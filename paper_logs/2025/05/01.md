## Daily Papers (2025-05-01)

### [Sadeed: Advancing Arabic Diacritization Through Small Language Model](https://arxiv.org/abs/2504.21635)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.21635.png)

Vote: 44

Authors: Khalil Hennara, Mohamed Motaism Hamed, Sara Chrouf, Zeina Aldallal, Safwan AlModhayan, Muhammad Hreden

- ***What's New***: Sadeed는 아랍어 '타슈킬(tashkīl)'을 위한 소형 언어 모델(Small Language Model)을 통해 아랍어의 모음 부호화(Diacritization)를 크게 개선했습니다. 본 연구에서는 최신 대형 언어 모델들이 접근하지 못하는 고유의 벤치마크 및 데이터 세트를 통해 더 포괄적이고 공정한 평가를 제공하는 Sadeed와 SadeedDiac-25의 도입을 강조합니다.
- ***Technical Details***: Sadeed는 기존의 Kuwain 1.5B 모델을 기반으로 하여 Fine-Tuning된 Decoder-Only Language Model입니다. 데이터 세트는 높은 품질을 보장하기 위해 정교한 데이터 클리닝과 정규화 파이프라인을 통해 구성되었습니다. 새롭게 제안된 SadeedDiac-25 벤치마크는 고전 아랍어(Classical Arabic)와 현대 표준 아랍어(Modern Standard Arabic; MSA) 텍스트를 배합하여 다양성 높은 평가를 가능하게 합니다.
- ***Performance Highlights***: Sadeed는 기존 대형 언어 모델과 비교하여 매우 경쟁력있는 성능을 보여주며, 특히 소형 모델이지만 훈련된 특정 도메인에서는 독점 모델을 능가합니다. 아랍어 모음 부호화의 최신 벤치마크에서, Word Error Rate(WER)와 Diacritic Error Rate(DER)에서 SOTA(State-of-The-Art) 성능을 기록하였습니다.

### [WebThinker: Empowering Large Reasoning Models with Deep Research Capability](https://arxiv.org/abs/2504.21776)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.21776.png)

Vote: 26

Authors: Hongjin Qian, Ji-Rong Wen, Yongkang Wu, Jiajie Jin, Zhicheng Dou, Xiaoxi Li, Yutao Zhu, Guanting Dong

- ***What's New***: WebThinker는 대형 추론 모델(LRMs)의 제한된 성능을 극복하기 위해 웹 서치를 통해 자율적으로 지식을 탐색하고 연구 보고서를 작성할 수 있는 딥 리서치 에이전트를 소개합니다. 이 모델은 특정 정보 격차를 만나면 웹을 탐색하고 필요한 정보를 수집하여 복잡한 문제를 해결할 수 있습니다.
- ***Technical Details***: WebThinker는 '딥 웹 탐색기(Deep Web Explorer)' 모듈을 통해 LRMs가 웹 페이지를 검색하고 탐색하며 관련 정보를 추출할 수 있도록 돕습니다. 또한 '자율적 사고-검색-초안 작성 전략(Autonomous Think-Search-and-Draft strategy)'을 사용하여 모델이 실시간으로 논리적 추론, 정보 수집, 보고서 작성 작업을 통합할 수 있게 합니다. RL 기반의 반복적 온라인 직접 선호 최적화(DPO) 훈련 전략을 도입하여 연구 도구 활용을 최적화합니다.
- ***Performance Highlights***: GPQA, GAIA, WebWalkerQA, HLE와 같은 복잡한 추론 벤치마크와 Glaive의 과학적 보고서 생성 작업에서 WebThinker가 기존 방법들보다 훨씬 우수한 성능을 보였습니다. 특히, WebThinker-32B-RL 모델은 GAIA와 같은 벤치마크에서 이전 모델들보다 상당한 성능 향상을 이뤘습니다.

### [COMPACT: COMPositional Atomic-to-Complex Visual Capability Tuning](https://arxiv.org/abs/2504.21850)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.21850.png)

Vote: 20

Authors: Olga Russakovsky, Xindi Wu, Hee Seung Hwang, Polina Kirichenko

- ***What's New***: COMPACT는 복합 시각 언어 모델(Multimodal Large Language Models; MLLMs)의 복잡한 시각-언어 작업 수행 능력을 향상시키기 위해 설계된 새로운 데이터 생성 방법입니다. COMPACT는 기존의 데이터 규모 확장 위주의 교육 방법론 대신, 교육 예제의 구성적 복잡성을 체계적으로 제어하여 더욱 복잡한 작업을 효율적으로 학습하도록 합니다.
- ***Technical Details***: COMPACT는 원자적 시각 능력(Atomic Visual Capabilities)을 정의하고, 이를 결합하여 복합적이고 구성상의 복잡성을 가진 교육 데이터를 생성합니다. 이 데이터셋은 구체적인 구성성 복잡도를 가진 3단계 복합성 레벨(𝑘=1, 2, 3)을 통해 다양한 작업 환경을 다루도록 설계되었습니다. 전체 VIT 데이터의 10% 이하로 동일한 성능을 제공하며, 특히 4개 이상의 원자적 능력을 필요로 하는 복잡한 과제에서 포괄적인 성능 향상을 이룹니다.
- ***Performance Highlights***: COMPACT는 MMStar와 MM-Vet에서 각각 83.3%와 94.0%의 성능 향상을 보여주며, 전체 VIT 데이터 대비 유의미한 성능을 발휘합니다. 모델이 다양한 구성 능력을 일반화하여 복잡한 과제에서도 뛰어난 성능을 발휘하도록 돕습니다.

### [Phi-4-Mini-Reasoning: Exploring the Limits of Small Reasoning Language Models in Math](https://arxiv.org/abs/2504.21233)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.21233.png)

Vote: 20

Authors: Baolin Peng, Dongdong Chen, Jianfeng Gao, Young Jin Kim, Yunsheng Li, Weijian Xu, Yelong Shen, Weizhu Chen, Mei Gao, Liliang Ren, Haoran Xu, Yen-Chun Chen, Hany Awadalla, Shuohang Wang

- ***What's New***: Phi-4-Mini-Reasoning는 작은 언어 모델(Small Language Models; SLMs)의 수학적 추론 능력을 극대화하기 위한 새로운 방식의 학습 레시피를 제안합니다. 이 모델은 대규모 추론 데이터 집합을 통한 대량 중간 학습과 고품질 데이터로의 지도 미세 조정, 선호 학습을 위한 Rollout DPO, 그리고 검증 가능한 보상을 활용한 강화 학습을 통해 SLM의 추론 능력을 향상시켰습니다.
- ***Technical Details***: 이 연구에서 제안된 학습 방법론은 네 단계로 구성됩니다: 첫째, 다양한 장르의 Chain-of-Thought(CoT) 데이터를 이용한 대량 중간 학습(Mid-training), 둘째, 고품질 CoT 데이터를 통한 지도 미세 조정(Supervised Fine-tuning), 셋째, Rollout DPO를 이용한 선호 학습, 넷째, 검증 가능한 보상의 원칙을 활용한 강화 학습 처리입니다. Phi-4-Mini는 이러한 접근을 통해 Phi-4-Mini-Reasoning 모델로 변모하여, 기존보다 개선된 수학적 추론 능력을 보입니다.
- ***Performance Highlights***: Phi-4-Mini-Reasoning 모델은 3.8B-파라미터 모델임에도 불구하고 같은 규모의 기존 오픈 소스 모델보다 우수한 성능을 보였습니다. Math-500 벤치마크에서 DeepSeek-R1-Distill-Qwen-7B 모델을 3.2 포인트, DeepSeek-R1-Distill-Llama-8B 모델을 7.7 포인트 상회하는 성능을 기록하여 작은 모델도 철저한 데이터 선택 및 학습 전략을 통해 강력한 추론 능력을 확보할 수 있다는 점을 실증하였습니다.

### [Beyond the Last Answer: Your Reasoning Trace Uncovers More than You Think](https://arxiv.org/abs/2504.20708)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.20708.png)

Vote: 17

Authors: Hani Itani, Bernard Ghanem, Hasan Abed Al Kader Hammoud

- ***What's New***: 이 논문에서는 대형 언어 모델(LLMs)의 최종 답변 만을 평가하는 관행에 의문을 제기하고, 중간 추론 단계를 분석하여 보다 신뢰할 수 있는 평가 방법을 제안합니다. 이 연구는 다른 중간 추론 단계에서 파생된 답변을 모드(Mode)로 집계하여 정확성을 크게 향상시킬 수 있음을 보여줍니다.
- ***Technical Details***: 제안된 방법은 전체 추론 과정을 자연어 마커에 따라 '서브사고(subthoughts)'로 분리하고, 각 중간 상태 이후에 모델이 추론을 계속 진행하도록 유도하는 새로운 프롬프트를 생성합니다. 각 서브사고의 최종 답변을 추출하여, 이 답변의 분포를 분석하고 모드(Mode)로 집계하여 보다 일관된 결론을 도출합니다.
- ***Performance Highlights***: 수학적 추론 데이터셋(AIME2024, AIME2025)에 대한 실험에서 LLM들의 정확도가 13% (AIME2024) 및 10% (AIME2025)까지 개선되었습니다. 이 방법은 특히 변동이 높은 문제에 대해 안정된 답을 제공하며, 기존 방식에 비해 일관성을 강조하여 정확도를 높였습니다.

### [Phi-4-reasoning Technical Report](https://arxiv.org/abs/2504.21318)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.21318.png)

Vote: 14

Authors: Neel Joshi, Besmira Nushi, Shital Shah, Vaishnavi Shrivastava, Gustavo de Rosa, Guoqing Zheng, Sahaj Agarwal, Ahmed Awadallah, Suriya Gunasekar, Marah Abdin, Piero Kauffmann, Vibhav Vineet, Harkirat Behl, Yash Lara, Caio César Teodoro Mendes, Lingjiao Chen, Olli Saarikivi, Mojan Javaheripi, Yue Wu, Arindam Mitra, Safoora Yousefi, Dimitris Papailiopoulos, Vidhisha Balachandran

- ***What's New***: Phi-4-reasoning 모델은 복잡한 추론 작업에서 강력한 성능을 보이는 140억 개의 파라미터를 갖춘 모델입니다. 구체적으로 훈련 데이터를 철저히 선별하여 Phi-4를 대상 추론 데이터로 슈퍼바이저드 파인튜닝(Supervised Fine-Tuning; SFT)했으며, o3-mini 모델을 이용하여 생성된 추론 시연을 통해 세밀한 추론 체인을 생성합니다. 이후 강화 학습을 적용하여 성능을 더욱 향상시켰습니다.
- ***Technical Details***: Phi-4-reasoning 모델은 목적 기반 강화 학습을 통한 변형 모델(Variant)인 Phi-4-reasoning-plus도 개발했습니다. 이 모델은 긴 추론 흔적을 생성하며, 학습 데이터는 수학, 과학적 추론, 코딩, 알고리즘 문제 해결 등 다양한 분야를 포함합니다. Phi-4-reasoning는 1.4M 개의 고품질 질문을 포함하는 데이터 세트를 사용하여 훈련되었으며, Phi-4-reasoning-plus는 검증 가능한 수학 문제를 6K개 정도 더 학습합니다.
- ***Performance Highlights***: Phi-4-reasoning 모델은 AIME, GPQA 등 여러 벤치마크에서 DeepSeek-R1-Distill-Llama-70B 모델과 같은 훨씬 더 큰 모델들을 능가하는 성과를 보여주었습니다. 특정 벤치마크에서 671B 파라미터를 갖춘 DeepSeek-R1 모델에 비견될 정도의 성능을 발휘하고 있습니다. 또한, 오픈 소스와 크로즈된 최신 모델들의 성능을 상회하는 결과를 보였습니다.

### [Softpick: No Attention Sink, No Massive Activations with Rectified Softmax](https://arxiv.org/abs/2504.20966)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.20966.png)

Vote: 14

Authors: Zayd M. K. Zuhri, Alham Fikri Aji, Erland Hilman Fuadi

- ***What's New***: 이 논문에서는 소프트픽(Softpick)을 소개하며, 이는 트랜스포머 주의 메커니즘에서 소프트맥스(Softmax)를 대체할 수 있도록 설계되었습니다. 소프트맥스의 주의 집중 문제(attention sink) 및 대규모 활성화(massive activations)를 제거하여, 주의 메커니즘의 효율성을 높입니다. 소프트픽 사용 시, 모델의 성능을 유지하면서도 주의 집중 문제를 0%로 낮추고, 활성화 지도의 희소성을 증가시킵니다.
- ***Technical Details***: 소프트픽 함수는 정규화가 필요 없는 형태로 설계되었으며, '합이 1이 되는 것(sum-to-one)' 방식을 제거합니다. 실험에서는 340M 파라미터의 트랜스포머 모델에 소프트픽을 적용하여 소프트맥스와 유사한 성능을 유지했으며, 특히 양자화(quantization) 상황에서 더 우수한 성능을 보였습니다. 소프트픽은 주의 집중 문제를 해결하여 희소성이 높은 주의 지도를 생성하며, 이는 모델 해석 가능성과 양자화, 저정밀 훈련, 가지치기(pruning) 등에 새로운 가능성을 열어줄 수 있습니다.
- ***Performance Highlights***: 소프트픽을 사용한 모델은 소프트맥스를 사용한 모델에 비해 양자화 시 일관되게 더 높은 성능을 보였고, 특히 낮은 비트 정밀도에서 두드러진 장점을 갖고 있습니다. 예를 들어, 소프트맥스 사용 시 최종 훈련 손실과 비교하여 소프트픽은 훈련 손실의 격차를 0.004까지 줄였으며, 양자화 후 낮은 비트 정밀도에서도 낮은 손실과 높은 정확도를 보였습니다. 이는 현재 LLMs의 시각적 추론과 코드 생성에서 상당한 도전 과제를 안고 있음을 보여주며, 향후 연구 방향성을 제시합니다.

### [Taming the Titans: A Survey of Efficient LLM Inference Serving](https://arxiv.org/abs/2504.19720)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.19720.png)

Vote: 9

Authors: Yixin Ji, Xinyu Duan, Juntao Li, Ranran Zhen, Zhefeng Wang, Min Zhang, Zhenlin Yang, Baoxing Huai, Tong Liu, Qingrong Xia

- ***What's New***: 대형 언어 모델(LLMs)의 추론 효율성을 높이기 위한 다양한 방법론이 체계적으로 정의되었습니다. 이 설문 조사에서는 인스턴스 수준(methodologies)과 클러스터 수준(strategies)에서의 최적화 방법을 포함하여 emerging scenarios를 다루고 있습니다.
- ***Technical Details***: 인스턴스 수준에서는 모델 배치(model placement), 요청 스케줄링(request scheduling), 디코딩 길이 예측(decoding length prediction), 스토리지 관리(storage management) 등의 최적화 기법이 설명됩니다. 클러스터 수준에서는 GPU 클러스터 배치와 로드 밸런싱(load balancing) 등이 다루어지며, 서비스 지향적(scheduleing) 클러스터 관리 등도 포함됩니다. emerging scenario에서 다루어진 추가적 주제에는 장문 컨텍스트 처리를 위한 parallel processing과 retrieval-augmented generation(RAG), mixture of experts(MoE), speculative decoding 등이 언급됩니다.
- ***Performance Highlights***: 다양한 방법론이 LLM 추론 서비스의 latency와 throughput을 개선하기 위한 연구가 수행되었습니다. 예를 들어, decoding 길이 예측을 통해 요청 batch의 효율성을 높일 수 있으며, speculative decoding을 이용하여 inference 속도를 증가시킬 수 있습니다. 추가적으로, cloud 기반 LLM 서비스는 local 자원이 충분치 않은 경우 경제적 대안이 될 수 있습니다.

### [ReVision: High-Quality, Low-Cost Video Generation with Explicit 3D Physics Modeling for Complex Motion and Interaction](https://arxiv.org/abs/2504.21855)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.21855.png)

Vote: 7

Authors: Liang-Chieh Chen, Qihao Liu, Qihang Yu, Alan Yuille, Ju He

- ***What's New***: ReVision는 대형 비디오 생성 모델을 강화하기 위해 매개변수화된 3D 물리적 지식을 명시적으로 통합하는 플러그 앤 플레이(Plug-and-Play) 프레임워크를 도입하여 고품질 비디오와 복잡한 움직임 및 상호작용을 생성할 수 있게 했습니다.
- ***Technical Details***: ReVision는 세 가지 단계로 구성됩니다. 첫째, 비디오 확산 모델(Video Diffusion Model)을 사용해 대략적인 비디오를 생성합니다. 둘째, 이 대략적인 비디오에서 2D 및 3D 피쳐를 추출하여 3D 객체 중심 표현을 만듭니다. 세 번째로, 이를 최적화된 3D 움직임 시퀀스로 피드백하여 모션 일관성을 강화하고 복잡한 동작과 상호작용을 포함한 비디오를 생성합니다. 이 과정에서 Parameterized Physical Prior Model (PPPM)을 활용하여 더욱 정교하고 자연스러운 3D 움직임 시퀀스를 생성합니다.
- ***Performance Highlights***: ReVision는 Stable Video Diffusion (SVD) 상에서의 실험에서 그 효과를 입증했으며, 1.5B 파라미터만으로도 13B 파라미터 이상을 사용한 최첨단 비디오 생성 모델들을 복잡한 비디오 생성 성능에서 능가했습니다. 이로써 3D 물리적 지식을 포함한 작은 비디오 생성 모델도 더 현실적이고 컨트롤 가능한 복잡한 움직임과 상호작용을 생성할 수 있는 가능성을 보여줍니다.

### [RoboVerse: Towards a Unified Platform, Dataset and Benchmark for Scalable and Generalizable Robot Learning](https://arxiv.org/abs/2504.18904)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.18904.png)

Vote: 7

Authors: Charlie Tianyue Cheng, Pieter Abbeel, Yuyang Li, Haoran Lu, Carlo Sferrazza, Haozhe Chen, Yen-Jen Wang, Ruihai Wu, Siheng Zhao, Feishi Wang, Jiazhao Zhang, Haoran Geng, Jiangran Lyu, Yufei Ding, Baoxiong Jia, Peihao Li, Mingtong Zhang, Bangjun Wang, Yiran Geng, Yue Wang, Siyuan Huang, Dylan Goetting, Boshi An, Ran Gong, Yuxi Qian, Jialiang Zhang, Haozhe Lou, Chaoyi Xu, Yuran Wang, Weikang Wan, Chengyang Zhao, Yuxuan Kuang, Hao Dong, Yutong Liang, Jitendra Malik, Songlin Wei, Jiageng Mao

- ***What's New***: ROBOVERSE는 확장 가능하고 일반화 가능한 로봇 학습을 위해 통합 플랫폼, 데이터셋, 벤치마크를 제공합니다. METASIM이라는 핵심 인프라를 통해 서로 다른 시뮬레이터 간의 매끄러운 전환을 가능하게 하여 다양하고 대규모의 합성 데이터를 생성하고, 모방 학습 및 강화 학습을 위한 일관된 평가를 가능하게 합니다.
- ***Technical Details***: ROBOVERSE는 여러 시뮬레이터와 로봇 구현을 지원하는 시뮬레이션 플랫폼으로, METASIM이라는 인프라를 통해 시뮬레이터에 중립적인 인터페이스 시스템을 구축하였습니다. 이 시스템은 에이전트, 객체, 센서, 작업 및 물리적 매개변수를 표준화하여 다양한 시뮬레이션 환경을 통합합니다. 데이터를 증대하고 도메인 무작위화를 사용하여 데이터를 생성하며, 다양한 원본 시뮬레이션 환경에서 작업, 자산 및 궤적을 수집하여 대규모 고품질 데이터셋을 생성합니다.
- ***Performance Highlights***: ROBOVERSE를 기반으로 한 실험은 모방 학습, 강화 학습, 세계 모델 학습에서 안정적이고 강력한 정책 학습과 평가를 가능하게 하였습니다. 시뮬레이션에서 실제 환경으로의 전이에서 고충실도의 물리 및 렌더링을 통해 강력한 성능을 보여주었으며, 데이터의 확장 가능성과 실환경에서의 적용 가능성을 입증하였습니다.

### [Generative AI for Character Animation: A Comprehensive Survey of Techniques, Applications, and Future Directions](https://arxiv.org/abs/2504.19056)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.19056.png)

Vote: 7

Authors: Omid Ghahroodi, Kiarash Kiani Feriz, Pardis Sadat Zahraei, Arash Rasouli, Mahdieh Soleymani Baghshah, Ali Moghadasi, Alireza Mirrokni, Sara Azarnoush, Mohammad Mahdi Abootorabi, Bahar Behzadipour, Benyamin Maleki, Hossein Behzadasl, Mobina Salimipanah, Hamid R. Rabiee, Erfan Sadraiye, Nizi Nazar, Abolfazl Eshagh Abianeh, Ehsaneddin Asgari, Meisam Ahmadi, Mahdi Teymouri Nahad

- ***What's New***: 이 논문은 캐릭터 애니메이션을 위한 생성 AI의 기술, 응용 및 미래 방향에 대한 종합적인 조사 연구를 제공합니다. 얼굴 애니메이션, 제스처 모델링, 모션 합성, 아바타 생성 및 텍스처 합성과 같은 다양한 캐릭터 애니메이션의 주요 생성 AI 응용을 한 관점에서 통합하여 제공하는 첫 시도입니다.
- ***Technical Details***: 논문은 획기적인 언어와 확산 모델을 활용하여 시간이 덜 들고 저렴하게 애니메이션 콘텐츠를 생성하는 방법을 설명합니다. 이 문서는 SMPL 및 최신 생성 AI를 변형시킨 모델 아키텍처, 예를 들어 컨볼루션 신경망(CNN), 일시적 컨볼루션 네트워크(TCN), 생성적 적대 신경망(GAN), 변분 자동인코더(VAE), 변환기(Transformer), 디노이즈 확산 확률 모델(DDPM), CLIP 및 ControlNet 등을 포괄적으로 다룹니다.
- ***Performance Highlights***: 제안된 AI 시스템은 다양한 캐릭터 애니메이션 작업에 대해 탁월한 성능을 보였습니다. 텍스트-주도 방식의 다양한 생성 작업에서 기존 방식들에 비해 시각적 사실성, 형태 파악 능력이 뛰어난 것으로 나타났습니다. 이는 향후 연구에 있어 시각적 컨텍스트 평가와 코드 생성능력을 크게 개선할 수 있는 토대를 마련한 것입니다.

### [Llama-3.1-FoundationAI-SecurityLLM-Base-8B Technical Report](https://arxiv.org/abs/2504.21039)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.21039.png)

Vote: 6

Authors: Paul Kassianik, Adam Swanda, Amin Karbasi, Fraser Burch, Blaine Nelson, Dhruv Kedia, Massimo Aufiero, Avi Zohary, Amy Chang, Sajana Weerawardhena, Aman Priyanshu, Yaron Singer, Alexander Chen, Baturay Saglam, Hyrum Anderson, Kojin Oshiba, Anu Vellore, Omar Santos

- ***What's New***: Llama-3.1-FoundationAI-SecurityLLM-Base-8B는 Llama 3.1 아키텍처를 기반으로 한 사이버 보안 중심의 대형 언어 모델(LLM)입니다. 이 모델은 carefully curated cybersecurity corpus로 추가적인 pre-training을 거쳐 개발되었으며, 기존의 사이버 보안 벤치마크와 새로운 벤치마크에서 뛰어난 성능을 보입니다.
- ***Technical Details***: Foundation-Sec-8B는 Llama 3.1-8B 모델을 중심으로 개발되었으며, 사이버 보안 데이터에 대해 추가적인 pre-training을 진행했습니다. 데이터 수집은 URL을 기반으로 한 보안 정보 소스에 포커스를 맞추었으며, 일반적인 웹 데이터셋에서 발견하기 어려운 자료를 포괄하는 custom scrapers를 사용하여 이루어졌습니다. 벤치마크 평가를 위해 multiple-choice question answering (MCQA)와 root cause mapping (RCM) 형식의 세 가지 사이버 보안 벤치마크가 사용되었습니다.
- ***Performance Highlights***: Foundation-Sec-8B는 CTIBench-MCQA와 CTIBench-RCM에서 GPT-4o-mini와 유사한 성능을 보이며, CTIBench-RCM의 경우 6.1포인트의 우위를 차지했습니다. 본 모델은 Llama 3.1-70B 및 WhiteRabbitNeo-V2-70B보다 약 1포인트 높은 성능을 나타냈으며, 동일한 Llama 3.1-8B-base에 비해 3포인트 이상의 정확도 향상을 기록했습니다. 이는 일반적인 대형 모델에서 특정 전문 분야로의 성능 개선을 나타냅니다.

### [UniBiomed: A Universal Foundation Model for Grounded Biomedical Image Interpretation](https://arxiv.org/abs/2504.21336)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.21336.png)

Vote: 2

Authors: Yuxiang Nie, Hao Chen, Linshan Wu, Jiaxin Zhuang, Sunan He

- ***What's New***: UniBiomed는 새로운 통합의 Multi-modal Large Language Model (MLLM)과 Segment Anything Model (SAM)을 사용하여 생의학 이미지의 통합 해석을 제공하는 첫 번째 범용 기초 모델입니다. 이는 임상 텍스트 생성과 생의학 객체의 분할을 통합하여 다양한 생의학 과제를 다룰 수 있는 능력을 갖추고 있습니다.
- ***Technical Details***: UniBiomed는 27백만 쌍의 이미지, 주석, 텍스트 설명을 포함하는 대규모 데이터셋을 활용하여 10가지의 생의학 이미징 모달리티에 걸쳐 훈련되었습니다. MLLM은 다중 모달 이미지를 해석하고 임상 텍스트 설명을 생성하는 데 사용되며, SAM은 해당 생의학 객체를 분할합니다. 이로써 UniBiomed는 임상 전문가의 개입 없이 자동화된 종단 간 생의학 이미지 해석을 제공합니다.
- ***Performance Highlights***: UniBiomed는 다양한 생의학 과제에서 최첨단 성능을 달성했습니다. 예를 들어, 구체적인 생의학 이미지 분할에서 10.25%의 Dice 점수를 향상시켰습니다. 또한, 질병 인식 및 영역 인식 보고서 생성 임무에서도 우수한 성과를 보이며, 임상 워크플로 최적화를 통해 진단 효율성을 크게 향상시킬 수 있음을 보여줍니다.

### [Selecting Optimal Candidate Profiles in Adversarial Environments Using Conjoint Analysis and Machine Learning](https://arxiv.org/abs/2504.19043)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.19043.png)

Vote: 2

Authors: Connor T. Jerzak, Rishi Hazra, Priyanshi Chandra

- ***What's New***: 이 연구는 결합분석(Conjoint Analysis)과 머신러닝을 활용하여 경쟁적인 정치 환경에서 최적의 후보자 프로필을 식별하는 방법을 제안합니다. 이 연구는 두 정당이 동시에 최적의 후보자를 선택하는 상황으로 확장하여, 역사적 선거 결과 범위 내에서 최적 전략을 찾아 관찰된 실제 후보자와 더 밀접하게 일치할 가능성을 제시합니다.
- ***Technical Details***: 이 연구에서는 예시로 미국 대통령 선거를 선택하여 결합 실험 데이터를 사용하여 가장 유리한 결과를 얻을 수 있는 후보자 속성의 확률 분포를 나타내는 최적의 확률적 개입을 도출합니다. 이때, 데이터가 과적합되지 않도록 정규화를 적용하였으며, 각 후보의 이질적인 반응을 반영하여 정당성을 평가합니다.
- ***Performance Highlights***: 비적대적 접근법과 달리 적대적 환경에서의 기대 결과는 실제 관찰된 후보자 전략과 더 밀접하게 일치했으며, 실제 본선거 결과와 유사한 투표 점유율과 관련한 예측 정확성을 보였습니다. 이 연구는 향후 사회과학 실험 데이터 분석에 중요한 통찰을 제공할 수 있습니다.

