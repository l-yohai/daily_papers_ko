## Daily Papers (2025-05-07)

### [Unified Multimodal Chain-of-Thought Reward Model through Reinforcement Fine-Tuning](https://arxiv.org/abs/2505.03318)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.03318.png)

Vote: 63

Authors: Qinglin Lu, Chunyu Wang, Yibin Wang, Zhimin Li, Yuhang Zang, Cheng Jin, Jiaqi Wang

- ***What's New***: 이 논문에서는 시각적 이해 및 생성 작업에 대해 다차원적이고 단계별의 긴 연쇄 사고(CoT) 기반 보상 모델을 통합한 UNIFIEDREWARD-THINK을 제안하며, 이는 기존 모델들이 주로 단일 응답을 제공하거나 얕은 사고 과정에 그치는 문제를 해결합니다. 이 모델은 기존 보상 모델들이 자주 직면했던 부정확한 보상 신호를 강화학습을 통해 개선합니다.
- ***Technical Details***: UNIFIEDREWARD-THINK 모델은 단계별 긴 연쇄 사고 능력을 활성화하고 강화하기 위해 탐색 기반의 강화 미세 튜닝 접근 방식을 취합니다. 첫 번째 단계에서 GPT-4o의 사고 과정을 정제된 이미지 생성 선호 데이터로부터 모방하여 모델의 CoT 사고 형식을 학습하게 하고, 그 후 대규모 통합 멀티모달 선호 데이터를 사용하여 다양한 시각 과제에서 모델의 사고 과정을 유도하는 방식을 활용합니다. 마지막으로 Group Relative Policy Optimization (GRPO)를 통해 강화학습을 수행하며, 이 과정에서 잘못된 예측 샘플이 다양한 사고 경로를 탐색하며 올바른 해결책을 최적화하도록 합니다.
- ***Performance Highlights***: 제안된 모델은 기존 기반 모델인 UnifiedReward 대비 의미 있는 성능 향상을 보여주며, 특히 이미지 이해 보상 작업에서 두드러진 성과를 냈습니다. 다차원 및 다단계 사고 도입이 모든 작업에서 상당한 성능 향상을 가져왔으며, 긴 CoT 사고 학습 후, 명시적으로 CoT 사고를 활용하지 않아도 암묵적 논리 추론을 통해 기존 방법들보다 우수한 성능을 보여줍니다.

### [Absolute Zero: Reinforced Self-play Reasoning with Zero Data](https://arxiv.org/abs/2505.03335)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.03335.png)

Vote: 59

Authors: Gao Huang, Yang Yue, Shenzhi Wang, Tong Wu, Zilong Zheng, Yiran Wu, Qingyun Wu, Andrew Zhao, Quentin Xu, Matthieu Lin

- ***What's New***: Absolute Zero는 외부 데이터에 의존하지 않고 모델이 스스로 학습할 과제를 정하고 해결함으로써 자기 진화를 통해 추론 능력을 향상시키는 새로운 RLVR(Reinforcement Learning with Verifiable Rewards) 패러다임입니다. 모델은 코드 실행기를 사용하여 제안된 코드 추론 과제를 검증하고 답을 검증하며, 이를 통해 신뢰할 수 있는 학습 피드백을 제공합니다.
- ***Technical Details***: Absolute Zero Reasoner(AZR)는 추론 과제를 제안하고 해결하는 자기 강화 학습 시스템입니다. 코드 실행 환경을 통해 과제의 무결성을 검증하고 안정적인 학습을 위한 검증 가능한 피드백을 제공합니다. AZR은 귀납(induction), 오귀납(abduction), 연역(deduction)과 같은 다양한 추론 모드를 사용하여 코드 기반의 학습 과제를 제안합니다. 모델은 새롭게 제안된 다중 과제를 학습하는 강화 학습 방법을 채택하여 제안자 및 해결자 역할을 동시에 수행합니다.
- ***Performance Highlights***: AZR는 외부 데이터 없이도 SOTA(state-of-the-art) 성능을 달성하였으며, 수만 개의 인공지능 데이터에 의존하는 기존의 0-세팅 모델을 초과하는 성능을 발휘합니다. 코딩 환경에서 학습된 AZR 모델은 수학 정확도를 평균 10.9% 증가시키며, 코더 모델은 15.2% 포인트를 추가하며 강력한 일반화된 추론 능력 향상을 시현했습니다. 더 큰 모델 크기에서도 성능 향상이 지속되어 AZR은 확장성에 있어 유리함을 보여주었습니다.

### [RADLADS: Rapid Attention Distillation to Linear Attention Decoders at Scale](https://arxiv.org/abs/2505.03005)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.03005.png)

Vote: 22

Authors: Daniel Goldstein, Janna Lu, Eugene Cheah, Eric Alcaide

- ***What's New***: 이 논문은 소프트맥스 주의 메커니즘을 사용하는 트랜스포머 모델을 선형 주의 디코더(Linear Attention Decoders) 모델로 빠르게 변환하는 프로토콜인 RADLADS를 제안합니다. 또한, 새로운 RWKV-변형 아키텍처 2종과 Qwen2.5 오픈 소스 모델을 7B, 32B, 72B 크기로 변환하는 방법을 소개합니다.
- ***Technical Details***: RADLADS 프로세스는 350-700M 토큰만 필요로 하며, 오리지널 최고 모델을 학습하는데 사용하는 토큰 수의 0.005% 미만을 사용합니다. 또한, RAD-RWKV6 ('RADFinch') 및 RAD-RWKV7 ('RADGoose') 두 가지 새로운 아키텍처를 개발했습니다. 모델 전환 과정은 3단계로 나뉘며, 첫 번째 단계에서는 주의 감추기 상태를 맞춥니다. 두 번째 단계에서는 지식 증류를 통해 모델 레이어를 미세 조정합니다. 마지막으로 세 번째 단계에서는 더 긴 컨텍스트로 모델을 미세 조정합니다.
- ***Performance Highlights***: RADLADS는 동일한 주어진 데이터셋에서 다른 변환 프로세스를 사용하는 모델보다 높은 성능 비율을 달성했습니다. 특히 Qwen2.5에서 변환된 RADLADS 모델은 거의 모든 벤치마크에서 더 높은 점수 비율을 기록했으며, 이를 통해 순수 RNN 기반 언어 모델의 새로운 최첨단 성능을 보여줍니다.

### [FlexiAct: Towards Flexible Action Control in Heterogeneous Scenarios](https://arxiv.org/abs/2505.03730)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.03730.png)

Vote: 20

Authors: Junhao Zhuang, Shiyi Zhang, Yansong Tang, Zhaoyang Zhang, Ying Shan

- ***What's New***: FlexiAct은 이질적인 시나리오에서 유연한 동작 제어를 목표로 한 새로운 Image-to-Video (I2V) 프레임워크입니다. 기존의 방법들이 공간 구조나 관점의 일치가 필요한데 비해, FlexiAct는 이러한 제약 없이 참조 비디오의 동작을 목표 이미지로 전이할 수 있습니다.
- ***Technical Details***: FlexiAct는 RefAdapter와 Frequency-aware Action Extraction (FAE)이라는 두 가지 컴포넌트로 구성됩니다. RefAdapter는 이미지 조건부 어댑터로 다양한 공간 구조에 적응하면서도 외형 일관성을 유지합니다. FAE는 노이즈 제거 과정에서 주파수 인식 임베딩을 사용하여 동작 정보를 직접 추출하며, 다른 시공간 아키텍처 없이도 동작 추출을 실현합니다.
- ***Performance Highlights***: 실험 결과에서 FlexiAct는 모션 충실도와 외형 일관성에서 기존의 베이스라인 방법들을 크게 능가했습니다. 특히, FlexiAct는 다양한 레이아웃과 시각 구조에서 동작의 정확성과 외형 일관성을 균형 있게 유지할 수 있음을 보여줍니다.

### [RetroInfer: A Vector-Storage Approach for Scalable Long-Context LLM Inference](https://arxiv.org/abs/2505.02922)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02922.png)

Vote: 18

Authors: Chen Chen, Fan Yang, Baotong Lu, Mingxing Zhang, Mao Yang, Jinkai Zhang, Jing Liu, Chengruidong Zhang, Xiao Yan, Jiawei Jiang, Qianxi Zhang, Huiqiang Jiang, Di Liu, Yaoqi Chen, Yuqing Yang, Jingjia Luo, Qi Chen, Bailu Ding

- ***What's New***: RetroInfer는 장문 맥락 LLM(Large Language Model) 추론 속도를 향상시키기 위해 키-값(KV) 캐시를 벡터 스토리지 시스템으로 재정립한 새로운 시스템입니다. 이는 주의력의 희소성을 활용하고 주의력 인식을 통해 중요한 토큰을 효율적으로 검색하는 'wave index'와 GPU와 CPU 간의 계산 및 데이터 전송을 중복되는 'wave buffer'로 구성되어 높은 처리량을 유지합니다.
- ***Technical Details***: RetroInfer는 Wave Index라 불리는 주의력 인식 벡터 인덱스를 통해 중요 토큰을 정확하고 효율적으로 검색합니다. 이 인덱스는 세 가지 주요 기술로 이루어져 있습니다: 트라이파트 주의력 근사, 정확도 경계가 있는 주의력 추정, 세그먼트 클러스터링. Wave Buffer는 GPU 메모리에 KV 캐시를 효율적으로 배치하고 CPU와의 데이터 이동을 조정하여 높은 처리량을 유지합니다.
- ***Performance Highlights***: RetroInfer는 128K 토큰 컨텍스트에서 풀 어텐션과 동일한 수준의 정확성을 유지하면서도 속도 면에서 최대 4.5배까지 높은 속도를 기록했으며, CPU 메모리에 KV 캐시를 확장할 때 희소한 어텐션 베이스라인보다 최대 10.5배 더 빠른 속도를 달성했습니다. 이러한 결과는 RetroInfer가 현재의 희소 어텐션 방법들보다 더 나은 정확도와 효율성을 제공함을 나타냅니다.

### [Decoding Open-Ended Information Seeking Goals from Eye Movements in Reading](https://arxiv.org/abs/2505.02872)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02872.png)

Vote: 14

Authors: Omer Shubi, Cfir Avraham Hadar, Yevgeni Berzak, Yoav Meiri

- ***What's New***: 이 연구는 처음으로 독자의 열린 정보 탐색 목표를 읽는 동안의 안구 운동을 통해 자동으로 해독할 수 있는지를 탐구합니다. 구체적으로, 독자가 특정한 질문을 염두에 두고 있는 경우 그 질문을 안구 운동 데이터를 기반으로 해독할 수 있는지에 대한 문제를 제시합니다.
- ***Technical Details***: 이 논문에서는 목표 분류(goal classification)와 목표 재구성(goal reconstruction)이라는 두 가지 과제와 평가 체계를 소개합니다. 수백 가지 텍스트별 정보 탐색 작업에 대한 대규모 안구 추적 데이터를 활용하며, 안구 운동 데이터와 텍스트를 결합한 여러 구별적(discriminative) 및 생성적(generative) 멀티모달 LLMs를 개발하여 비교합니다. 이러한 모델은 안구 운동과 텍스트를 결합하여 목표 분류와 목표 재구성을 수행합니다.
- ***Performance Highlights***: 안구 운동과 텍스트를 결합한 모델은 목표 분류에서 상당히 높은 성과를 보였으며, 특히 RoBERTEye-Fixations 모델이 모든 평가 체계에서 최고의 정확도를 기록했습니다. 목표 재구성에서는 DalEye-Llama 모델이 새로운 참여자에 대해서는 좋은 성과를 보였으나, 새로운 항목에 대해서는 성능이 하락했습니다. 이는 신경망 모델이 안구 운동 데이터를 통해 독자의 텍스트별 목표를 추출할 수 있음을 시사합니다.

### [An Empirical Study of Qwen3 Quantization](https://arxiv.org/abs/2505.02214)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02214.png)

Vote: 12

Authors: Xudong Ma, Haoran Chu, Yue Feng, Haotong Qin, Jinyang Guo, Yuye Li, Jie Luo, Michele Magno, Xianglong Liu, Xingyu Zheng

- ***What's New***: Qwen3는 오픈 소스 대형 언어 모델(Open-source LLMs) 패밀리 중 하나로, 최근 뛰어난 성능을 보이며 주목을 받고 있습니다. 이번 연구는 Qwen3의 다양한 양자화 설정(Quantization Settings) 하에서의 성능을 체계적으로 평가하고, 정확도 저하와 같은 문제점을 밝혀내어 향후 연구에 필요한 방향성을 제시합니다.
- ***Technical Details***: 본 연구는 Qwen3의 다양한 크기의 모델(0.6B, 1.8B 등)에서 다양한 언어 작업(Perplexity, 0-shot Reasoning 등)을 통해 1비트에서 8비트까지 5가지의 포스트 트레이닝 양자화(Post-Training Quantization; PTQ) 기법을 평가하였습니다. 실험은 NVIDIA A800 80GB GPU에서 일관된 조건 하에 수행되었으며, 모든 양자화 방법은 동일한 C4 데이터셋의 128개의 샘플을 사용한 일관적인 보정을 통해 수행되었습니다.
- ***Performance Highlights***: Qwen3는 8비트에서 거의 손실이 없는 성능을 유지할 수 있으나, 4비트 이하로 감소하면 성능 저하가 두드러집니다. GPTQ는 2비트에서도 보정 기반 보상을 통해 최소한의 성능을 유지하며, BiLLM과 같은 이진화(binarization) 방법이 일부 경우에 유망한 결과를 나타냈습니다. 특히, 더 큰 모델은 양자화 하에서 더 안정적인 경향을 보였습니다.

### [Multi-Agent System for Comprehensive Soccer Understanding](https://arxiv.org/abs/2505.03735)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.03735.png)

Vote: 9

Authors: Zifeng Li, Weidi Xie, Jiayuan Rao, Haoning Wu, Yanfeng Wang, Ya Zhang

- ***What's New***: 이 논문은 축구 이해를 위한 포괄적인 프레임워크를 소개합니다. 특히, SoccerWiki라는 대규모 멀티모달 석커 도메인 지식 기반을 구축하였으며, SoccerBench라는 가장 크고 포괄적인 축구 벤치마크를 제시합니다. 또한, SoccerAgent라는 새로운 멀티 에이전트 시스템을 개발하여, 복잡한 축구 질문을 협업적 추론을 통해 해결하고자 하였습니다.
- ***Technical Details***: SoccerWiki는 9,471명의 선수, 266개의 팀, 202명의 심판 및 235개의 경기장에 대한 정보를 포함하는 대규모 지식 기반입니다. SoccerBench는 약 10K의 멀티모달 다중 선택 QA 쌍을 포함하며, 13개의 축구 이해 작업을 커버합니다. SoccerAgent는 18개의 전문 도구를 통합하여 여려 질문을 해체하고, 기존 도구들을 통해 축구에 관한 문제를 해결합니다. 이 시스템은 특히 문맥 지식을 활용하여 축구 비디오 분석에서 뛰어난 성능을 발휘합니다.
- ***Performance Highlights***: SoccerBench의 실험 결과, SoccerAgent가 기존의 약 11개의 최신 MLLM들과 비교했을 때 탁월한 성능을 보였습니다. SoccerAgent는 TextQA에서 85.0%, ImageQA에서 73.3%, VideoQA에서 60.9%의 정확도를 기록하며, 특히 축구 도메인 지식을 필요로 하는 질문에서 우월한 성능을 보여주었습니다.

### [VITA-Audio: Fast Interleaved Cross-Modal Token Generation for Efficient Large Speech-Language Model](https://arxiv.org/abs/2505.03739)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.03739.png)

Vote: 6

Authors: Peixian Chen, Hang Shao, Jinlong Peng, Haoyu Cao, Ke Li, Xing Sun, Heting Gao, Chaoyou Fu, Lijiang Li, Yunhang Shen, Jian Li, Rongrong Ji, Mengdan Zhang, Zuwei Long

- ***What's New***: VITA-Audio는 빠른 교차모달 토큰 생성(Fast Interleaved Cross-Modal Token Generation)을 통한 대형 음성-언어 모델(Speech-Language Model)의 효율적인 구현을 목표로 한 혁신적인 프레임워크입니다. 이 모델은 최초의 전방향 패스에서 오디오 출력을 생성할 수 있는 최초의 다중 모달 대형 언어 모델입니다.
- ***Technical Details***: VITA-Audio는 다중 교차모달 토큰 예측(Multiple Cross-modal Token Prediction; MCTP) 모듈을 도입해, 단일 모델 전방향 패스를 통해 여러 오디오 토큰을 효율적으로 생성합니다. MCTP 모듈은 LLM의 숨겨진 상태를 오디오 토큰과 매핑하는 경량 모듈입니다. 이 모델은 4단계의 점진적 학습 전략을 통해 오디오 품질의 손실을 최소화하며 모델의 가속화를 달성합니다.
- ***Performance Highlights***: 7B 파라미터 규모에서 VITA-Audio는 3에서 5배의 추론 가속을  달성하였으며, 자동 음성 인식(ASR), 텍스트-형성(TTS), 음성 질문 응답(SQA) 작업에서 비슷한 규모의 공개 소스 모델보다 뛰어난 성능을 보였습니다.

### [HoloTime: Taming Video Diffusion Models for Panoramic 4D Scene Generation](https://arxiv.org/abs/2504.21650)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.21650.png)

Vote: 5

Authors: Wangbo Yu, Haiyang Zhou, Li Yuan, Xinhua Cheng, Yonghong Tian, Jiawen Guan

- ***What's New***: HoloTime은 비디오 확산 모델(Video Diffusion Models)을 조율하여 단일 입력 이미지나 모델 생성 파노라마 이미지를 360도 4D 씬으로 변환하는 프레임워크를 제안합니다. 최초로 4D 씬 생성을 위한 포괄적인 360World 데이터셋을 도입하여 VR과 AR에서 몰입감을 극대화합니다.
- ***Technical Details***: HoloTime은 두 단계의 이미지-비디오 확산 모델을 사용하는 Panoramic Animator와 공간 및 시간 정렬을 통해 4D 포인트 클라우드를 생성하는 Panoramic Space-Time Reconstruction을 제안합니다. 특히, Panoramic Animator는 두 단계 Motion Guided Generation과 Panoramic Circular Techniques를 통해 이미지의 동적 파노라마 비디오 변환을 처리합니다.
- ***Performance Highlights***: 기존 방법과의 비교를 통해 HoloTime은 파노라마 비디오 생성과 4D 씬 재구성 모두에서 뛰어난 성능을 입증하였으며, 이를 통해 VR 및 AR 애플리케이션에서 더 사실적이고 생동감 있는 몰입형 환경을 조성합니다.

### [Geospatial Mechanistic Interpretability of Large Language Models](https://arxiv.org/abs/2505.03368)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.03368.png)

Vote: 5

Authors: Stef De Sabbata, Stefano Mizzaro, Kevin Roitero

- ***What's New***: 이 논문은 대형 언어 모델(LLMs)의 지리 정보 처리 방식을 공간 분석을 통해 역공학적으로 이해하는 새로운 프레임워크를 제안합니다. 이는 LLMs가 지리 정보에 대해 생성하는 내부 표현을 이해하는 데 기여하며, 지리적 편향과 다양성을 밝히는 데 도움이 됩니다.
- ***Technical Details***: 프레이밍 기법(probing)은 LLM의 활성화 상태를 분석하여 내부 구조를 밝히는 데 사용됩니다. 주요 초점은 지리적 정보에 대한 수치 모형이 아닌 공간 분석을 활용하여 내재적인 지리적 패턴을 탐구하는 것입니다. 실험에서는 공간 자기상관 분석을 통해 지리적으로 가까운 지역의 내부 표현이 얼마나 유사한지를 평가했습니다.
- ***Performance Highlights***: 실험 결과, 공간 자기상관 분석을 통해 LLM의 내부 표현이 지리적으로 유의미한 패턴을 따르는지를 직접 관찰할 수 있었으며, LLM이 지리 정보를 처리하는 방식에 대한 명확한 이해를 제공하는 적은 수의 특성을 확인했습니다. 특히 가느다란 오토인코더(sparse autoencoder)를 통해 다의적 구조를 보다 해석 가능한 단일 의미적(monosemantic) 특징으로 분리하는 데 성공하였습니다.

### [InfoVids: Reimagining the Viewer Experience with Alternative Visualization-Presenter Relationships](https://arxiv.org/abs/2505.03164)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.03164.png)

Vote: 4

Authors: Ryan A. Rossi, Ji Won Chung, Tongyu Zhou, Ivy Chen, Shunan Guo, Kevin Hsu, Jeff Huang, Alexa Siu, James Tompkin, Franck Dernoncourt

- ***What's New***: InfoVids는 새로운 시각자료-발표자 관계를 통해 시청 경험을 새롭게 상상합니다. 전통적인 2D 화면과 3D 세계로 구분되는 발표 방식을 넘어, InfoVids는 시각자료와 발표자를 동일한 3D 공간 내에 통합함으로써 더 인간 중심적인 시청 경험을 제공합니다.
- ***Technical Details***: InfoVids는 ARKit을 이용해 발표자와 시각적 자료가 공유된 3D 공간 내에서 공존하도록 설계되었습니다. 본 연구에서는 각각 다른 형태와 상호작용 방식을 가지는 네 종류의 InfoVids(AirplaneVis, NapoleonVis, InjuryVis, WalmartVis)를 설계하여 시청자 경험을 탐색했습니다. 각 영상은 2D 화면과 비교하기 위해 동등한 내용의 전통적 슬라이드 발표를 포함합니다.
- ***Performance Highlights***: 연구 결과에 따르면, InfoVids는 시청자의 주의를 집중시키고 발표자의 참여도와 몰입감을 향상시키는 것으로 나타났습니다. AirplaneVis와 NapoleonVis는 시청자들에게 더 자연스러운 본체 움직임과 스토리텔링을 제공하며, WalmartVis는 전통적인 슬라이드 발표보다 덜 선호되었습니다. 이를 통해 시각자료와 발표자가 동일한 공간을 공유하는 새로운 동적 관계가 시청자 경험에 의미있는 영향을 미친다는 것을 알 수 있습니다.

### [SWE-smith: Scaling Data for Software Engineering Agents](https://arxiv.org/abs/2504.21798)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.21798.png)

Vote: 3

Authors: Kilian Leret, Ludwig Schmidt, Diyi Yang, John Yang, Carlos E. Jimenez, Yanzhe Zhang, Binyuan Hui, Ofir Press, Kabir Khandpur, Alexander Wettig

- ***What's New***: SWE-smith는 대규모 소프트웨어 엔지니어링 훈련 데이터를 생성하기 위한 새로운 파이프라인입니다. 이를 통해 128개의 GitHub 리포지토리에서 50,000개의 instance를 생성해 기존 대비 훨씬 큰 데이터세트를 제공합니다.
- ***Technical Details***: SWE-smith는 Python 코드베이스에서 실행 환경을 구성하고, 기존 테스트를 깨뜨릴 수 있는 수백에서 수천 개의 task instance를 자동으로 합성합니다. 각 인스턴스는 오류를 포함하도록 코드베이스를 수정하며, 실험실 설정에서 실행 기반 검증을 통해 버그를 효과적으로 감지합니다.
- ***Performance Highlights***: SWE-smith를 사용하여 훈련된 SWE-agent-LM-32B 모델은 SWE-bench Verified benchmark에서 40.2%의 Pass@1 해결율을 기록하며, 이는 오픈소스 모델 중 최고 수준의 성능입니다. 또한, SWE-smith는 기존보다 훨씬 낮은 비용으로 더 많은 task instance를 생성할 수 있도록 하였습니다.

### [Teaching Models to Understand (but not Generate) High-risk Data](https://arxiv.org/abs/2505.03052)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.03052.png)

Vote: 2

Authors: Swabha Swayamdipta, Matthew Finlayson, Luca Soldaini, Ryan Wang, Robin Jia

- ***What's New***: 이 논문은 기존에 높은 위험(Risk) 데이터 필터링으로 인해 제한된 언어 모델의 이해 능력을 개선하기 위한 새로운 훈련 패러다임인 SLUNG(Selective Loss to Understand but Not Generate)을 제시합니다. SLUNG은 모델이 높은 위험 데이터를 이해할 수 있도록 하면서도 이를 생성하지 않도록 설계되었습니다.
- ***Technical Details***: SLUNG은 차세대 토큰 예측 손실을 특정 토큰의 위험 수준에 따라 조정하며, 높은 위험 레이블을 가진 토큰에 대해 손실을 제로로 설정하거나 비호감(Unlikelihood) 손실을 적용합니다. 이 방법은 모델이 이전 높은 위험 토큰을 염두에 두고 낮은 위험 토큰을 생성하도록 합니다. SLUNG는 두 가지 구체적 예로 'Masked SLUNG'과 'Unlikelihood SLUNG'을 제시합니다.
- ***Performance Highlights***: 실험 결과, SLUNG 방법론은 모델이 독성(Toxic) 데이터를 이해하면서도 이를 생성하지 않도록 하는 데 효과적임을 보여주었습니다. 또한, 모델이 저작권이 있는 데이터를 생성하지 않으면서도 해당 데이터를 이해하는 것을 가능하게 했으며, 이로 인해 안전하고 역량 있는 언어 모델 개발이 가능해졌습니다.

### [Auto-SLURP: A Benchmark Dataset for Evaluating Multi-Agent Frameworks in Smart Personal Assistant](https://arxiv.org/abs/2504.18373)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.18373.png)

Vote: 2

Authors: Lei Shen, Xiaoyu Shen

- ***What's New***: Auto-SLURP는 대형 언어 모델(LLMs)을 기반으로 한 멀티 에이전트 프레임워크를 지능형 개인 비서 컨텍스트에서 평가하기 위한 벤치마크 데이터셋입니다. 이는 원래 SLURP 데이터셋을 확장하여 데이터 재라벨링과 시뮬레이션 서버 및 외부 서비스를 통합하여 종합적인 엔드 투 엔드 평가가 가능하도록 합니다.
- ***Technical Details***: Auto-SLURP 데이터셋은 SLURP 데이터셋의 의도-슬롯 구조를 확장하여 자연어 이해, 작업 실행 및 응답 생성의 평가를 가능하게 합니다. 데이터는 사용자의 상호작용 파이프라인과 일치하도록 재구성되었으며, 시뮬레이션 서버와 외부 서비스를 통합하여 복잡한 멀티 스텝 작업을 처리할 수 있도록 지원합니다. 이 설정은 API 접근, 모듈 간 상태 관리, 특수화된 에이전트 간의 조정을 포함한 다양한 작업 도메인을 포괄합니다.
- ***Performance Highlights***: Auto-SLURP는 최신 멀티 에이전트 프레임워크에서도 상당한 도전 과제를 제시하며, 가장 우수한 성능을 기록한 AgentLite 프레임워크가 성공률 46%를 달성했습니다. 실패 분석 결과, 대부분의 오류는 의도 에이전트에서 발생했으며, 이를 개선하기 위한 모델의 파인튜닝이 성능을 55% 향상시켰습니다. 이러한 결과는 신뢰할 수 있는 지능형 개인 비서를 구축하는 데 있어 여전히 상당한 어려움이 있음을 보여줍니다.

### [Invoke Interfaces Only When Needed: Adaptive Invocation for Large Language Models in Question Answering](https://arxiv.org/abs/2505.02311)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02311.png)

Vote: 2

Authors: Biao Qin, Chunlai Zhou, Jihao Zhao

- ***What's New***: 이 연구는 대규모 및 소규모 언어 모델(Large and Small Language Models)의 협력 시스템에서 환각(hallucination) 검출 문제와 프롬프트 재정렬 문제를 다루고 있습니다. 연구진은 이와 관련하여 AttenHScore라는 새로운 호출 평가 지표를 제안하였으며, 이 지표는 소규모 언어 모델의 생성 도중 환각의 증가와 확산을 정량화합니다. 이로써 추론 오류를 보다 정확하게 검출할 수 있으며, 회수 기반의 QA(Context)를 개선할 수 있는 방법론이 제시되었습니다.
- ***Technical Details***: 제안된 AttenHScore는 생성 과정에서 언어 모델의 주의(attention) 지표 및 생성 확률을 기반으로 하여 환각 발생 여부를 실시간으로 판단합니다. 각 토큰에 대해 주의 수준과 최대 확률을 결합하여 환각의 누적 및 확산을 측정하며, 이를 통해 실시간으로 대규모 언어 모델의 호출이 필요한지를 판별합니다. 또한, 질의와 텍스트 청크 사이의 불확실성을 평가하여 정보의 중요도를 기반으로 정렬을 최적화하는 메커니즘을 도입했습니다.
- ***Performance Highlights***: AttenHScore는 CoQA와 SQuAD 데이터세트에서 탁월한 성능을 보여주었으며, 우수한 평가 지표를 만족시켰습니다. 특히 복잡한 질문을 다룰 때 다른 최첨단 방법들보다 성능이 우수했습니다. 대규모-소규모 언어 모델의 협력 시스템에서 AttenHScore를 통해 얻은 방법은 성능 향상을 가져왔으며, 대부분의 큰 언어 모델을 독립적으로 사용할 때와 유사한 성능을 나타냈습니다.

### [Which Agent Causes Task Failures and When? On Automated Failure Attribution of LLM Multi-Agent Systems](https://arxiv.org/abs/2505.00212)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.00212.png)

Vote: 1

Authors: Zhiguang Han, Jiale Liu, Ming Yin, Yiran Chen, Shaokun Zhang, Huazheng Wang, Jieyu Zhang, Jingyang Zhang, Chi Wang, Qingyun Wu, Beibin Li

- ***What's New***: 이 논문은 LLM 멀티에이전트 시스템에서의 실패 원인 자동화를 연구한 새로운 영역을 제안합니다. 이를 지원하기 위해 Who&When 데이터셋을 소개하며, 127개의 LLM 멀티에이전트 시스템의 세부적인 실패 로그와 함께 결함이 특정 에이전트와 결정적 오류 단계에 연결되는 주석을 제공합니다.
- ***Technical Details***: Who&When 데이터셋은 127개의 다양한 에이전트 시스템에서 수집된 광범위한 실패 로그를 포함하고 있으며, 각 로그는 실패-책임 에이전트와 결정적 오류 단계를 지정하는 세부 주석이 포함되어 있습니다. 이 연구는 자동화된 실패 귀속 기법 세 가지를 개발하고 평가하며, 각 방법의 장단점을 요약합니다.
- ***Performance Highlights***: 오늘날의 최첨단(STOA) 추론 모델조차도 실질적인 사용성에 도달하지 못하며, 가장 우수한 방법은 실패-책임 에이전트를 식별하는 데 53.5%의 정확도를, 실패 단계를 정확히 찾는 데는 14.2%의 정확도를 기록했습니다.

