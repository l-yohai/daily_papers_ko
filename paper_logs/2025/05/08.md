## Daily Papers (2025-05-08)

### [Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities](https://arxiv.org/abs/2505.02567)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02567.png)

Vote: 52

Authors: Weihua Luo, Xinjie Zhang, Qing-Guo Chen, Jintao Guo, Zhao Xu, Guo-Hua Wang, Kaifu Zhang, Lunhao Duan, Shanshan Zhao, Minghao Fu

- ***What's New***: 이 논문은 단일 프레임워크에서 다중 모달리티 이해 및 생성 모델을 통합하는 최신 연구를 개괄적으로 소개합니다. GPT-4o와 같은 새로운 기능이 이를 잘 보여주며, 텍스트를 이미지로 변환하거나 반대로 이해하고 생성하는 통합 모델의 가능성을 강조합니다. 이 논문은 이러한 통합이 멀티모달 아키텍처 패러다임의 차이로 인해 큰 도전 과제를 안고 있음을 설명합니다.
- ***Technical Details***: 이 논문은 멀티모달 이해 및 이미지 생성 모델의 최근 발전을 소개하며, 기존 통합 모델을 세 가지 주된 구조적 패러다임으로 분류합니다: diffusion-based approach, autoregressive-based approach, 그리고 이 둘을 결합하는 hybrid approach. 각 카테고리에서 관련 기술과 접근 방식을 분석하며 다양한 벤치마크와 데이터셋을 모아 향후 탐구에 필요한 자원을 제공합니다.
- ***Performance Highlights***: 현재 통합 멀티모달 모델들은 대형 데이터셋에서의 실행 가능성을 보여주지만, 이미지 토큰화를 효율적으로 수행하고 텍스트와 이미지 간의 초모달 주의집중(cross-modal attention) 병목을 해결하는 것이 중요한 과제로 남아 있습니다. 기술적인 발전과 더불어 데이터 필터링, 비편향화, 합성을 통해 모델의 공정성과 강력함을 확보하는 방법론이 중요하게 대두되고 있습니다.

### [ZeroSearch: Incentivize the Search Capability of LLMs without Searching](https://arxiv.org/abs/2505.04588)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.04588.png)

Vote: 30

Authors: Pengjun Xie, Yingyan Hou, Xuanbo Fan, Hao Sun, Yan Zhang, Fei Huang, Zile Qiao, Yong Jiang, Jiayan Guo

- ***What's New***: ZEROSEARCH는 검색 엔진을 사용하지 않고 대형 언어 모델(LLMs)의 검색 능력을 강화하는 새로운 강화 학습(하강 강화)이 적용된 프레임워크입니다. 실시간 검색 엔진과의 상호작용 없이 LLM의 추론 및 문서 생성 능력을 독려합니다.
- ***Technical Details***: ZEROSEARCH는 경량 감독 학습(fine-tuning)을 통해 쿼리에 응답하여 관련 및 노이즈 문서를 생성할 수 있는 검색 모듈처럼 LLM을 변형합니다. 학습 과정에서 점진적으로 문서의 품질을 저하시켜 모델의 추론 능력을 도출하는 커리큘럼 롤아웃 기법(curriculum-based rollout strategy)을 사용합니다. Proximal Policy Optimization(PPO), Group Relative Policy Optimization(GRPO), Reinforce++ 등의 다양한 RL 알고리즘과 호환됩니다.
- ***Performance Highlights***: ZEROSEARCH는 3B LLM을 사용한 실험에서 실제 검색 엔진을 활용한 모델과 유사한 성능을 보였으며, 14B LLM에서는 이를 초과하는 성능을 보여주었습니다. 비용 효율성을 높이고, 대규모로 운용이 가능해 상용 API의 높은 비용 문제를 해결했습니다. 또한 다양한 파라미터 크기의 모델에서 일반화가 잘 되어 다양한 환경에서도 강력한 성능을 발휘합니다.

### [Beyond Recognition: Evaluating Visual Perspective Taking in Vision Language Models](https://arxiv.org/abs/2505.03821)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.03821.png)

Vote: 19

Authors: Michał Nauman, Maciej Wołczyk, Alicja Ziarko, Michał Kosiński, Piotr Miłoś, Gracjan Góral

- ***What's New***: 이 연구는 Vision Language Models (VLMs)의 시각적 관점 이해 능력을 평가하기 위한 새로운 시각 과제 세트를 제안합니다. 인간의 시각적 관점 이해 테스트에 영감을 받아, 단일 인물 피규어와 단일 객체를 다양한 공간 구성으로 배치하여 총 144개의 고유한 시각 과제를 디자인하였습니다.
- ***Technical Details***: 각 시각 과제는 장면 이해, 공간 추론, 시각적 관점 이해의 세 가지 수준을 평가하기 위해 7개의 진단 질문과 짝지어져 있습니다. GPT-4-Turbo, GPT-4o, Llama-3.2-11B-Vision-Instruct 등 최신 모델의 성과를 평가했으며, 모델들은 장면 이해에는 뛰어나지만 공간 추론에서 성능이 급감하고 관점 이해에서는 더욱 악화됩니다.
- ***Performance Highlights***: 모델들은 장면 이해 평가에서는 좋은 성과를 보였지만, 시각적 관점 이해 평가에서는 모든 모델이 어려움을 겪었습니다. 특히 GPT-4o 모델은 관점 이해에서 다른 모델보다 나은 성과를 내었지만 Q6 질문에서 발생하는 Claude 3 Sonnet의 주요 오류 패턴이 드러났습니다. 이 연구는 VLMs가 단순한 객체 인식 이상의 공간적 관계 추론 능력을 필요로 한다는 점을 강조합니다.

### [R&B: Domain Regrouping and Data Mixture Balancing for Efficient Foundation Model Training](https://arxiv.org/abs/2505.00358)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.00358.png)

Vote: 15

Authors: Satya Sai Srinath Namburi GNVV, Ziyi Chu, Avi Trost, Albert Ge, John Cooper, Ziyang Cai, Frederic Sala, Nicholas Roberts, Tzu-Heng Huang, Kendall Park

- ***What's New***: R&B는 대규모 모델 학습 효율성을 높이기 위해 도메인 재구성(Domain Regrouping)과 데이터 혼합 균형(Data Mixture Balancing)을 제안합니다. 이는 데이터셋을 의미 기반으로 세분화하고, 학습 중 획득한 도메인 그래디언트로 최적의 데이터 구성을 동적으로 조정함으로써 기존 방법의 한계를 극복합니다.
- ***Technical Details***: R&B 프레임워크는 임베딩 유사성을 바탕으로 데이터를 의미적으로 유사한 클러스터로 재구성(Regroup)하고, 정규 학습 중 계산된 도메인 그래디언트를 사용해 도메인 가중치를 동적으로 최적화(Balance)합니다. 이 방법은 추가적인 손실 또는 그래디언트 계산 없이 기존에 활용된 정보를 통해 도메인 간의 관계를 고려해 최적의 데이터 혼합을 구현합니다.
- ***Performance Highlights***: R&B는 다양한 자연어 처리 및 멀티모달 작업에서 0.01% 미만의 추가 계산 오버헤드로 기존 최첨단 데이터 혼합 전략의 성능을 능가하거나 추월합니다. 본 연구에서는 5개의 다양한 데이터셋에 대한 실험 결과, 기존 방식보다 99% 이상의 계산 자원 절감 효과를 보이며, 성능 유지 혹은 향상을 입증했습니다.

### [HunyuanCustom: A Multimodal-Driven Architecture for Customized Video Generation](https://arxiv.org/abs/2505.04512)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.04512.png)

Vote: 13

Authors: Qinglin Lu, Zhengguang Zhou, Sen Liang, Qin Lin, Yuan Zhou, Teng Hu, Zhentao Yu

- ***What's New***: HunyuanCustom은 사용자가 정의한 조건하에 특정 주제를 특징으로 하는 동영상을 생성하는 멀티모달 프레임워크입니다. 이 시스템은 이미지, 오디오, 비디오, 텍스트를 모두 지원하며 ID 일관성을 중시합니다. HunyuanVideo를 기반으로 하여 이미지-텍스트 융합 모듈을 도입하고, 이미지 ID강화 모듈을 통해 프레임 전반에 걸쳐 ID 특징을 강화합니다.
- ***Technical Details***: HunyuanCustom은 텍스트-이미지 융합 모듈을 도입하여 LLaVA 기반으로 상호작용을 가능하게 하며, 가로 방향 시간축을 활용한 강화 모듈로 ID 일관성을 높입니다. 오디오 및 비디오 조건 생성을 위해, 공간적인 크로스-어텐션을 통해 오디오 정렬이 이루어지는 AudioNet 모듈과 패치 기반 피쳐-정렬 네트워크로 비디오 피쳐 주입이 이루어집니다. 이러한 멀티모달 접근은 주제 일관성과 제어성을 강화합니다.
- ***Performance Highlights***: 구성이 단일 객체 및 다중 객체 시나리오에서 산업 표준 대비 뛰어난 성능을 보였습니다. ID 일관성, 사실성, 텍스트-비디오 정렬에 있어 비교 대상들을 능가하는 성과를 보여주었으며, 오디오 및 비디오 기반으로 맞춤화한 동영상 생성에서도 강력한 성능을 발휘했습니다. 이는 HunyuanCustom의 멀티모달 제어 능력과 ID 보존의 하이라이트를 보여줍니다.

### [PrimitiveAnything: Human-Crafted 3D Primitive Assembly Generation with Auto-Regressive Transformer](https://arxiv.org/abs/2505.04622)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.04622.png)

Vote: 13

Authors: Xiao Han, Yong-Jin Liu, Kaiwen Xiao, Jingwen Ye, Wei Yang, Yanning Zhou, Yiqin Zhu, Yuze He

- ***What's New***: PrimitiveAnything는 인간이 설계한 3D 프리미티브 조립체(primitives assembly)를 자동 회귀 트랜스포머(Auto-Regressive Transformer)를 사용하여 생성하는 새로운 프레임워크입니다. 이는 복잡한 3D 형태를 해석 가능한 기본 기하 요소로 분해하여, 다양한 형태 범주에 걸쳐 인간의 직관적인 분해 패턴을 캡처하고, 기존 메서드보다 더 정확하게 인간의 인식을 따르는 형상 분해를 가능하게 합니다.
- ***Technical Details***: 이 프레임워크는 다수의 프리미티브 타입을 통합된 파라미터화 방식으로 표현하고, 구조화된 시퀀스를 생성하여 교육성 및 준거성 있는 학습을 가능하게 합니다. 프리미티브 트랜스포머는 클래스 레이블, 변환, 회전 및 스케일링 매개변수를 학습 가능한 토큰으로 취급하여 시퀀셜한 생성 프로세스를 통해 형상 조건부 변수를 기반으로 프리미티브 시퀀스를 생성합니다. 또한, 혼란을 줄이기 위해 명확한 파라미터화를 사용하여 학습 모드를 안정화시켰습니다.
- ***Performance Highlights***:  PrimitiveAnything는 뛰어난 일반화 능력을 통해 다양한 형태 범주에서 높은 품질의 프리미티브 조립체를 생성하며, 기존의 최적화 기반 및 학습 기반 방법들보다 더 뛰어난 성능을 입증했습니다. 특히, 포인트 클라우드 형태의 입력 조건을 활용함으로써 3D 형상의 원본 표면 및 전역 구조를 충실히 재현할 수 있는 능력을 보여줍니다. 실험 결과, Chamfer Distance, Earth Mover's Distance 등의 여러 지표에서 기존 방법들 보다 낮은 값 또는 개선된 값을 기록했습니다.

### [Benchmarking LLMs' Swarm intelligence](https://arxiv.org/abs/2505.04364)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.04364.png)

Vote: 12

Authors: Hao Sun, Ji-Rong Wen, Kai Ruan, Mowen Huang

- ***What's New***: SwarmBench는 대형 언어 모델(Large Language Models; LLMs)의 분산된 에이전트로서의 군집 지능(swarm intelligence) 능력을 평가하기 위한 새로운 벤치마크입니다. SwarmBench는 다중 에이전트 시스템(Multi-Agent Systems; MAS)의 분산된 조정을 체계적으로 평가하기 위해 설계된 벤치마크로 다양한 코디네이션 작업을 포함하며, 지역적 감각 입력과 통신에만 주로 의존하도록 구성된 2D 그리드 환경을 특징으로 합니다.
- ***Technical Details***: SwarmBench는 다섯 가지 근본적인 MAS 조정 작업, 즉 추적(Pursuit), 동기화(Synchronization), 수집(Foraging), 군집(Flocking), 운송(Transport)을 특징으로 하며, 각 에이전트는 제한된 크기의 주변 k x k 그리드 뷰를 통해 정보를 인식하고 지역적, 익명적 메시지 전달을 사용할 수 있습니다. 에이전트들의 조정 효율성을 평가하기 위한 메트릭이 제안되었으며, 지역적 상호작용에서 집합적 전략의 출현이 요구됩니다.
- ***Performance Highlights***: 대표적인 LLM들을 사용하여 수행한 실험에서는 엄격한 지역 정보 제약 하에서 작업 간 상당한 성능 변동성이 드러났습니다. 이는 이러한 분산된 시나리오에서 강력한 계획 및 전략 형성의 한계를 보여줍니다.

### [Beyond Theorem Proving: Formulation, Framework and Benchmark for Formal Problem-Solving](https://arxiv.org/abs/2505.04528)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.04528.png)

Vote: 7

Authors: Junchi Yan, Xinhao Zheng, Qi Liu, Xingzhi Qi, Renqiu Xia, Qinxiang Cao

- ***What's New***: Beyond Theorem Proving 연구는 문제 해결을 결정론적 마르코프 의사결정 과정으로 수립하고, FPS(Formal Problem-Solving)라는 새로운 프레임워크를 소개하여 기존의 FTP(Formal Theorem Proving) 환경을 이용해 프로세스 수준의 확인 가능한 문제 해결을 수행합니다. D-FPS(Deductive FPS)는 해결과 검증을 분리하여 인간과의 일치를 향상시킵니다.
- ***Technical Details***: FPS 프레임워크는 FTP 환경에서 구현되며, 논리적 증명 생성 기능을 활용하여 문제 해결을 수행합니다. D-FPS는 모든 유효한 해답을 찾는 문제에 대해 연역적 추론을 강조하며, 문제 해결 과정을 앞으로과 뒤로 단계를 나눕니다. 벤치마크로는 FormalMath500, MiniF2F-Solving, PutnamBench-Solving이 있습니다. 응답의 정확성을 평가하기 위해 RPE(Restricted Propositional Equivalence)를 제안합니다.
- ***Performance Highlights***: FormalMath500에서 FPS의 최고 해결률은 23.77%, MiniF2F-Solving은 27.47%, PutnamBench-Solving은 0.31%로 확인되었습니다. 현재의 LLMs는 아직 프로세스 수준의 검증이 필요한 문제 해결에서 제한된 성능을 보여줍니다.

### [OpenHelix: A Short Survey, Empirical Analysis, and Open-Source Dual-System VLA Model for Robotic Manipulation](https://arxiv.org/abs/2505.03912)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.03912.png)

Vote: 6

Authors: Han Zhao, Xinyang Tong, Zirui Ge, Donglin Wang, Shuanghao Bai, Siteng Huang, Pengxiang Ding, Can Cui, Wanqi Zhou, Runze Suo, Yang Liu, Wenxuan Song, Bofang Jia

- ***What's New***: OpenHelix는 로봇 조작을 위한 듀얼 시스템 VLA(Vision-Language-Action) 모델의 오픈 소스 버전을 제시하여 해당 분야 연구를 가속화하려 합니다. 이 논문은 기존의 듀얼 시스템 구조를 비교하고, 핵심 설계 요소에 대해 체계적인 실험 분석을 제공하며, 비용 효율적인 오픈 소스 모델을 제공합니다.
- ***Technical Details***: OpenHelix 모델은 두 개의 시스템 체제를 기반으로 하여 구성되며, 이러한 구조는 인간의 인지에 관한 듀얼 프로세스 이론을 참고하여 두 시스템을 설정합니다. System 1은 직관적이고 빠른 반응을 책임지며, System 2는 고차원적인 이유와 논리를 기반으로 다소 느리지만 정확도가 높은 결정을 내립니다. MLLM(Multimodal Large Language Models) 선택, 정책 학습, 잠재적 특성 표현 등 다양한 설계 요소가 결합되어 로봇의 행동을 돕습니다. 주요 키워드로는 MLLM 선택, 정책 선택, 잠재적 특성 표현 등을 포함할 수 있습니다.
- ***Performance Highlights***: CALVIN, CALVIN-E 환경에서의 평가 결과, OpenHelix 시스템은 다양한 언어 및 동적 시나리오에서 높은 성능을 보였습니다. 특히 MLLM과 보조 작업을 통합한 경우, 일반화 성능이 크게 향상되었습니다. Asynchronous inference(비동기 추론)는 모델의 전반적인 성능에 미미한 영향을 미쳐, 고급 작업 성능 향상에 필수적이지는 않다는 결과를 보였습니다.

### [Scenethesis: A Language and Vision Agentic Framework for 3D Scene Generation](https://arxiv.org/abs/2505.02836)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02836.png)

Vote: 6

Authors: Chen-Hsuan Lin, Aniket Bera, Yunhao Ge, Lu Ling, Yifan Ding, Yichen Sheng, Ming-Yu Liu, Tsung-Yi Lin, Yu Zeng, Zhaoshuo Li

- ***What's New***: Scenethesis는 3D 장면 생성을 위한 언어와 비전 기반의 독창적 프레임워크로, 텍스트 기반의 코스 레이아웃 초안을 대규모 언어 모델(LLM)로 작성한 후, 비전 기반 입체적 개선을 통해 구현됩니다. 물리 기반 최적화를 적용하여 오브젝트의 충돌을 방지하고 안정성을 촉진하여 현실적이고 물리적으로 타당한 3D 상호작용 장면을 생성합니다.
- ***Technical Details***: Scenethesis는 LLM을 통해 코스 레이아웃을 작성하고, 비전 모듈을 통해 이미지를 생성하여 공간적 관계를 개선합니다. 그 후, 물리 인식 최적화 모듈은 시맨틱 대응 매칭과 서명 거리 필드(SDF)를 사용하여 객체 배열을 정밀하게 조정하여 물리적 타당성을 확보합니다. 마지막으로 판정 모듈은 공간 일관성을 검사합니다.
- ***Performance Highlights***: Scenethesis는 장면 다양성, 레이아웃 현실성, 물리적 타당성에서 최첨단 방법들보다 뛰어난 성능을 발휘합니다. 실험 결과, 충돌률이 크게 감소하고 안정성은 향상되었으며 접근성과 이동 가능성도 개선되어, 오브젝트의 기능적 배치와 상호작용성이 강화된 환경을 생성함을 보여주었습니다.

### [OmniGIRL: A Multilingual and Multimodal Benchmark for GitHub Issue Resolution](https://arxiv.org/abs/2505.04606)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.04606.png)

Vote: 5

Authors: Wei Tao, Xilin Liu, Hongyu Zhang, Mingzhi Mao, Yuchi Ma, Lianghong Guo, Zibin Zheng, Yanlin Wang, Jiachi Chen, Runhan Jiang

- ***What's New***: OmniGIRL은 GitHub Issue ResoLution을 평가하기 위한 다국어 및 멀티모달 벤치마크로 개발되었습니다. 이 벤치마크는 다른 프로그래밍 언어와 다양한 도메인, 그리고 멀티모달 입력 정보로 구성된 959개 작업 인스턴스를 포함하고 있으며, 이는 기존의 단일 언어 및 텍스트 기반 벤치마크의 한계를 극복하고, 보다 포괄적인 대규모 언어 모델(LLMs)의 성능 평가를 목표로 하고 있습니다.
- ***Technical Details***: OmniGIRL 벤치마크는 Python, JavaScript, TypeScript, Java의 네 가지 주요 프로그래밍 언어와 웹 개발, 코드 품질, 네트워크 도구 등 8개의 도메인에서 수집된 15개 리포지토리를 기반으로 하고 있습니다. 또한, 텍스트, 이미지, 웹사이트 링크 등 다양한 입력 모달리티를 포함하여 현재의 LLMs가 다양한 입력을 처리할 수 있는 능력을 평가하고 있습니다.
- ***Performance Highlights***: 현재의 선도적인 LLMs는 OmniGIRL에서 제한된 성능을 보였습니다. Agentless-X 접근법을 사용한 GPT-4o 모델은 8.6%의 이슈 해결 비율을 기록했고, 오라클 리트리벌(Oracle Retrieval) 방법을 사용한 Claude-3.5-Sonnet은 이미지가 포함된 문제에서 10.5%의 해결 비율을 보였습니다. 이러한 결과는 더 많은 연구 및 방법 개선의 필요성을 시사합니다.

### [OSUniverse: Benchmark for Multimodal GUI-navigation AI Agents](https://arxiv.org/abs/2505.03570)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.03570.png)

Vote: 4

Authors: Mariya Davydova, Patrick Barker, Sinéad Ryan, Arturo Márquez Flores, Daniel Jeffries

- **What's New**: OSUniverse는 고급 GUI-내비게이션 AI 에이전트를 위한 복잡한 멀티모달 데스크톱 지향 작업을 벤치마크하는 새로운 프레임워크입니다. 이 벤치마크는 사용 용이성, 확장성, 포괄적인 테스트 케이스 적용 및 자동화된 유효성을 검사하는 데 중점을 두고 있습니다. 특정 패턴의 작업들과이를 풀기 위해서 여러 애플리케이션을 순차적으로 처리하는 능력을 시험합니다. 최신 SOTA 에이전트가 50% 이상의 성취율을 보이지 못하도록 복잡도를 조정함으로써 고안되었습니다.

### [Knowledge Augmented Complex Problem Solving with Large Language Models: A Survey](https://arxiv.org/abs/2505.03418)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.03418.png)

Vote: 4

Authors: Huajun Chen, Da Zheng, Lanning Wei, Junwei Su, Jintian Zhang, Yuchen Tian, Ningyu Zhang, Lun Du, Yuqi Zhu

- ***What's New***: 이 설문은 대형 언어 모델(Large Language Models; LLMs)을 활용한 복합 문제 해결 능력을 지식 기반으로 향상시키는 방법에 대해 탐구합니다. CoT(Chain-of-Thought) 추론과 지식 보강, 다양한 LLM 기반 및 도구 기반 검증 기술에 대한 사례를 분석하여 LLMs의 한계를 밝히고자 합니다.
- ***Technical Details***: LLMs는 다단계 추론(Multi-step reasoning), 도메인 지식 통합(Domain Knowledge Integration) 및 결과 검증(Result Verification)의 측면에서 각기 다른 도메인의 복잡한 문제에 대한 해결책을 찾고자 여러 알고리즘을 사용합니다. CoT 추론은 LLMs가 생성하는 여러 추론 경로를 평가하여 정답까지 도달할 가능성을 높이는 과정입니다. 이를 위해 검증자(Verifier) 모델이 사용되며, 지식은 RAG(Retrieval-Augmented Generation) 및 GraphRAG와 같은 기법을 통해 문서로부터 추출하여 활용합니다.
- ***Performance Highlights***: LLMs는 탐지 가능하고 잘 정의된 목표를 가진 문제에 대해 뛰어난 성능을 보였으나, 공학 설계나 데이터 과학, 수학의 문제에서 각각의 복잡한 도메인 요건을 충족시키는 데 있어서는 여러 도전 과제를 안고 있습니다. 성능 향상을 위해 LLMs는 강화를 통한 자가 수정(Self-Correction)을 적용하고, 코드 생성 및 최적화 문제에서는 실행 피드백을 활용한 자동 디버깅을 진행합니다. 이러한 실험 결과는 여러 도메인에서 LLMs의 현재 한계를 보여주며, 각 분야 연구에 따른 개선 방안을 제시합니다.

### [LLM-Independent Adaptive RAG: Let the Question Speak for Itself](https://arxiv.org/abs/2505.04253)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.04253.png)

Vote: 2

Authors: Nikita Krayko, Maria Marina, Viktor Moskvoretskii, Daria Galimzianova, Mikhail Salnikov, Vasily Konovalov, Alexander Panchenko, Nikolay Ivanov, Sergey Pletenev

- ***What's New***: 이 연구에서는 대형 언어 모델(LLM; Large Language Models)의 불확실성 추정에 의존하지 않고도 기존의 RAG(Retrieval-Augmented Generation) 접근 방식의 한계를 극복할 수 있는 경량 저비용 적응형 검색법을 제안합니다. 외부 정보를 기반으로 하는 LLM 독립 적응형 검색 방식(LLM-Independent Adaptive RAG)을 통해 효율성을 크게 개선하면서도 성능을 유지할 수 있습니다.
- ***Technical Details***: 이 연구에서는 외부 정보에 기반한 27개의 새로운 특징을 7개 그룹으로 구분하여 LLM 독립 적응형 검색에 활용했습니다. 이 접근 방식은 질문 유형, 엔티티의 인기(Popularity), 문맥의 관련성을 판단하여 검색 필요성을 결정하며, LLM 호출을 줄여 효율성을 향상시킵니다. 이를 통해 재현 과정에서 LLM에 의존하지 않고도 높은 수준의 QA 성능을 달성할 수 있습니다.
- ***Performance Highlights***: 우리의 방법은 SQuAD, Natural Questions, MuSiQue 등 6개의 QA 데이터셋에서 기존의 불확실성 기반 적응형 검색 방법과 유사한 성능을 보여주면서도 LLM 호출을 줄여 효율성을 개선했습니다. 특히 복잡한 질문에 대해서는 불확실성에 기반한 기존 방법보다 더 나은 성능을 나타냅니다.

### [RAIL: Region-Aware Instructive Learning for Semi-Supervised Tooth Segmentation in CBCT](https://arxiv.org/abs/2505.03538)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.03538.png)

Vote: 2

Authors: Jie Liu, Ziyu Shen, Jiashuo Guo, Chuyu Zhao, Hao Huang, Zhongwei Zhou, Zekuan Yu

- ***What's New***: RAIL은 3D 치아 세분화를 위한 반지도 학습 프레임워크로서, Region-Aware Instructive Learning 기법을 활용하여 기존 방법들이 가지고 있던 문제를 해결합니다. 듀얼-그룹, 듀얼-학생 프레임워크를 기반으로, 두 학생 모델이 상호 교류하며 지도와 비지도 학습에서 오류와 불확실한 영역의 효과를 감소시킵니다.
- ***Technical Details***: 이 연구에서는 두 가지 주요 메커니즘을 소개합니다. Disagreement-Focused Supervision (DFS) Controller는 학생 모델의 예측과 실제 라벨 간의 차이에 중점을 두어 오차 영역에 집중하게 하고, Confidence-Aware Learning (CAL) Modulator는 모델의 신뢰도가 높은 영역에 중점을 두어 불안정한 패턴을 학습하는 것을 방지하며 전반적인 의사결정의 안정성을 향상시킵니다.
- ***Performance Highlights***: RAIL은 다수의 CBCT 치아 세분화 데이터세트에 대한 실험에서 최첨단 방법들을 초과하는 성능을 보여줍니다. 특히, 제한된 레이블 데이터 하에서도 경쟁력 있는 성능을 발휘하며, Dice 계수와 Jaccard 계수에서 각각 1% 이상 향상되고, Hausdorff Distance와 Average Surface Distance에서 획기적인 감소를 보였습니다.

### [OpenVision: A Fully-Open, Cost-Effective Family of Advanced Vision Encoders for Multimodal Learning](https://arxiv.org/abs/2505.04601)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.04601.png)

Vote: 2

Authors: Cihang Xie, Yanqing Liu, Haoqin Tu, Hongru Zhu, Xianhang Li

- ***What's New***: OpenVision은 완전한 공개형이며 비용 효율적인 고급 비전 인코더(Vision Encoders) 계열로, 대형 멀티모달 학습 모델(Multimodal Learning Models)에서 OpenAI의 CLIP와 경쟁하거나 그 성능을 초과합니다. 연산자들에게는 모델 용량과 효율성 간의 유연한 트레이드오프를 제공하며 보다 큰 모델은 향상된 멀티모달 성능을, 작은 모델은 경량화된 엣지 준비 배치를 가능하게 합니다.
- ***Technical Details***: OpenVision은 Recap-DataComp-1B와 CLIPS와 같은 최신 연구를 기반으로 합니다. 이 두 가지를 활용하여 전반적인 인코더 품질을 향상시킬 수 있는 키 핵심 요소를 밝히고, 멀티모달 모델 개발에 있어 실질적인 이점을 보여줍니다. CLIPS의 효율적인 학습 커리큘럼을 따라서, 모든 인코더는 세 가지 연속된 해상도 단계에서 사전 학습이 이루어집니다. 언어와 비전을 보다 효과적으로 통합하기 위한 멀티모달 대형 언어 모델(MLLM)의 시각적 인스트럭션 튜닝과 평가도 수행됩니다.
- ***Performance Highlights***: 다양한 멀티모달 평가 지표에서 OpenVision은 OpenAI의 CLIP와 Google의 SigLIP를 사용했을 때보다 일관되게 동등하거나 그 이상의 성능을 입증했습니다. 특히 5.9M에서 632.1M 파라미터까지 초점을 맞춘 25개 이상의 체크포인트를 공개하여, 엣지 장치부터 고용량 서버까지 부드러운 정확도-효율성 트레이드오프를 가능하게 했습니다. 제안된 OpenVision 모델들은 멀티모달 학습에서 CLIP-Bench, Text VQA와 같은 다양한 평가에서 상당한 성능을 보여주었습니다.

### [COSMOS: Predictable and Cost-Effective Adaptation of LLMs](https://arxiv.org/abs/2505.01449)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.01449.png)

Vote: 1

Authors: Frederic Sala, Jiayu Wang, Aws Albarghouthi

- ***What's New***: COSMOS는 대형 언어 모델(LLMs)의 적응 전략 선택 문제를 체계적으로 정형화하고, 성능과 비용을 고려하여 예측할 수 있는 새로운 프레임워크를 도입합니다. 이를 통해 실험 없이도 최적의 모델 및 적응 전략을 예측하는 것이 가능합니다.
- ***Technical Details***: COSMOS는 임베딩이 강화된 경량 프록시 모델과 저샘플 확장 법칙을 활용하여 미세 조정 성능을 예측합니다. 이러한 예측 모델들을 통해 LLM의 적응 결과를 최소 비용으로 예측할 수 있도록 설계되었습니다. 이 프레임워크는 다양한 적응 전략에 대한 성과와 비용을 데이터 효율적으로 예측하며, 학습 시간 및 테스트 시간 동안의 적응 전략을 캡쳐할 수 있습니다.
- ***Performance Highlights***: 8개의 대표적인 벤치마크에 대한 평가 결과, COSMOS는 평균적으로 92.72%의 계산 비용을 줄이면서도 높은 예측 정확도(1.09% 평균 절대 오차)를 달성했습니다. 리소스 집약적인 시나리오에서는 최대 98.71%까지 비용 감소를 보여, LLM 배포 시의 계산 부담을 크게 줄일 수 있으며, 성능 표준을 유지할 수 있음을 입증했습니다.

### [Uncertainty-Weighted Image-Event Multimodal Fusion for Video Anomaly Detection](https://arxiv.org/abs/2505.02393)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02393.png)

Vote: 1

Authors: Jihong Park, Sungheon Jeong, Mohsen Imani

- ***What's New***: 이 논문에서는 비디오 이상 탐지(Video Anomaly Detection) 문제를 해결하기 위해 이미지와 이벤트 모달리티를 불확실성 가중치(Uncertainty-Weighted)를 통해 결합하는 새로운 프레임워크인 IEF-VAD를 제안합니다. 특별한 이벤트 센서 없이도 RGB 비디오에서 합성된 이벤트 표현을 통해 모션 단서를 강조하여 정확하고 강력한 비디오 이해가 가능합니다.
- ***Technical Details***: IEF-VAD는 Bayesian 불확실성 추정을 기반으로 이미지와 이벤트 표현의 불확실성을 통합하여 역분산 가중치를 계산합니다. 이를 통해 시간적으로 중요한 이벤트 큐가 이미지의 공간적 맥락과 상호보완적으로 작용하게 하며, Kalman 필터 업데이트 및 반복적 정제를 통해 남아 있는 모달리티 간 노이즈를 제거합니다.
- ***Performance Highlights***: UCF-Crime, XD-Violence, ShanghaiTech, MSAD 등 여러 대규모 비디오 이상 탐지 벤치마크에서 기존의 최첨단 방법들과 비교하여 일관된 성능 향상을 보여주며, 특히 UCF-Crime에서 AUC 88.67%, XD-Violence에서 AP 87.63%, ShanghaiTech에서 AUC 97.98%의 성능을 기록하며 최고의 결과를 달성했습니다. 이 프레임워크는 RGB 스트림이 열화된 상황에서도 합성 이벤트 모달리티의 보완적인 정보를 활용하여 측정 가능한 성능 이득을 제공합니다.

### [AutoLibra: Agent Metric Induction from Open-Ended Feedback](https://arxiv.org/abs/2505.02820)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02820.png)

Vote: 1

Authors: Hao Zhu, Xinkai Yu, Diyi Yang, Charlotte Ka Yee Yan, Jason Zhang, Phil Cuvin

- ***What's New***: AutoLibra는 새로운 에이전트 평가 프레임워크로, 개방형 피드백(Open-ended Feedback)으로부터 정밀한 행동 분석을 위한 메트릭(Metric)을 유도하는 방법을 제안합니다. 이를 통해 에이전트의 세밀한 행위를 평가할 수 있는 메트릭을 생성하며, 사용자 피드백을 바탕으로 에이전트의 행동 개선을 위한 새로운 인사이트를 제공합니다.
- ***Technical Details***: AutoLibra는 두 가지 주요 프로세스를 통해 메트릭을 유도합니다: (1) 피드백 접지(Feedback Grounding): 사용자 피드백을 에이전트의 특정 행동 항목과 연결하여 축적합니다. (2) 행동 클러스터링(Behavior Clustering): 접지된 피드백 항목을 클러스터링하여 일관된 행동 패턴으로 요약합니다. 이 프로세스를 바탕으로, 확보된 메트릭을 통해 대형 언어 모델(LLM-as-a-Judge)을 활용하여 에이전트의 성능을 평가합니다.
- ***Performance Highlights***: AutoLibra는 다양한 에이전트 도메인에서 높은 커버리지(Coverage)와 낮은 중복성(Redundancy)을 가진 해석 가능한 메트릭을 유도하는 데 성공했습니다. 이를 통해 텍스트 게임 에이전트의 성능을 20% 이상 개선하고, 웹 탐색 에이전트의 세밀한 행동 평가를 지원하였습니다.

### [Cognitio Emergens: Agency, Dimensions, and Dynamics in Human-AI Knowledge Co-Creation](https://arxiv.org/abs/2505.03105)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.03105.png)

Vote: 1

Authors: Xule Lin

- ***What's New***: 이 논문은 Cognitio Emergens (CE)라는 새로운 프레임워크를 소개합니다. 이 프레임워크는 인간과 AI 시스템이 상호 작용을 통해 지식 창출의 진화를 어떻게 형성하는지를 설명하며, 기존 모델의 한계를 극복하고 공진화적 인식 파트너십을 강조합니다.
- ***Technical Details***: CE는 세 가지 주요 구성 요소를 통합합니다: (1) Agency Configurations는 인간과 AI 간의 권위 분배를 설명하며 동적인 파트너 관계를 캡처합니다. (2) Epistemic Dimensions는 Discovery, Integration, Projection 축을 따라 협력하여 발생하는 6개의 구체적 역량을 식별합니다. (3) Partnership Dynamics는 연구자들이 지식에 대해 해석적 통제력을 잃을 위험이 있는 Epistemic Alienation과 같은 관계 진화를 형성하는 힘을 식별합니다.
- ***Performance Highlights***: CE는 autopoiesis 이론, 사회 시스템 이론, 조직 모듈성 이론을 기반으로 하여, 인간과 AI 간 지식 공동 창조가 지속적인 역할, 가치, 그리고 조직 구조에 대한 협상을 통해 어떻게 발생하는지를 드러냅니다. 이 프레임워크는 연구자, 기관 리더 및 정책 입안자에게 지식 공동 창출의 복잡성을 탐색하기 위한 실질적 도구를 제공합니다.

### [Alpha Excel Benchmark](https://arxiv.org/abs/2505.04110)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.04110.png)

Vote: 0

Authors: David Noever, Forrest McKee

- ***What's New***: Alpha Excel Benchmark는 Financial Modeling World Cup (FMWC) Excel 대회의 과제를 사용하여 대형 언어 모델(LLM)의 성능을 평가하는 새로운 벤치마크입니다. 이는 실제 비즈니스 환경에서의 과제를 기반으로 LLM의 역량을 평가하는 데 중점을 두며, 기존의 추상적 학술 문제와의 차별화를 꾀합니다.
- ***Technical Details***: Alpha Excel Benchmark는 총 113개의 FMWC Excel 대회 과제를 표준화된 JSON 형식으로 변환하여 프로그램적으로 평가할 수 있도록 설계되었습니다. 이 과제들은 패턴 인식, 금융 계산, 게임 시뮬레이션, 데이터 처리 작업 등 다양한 범주를 포괄합니다. 각 과제는 Excel 고유의 구현 세부 사항을 배제하고, LLM이 문제의 본질을 이해할 수 있도록 변형되었습니다.
- ***Performance Highlights***: GPT-4o-mini 모델은 금융 모델링 및 데이터 분석 작업에서 특히 높은 성능을 보였으며, Qwen 2.5가 그 뒤를 따랐습니다. 그러나 모든 모델이 게임 시뮬레이션 과제에서 복잡한 규칙 해석과 멀티스텝 추론에서 어려움을 겪었습니다. 에러 분석을 통해 수치 계산에서의 오류와 규칙 적용의 일관성 결여가 주요 문제점으로 지적되었습니다.

