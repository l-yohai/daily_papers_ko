## Daily Papers (2025-05-02)

### [A Survey of Interactive Generative Video](https://arxiv.org/abs/2504.21853)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.21853.png)

Vote: 30

Authors: Quande Liu, Haoxuan Che, Jiwen Yu, Hao Chen, Yiran Qin, Pengfei Wan, Di Zhang, Kun Gai, Xihui Liu, Xintao Wang

- ***What's New***: Interactive Generative Video (IGV)는 인터랙티브 기능과 생성 능력을 결합하여 고품질의 비디오 콘텐츠를 제작하는 기술입니다. 이 논문에서는 IGV를 정의하고, 주로 게임, 구현 AI, 자율주행이라는 세 가지 주요 응용 분야에서의 발전 상황을 조망합니다. 또한, IGV의 이상적인 시스템을 다섯 개의 핵심 모듈(Generation, Control, Memory, Dynamics, Intelligence)로 분해하여 기술적 도전과 미래 연구 방향을 제시합니다.
- ***Technical Details***: IGV는 Generation 모듈을 통해 고품질 비디오를 생성하며, Control 모듈을 통해 사용자 상호작용을 제어합니다. Memory 모듈은 콘텐츠 일관성을 유지하며, Dynamics 모듈은 물리적 법칙을 시뮬레이션합니다. 마지막으로, Intelligence 모듈은 인과적 추론을 통합하여 IGV가 자가 발전 가능한 메타버스로 진화할 수 있는 가능성을 제시합니다.
- ***Performance Highlights***: IGV는 다양한 분야에서 실질적인 적용 가능성을 보이며, 특히 게임에서 무한히 탐험 가능한 가상 세계를 구현할 수 있습니다. 또한, 구현 AI에서는 로봇 훈련을 위한 물리적으로 일관된 시뮬레이션을 제공하고, 자율주행에서는 안전한 시뮬레이션 테스트 환경을 조성할 수 있는 능력을 보여줍니다.

### [DeepCritic: Deliberate Critique with Large Language Models](https://arxiv.org/abs/2505.00662)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.00662.png)

Vote: 26

Authors: Wenkai Yang, Yankai Lin, Ji-Rong Wen, Jingwen Chen

- ***What's New***: DeepCritic은 대형 언어 모델(Large Language Models; LLMs)의 수학적 비판 능력을 개선하기 위한 새로운 두 단계의 프레임워크를 소개합니다. 이 프레임워크는 LLM들이 각 추론 단계의 수학적 솔루션에 대해 심도 있는 비판을 수행할 수 있도록 설계되었습니다.
- ***Technical Details***: DeepCritic은 Qwen2.5-72B-Instruct를 사용하여 4.5K의 장기 비판 데이터를 생성하고, 이를 기반으로 감독 학습을 진행합니다. 각 비판 데이터는 다각적인 검증과 각 추론 단계에 대한 초기 비판의 심도 깊은 비판을 포함하고 있습니다. 이후 PRM800K의 인간 라벨 데이터 또는 Monte Carlo 샘플링 기반 추정으로 자동 주석된 데이터를 사용하여 강화 학습(Reinforcement Learning; RL)을 수행하여 비판 능력을 더욱 촉진합니다.
- ***Performance Highlights***: 개발된 DeepCritic 모델은 기존의 LLM 비판 모델(DeepSeek-R1-distill 모델 및 GPT-4o 포함)을 뛰어넘는 성능을 보여주었으며, 특히 오류 식별 벤치마크에서 탁월한 결과를 보였습니다. 또한 LLM 생성기의 잘못된 단계를 더 효과적으로 수정하는 데 도움을 주었습니다.

### [T2I-R1: Reinforcing Image Generation with Collaborative Semantic-level and Token-level CoT](https://arxiv.org/abs/2505.00703)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.00703.png)

Vote: 19

Authors: Dongzhi Jiang, Renrui Zhang, Shilin Yan, Pheng-Ann Heng, Zhuofan Zong, Le Zhuo, Hao Li, Hongsheng Li, Ziyu Guo

- ***What's New***: T2I-R1은 텍스트-이미지 생성(Text-to-Image Generation) 작업에 강화 학습(Reinforcement Learning; RL) 기반의 이중 레벨 체인 오브 띵킹(Chain-of-Thought; CoT) 추론 프로세스를 결합하여 이미지 생성의 성능을 강화했습니다. T2I-CompBench에서 13%, WISE 벤치마크에서 19%의 개선 성과를 거두었습니다.
- ***Technical Details***: T2I-R1 모델은 텍스트 설명의 고차원적인 설계를 담당하는 세멘틱 레벨(Semantic-level) CoT와 패치 단위로 이미지 토큰을 생성하는 토큰 레벨(Token-level) CoT의 두 가지 레벨로 구성되어 있습니다. 이러한 두 레벨의 CoT는 BiCoT-GRPO라는 RL 프레임워크에서 하나의 학습 단계 내에서 최적화됩니다. 또한, 각 생성 과정에서 여러 비전 전문가(vision experts)를 활용해 리워드 모델(reward model)로 평가됩니다.
- ***Performance Highlights***: T2I-R1은 기존 모델들과 비교했을 때 T2I-CompBench와 WISE 벤치마크에서 각각 13%와 19%의 성능 향상을 달성하였으며, 특히 공간 과업(spatial task)에서는 이전 최첨단 모델인 FLUX.1을 초과하여 5% 이상의 개선을 보여주었습니다. 이러한 성과는 CoT 기반의 개선된 설계 및 생성 과정이 사용자 의도 파악 및 드문 시나리오에 대한 견고함을 제공함을 입증합니다.

### [100 Days After DeepSeek-R1: A Survey on Replication Studies and More Directions for Reasoning Language Models](https://arxiv.org/abs/2505.00551)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.00551.png)

Vote: 15

Authors: Yao Xiao, Hai Ye, Bin Wang, Xiang Lin, Xingxuan Li, Yue Deng, Dianwen Ng, Chong Zhang, Zhanfeng Mo, Qi Zhang, Lidong Bing

- ***What's New***: 이번 연구에서는 최근 출시된 DeepSeek-R1의 복제 연구를 통해 추론 언어 모델(Reasoning Language Models; RLMs)의 향후 연구 방향을 제시합니다. 이 연구는 주로 감독된 미세 조정(Supervised Fine-tuning; SFT)과 검증 가능한 보상을 통한 강화 학습(Reinforcement Learning from Verifiable Rewards; RLVR)에 중점을 두고 있으며, 데이터 준비와 방법 설계를 통해 유용한 통찰력을 제공합니다.
- ***Technical Details***: DeepSeek-R1의 복제 연구들은 주로 SFT와 RLVR 두 가지 방향에 초점을 맞추고 있습니다. SFT의 경우, 데이터 수집 및 준비에 대한 세부 정보와 트레이닝 절차를 소개하고, RLVR의 경우, 알고리즘과 보상 시스템 디자인을 설명합니다. 데이터 준비 과정에서는 주로 수학과 코딩 문제를 기반으로 하며, 각 데이터셋은 특정한 검증 과정을 거쳐 고품질의 문제와 해답을 구성합니다.
- ***Performance Highlights***: DeepSeek-R1-Zero와 같은 초기 모델들은 수학적 추론과 지식 작업에서 강력한 성능을 보였으며, 이는 스탠드얼론 RLVR 과정을 통해 이루어졌습니다. SFT와 RLVR을 통해 향상된 RLMs는 다양한 과제에 대한 강력한 일반화 능력을 보여주었으며, 이는 단순 모방 학습으로는 불가능했던 성과입니다. 그러나, RLMs의 안전성 및 해석 가능성 향상, 보상 메커니즘의 정교화 등은 여전히 미래 연구에서 해결해야 할 과제로 남아 있습니다.

### [Self-Generated In-Context Examples Improve LLM Agents for Sequential Decision-Making Tasks](https://arxiv.org/abs/2505.00234)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.00234.png)

Vote: 7

Authors: Zhiqiang Xie, Vishnu Sarukkai, Kayvon Fatahalian

- ***What's New***: 이 논문에서는 큰 언어 모델(Large Language Model; LLM) 에이전트들이 순차적 결정 과제를 해결할 때, 자신이 이전에 성공했던 사례들을 자동으로 학습하여 퍼포먼스를 향상시키는 새로운 방법을 제안합니다. 이러한 접근은 구체적인 과제에 맞춘 지식 엔지니어링에 의존하지 않고, 자가 생성 예제의 데이터베이스를 구축하고 세련되게 하는 데 초점을 맞춥니다.
- ***Technical Details***: 이 연구는 ReAct 스타일의 에이전트를 가정하며, 에이전트가 각각의 결정 지점에서 가장 관련성이 높은 예제를 자가 생성 데이터베이스에서 선택하여 사용할 수 있도록 하는 '자가 생성 예제 채택(Traj-Bootstrap)' 알고리즘을 개발했습니다. 또한, 데이터베이스 수준에서 고성능 예제 컬렉션을 식별하는 '인구기반 훈련(DB-Selection)' 및 관찰 가능한 유틸리티에 기반하여 개별 경로를 보유하는 '예제-수준 선택(Exemplar-Selection)'이 포함됩니다.
- ***Performance Highlights***: 이 방법은 ALFWorld에서 89%의 성공률을 기록하며, 초기 에이전트가 각 과제당 두세 번 정도의 시도를 할 수 있도록 한 것과 비슷한 성능을 보였습니다. Wordcraft와 InterCode-SQL에서도 각각 64%와 79%로 성능이 증가했습니다. DB-Selection 및 Exemplar-Selection 확장 방법을 통해 ALFWorld에서는 91%의 성공률을 달성하여 더 복잡한 접근 방식을 사용할 때와 비슷한 결과를 얻었습니다.

### [KeySync: A Robust Approach for Leakage-free Lip Synchronization in High Resolution](https://arxiv.org/abs/2505.00497)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.00497.png)

Vote: 6

Authors: Antoni Bigata, Stella Bounareli, Michał Stypułkowski, Rodrigo Mira, Konstantinos Vougioukas, Stavros Petridis, Maja Pantic

- ***What's New***: 이 연구는 고해상도 립 싱크(Lip Synchronization) 성능을 향상시키기 위한 새로운 접근 방식인 KeySync를 소개합니다. 기존의 립 싱크 모델들이 가지는 표현 누출(Expression Leakage)과 얼굴 가림 현상(Occlusions)에 대한 문제를 해결할 목적으로 설계된 두 단계 프레임워크를 도입하여, 실제 응용 분야에서 더욱 효과적인 립 싱크 성능을 보장합니다.
- ***Technical Details***: KeySync는 최근의 얼굴 애니메이션 기술을 활용한 두 단계 프레임워크를 통해 고품질의 립 싱크 영상을 생성합니다. 이 모델은 얼굴 하부를 적절하게 가리는 마스킹 전략으로 입력 영상으로부터의 누출을 최소화하고, 영상 분할 모델을 사용해 얼굴 가림 현상을 일관되게 처리합니다. 또한, 학습 과정에서 음악 임베딩을 추가하여 비디오와 오디오 프레임 간의 정렬을 개선합니다.
- ***Performance Highlights***: KeySync는 립 싱크 성능 면에서 최고 수준을 달성하였으며, 특히 교차 싱크 동기화를 통해 오디오와 비디오 간의 불일치를 효과적으로 처리할 수 있음을 입증했습니다. 제안된 모델은 여러 객관적 척도 및 사용자 연구에서 기존 방법들을 능가하는 결과를 보였고, 누출 측정 기준인 LipLeak을 통해 모델의 표현 누출 개선을 확인할 수 있었습니다.

### [TF1-EN-3M: Three Million Synthetic Moral Fables for Training Small, Open Language Models](https://arxiv.org/abs/2504.20605)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.20605.png)

Vote: 4

Authors: Laura Diosan, Mihai Nadas, Andreea Tomescu, Andrei Piscoran

- ***What's New***: TF1-EN-3M은 소규모의 개방형 언어 모델들(Open Language Models; OLMs)을 훈련시키기 위해 만들어진 세 백만 개의 영어 도덕 우화 데이터를 세계 최초로 공개합니다. 이 데이터셋은 8B 파라미터 이내의 모델들로 생성된 이야기로, 각 우화는 전형적인 6개 요소(캐릭터, 특성, 배경, 갈등, 해결, 도덕)를 따르는 구조로 생성되었습니다. 이 데이터셋은 도덕적 스토리 생성과 관련해 새로운 연구방향을 열어줄 것입니다.
- ***Technical Details***: TF1-EN-3M은 각 요소별로 값 공간을 넓게 설정한 후, 조합적 확장을 통해 다양한 우화를 생성하는 템플릿 기반의 프롬프트 설계를 사용합니다. 평가에는 GPT 기반의 비평가 모델이 활용되어 문법, 창의성, 도덕 명확성 및 템플릿 준수도를 검토하며, 레퍼런스 없이 다양성과 가독성을 측정하는 메트릭도 사용됩니다. 여러 개방형 LLM 후보들 중 Llama-3.1-8B-Instruct 모델이 선택되어 고품질 우화를 지속적으로 생성하는 데 최적의 효율성을 보였습니다.
- ***Performance Highlights***: LLM 기반 평가 지표에서 Llama-3.1-Tulu-3-8B 모델은 창의성 및 도덕 명확성에서 높은 점수를 기록하며 최고의 성능을 보였습니다. 그러나 Llama-3.1-8B-Instruct는 문법과 템플릿 준수에서 가장 높은 점수를 받아 전반적인 우화 생성 모델로 가장 적합하다는 평가를 받았습니다. 성능 평가 지표 중에서 Llama-3.1-8B-Instruct는 Self-BLEU, Distinct-1, 가독성 모두에서 균형 잡힌 결과를 보여주었습니다.

### [LLMs for Engineering: Teaching Models to Design High Powered Rockets](https://arxiv.org/abs/2504.19394)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.19394.png)

Vote: 4

Authors: Toby Simonds

- ***What's New***: 이 연구는 대형 언어 모델(LLMs)이 소프트웨어 엔지니어링을 넘어 물리적 엔지니어링, 특히 고출력 로켓 설계에 어떻게 응용될 수 있는지를 탐색합니다. RocketBench라는 시뮬레이션 환경을 구축하여 LLMs가 실제 로켓 설계 작업을 통해 자신의 설계 능력을 평가할 수 있도록 하였으며, 강화학습(RL)을 활용하여 이러한 모델들이 인간 전문가와 비교하여 탁월한 성과를 보일 수 있음을 보여주었습니다.
- ***Technical Details***: RocketBench는 RocketPy라는 고정밀 궤적 시뮬레이션 도구를 사용하여 모델 로켓 설계를 평가합니다. 특정 고도 최적화 및 정밀 착륙이라는 두 가지 설계 과제를 통해 LLM의 성능을 테스트합니다. 이 연구에서는 LLMs가 기본적인 엔지니어링 지식을 바탕으로 효율적인 초기 설계를 생성할 수 있지만, 시뮬레이션 피드백을 통한 반복적 설계 개선에서는 제한적인 성과를 보인다는 것을 발견했습니다. 그러나 Qwen 2.5 7B 모델을 사용한 RL 트레이닝은 이러한 한계를 극복하고, 인간과 비교하여 우수한 성능을 나타냅니다.
- ***Performance Highlights***: 강화학습으로 훈련된 모델은 목표 고도 도전과 정밀 착륙 시험에서 인간 전문가의 성능을 뛰어넘었습니다. 구체적으로, 7B 매개변수 모델이 인간 전문가의 최고 점수 76.57을 넘어서며, 12미터 이내로 정확하게 착륙하는 성과를 이루었습니다. 이는 RL-trained LLMs이 복잡한 엔지니어링 최적화 문제 해결에 큰 잠재력을 가지고 있음을 보여줍니다.

### [AdaR1: From Long-CoT to Hybrid-CoT via Bi-Level Adaptive Reasoning Optimization](https://arxiv.org/abs/2504.21659)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.21659.png)

Vote: 3

Authors: Li Shen, Xiaochun Cao, Dacheng Tao, Haotian Luo, Rui Liu, Yibo Wang, Naiqiang Tan, Haiying He, Jinluan Yang

- ***What's New***: 이번 연구에서는 장기적인 Chain-of-Thought (CoT) 추론 모델의 효율성을 개선하기 위한 새로운 2단계 프레임워크를 제안합니다. 이 프레임워크는 다양한 추론 스타일을 생성할 수 있도록 장단기 CoT 모델을 결합한 혼합 추론 모델을 구축하고, Bi-Level Adaptive Reasoning Optimization을 통해 입력 문제의 복잡성에 따라 적절한 추론 스타일을 선택하게 합니다.
- ***Technical Details***: 제안된 방법은 두 단계로 구성됩니다. 첫째, Long-CoT 모델과 Short-CoT 모델을 병합하여 다양한 추론 경로를 생성할 수 있는 통합 추론 모델을 제공합니다. 둘째, Bi-Level Preference Training을 적용하여 그룹 수준에서 적절한 추론 스타일을 선택하고, 인스턴스 수준에서는 선택된 그룹 내에서 더 효율적인 추론을 하도록 모델을 최적화합니다. 이를 통해 모델은 문제의 복잡성에 따라 적절한 추론 스타일을 유연하게 선택할 수 있습니다.
- ***Performance Highlights***: 제안된 방법은 7B와 1.5B 모델 모두에서 장-단기 병합 모델에 비해 길이 감소 효과가 50.93% 및 43.28%에 달하면서 정확도 감소는 각각 1.65%와 1.21%로 최소화되었습니다. 이는 기존의 DPO나 O1-Pruner 등과 비교하여 뛰어난 효율성 향상을 보여줍니다.

### [MediAug: Exploring Visual Augmentation in Medical Imaging](https://arxiv.org/abs/2504.18983)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.18983.png)

Vote: 3

Authors: Lei Zhang, Canxuan Gang, Hao Zhang, Zhiwei Zhang, Xuyin Qi, Zeyu Zhang, Yang Zhao

- ***What's New***: MediAug는 의료 영상에서의 시각적 증강(Visual Augmentation)을 위한 포괄적 벤치마크를 제안하여, 의료 영상 분석에서 데이터 증강의 중요성에 대해 체계적이고 통합된 평가를 수행했습니다.
- ***Technical Details***: MediAug 프레임워크는 뇌종양 MRI와 안구 질환 망막 데이터셋에 대해 MixUp, YOCO, CropMix, CutMix, AugMix, SnapMix와 같은 6가지 혼합 기반 데이터 증강 기법을 ResNet-50 및 ViT-B 백본에서 평가하였습니다. 각 증강법은 데이터를 다양한 방식으로 변형하여 학습 효율성을 높이고 특정한 문제를 해결하는 데 중점을 둡니다.
- ***Performance Highlights***: 뇌종양 분류 작업에서 ResNet-50 기반의 MixUp은 79.19%의 정확도를 기록하며 성능 개선을 증명했으며, ViT-B 기반의 SnapMix는 99.44%의 정확도를 차지했습니다. 안구 질환 분류에서는 ResNet-50 기반의 YOCO가 91.60%, ViT-B 기반의 CutMix가 97.94%의 정확도를 보였으며, 이는 데이터 증강 기법과 모델 구조 간의 최적 조합이 모델 성능에 미치는 영향을 방증합니다.

### [Spatial Speech Translation: Translating Across Space With Binaural Hearables](https://arxiv.org/abs/2504.18715)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.18715.png)

Vote: 1

Authors: Runlin He, Shyam Gollakota, Tuochao Chen, Qirui Wang

- ***What's New***: Spatial Speech Translation은 청각 환경 내 다수의 화자를 실시간으로 번역하여 사용하는 착용형 기기에서 공간적 인식을 통합한 최초의 시스템입니다. 이 시스템은 화자의 방향과 고유한 음성 특징을 보존하여 번역된 이중채널(binaural) 출력을 제공합니다.
- ***Technical Details***: 이 시스템은 블라인드 소스 분리(Blind Source Separation), 위치 추정(Localization), 실시간 표현 번역(Expressive Translation), 이중채널 렌더링(Binaural Rendering)을 위한 복잡한 기술 문제를 해결하고 있습니다. Apple M2 실리콘에서 실시간 추론을 달성할 수 있습니다. 제안된 모델은 프랑스어, 독일어, 스페인어를 영어로 번역하며, 175M 파라미터의 번역 모델이 사용됩니다.
- ***Performance Highlights***: 실제 환경에서 수행한 사용자 연구에서, 번역 시스템은 기존 모델 대비 더 높은 의미적 일관성을 보여주었으며, 번역 전후의 사용자 지각 방향 인식 오류가 거의 없었습니다. BLEU 점수는 최대 22.01을 기록하였으며, 평균 지연 시간(AL)은 약 3초 수준으로 측정되었습니다. 모델 튜닝 후 비특수 모델에서 BLEU 점수가 약 4점 상승한 것도 확인할 수 있었습니다.

### [A Robust Deep Networks based Multi-Object MultiCamera Tracking System for City Scale Traffic](https://arxiv.org/abs/2505.00534)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.00534.png)

Vote: 1

Authors: Muhammad Imran Zaman, Gulshan Saleem, Rana Hammad Raza, Usama Ijaz Bajwa

- ***What's New***: 이 연구는 도시 규모의 교통 트래킹(tacking)을 위한 견고한 멀티 오브젝트 멀티카메라 트래킹 시스템(Multi-Object Multi-Camera Tracking)을 제안합니다. 제안된 프레임워크는 다양한 차량 속성, 가림, 조명 변화 등으로 인한 도전 과제를 해결하기 위해 Mask R-CNN을 사용합니다.
- ***Technical Details***: 제안된 MCT 시스템은 차량의 트래킹과 재식별(re-identification)을 강화하기 위해 Mask R-CNN과 Non-Maximum Suppression (NMS)을 결합하여 객체를 감지합니다. Deep SORT과 ResNet-152는 차량의 속성 추출 및 올바른 궤적 연결을 위한 백본 모델로 사용됩니다. 다양한 손실 함수(loss function)을 활용하여 가림, 조명 변화, 그림자 등의 문제를 극복합니다.
- ***Performance Highlights***: 제안된 프레임워크는 5th AI City Challenge의 데이터셋에서 IDF1 점수 0.8289와 정밀도(precision) 0.9026, 재현율(recall) 0.8527을 달성하며, 이는 복잡한 교통 트래킹 시나리오에서 견고하고 정확한 차량 트래킹을 수행하는 프레임워크의 효과성을 보여줍니다. ResNet-152와 Deep SORT를 기반으로 하는 모델은 IDF1 점수 0.8289로 가장 높은 성능을 기록했습니다.

