## Daily Papers (2025-03-25)

### [I Have Covered All the Bases Here: Interpreting Reasoning Features in Large Language Models via Sparse Autoencoders](https://arxiv.org/abs/2503.18878)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18878.png)

Vote: 82

Authors: Alexey Dontsov, Oleg Y. Rogov, Polina Druzhinina, Andrey Galichin, Ivan Oseledets, Anton Razzhigaev, Elena Tutubalina

- ***What's New***: 이 논문은 큰 언어 모델(Large Language Models; LLMs)이 이유하는 방식에 대한 메커니즘을 Sparse Autoencoders(SAEs)를 활용하여 설명하려는 시도를 최초로 제시하고 있습니다. SAEs를 통해 DeepSeek-R1 모델 시리즈의 이유 메커니즘을 규명함으로써, 이유의 성능을 체계적으로 향상시키는 기능을 찾아내고 이를 증명했습니다.
- ***Technical Details***: Sparse Autoencoder(SAE)는 신경 네트워크의 잠재 표현을 희박하게 해석 가능한 특징으로 분해하는 방법을 사용합니다. 제안된 접근법은 후보 '이유 특징'을 SAE 표현에서 추출하고, 그 특징들이 모델의 이유 능력과 직접적으로 상관관계가 있음을 검증합니다. 또한, 새로운 평가 척도인 ReasonScore를 통해 SAE 특징이 이유성과 관련된지를 자동으로 평가합니다.
- ***Performance Highlights***: 실험 결과, 감지된 특징의 활성화를 증폭시킬 때 모델 출력의 구조적 이유가 체계적으로 증가했음을 보여 주었습니다. 이유 관련 벤치마크에서, 특징을 증폭시키는 과정은 이유의 질과 성능을 향상시켜 여러 이유 문제 해결 과제에서 증명되었습니다.

### [Position: Interactive Generative Video as Next-Generation Game Engine](https://arxiv.org/abs/2503.17359)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.17359.png)

Vote: 52

Authors: Haoxuan Che, Jiwen Yu, Di Zhang, Pengfei Wan, Xintao Wang, Yiran Qin, Xihui Liu, Quande Liu

- ***What's New***: Interactive Generative Video(IGV)는 다음 세대 게임 엔진의 핵심 기술로 제안되어, 발전된 비디오 생성 모델을 이용해 무한한 새로운 콘텐츠를 생성할 수 있는 기회를 제공합니다. 이 기술은 사용자 제어 가능한 상호작용성, 장기 기억 능력, 인과 추론을 통해 물리적 인식 세계 모델링을 가능하게 하며, 미래의 게임 개발 방향을 새롭게 설정합니다.
- ***Technical Details***: Generative Game Engine(GGE)에 대한 포괄적인 프레임워크와 위계적인 성숙도 로드맵(L0-L4)을 제시하며, IGV의 상호작용 기능을 비디오 생성 기능에 확장합니다. GGE의 구현에 있어 IGV는 무한한 새로운 게임 콘텐츠 생성, 현실적인 상호작용을 위한 물리적 인식 세계 모델링, 그리고 상호작용 경험을 위한 사용자 제어 생성이라는 세 가지 주요 특성을 갖춥니다. IGV 중심의 GGE는 다섯 가지 핵심 모듈로 구성됩니다: 생성 모듈, 제어 모듈, 메모리 모듈, 동적 모듈, 지능 모듈이 비디오 인터페이스를 통해 독립된 가상 세계를 창출합니다.
- ***Performance Highlights***: 현재 비디오 생성 모델은 게임 개발에 사용될 수 있을 만큼 충분한 물리적 이해력을 갖추지 못하고 있음이 여러 연구에서 밝혀졌습니다. 하지만 물리적 상호작용이 풍부한 대규모 데이터셋을 통해 훈련함으로써 이러한 모델들이 기본적인 물리적 원리를 관찰을 통해 학습할 수 있음을 보여주는 연구가 수행되고 있습니다. 이는 향후 더 급진적인 성능의 혁신이 가능하도록 방향을 제시합니다.

### [Video-T1: Test-Time Scaling for Video Generation](https://arxiv.org/abs/2503.18942)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18942.png)

Vote: 50

Authors: Yimo Cai, Fangfu Liu, Kaiyan Zhang, Hanyang Wang, Yueqi Duan, Xiaohang Zhan

- ***What's New***: 비디오 생성의 새로운 접근 방식으로 '테스트 타임 스케일링(Test-Time Scaling; TTS)'을 도입한 Video-T1 프레임워크가 제안되었습니다. 이 기술은 비싼 재훈련이 필요 없이, 테스트 타임에서의 추가 연산을 통해 비디오의 생성 품질을 향상시키려는 새로운 시도를 보여줍니다.
- ***Technical Details***: Video-T1은 비디오 생성을 가우시안 노이즈에서 목표 비디오 분포로의 경로 탐색 문제로 재해석하며, 테스트 타임에서 계산을 증가하여 더 넓은 해 탐색 공간을 구축합니다. 두 가지 탐색 알고리즘인 '랜덤 선형 검색(Random Linear Search)'과 '프레임의 나무(Tree-of-Frames; ToF) 검색'을 활용하여, 멀티모달 모델을 통해 생성된 비디오의 품질을 평가하고 탐색 과정을 안내합니다.
- ***Performance Highlights***: 실험 결과, TTS를 통한 계산 시간 증가가 비디오 생성 품질을 일관되게 향상시키는 것으로 나타났습니다. 특히 ToF 검색을 통해 낮은 계산 비용으로 높은 샘플 다양성을 유지할 수 있었고, 다양한 비디오 생성 모델에서 성능이 크게 향상되었습니다.

### [A Comprehensive Survey on Long Context Language Modeling](https://arxiv.org/abs/2503.17407)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.17407.png)

Vote: 38

Authors: Yifan Song, Ruijie Zhu, Jiebin Zhang, Jian Yang, Shilong Li, Zhoujun Li, Huanxuan Liao, Yingshui Tan, Jiaheng Liu, Ziqiang Liu, Haoran Que, Yong Shan, Zhejian Zhou, Hangyu Guo, Tianyu Liu, Jiayi Tian, Yuanxing Zhang, Zhaoxiang Zhang, Wangchunshu Zhou, Zili Wang, Sujian Li, Wenhao Huang, Chenchen Zhang, Zhiqi Bai, Yancheng He, Wei Ye, Dawei Zhu, Bo Zheng, Zhuo Chen, Wenbo Su, Shizhu He, Wenhao Wu, Ge Zhang, Zekun Wang, Yang Gao, Fanyu Meng, Junlan Feng

- ***What's New***: 이 논문은 최근 대형 언어 모델(Large Language Models; LLMs)의 긴 컨텍스트(Language Context) 모델링을 위한 발전을 포괄적으로 조사합니다. 긴 컨텍스트 모델링의 효과적, 효율적 개발 방법, 훈련 및 배포 방법, 그리고 평가 및 분석 방법을 중심으로 구조화되어 있습니다.
- ***Technical Details***: 데이터 전략, 아키텍처 설계 및 워크플로우 디자인을 검토하여 긴 컨텍스트 대형 언어 모델(LLCMs)을 효과적이고 효율적으로 만들기 위한 방법을 제안합니다. 또한, LCLMs 훈련 및 추론 시 효율성을 높이기 위한 AI 인프라 최적화 전략을 제공하며, 긴 문맥 이해와 생성에 대한 평가를 수행합니다.
- ***Performance Highlights***: 여러 벤치마크를 통해 대형 언어 모델이 긴 텍스트를 처리하는 능력을 실험적으로 평가했습니다. 다양한 길이의 텍스트에 대한 perplexity의 감소는 모델이 더 많은 문맥 정보를 효과적으로 사용할 수 있음을 시사합니다. 하지만 지원하는 컨텍스트 길이와 실제 효과적인 길이 사이에는 여전히 의미 있는 갭이 존재합니다.

### [SimpleRL-Zoo: Investigating and Taming Zero Reinforcement Learning for Open Base Models in the Wild](https://arxiv.org/abs/2503.18892)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18892.png)

Vote: 21

Authors: Wei Liu, Qian Liu, Keqing He, Yuzhen Huang, Junxian He, Zejun Ma, Weihao Zeng

- ***What's New***: SimpleRL-Zoo는 Open Base Models를 대상으로 Zero Reinforcement Learning(RL)을 조사하여 다양한 기초 모델에서의 CoT 사고 및 자가 반성 행동의 자연 발현을 평가합니다. GRPO 알고리즘을 활용하여 다양한 설계 전략을 도입해 특정 기초 모델에서 이전에 관찰되지 않았던 'aha moment'를 소형 모델에서도 발견하였습니다.
- ***Technical Details***: 제시된 논문에서는 Llama3-8B, Mistral-7B/24B, DeepSeek-Math-7B 등 10개의 다양한 기초 모델에서 Zero RL 훈련을 수행하였습니다. 각 모델은 GSM8K와 MATH 데이터셋을 사용하여 규칙 기반 보상 모델링을 적용했습니다. GRPO 알고리즘과 주요 설계 요소를 결합하여 모든 기초 모델에서 정확도 향상과 응답 길이 증가를 달성했지만, 모델마다 훈련 중 다른 패턴을 보였습니다.
- ***Performance Highlights***: 테스트의 결과, Qwen-2.5-32B은 AIME24에서 pass@1 정확도가 10.0에서 36.7로 급증했습니다. DeepSeek-Math-7B의 경우, 초기 약 10.0의 성능이 불과 80번의 훈련 반복 후 세 배 이상 향상되었으며, 응답 길이는 약 300에서 1200 이상으로 성장했습니다.

### [Aether: Geometric-Aware Unified World Modeling](https://arxiv.org/abs/2503.18945)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18945.png)

Vote: 21

Authors: Zizun Li, Chunhua Shen, Aether Team, Tong He, Wenzheng Chang, Yang Zhou, Haoyi Zhu, Yifan Wang, Junyi Chen, Jianjun Zhou, Jiangmiao Pang

- ***What's New***: 이 논문은 AETHER라는 새로운 프레임워크를 제안하며, 이는 AI 시스템이 인간과 같은 공간 추론을 가능하게 하는 통합된 세계 모델링을 선보입니다. 다시 말해, AETHER는 4차원 동적 재구성(4D Dynamic Reconstruction), 동작 기반의 비디오 예측(Action-Conditioned Video Prediction), 목표 기반의 시각적 계획(Goal-Conditioned Visual Planning)의 세 가지 핵심 기능을 동시에 최적화하여 구현합니다.
- ***Technical Details***: AETHER는 사전에 학습된 비디오 생성 모델(Video Generation Models)을 활용하여, 합성된 4D 데이터로 후학습을 통해 정교한 성능을 발휘합니다. 여기에는 자동화된 4D 합성 데이터 라벨링 파이프라인이 포함되며, 깊이 비디오(Depth Video)와 카메라 이동 궤적(Camera Pose Trajectory)을 사용해 다양한 태스크에 대한 조건 설정이 가능합니다. 이 프레임워크는 서로 다른 입력 조건을 가진 다중태스크 생성 모델로 다중작업의 공동 최적화를 지원합니다.
- ***Performance Highlights***: AETHER는 이미지 및 비디오 생성 분야에서 선도적인 성능을 보여주었습니다. 특히, 깊이 예측(Depth Prediction) 및 카메라 위치 예측(Camera Pose Estimation)에서 최첨단(SOTA) 성능을 상회하거나 그와 동등한 성능을 발휘했습니다. 더불어, 동작 조건 기반 비디오 예측에서는 강력한 성능을 기록하며, 기존 모델들에 비해 더 정확한 예측을 수행합니다.

### [OmnimatteZero: Training-free Real-time Omnimatte with Pre-trained Video Diffusion Models](https://arxiv.org/abs/2503.18033)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18033.png)

Vote: 20

Authors: Nir Darshan, Dvir Samuel, Rami Ben-Ari, Gal Chechik, Matan Levy

- ***What's New***: OmnimatteZero는 사전 훈련된 비디오 확산 모델(Pre-trained Video Diffusion Models)을 활용하여 실시간으로 객체를 제거하고 추출하며, 새로운 배경에 결합하는 최초의 무훈련 Omnimatte 기법입니다. 기존 방법들과 달리 모델 훈련이나 최적화를 필요로 하지 않고, A100 GPU에서 프레임 당 0.04초의 속도로 효율성을 크게 향상시킵니다.
- ***Technical Details***: OmnimatteZero는 사전 훈련된 비디오 확산 모델을 활용하여 영상을 배경과 여러 전경 레이어로 분해하며, 각 전경 레이어에는 그림자와 반사와 같은 객체의 관련된 효과가 함께 포함됩니다. 이는 객체 제거를 위해 기존 이미지 인페인팅 기법을 비디오에 적응시키고, 셀프 어텐션(Salf-attention) 맵을 사용하여 객체와 그 흔적 정보(추적, 그림자, 반사)를 포착하여 이를 인페인팅합니다. 객체와 그 효과를 제거하고 객체 레이어를 추출하여 새로운 영상에 결합하기 위해 간단한 잠재 산술을 통해 객체 레이어를 분리하고 다시 결합할 수 있습니다.
- ***Performance Highlights***: OmnimatteZero는 PSNR과 LPIPS와 같은 모든 벤치마크에서 기존의 감독 학습 및 자가 감독 학습 기법을 능가하며, 백그라운드 재구축 및 객체 레이어 추출에 있어서 우수한 성능을 보입니다. 특히, LTXVideo 모델 기반으로 0.04초 프레임 시간 내 실시간 성능을 달성하여 가장 빠른 Omnimatte 기법의 기록을 세웠습니다.

### [Judge Anything: MLLM as a Judge Across Any Modality](https://arxiv.org/abs/2503.17489)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.17489.png)

Vote: 14

Authors: Yaochen Wang, Yao Wan, Yi Gui, Zetong Zhou, Qi Qin, Shu Pu, Dongping Chen, Guohao Wang, Zhongyi Zhang, Shuang Gong, Philip S. Yu, Zhiyuan Zhang, Yuhang Chen

- ***What's New***: 이 논문은 다양한 모달리티(이미지, 오디오, 비디오 등)를 이해하고 생성하는 임무에서 MLLM(Multimodal Large Language Models)을 자동 평가자로 사용하는 아이디어를 확장했습니다. 이를 위해 두 가지 벤치마크, TASKANYTHING과 JUDGEANYTHING을 소개하여 MLLM의 전체 성능과 평가 능력을 종합적으로 평가합니다.
- ***Technical Details***: TASKANYTHING 벤치마크는 15개의 다양한 모달리티 카테고리에서 연구하기 위해 1,500개의 질문으로 구성되어 있으며, JUDGEANYTHING은 5개의 최신 MLLM을 대상으로 평가 능력을 조사하고 인간 판정과 상세한 평가 기준을 포함한 표준화된 테스트베드를 제공합니다.
- ***Performance Highlights***: MLLMs는 Multimodal Understanding(MMU)에서 Pair Comparison 설정에서 평균 66.55%, Score Evaluation 설정에서 평균 42.79%를 달성했지만, Multimodal Generation(MMG) 작업에서는 Pair Comparison에서 평균 53.37%, Score Evaluation에서 평균 30.05%만 달성하여 교차 모달리티 편향성과 환각 문제에 직면하고 있음을 드러냈습니다.

### [Defeating Prompt Injections by Design](https://arxiv.org/abs/2503.18813)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18813.png)

Vote: 14

Authors: Tianqi Fan, Ilia Shumailov, Jamie Hayes, Florian Tramèr, Christoph Kern, Nicholas Carlini, Edoardo Debenedetti, Andreas Terzis, Daniel Fabian, Chongyang Shi

- ***What's New***: CaMeL(CApabilities for MachinE Learning)은 대규모 언어 모델(LLM; Large Language Model) 기반 시스템에서 발생할 수 있는 프롬프트 주입 공격(Prompt Injection Attacks)을 방어하기 위한 새로운 보안 프레임워크입니다. 모델 자체를 수정하지 않고, 시스템 레이어에서 보안 정책을 강제하여 시스템 전반의 안전성을 개선합니다.
- ***Technical Details***: CaMeL은 제어 흐름 무결성(Control Flow Integrity), 접근 제어(Access Control), 정보 흐름 제어(Information Flow Control) 같은 전통적인 소프트웨어 보안 개념을 차용하여 설계되었습니다. 사용자 쿼리로부터 제어 및 데이터 흐름을 추출하고, Python 인터프리터를 사용하여 보안 정책을 강제합니다. 또한, 'capabilities'를 통해 데이터 흐름을 관리 및 추적하며, 각 값에 대한 메타데이터를 정의하고 저장합니다.
- ***Performance Highlights***: CaMeL은 NeurIPS 2024의 에이전트 보안 벤치마크인 AgentDojo에서 67%의 작업을 보안적으로 해결하였습니다. 이는 기존 모델보다 상당히 향상된 보안을 제공하며, 유틸리티가 중요한 일부 영역에서는 성능 저하가 있으나, 전반적으로 시스템의 보안성을 크게 강화합니다. 특히 LLM의 본연의 행동 변경 없이 시스템 보안을 보장합니다.

### [FFN Fusion: Rethinking Sequential Computation in Large Language Models](https://arxiv.org/abs/2503.18908)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18908.png)

Vote: 13

Authors: Najeeb Nabwani, Ido Shahaf, Ido Galil, Akhiad Bercovich, Itay Levy, Tomer Ronen, Ehud Karpas, Yonatan Geifman, Itamar Schen, Izhak Golan, Ran Zilberstein, Zach Moshe, Omri Puny, Amnon Geifman, Mohammad Dabbah, Oren Tropp, Elad Segal, Ran El-Yaniv

- ***What's New***: FFN Fusion은 대형 언어 모델(Large Language Models; LLMs)에서 연속적인 계산을 병렬화하여 최적화하는 혁신적인 기술입니다. Feed-Forward Network(FFN) 레이어를 결합하여 순차적인 층을 병렬로 변환함으로써 추론 지연 시간을 줄이고, 모델 성능을 유지합니다. 이를 통해 Ultra-253B-Base 모델은 1.71배의 추론 속도 향상과 토큰당 비용을 35배 절감하는 동시에 높은 성능을 달성합니다.
- ***Technical Details***: FFN Fusion은 Attention 레이어 제거 후 남은 FFN 연속 레이어를 병합하여 병렬 처리를 가능하게 합니다. 이러한 병합은 여러 GPU에 걸쳐 실행할 수 있으며, 이는 특히 최신 GPU 노드에서 효과적입니다. Ultra-253B-Base 모델은 405B-파라미터 Llama-3.1-405B-Instruct 모델을 기반으로 attention pruning과 FFN Fusion을 결합하여 개발되었습니다. FFN Fusion은 FFN 레이어의 독립적인 계산 가능성을 활용하여 모델 기능을 유지하면서 하드웨어 효율성을 개선합니다.
- ***Performance Highlights***: Ultra-253B-Base는 Arena Hard에서 84.92%, HumanEval에서 86.58%, MMLU Instruct에서 87.54%의 성능을 기록했으며, 사용자의 대기 시간을 1.71배 단축하고 토큰 당 비용을 35배 낮췄습니다. FFN Fusion 기법은 특히 모델의 규모가 커질수록 더 효과적이며, 기존의 최적화 기술인 모형 압축(quantization) 및 가지치기(pruning)와 상호 보완되며 큰 성능 향상을 이끌어냅니다.

### [LEMMA: Learning from Errors for MatheMatical Advancement in LLMs](https://arxiv.org/abs/2503.17439)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.17439.png)

Vote: 13

Authors: Wei Wu, H. Vicky Zhao, Lijun Wu, Zinan Tang, Conghui He, Chenlin Ming, Honglin Lin, Qizhi Pei, Yu Li, Zhuoshi Pan

- ***What's New***: LEMMA는 대형 언어 모델(Large Language Models; LLMs)의 수학적 문제 해결 능력을 향상시키기 위한 새로운 접근 방법으로, 모델이 생성하는 오류 데이터를 학습에 활용하여 추론 능력을 증진합니다. LEMMA는 오류 유형을 기반으로 한 오류 증강 전략을 도입하여 다양한 오류 데이터셋을 구축하고, 잘못된 해답과 이를 수정한 정확한 해답을 연결하는 반영 데이터를 만들어 미세 조정합니다.
- ***Technical Details***: LEMMA는 LLMs가 생성한 오류 유형을 세분화하여 분석하고, 이를 기반으로 오류 유형에 따른 오류 증강 전략을 설계합니다. 대상 모델의 추론 흔적에서 오류를 추출하고, 발전된 모델을 사용해 대표적인 오류를 생성합니다. 오류가 포함된 해답은 'Fix & Continue'와 'Fresh & Restart'라는 두 가지 보완 기제를 통해 수정됩니다. 이를 통해 매끄러운 반영 연결을 통해 잘못된 해답이 정확한 해답으로 전환되며, 모델은 외부 평가 모델에 의존하지 않고도 오류를 자동으로 수정할 수 있습니다.
- ***Performance Highlights***: LEMMA는 수학적 추론 벤치마크(GSM8K, MATH 등)에서 실험 결과, 기존의 강력한 기준선 대비 최대 13.3%의 정확도 향상을 보여주었으며, 특히 분포 외 벤치마크에서 뛰어난 일반화 능력을 입증하였습니다. LEMMA를 통해 훈련된 모델은 일관되게 대표적인 오류 유형의 발생을 감소시켰으며, 이는 구조적인 오류 학습이 수학적 추론을 강화하는 강력한 도구임을 시사합니다.

### [AgentRxiv: Towards Collaborative Autonomous Research](https://arxiv.org/abs/2503.18102)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18102.png)

Vote: 13

Authors: Samuel Schmidgall, Michael Moor

- ***What's New***: AgentRxiv는 자율적인 연구 에이전트들이 연구를 생성하고 공유하며 기존 연구를 기반으로 구축할 수 있는 자율 연구 협업 프레임워크를 소개합니다. 에이전트가 이전 연구를 활용할 수 있게 하여, 독립적으로 운영되는 에이전트보다 성능이 향상되는 것을 관찰했습니다.
- ***Technical Details***: AgentRxiv는 자율 연구 에이전트를 위해 중앙 집중식, 오픈 소스 프리프린트 서버로 구현되었으며 에이전트 사이의 연구 결과 시스템적 공유를 가능하게 합니다. 에이전트는 'Simultaneous Divergence Averaging (SDA)'와 같은 새로운 추론 기법을 연속적 연구 과정을 통해 발견했습니다.
- ***Performance Highlights***: MATH-500 벤치마크에서의 정확도는 처음 70.2%에서 78.2%로 지속적인 개선을 보였습니다. 각 연구 세대의 성과는 이전 연구를 참조하며 기하급수적으로 발전했으며, SDA 알고리즘은 세 가지 다양한 벤치마크에서 평균 9.3%의 성과 향상을 나타냈습니다.

### [Vision-R1: Evolving Human-Free Alignment in Large Vision-Language Models via Vision-Guided Reinforcement Learning](https://arxiv.org/abs/2503.18013)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18013.png)

Vote: 12

Authors: Jinqiao Wang, Shurong Zheng, Yufei Zhan, Ming Tang, Fan Yang, Yousong Zhu, Hongyin Zhao

- ***What's New***: Vision-R1은 대형 비전-언어 모델(Large Vision-Language Models; LVLMs)의 정렬을 시각 유도 강화 학습(Vision-Guided Reinforcement Learning) 방식을 통해 인간의 주석 없이 개선할 수 있는 새로운 접근법을 제안합니다. 이 방법은 수작업 선호 데이터셋 없이 큐레이션된 지시 데이터를 활용하여 시각 피드백을 통한 보상을 제공하여 모델의 임무 이해력을 높입니다.
- ***Technical Details***: Vision-R1은 전통적인 보상 모델 없이 시각적 피드백을 통한 절대적인 보상을 제공하기 위해, 시각 기준에 기반한 보상 함수(Criteria-Driven Reward Function)를 도입했습니다. 이를 통해 모델이 작업의 특성을 더 깊이 이해하고, 더 정확한 응답을 생성하도록 합니다. 또한, 학습 도중 계속해서 보상 기준을 동적으로 조정하는 '점진적 규칙 정제 전략(Progressive Rule Refinement Strategy)'을 도입하여 보상 해킹을 방지하고 모델의 지속적인 개선을 촉진합니다.
- ***Performance Highlights***: Vision-R1은 다양한 도메인에서 LVLM의 성능을 최대 50% 향상시켰으며, 특히 Qwen2.5-VL-72B 모델을 능가하는 성능을 보였습니다. SFT(Supervised Fine-Tuning)와 비교했을 때, 미보는 시나리오에서 평균적으로 6% 개선된 성능을 보였으며, QA(질문 응답) 능력에서도 유지가 되었습니다. 이러한 성과는 Vision-R1이 특정 작업이나 실제 응용에서 LVLM을 강화하는 데 효과적임을 보여줍니다.

### [CFG-Zero*: Improved Classifier-Free Guidance for Flow Matching Models](https://arxiv.org/abs/2503.18886)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18886.png)

Vote: 12

Authors: Ziwei Liu, Amber Yijia Zheng, Raymond A. Yeh, Weichen Fan

- ***What's New***: 이 논문에서는 CFG-Zero⋆라는 새로운 기법을 제안하여 기존의 Classifier-Free Guidance (CFG) 기법을 개선했습니다. 이를 통해 플로우 매칭(Flow Matching) 모델에서 이미지 생성과 제어 가능성을 획기적으로 향상시켰습니다. 특히, 초기 학습 단계에서 CFG가 잘못된 방향으로 샘플을 유도하는 문제점을 발견하고 이를 보완한 것입니다.
- ***Technical Details***: CFG-Zero⋆는 두 가지 주요 개선점을 포함합니다: (a) 학습된 속도의 부정확성을 보정하기 위해 최적화된 스케일을 도입, (b) ODE(Oordinary Differential Equation) 풀이기의 첫 몇 스텝을 0으로 초기화합니다. 이러한 수정 사항은 기존의 CFG 코드 기반에 쉽게 통합할 수 있으며 추가적인 계산 비용을 거의 발생시키지 않습니다.
- ***Performance Highlights***: 실험 결과에 따르면, CFG-Zero⋆는 기존의 CFG보다 텍스트-이미지와 텍스트-비디오 생성에서 더 우수한 성능을 보였습니다. 특히, ImageNet에서의 계층 조건 생성 실험에서 일관된 성능 향상을 확인할 수 있었으며, CFG-Zero⋆는 미려한 이미지 품질과 더 나은 텍스트 정렬을 달성하였습니다.

### [Training-free Diffusion Acceleration with Bottleneck Sampling](https://arxiv.org/abs/2503.18940)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18940.png)

Vote: 11

Authors: Yunhai Tong, Xuefeng Xiao, Ling Yang, Shanchuan Lin, Xing Wang, Ye Tian, Xin Xia, Bin Cui, Yuxi Ren

- ***What's New***: 이 논문은 Bottleneck Sampling이라는 새로운 프레임워크를 소개합니다. 이 프레임워크는 낮은 해상도의 프리트레인된(prior-trained) 요소를 활용하여 훈련 없이도 고해상도 생성의 계산 부하를 줄이도록 설계되었습니다. 고-저-고(high-low-high) 노이즈 제거(workflow) 과정을 통해 성능을 보존하면서도 컴퓨팅 오버헤드를 줄일 수 있습니다.
- ***Technical Details***: Bottleneck Sampling은 초기와 마지막 단계에서 고해상도 노이즈 제거를 수행하고 중간 단계에서는 낮은 해상도로 작동합니다. 이를 통해서 효율성을 높이고 아티팩트 문제를 줄입니다. 각 단계에서 해상도 전환 지점을 최적화하고 노이즈 제거 타임스텝을 적응적으로 조정함으로써, aliasing과 블러 아티팩트를 최소화합니다. 이 프레임워크는 텍스트 이미지 및 비디오 생성 시스템에 적용되어 유연성을 입증했습니다.
- ***Performance Highlights***: Bottleneck Sampling은 이미지 생성에서는 최대 3배, 비디오 생성에서는 최대 2.5배의 속도 향상을 이루는 한편, 여러 평가 지표에서 표준 고해상도 샘플링 프로세스와 동등한 수준의 출력 품질을 유지합니다. 이 방법은 기존의 아키텍처 수정이나 재훈련 없이 적용 가능하여, 효율성과 품질의 균형을 성공적으로 이룹니다.

### [Equivariant Image Modeling](https://arxiv.org/abs/2503.18948)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18948.png)

Vote: 11

Authors: Li Li, Ruixiao Dong, Zigang Geng, Shuyang Gu, Han Hu, Mengde Xu

- ***What's New***: 이 논문은 자연 시각 신호의 평행이동 불변성을 활용하여 생성 모델의 서브태스크를 최적화하는 새로운 등변 이미지 모델링(Equivariant Image Modeling) 프레임워크를 제안합니다. 컬럼 기반의 토큰화와 창문형 인과적 주의(windowed causal attention)를 도입하여 수평축에서의 변환 대칭성을 강화합니다.
- ***Technical Details***: 이 방법은 컬럼 기반의 토큰화를 통해 전통적인 2D 패치 그리드를 대체하고, 자연 이미지의 통계적 특징을 더욱 잘 보존합니다. 또한, 인과적 주의 메커니즘을 창문형으로 조정하여 위치 간의 일관된 컨텍스트 관계를 부여합니다. 이를 통해 높은 차원의 데이터 분포를 간단한 서브태스크로 분해하여 모델의 학습 효율성을 높입니다.
- ***Performance Highlights***: 256×256 해상도에서 클래스 조건부 ImageNet 생성 작업을 평가한 결과, 우리 접근 방법은 최신 AR 모델과 비교하여 적은 컴퓨팅 자원을 사용하면서도 유사한 성능을 나타냈습니다. 체계적인 분석을 통해 향상된 등변성이 서브태스크 간의 갈등을 줄여 제로샷 일반화 성능을 크게 개선하는 것을 확인했습니다. 이는 특히 무한히 긴 이미지 합성에 유리합니다.

### [Feather-SQL: A Lightweight NL2SQL Framework with Dual-Model Collaboration Paradigm for Small Language Models](https://arxiv.org/abs/2503.17811)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.17811.png)

Vote: 10

Authors: Wenqi Pei, Hengyuan Zhao, Pingyi Luo, Han Chen, Hailing Xu, Zining Zhang, Shizheng Hou, Bingsheng He

- ***What's New***: Feather-SQL은 작은 언어 모델(Small Language Models; SLMs)의 자연어-쿼리 변환(NL2SQL) 성능을 향상시키기 위한 경량 프레임워크로, SLMs의 SQL 실행 가능성과 정확성을 높이도록 설계되었습니다. 이는 강력한 범용 대화 모델과 세밀하게 조정된 SQL 전문가의 협력 패러다임을 도입하여 강력한 분석적 추론과 높은 정밀도의 SQL 생성을 결합합니다.
- ***Technical Details***: Feather-SQL은 스키마 가지치기(Schema Pruning)와 연결(Schema Linking), 복수 경로(Multi-path) 및 복수 후보(Multi-candidate) 생성, 그리고 수정 및 선택 모듈로 구성되어 있습니다. 스키마 가지치기는 불필요한 테이블을 제거하여 필수 데이터베이스 요소에 집중하게 하며, 다중 경로 및 후보 생성은 생성된 SQL 쿼리의 실행 가능성과 정확성을 개선하기 위해 다양한 쿼리 생성 전략을 탐색합니다. 또한, 1+1 모델 협업 패러다임을 소개하여 일반 대화 모델이 연결과 후보 선택을, SQL 전문가가 쿼리 생성을 처리하도록 하여 나쁜 기계 학습(catastrophic forgetting)을 방지합니다.
- ***Performance Highlights***: Feather-SQL은 BIRD와 Spider 데이터셋에서 NL2SQL 성능을 약 10%까지 향상시키며, 파인튜닝 없이도 SLM의 정확성 한계를 54.76%로 높였습니다. 또한, 다양한 SLMs 모델과 함께 사용될 때 일관된 최상위 성능(State-of-the-Art)을 보여주며, 이 협업 패러다임은 전반적인 정확성을 지속적으로 향상시킵니다.

### [Video SimpleQA: Towards Factuality Evaluation in Large Video Language Models](https://arxiv.org/abs/2503.18923)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18923.png)

Vote: 10

Authors: Jiahua Dong, Ian Reid, Wangbo Yu, Ge Zhang, Yingyao Wang, Pengfei Hu, Haoze Zhao, Jihao Gu, Xiaodan Liang, Meng Cao, Haoran Tang

- ***What's New***: Video SimpleQA는 대형 비디오 언어 모델(Large Video Language Models; LVLMs)의 사실 기반 평가를 위한 최초의 종합 벤치마크입니다. 이 벤치마크는 비디오 자체의 내러티브를 넘어 외부 지식을 결합하여 사실 확인형 질문과 정의적이고 간결한 답변을 통해 비주관적인 해석을 배제하며, 외부 출처를 통해 검증된 모든 주석이 포함됩니다.
- ***Technical Details***: Video SimpleQA 벤치마크는 짧은 질문과 짧은 형식의 분명한 답변으로 구성되며, 이러한 답변은 사실적인 근거 원칙에 철저히 따라야 합니다. 이 벤치마크는 고정된 단일 프레임 이해와 동적 시간적 추론을 모두 포괄하는 주석 질문 유형을 포함하여 LVLMs의 장기적 종속성을 명시적으로 평가합니다. 또한, 이 벤치마크는 총 41개의 최첨단 LVLMs를 평가합니다.
- ***Performance Highlights***: 현재 LVLMs는 특히 오픈소스 모델에서 사실 준수에 눈에 띄는 결함을 보이며, 성능의 점진적 향상에 대한 제약을 드러냈습니다. 가장 우수한 성능을 보인 모델인 Gemini-1.5-Pro가 F-score 54.4%를 기록했으며, 이는 인간 전문가와 비교할 때 여전히 많은 개선이 필요함을 시사합니다. 테스트 시점에서의 계산 패러다임은 성능 향상에 있어 미미한 이익을 얻었고, RAG(Retrieval-Augmented Generation)는 추가적인 추론 시간이 소모되지만 일관된 성능 개선을 보여주었습니다.

### [MagicComp: Training-free Dual-Phase Refinement for Compositional Video Generation](https://arxiv.org/abs/2503.14428)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.14428.png)

Vote: 7

Authors: Peng Jin, Yian Zhao, Shenghai Yuan, Yufan Deng, Jie Chen, Chang Liu, Zesen Cheng, Hongyu Zhang

- ***What's New***: MagicComp는 텍스트-비디오 변환(T2V) 생성에서 복합적인 요소의 효과적 처리를 위해 새로운 교육없는(taining-free) 방법을 제안합니다. 이 방법은 주제 간 의미 혼란을 분해하고, SAD(Semantic Anchor Disambiguation)와 DLFA(Dynamic Layout Fusion Attention)를 활용하여 시간적·공간적 견고성을 확보합니다.
- ***Technical Details***: MagicComp는 두 단계의 세분화 전략을 사용합니다. 첫 번째는 SAD 모듈로, 주제 간 의미 혼란을 줄이고, 각 주제에 독립적 인코딩을 통해 방향 벡터를 문맥에 적용합니다. 두 번째는 DLFA 모듈로, LLM 기반의 레이아웃과 모델 인식 레이아웃을 결합하여 더 유연한 주의력 조작을 실현합니다. 이렇게 함으로써 문맥적 일관성을 유지하면서 주제 속성을 세밀하게 묶을 수 있습니다.
- ***Performance Highlights***: MagicComp는 T2V-CompBench와 VBench에서 기존의 최첨단 컴포지션 방법을 뛰어넘는 성능을 보여주며, 특히 속성 지속성, 동작, 수치적 이해 및 다중 객체 처리에서 우수한 점수를 기록했습니다. SAD와 DLFA 모듈을 결합함으로써 시각적 일관성과 문맥 정확성이 강화되었습니다.

### [Typed-RAG: Type-aware Multi-Aspect Decomposition for Non-Factoid Question Answering](https://arxiv.org/abs/2503.15879)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.15879.png)

Vote: 6

Authors: DongGeon Lee, Hyeonseo Nam, Hyeri Lee, Ahjeong Park, Yunho Maeng

- ***What's New***: Typed-RAG은 비사실적 질의응답(Non-Factoid Question Answering; NFQA) 분야에서 타입 인식 다중 측면 분해(type-aware multi-aspect decomposition) 프레임워크를 도입한 연구입니다. NFQA의 다양한 의도와 응답 관점의 복잡성을 해결하기 위해 각 NFQ 유형에 따라 질의를 분류하고 RAG 파이프라인에 통합하여 답변 생성 전략을 최적화합니다.
- ***Technical Details***: Typed-RAG 프레임워크는 사전 훈련된 분류기를 통해 NFQs를 분류하고 이에 따라 검색 및 생성 전략을 최적화합니다. 여러 측면의 NFQs를 단일 측면의 하위 질의로 분할하고, 결과를 모아 보다 정보가 풍부하며 맥락적으로 관련성 높은 응답을 생성합니다.
- ***Performance Highlights***: 실험 결과 Typed-RAG은 Wiki-NFQA 데이터셋에서 기존의 LLMs와 RAG 기반 모델들을 능가하는 성능을 보여줍니다. 특히 복잡한 NFQ의 측면들을 효과적으로 캡처하고, 사용자와의 의도를 잘 반영한 세밀하고 맥락적인 응답을 생성하는 데 있어서 그 중요성을 강조합니다.

### [AlphaSpace: Enabling Robotic Actions through Semantic Tokenization and Symbolic Reasoning](https://arxiv.org/abs/2503.18769)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18769.png)

Vote: 6

Authors: Alan Dao, Dinh Bach Vu, Bui Quang Huy

- ***What's New***: AlphaSpace는 'Semantic Tokenization'과 'Symbolic Reasoning'을 활용하여 언어 모델의 3D 공간에서 로봇 조작 능력을 강화하는 혁신적인 방법론을 제시합니다. 이를 통해 LLMs가 전통적인 시각 기반 임베딩 없이 특정 [x, y, z] 좌표에 정확히 물체를 조작할 수 있습니다.
- ***Technical Details***: AlphaSpace는 계층적 의미 기반 'Tokenization' 전략을 사용하여 공간 정보를 거칠고 세밀한 수준에서 인코딩합니다. 객체의 속성, 위치, 높이 정보를 구조화된 토큰으로 표현하여 LLMs가 3D 공간에서 객체를 조작하게 합니다. 'Supervised Fine-Tuning(SFT)'과 'Group Relative Policy Optimization(GRPO)'의 두 단계 학습 프레임워크를 채택하여 객체 조작을 위한 명령어를 예측합니다.
- ***Performance Highlights***: 실험 결과 AlphaSpace는 객체 조작 부문에서 66.67%의 정확도를 달성하며, 기존 모델인 GPT-4o(37.5%)와 Claude 3.5 Sonnet(29.17%)를 능가합니다. 이는 AlphaSpace의 3D 공간 추론 능력이 매우 우수함을 나타냅니다.

### [Reasoning to Learn from Latent Thoughts](https://arxiv.org/abs/2503.18866)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18866.png)

Vote: 6

Authors: Chris J. Maddison, Neil Band, Yangjun Ruan, Tatsunori Hashimoto

- ***What's New***: 이 논문은 '잠재적 사고'를 통한 학습 방법을 제안하여 데이터가 제한적인 환경에서도 언어 모델(LM)의 프리트레이닝 효율성을 향상시키는 방법을 설명합니다. 이는 웹 텍스트를 인간의 복잡한 사고 과정을 압축하여 표현된 것으로 보고, 이러한 잠재적 사고가 더 높은 효율성을 가져올 수 있음을 제안합니다.
- ***Technical Details***: 저자는 '잠재적 사고 모델'을 훈련하여 관찰된 데이터에 숨겨진 인간의 생각을 해독하고, 이를 통해 데이터에서 학습하는 방식을 설명합니다. 이를 위해 저자는 잠재 변수 모델(latent variable model) 접근 방식을 사용하여 p(Z, X) 형태의 합동 분포를 모델링하고, 전통적인 EM 알고리즘을 통해 '잠재적 사고 부트스트래핑'(BoLT)이라는 방법을 소개합니다.
- ***Performance Highlights***: 저자는 BoLT가 여러 EM 반복을 통해 기존 성능을 지속적으로 개선함을 보였습니다. 특히, 데이터 효율성을 높이는데 있어 추가적인 추론 연산을 활용할 때 관측 가능한 성능 향상을 이뤄냈습니다. 실험은 사전 훈련된 TinyLlama 모델을 수학 문제에서 지속적으로 강화 학습하여 실현되었습니다.

### [V-Seek: Accelerating LLM Reasoning on Open-hardware Server-class RISC-V Platforms](https://arxiv.org/abs/2503.17422)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.17422.png)

Vote: 5

Authors: Mohamed Amine Ahmdi, Javier J. Poveda Rodrigo, Alessio Burrello, Daniele Jahier Pagliari, Luca Benini

- ***What's New***: V-Seek는 오픈 하드웨어 서버급 RISC-V 플랫폼에서 대규모 언어 모델(LLMs; Large Language Models)의 추론 속도를 향상시키기 위해 설계된 최적화 기법을 소개합니다. 이는 특히 Sophon SG2042라는 첫 상용 다중 코어 RISC-V CPU를 최적화하여 LLM 추론을 가속화하는 데 초점을 맞추고 있습니다.
- ***Technical Details***: 이 연구는 Sophon SG2042 플랫폼에서 LLM 추론을 최적화하기 위해 세 가지 방법을 사용했습니다: i) 하드웨어의 메모리 인프라와 벡터화 기능을 최대한 활용하여 주요 LLM 레이어에 최적화 및 양자화된 커널을 개발, ii) Sophon SG2042의 벡터 유닛을 지원하는 Xuantie 포크의 GCC 10.4를 활용하여 적합한 컴파일 도구체인을 선택, iii) 비균일 메모리 접근(NUMA; Non-uniform Memory Access) 정책을 최적화하여 시스템의 메모리 계층을 효과적으로 활용.
- ***Performance Highlights***: 최적화 후, DeepSeek R1 Distill Llama 8B 모델은 토큰 생성 속도 4.32 tok/s, 사전 처리(prefill) 시 6.54 tok/s를 달성했으며, 이는 기준 모델 대비 각각 3배, 2.9배의 속도 향상입니다. 또한, x86 플랫폼 대비 에너지 효율성이 1.2배 향상되었습니다.

### [Lost in Cultural Translation: Do LLMs Struggle with Math Across Cultural Contexts?](https://arxiv.org/abs/2503.18018)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18018.png)

Vote: 4

Authors: Jaswinder Singh, Abdul Sattar, Bhoomika Lohana, Matt Keon, Aabid Karim, Abdul Karim

- ***What's New***: 이 연구는 대형 언어 모델(LLMs; Large Language Models)이 문화적으로 적응된 수학 문제를 해결할 때 그들의 수학적 추론 능력이 어떻게 변화하는지를 평가합니다. 이를 위해 저자들은 GSM8K라는 광범위하게 사용되는 기준 데이터셋을 다양한 문화적 요소를 반영한 여섯 개의 인공 데이터셋으로 변형하여, LLMs의 수학적 추론이 문화적 맥락에 따라 어떻게 영향을 받는지를 탐구했습니다.
- ***Technical Details***: 원본 GSM8K 데이터셋의 수학적 논리와 수치를 유지하면서 개인 이름, 음식, 장소 이름 등의 문화적 요소를 수정하여 여섯 개의 문화적으로 적응된 데이터셋을 생성했습니다. 각각의 데이터셋은 해당 문화에서 대표적인 요소를 반영하도록 설계되었으며, 이를 통해 다양한 문화적 맥락에서 LLMs의 성능을 평가할 수 있습니다. 이 데이터셋과 결과 재현을 위한 스크립트는 https://github.com/akarim23131/Lost_in_Cultural_Translation에서 제공됩니다.
- ***Performance Highlights***: 실험 결과, LLMs는 문화적 참조가 변경되면 수학 문제 해결에 어려움을 겪는 것으로 나타났습니다. 특히, 작은 모델들이 큰 모델들보다 더 큰 성능 저하를 경험했습니다. 흥미롭게도, 수학적 훈련이 명시적으로 이루어지지 않은 모델도 관련 문화적 맥락에 노출되면 더 큰 수학적으로 숙련된 모델보다 더 나은 성능을 보이기도 했습니다. 이는 LLMs의 수학적 추론 능력이 훈련 데이터의 문화적 다양성에 크게 영향을 받는다는 것을 시사합니다.

### [AMD-Hummingbird: Towards an Efficient Text-to-Video Model](https://arxiv.org/abs/2503.18559)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18559.png)

Vote: 4

Authors: Emad Barsoum, Dong Li, He Cui, Mengmeng Ge, Takashi Isobe, Dong Zhou

- ***What's New***: AMD-Hummingbird는 텍스트-비디오 생성(Text-to-Video; T2V) 모델의 효율성을 향상시키기 위해 설계된 경량 텍스트-비디오 프레임워크입니다. 기존 모델을 가지치기(pruning)하여 시각적 피드백 학습을 통해 시각적 품질을 개선합니다. U-Net의 파라미터를 1.4억에서 0.7억으로 줄여 효율성을 크게 개선하면서도 고품질 비디오 생성을 유지합니다. VBench에서 가장 높은 점수를 기록하였으며, VideoCrafter2와 같은 최신 모델에 비해 31배의 속도 향상을 이루었습니다.
- ***Technical Details***: Hummingbird는 두 단계의 확산 모델 증류 파이프라인을 소개합니다. 첫 번째 단계에서는 모델 파라미터를 줄이고, 두 번째 단계에서는 시각적 피드백 학습을 통해 시각적 품질을 회복합니다. LLMs를 활용한 데이터 처리 파이프라인을 도입하여 텍스트 프롬프트를 재구성하고, 비디오 품질 평가(VQA) 모델을 사용해 훈련 데이터의 품질을 향상시킵니다. 각 단계는 AMD Instinct MI250 가속기에서 훈련되며, AMD의 최신 하드웨어 및 소프트웨어 기술을 활용합니다.
- ***Performance Highlights***: AMD-Hummingbird는 VideoCrafter2에 비해 약 31배의 속도 향상을 달성했으며, VBench의 전반적인 점수에서 최고점을 기록했습니다. 또한, 최대 26프레임의 영상을 생성할 수 있어 기존 U-Net 기반 방법의 긴 비디오 생성 한계를 해결합니다. 이를 통해 Hummingbird는 실세계 응용에 적합한 고성능, 확장 가능성 및 유연성을 제공합니다.

### [Variance Control via Weight Rescaling in LLM Pre-training](https://arxiv.org/abs/2503.17500)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.17500.png)

Vote: 4

Authors: Louis Owen, Abhay Kumar, Nilabhra Roy Chowdhury, Fabian Güra

- ***What's New***: 이 논문에서는 대형 언어 모델(LLM) 사전 학습 시 가중치의 초기화 및 분산 제어의 중요성을 강조하며, Layer Index Rescaling(LIR)과 Target Variance Rescaling(TVR)이라는 새로운 기법을 도입했습니다. 이를 통해 모델의 하위 작업 성능을 최대 4.6% 개선하고, 극단적인 활성화 값을 줄여 양자화 및 저정밀 학습의 문제를 완화할 수 있음을 밝혔습니다.
- ***Technical Details***: LIR 가중치 초기화는 층의 인덱스에 따라 가중치를 조정하여, 훈련 과정 중 가중치의 초기 분포에서의 변동을 방지합니다. 반면 TVR은 학습 중 원하는 표준 편차 값을 목표로 가중치를 조정하여, 가중치의 무한한 성장을 방지하고 학습의 안정성을 확보합니다. 이러한 기법은 1B 파라미터 LLaMA 모델의 시스템적 튜닝 및 소거 실험을 통해 검증되었습니다.
- ***Performance Highlights***: 제안된 기법들(LIR과 TVR)을 활용한 실험에서 나쁜 극단 활성화 값을 피하며, 훈련 중 가중치 표준 편차를 효과적으로 관리하여 하위 테스트 성능을 기존 방법보다 4.6%까지 개선할 수 있음을 확인했습니다. 이는 현재 LLM의 중요한 과제 중 하나인 안정적인 학습 역학을 보장하는 데 많은 기여를 합니다.

### [Diffusion-4K: Ultra-High-Resolution Image Synthesis with Latent Diffusion Models](https://arxiv.org/abs/2503.18352)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18352.png)

Vote: 3

Authors: Xiefan Guo, Qiuyu Huang, Junjie Liu, Di Huang, Jinjin Zhang

- ***What's New***: Diffusion-4K는 초고해상도 이미지 생성에 대한 새로운 접근 방식을 제시하며, 인간 인식에 기반한 Aesthetic-4K 벤치마크를 최초로 개발했습니다. 또한 Wavelet 기반의 미세 조정 방법을 제안하여 다양한 잠재 확산 모델에 대해 사실적인 4K 이미지 생성 능력을 강화합니다.
- ***Technical Details***: Aesthetic-4K 벤치마크는 4K 이미지 합성을 위한 데이터셋으로, GPT-4o가 생성한 세밀한 텍스트 캡션과 고품질 이미지로 구성되어 있습니다. 이 모델에서는 고주파 구성 요소를 강조하는 Wavelet 기반의 미세 조정(Wavelet-based Fine-tuning) 기술을 활용하여, 기존 잠재 확산 모델(latent diffusion model)의 성능을 향상시켰습니다. 각종 평가 메트릭에는 FID, Aesthetics, CLIPScore 외에도 GLCM 점수와 압축 비율이 포함되어 있어 텍스처와 세부 사항을 평가할 수 있습니다.
- ***Performance Highlights***: Diffusion-4K는 텍스트 프롬프트 준수 및 고품질 이미지 생성에서 우수한 성능을 나타내었으며, 특히 SD3-2B 및 Flux-12B와 같은 최신 대규모 확산 모델에서 4K 해상도 이미지를 처리하는 데 효과적입니다. 실험 결과, 사람과 AI 모두에서 향상된 시각적 미학과 세부 사항 표현을 통해 Diffusion-4K가 PixArt-Σ 및 Sana에 비해 우월한 결과를 보였습니다.

### [RDTF: Resource-efficient Dual-mask Training Framework for Multi-frame Animated Sticker Generation](https://arxiv.org/abs/2503.17735)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.17735.png)

Vote: 2

Authors: Zhiqiang Yuan, Yeshuang Zhu, Ying Deng, Jie Zhou, Jinchao Zhang, Zexi Jia, Ting Zhang, Jiapei Zhang

- ***What's New***: RDTF(Resource-efficient Dual-mask Training Framework)은 애니메이션 스티커 생성(Multi-frame Animated Sticker Generation)을 위한 새로운 자원 효율적 기법입니다. 제한된 자원 하에서 대규모 모델 대신 소규모 모델을 처음부터 훈련하여 더 나은 성능을 낳을 수 있음을 보여줍니다.
- ***Technical Details***: RDTF는 두 가지 마스크(Dual-mask) 기반 데이터 활용 전략을 통해 제한된 데이터의 가용성을 높이고 다양성을 확장합니다. 또한, 난이도 적응형 커리큘럼 학습(Difficulty-adaptive Curriculum Learning)을 도입하여 점진적으로 어려운 샘플을 학습하게 함으로써 모델 수렴을 촉진합니다.
- ***Performance Highlights***: RDTF는 SimDA나 I2V-Adapter 같은 효율적 파라미터 튜닝 방법과 비교하여 우수한 정량적 및 정성적 성능을 보여줍니다. ASG(Animated Sticker Generation) 작업에서 가장 우수한 결과를 얻었습니다. 특히 제한된 자원 환경에서도 탁월한 성능을 발휘했습니다.

### [MetaSpatial: Reinforcing 3D Spatial Reasoning in VLMs for the Metaverse](https://arxiv.org/abs/2503.18470)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18470.png)

Vote: 2

Authors: Han Liu, Zhenyu Pan

- ***What's New***: MetaSpatial는 강화학습(RL) 기반의 새로운 프레임워크로, 시각-언어 모델(VLMs)의 3D 공간 추론 능력을 향상시킵니다. 이 프레임워크는 메타버스를 위한 실시간 3D 장면 생성을 가능하게 합니다. 강화학습을 활용하여 보다 물리적으로 일관되고 미적으로 지속적인 공간 레이아웃을 생성할 수 있는 새로운 방법을 제안합니다.
- ***Technical Details***: MetaSpatial은 멀티턴 강화학습 최적화 메커니즘을 도입하여, 물리적 제약과 렌더링 된 이미지 평가를 통합합니다. GRPO(Group Relative Policy Optimization)를 사용하여 그룹화된 정책을 최적화하여, 다양한 세분화 결과로부터 VLM이 더 깊은 공간 추론을 학습할 수 있도록 합니다. 레이아웃은 JSON 형식으로 예측되고, 조정된 보상 신호들이 학습 최적화를 유도합니다. 이를 통해 물리적 탐지, 형식 탐지, 렌더링 기반 보상과 같은 메커니즘이 포함된 3단계 평가가 이루어집니다.
- ***Performance Highlights***: MetaSpatial을 적용했을 때, Qwen-VL 7B 모델은 포맷 정확도에서 0.85에서 0.98로, 물리적 충돌 비율을 24.5% 감소시켰습니다. 또한, 렌더링 평가 점수는 0.35에서 0.58로 증가하였으며, 전반적인 장면 품질과 물리적 실현 가능성이 크게 향상되었습니다. 이 실험 결과는 VLM의 공간 추론과 생성 품질이 강화학습을 통해 크게 개선된다는 것을 보여줍니다.

### [Optimized Minimal 3D Gaussian Splatting](https://arxiv.org/abs/2503.16924)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16924.png)

Vote: 2

Authors: Eunbyung Park, Joo Chan Lee, Jong Hwan Ko

- ***What's New***: 이 논문에서는 대폭 축소된 Gaussian primitives를 활용하여 저장 용량을 줄이면서도 높은 렌더링 품질을 유지하는 Optimized Minimal Gaussian (OMG) 프레임워크를 제안했습니다. OMG는 특히 중요한 Gaussians를 식별하여 유지하는 방식으로 중복성을 최소화하고 공간 연속성과 불규칙성을 정밀하게 표현합니다.
- ***Technical Details***: OMG는 Gaussian 특성을 정밀하게 표현하기 위해 서브-벡터 양자화(Sub-Vector Quantization; SVQ)를 도입하여 각 속성을 여러 서브-벡터로 나누고 이를 각각 양자화 함으로써 계산 비용과 저장 효율성을 유지하며 높은 정밀도를 달성합니다. 또한 서브-벡터 양자화는 강력한 공간 특성을 활용하여 소규모의 시멘틱 정보를 압축하여 표현합니다.
- ***Performance Highlights***: OMG는 Mip-NeRF 360 데이터셋에서 600 FPS 이상의 렌더링 성능을 유지하면서도 PSNR 24.95에서 27.34까지의 성능으로 기존 최첨단 방법에 비해 저장 요구사항을 약 50% 줄였습니다. OMG-XS는 4.06MB에서 PSNR 27.06을 기록하며, OMG-XL은 6.82MB에서 PSNR 27.34를 기록하여 높은 압축 효율성과 계산 성능을 증명했습니다.

### [CODA: Repurposing Continuous VAEs for Discrete Tokenization](https://arxiv.org/abs/2503.17760)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.17760.png)

Vote: 2

Authors: Yeguo Hua, Gao Huang, Zeyu Liu, Xin Deng, Cheng Zhong, Zanlin Ni, Xiao Ma

- ***What's New***: CODA는 기존의 연속 VAE를 활용하여 이미지의 비연속 토큰화를 수행하는 새로운 프레임워크입니다. 연속적인 압축과 비연속화 과정을 분리하여, 이미 최적화된 인지적 압축을 유지하면서 강력한 시각적 충실도를 보장합니다. 이를 통해 비연속 토크나이저의 훈련 안정성과 효율성을 크게 향상시켰습니다.
- ***Technical Details***: CODA는 기존의 연속 VAE의 연속 잠재 공간을 활용하여 비연속 토큰화하기 위해 만들어진 프레임워크입니다. 잔여 양자화(Residual Quantization)와 주의 기반 양자화(Attention-based Quantization)를 사용하여, 연속적인 잠재 벡터를 점진적으로 비연속 토큰으로 근사하면서 정보 손실을 최소화합니다. 주의 기반 양자화는 코드 선택의 희소성과 명확성을 보장하는 데 기여합니다.
- ***Performance Highlights***: CODA는 ImageNet 256×256 데이터셋에서 16배 압축 시 1.34, 8배 압축 시 0.43의 재구성 FID를 달성하여 표준 VQGAN보다 뛰어난 성능을 보여주었습니다. 또한, 코드북 활용률도 100%에 달하며, MaskGIT 기반 이미지 생성에서도 FID 3.17을 기록하여 비연속 토큰화 전략의 우수성을 입증했습니다.

### [Revisiting Image Fusion for Multi-Illuminant White-Balance Correction](https://arxiv.org/abs/2503.14774)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.14774.png)

Vote: 1

Authors: Luis Herranz, David Serrano-Lozano, Javier Vazquez-Corral, Konstantinos G. Derpanis, Aditya Arora, Michael S. Brown

- ***What's New***: 이번 연구에서는 다중 조명 화이트 밸런스(Multi-Illuminant White Balance) 보정 문제를 해결하기 위한 새로운 트랜스포머 기반 방법을 제안합니다. 이 방법은 기존의 선형 융합법을 넘어 보다 유연하고 정확한 WB(WB; White Balance) 블렌딩 기능을 학습하며, sRGB 공간에서 다중 조명 이미지 데이터셋을 제안하는 것입니다.
- ***Technical Details***: 다섯 가지 WB 프리셋을 융합하여 비선형 방식으로 WB 보정 이미지를 생성하는 트랜스포머 블록을 활용하며, 트랜스포즈드 어텐션(Transposed Attention) 메커니즘을 통해 긴 거리의 공간적 종속성을 효과적으로 포착합니다. 이 외에도 학습 및 평가를 위해 16,284개의 sRGB 이미지를 포함하는 다중 조명 데이터셋을 소개하였습니다.
- ***Performance Highlights***: 우리의 방법은 기존의 선형 융합 기반 방법들보다 100% 이상의 성능 향상을 달성하며, ∆E2000 측정에서 DeepWB 대비 2.28 정도의 개선을 보여주었습니다. 이 모델은 7.9K의 훈련 가능한 파라미터만을 필요로 하며, 기타 방법들과 비교해 뛰어난 효율성을 자랑합니다.

### [Mind with Eyes: from Language Reasoning to Multimodal Reasoning](https://arxiv.org/abs/2503.18071)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18071.png)

Vote: 1

Authors: Yunfan Yang, Xian Zhao, Zhiyu Lin, Jitao Sang, Yifei Gao

- ***What's New***: 이 논문은 언어 중심의 멀티모달 추론 (Language-centric Multimodal Reasoning)과 협력적 멀티모달 추론 (Collaborative Multimodal Reasoning)으로 멀티모달 추론 접근 방식을 체계적으로 분류합니다. 특히, 언어와 시각, 청각 등 다양한 모달리티의 통합을 통해 차세대 멀티모달 추론 시스템을 형성할 수 있는 통찰을 제공합니다.
- ***Technical Details***: 연구는 현재의 Multimodal Large Language Models (MLLMs)의 발전을 분석하고, 멀티모달 추론 성능을 평가할 수 있는 핵심 벤치마크 작업과 평가 메트릭을 소개합니다. 언어 공간 내에서 추론 체인을 명시적으로 구성하는 것이 대부분의 솔루션에서 공통적인 방법으로, 이 논문은 이를 바탕으로 시각 정보 처리의 전략적 계층에서 차이를 설정합니다.
- ***Performance Highlights***: MLLMs가 언어와 시각의 상호작용에 있어 심각한 도전 과제에 직면해 있음이 드러났습니다. 특히, 현재의 'vision-language feature concatenation + text generation' 패러다임이 시각 정보의 생성 잠재력을 제한하고 역동적 상호작용을 방해하고 있음이 보여졌습니다. 향후 연구는 MLLMs의 시각 및 언어 생성 역량을 통합하여 좀 더 심리스하고 자율적인 추론 과정을 이루는 것이 중요합니다.

### [Rethinking Image Evaluation in Super-Resolution](https://arxiv.org/abs/2503.13074)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.13074.png)

Vote: 1

Authors: David Serrano-Lozano, Shaolin Su, Lei Sun, Javier Vazquez-Corral, Josep M. Rocafort, Danna Xue

- ***What's New***: 이 논문은 기존 초해상도(Super-Resolution; SR) 데이터셋의 참조 기준 이미지인 Ground Truth(GT)의 품질이 낮을 수 있음을 지적하며, 초해상도 모델 평가의 신뢰성을 높이기 위한 새로운 지표인 RQI(Relative Quality Index)를 제안합니다.
- ***Technical Details***: 제안된 RQI는 이미지 쌍의 상대 품질 차이를 평가하는 새로운 지표로, 완벽하다고 간주되지 않는 GT를 기준으로 삼아 평가합니다. 이는 기존의 절대적 유사성 측정 방식을 넘어, GT가 낮은 품질인 경우에도 공정한 평가를 가능하게 합니다. 연구는 7개의 최신 SR 모델과 3개의 실제 SR 데이터셋을 분석해 GT 품질이 모델 평가에 미치는 영향을 체계적으로 보여줍니다.
- ***Performance Highlights***: 제안된 RQI 지표는 사용자 기반 의견과 공공 벤치마크 모두에서 인간의 의견과 높은 일치를 보이며, 기존의 SSIM, PSNR, LPIPS 같은 지표들을 대체할 수 있는 가능성을 보여줍니다. 다양한 데이터셋에서의 실험은 RQI가 불완전한 GT로 발생하는 평가 편향을 완화하는 데 효과적임을 입증합니다.

### [Verbal Process Supervision Elicits Better Coding Agents](https://arxiv.org/abs/2503.18494)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18494.png)

Vote: 0

Authors: Jui-Ming Yao, Hao-Yuan Chen, Cheng-Pong Huang

- ***What's New***: 이 연구에서는 대형 언어 모델(large language models) 기반의 코드 생성 에이전트, CURA를 소개하며, 이 시스템은 말하는 과정 감독(Verbal Process Supervision; VPS)을 통해 성능을 향상시킵니다. CURA는 BigCodeBench와 같은 어려운 벤치마크에서 기준 모델 대비 3.65% 향상된 결과를 보여주며, o3-mini 모델과 결합될 때 최상의 성능을 발휘합니다.
- ***Technical Details***: CURA는 코드 이해 및 추론 에이전트 시스템으로, 복잡한 소프트웨어 엔지니어링 문제를 해결하기 위해 개발되었습니다. CURA는 말하는 과정 감독(VPS)을 통해 언어 모델이 언어적 보상 신호를 생성하여 추론 과정을 안내하도록 합니다. 이는 기존의 강화 학습 방법과 대조적으로, 모델의 행동을 강화하고 추론 능력을 향상시키는 방법으로 사용됩니다.
- ***Performance Highlights***: 실험 결과, CURA와 VPS를 통합한 o3-mini 모델이 BigCodeBench - Hard 벤치마크의 'Complete'와 'Average' 평가 카테고리에서 기준 모델보다 뛰어난 성능을 보여주었습니다. 또한, 다양한 온도 설정에서의 모델 성능 분석 결과, Mistral Large Latest 모델이 결정적(decoding temperature = 0) 방식에서 가장 높은 성능을 기록, 코드 생성 작업에 있어 구조화된 접근이 효과적임을 시사합니다.

### [Human Motion Unlearning](https://arxiv.org/abs/2503.18674)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18674.png)

Vote: 0

Authors: Alessio Sampieri, Matteo Migliarini, Fabio Galasso, Edoardo De Matteis, Indro Spinelli

- ***What's New***: 이 연구는 인간 동작의 비학습(Human Motion Unlearning)을 통해 위험한 애니메이션 생성 방지를 목적으로 합니다. 이는 기존의 안전한 동작을 포함한 데이터에서도 유해한 조합이 생성될 수 있어 도전적입니다. HumanML3D 및 Motion-X 데이터셋을 바탕으로 유해한 동작을 필터링하여 최초의 비학습 벤치마크를 제안하였습니다.
- ***Technical Details***: 제안된 모델은 Latent Code Replacement (LCR)라 명명되어 있으며, 트레이닝이 필요 없는 기법으로, 코드북의 이산적 잠재 공간에서 작동합니다. LCR은 MoMask 및 BAMM과 같은 최신 텍스트-동작 생성 모델에 적합하며 코드를 변경하여 유해 패턴을 피하도록 생성 과정을 조정합니다.
- ***Performance Highlights***: LCR 모델은 기존의 비학습 기법들과 비교하여 정성적 및 정량적 성능에서 뛰어난 결과를 보여줍니다. HumanML3D 및 Motion-X 데이터셋 모두에서 MoMask 및 BAMM 모델을 활용했으며, 유해한 동작을 성공적으로 제거하면서도 생성 품질을 유지하는 데 초점을 맞추었습니다.

### [QuartDepth: Post-Training Quantization for Real-Time Depth Estimation on the Edge](https://arxiv.org/abs/2503.16709)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16709.png)

Vote: 0

Authors: Jiuxiang Gu, Jing Liu, Weize Ma, Quanyi Wang, Jun Lin, Henghui Ding, Changdi Yang, Xuan Shen, Pu Zhao, Wei Niu, Yanzhi Wang, Rui Ding

- ***What's New***: QuartDepth는 가장 최신의 모노큘러 깊이 추정(Monocular Depth Estimation; MDE) 모델을 매우 절전적인 에지 디바이스(Application-Specific Integrated Circuits; ASICs)에 배포하기 위해 개발된 포스트 트레이닝 양자화(Post-Training Quantization; PTQ) 프레임워크입니다. 이 작업은 모델의 정확성을 유지하면서 에지 디바이스에서 실시간 추론을 가능하게 합니다.
- ***Technical Details***: QuartDepth는 단일 이미지에서 깊이 정보를 추정하는 MDE 모델의 가중치와 활성화를 4비트로 양자화하여 모델 크기 및 계산 비용을 줄입니다. 이 과정에서 발생할 수 있는 성능 저하를 완화하기 위해 활성화 폴리싱(Activation Polishing)과 보상 알고리즘, 그리고 가중치 재구성 방법을 사용하여 정확도를 최소화합니다. 또한 커널 융합과 맞춤형 명령어 프로그래밍을 지원하는 유연한 프로그래머블 하드웨어 가속기를 설계해 처리 효율성을 향상시킵니다.
- ***Performance Highlights***: QuartDepth 프레임워크는 주요 벤치마크 데이터셋에서 W4A8 및 W4A4 구성의 다양한 백본(backbone)을 테스트했으며, 대부분의 기존 PTQ 방법들보다 개선된 성능을 보여주었습니다. ASICs에서 수행한 실험 결과는 기존 모델 대비 빠른 추론 속도와 더 높은 에너지 효율성을 입증했습니다. 특히 W4A4 구성에서는 5.3배 빠른 추론과 5.5배 높은 전력 효율을 기록했습니다.

### [Instruct-CLIP: Improving Instruction-Guided Image Editing with Automated Data Refinement Using Contrastive Learning](https://arxiv.org/abs/2503.18406)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18406.png)

Vote: 0

Authors: Misha Sra, Sherry X. Chen, Pradeep Sen

- ***What's New***: Instruct-CLIP은 대비학습(Contrastive Learning)을 사용하여 이미지 편집 지침을 개선하는 자가지도식 방법을 제안합니다. 전통적인 텍스트-이미지(T2I) 모델의 한계를 극복하면서 InstructPix2Pix 데이터셋을 120,000개 이상의 개선된 샘플로 정제하여 모델을 개선합니다.
- ***Technical Details***: Instruct-CLIP은 원본 이미지와 편집된 이미지를 같은 특징 공간에 임베딩하고, CLIP의 개념을 응용하여 시각적 변화와 대응되는 편집 지침 사이의 의미론적 정렬을 학습합니다. DINOv2 백본을 수정하여 노이즈가 많은 잠재 이미지를 처리할 수 있도록 하고, 이러한 방법을 통해 수정된 지침을 생성합니다.
- ***Performance Highlights***: Instruct-CLIP 기반으로 학습된 모델은 행위와 편집 지침 간의 정렬을 강화하여 기존의 InstructPix2Pix 모델과 비교해 우수한 편집 결과를 제공합니다. 사용자 연구에서도 Instruct-CLIP의 결과물이 InstructPix2Pix와 MagicBrush보다 선호되는 것으로 나타났습니다.

### [DynamicVis: An Efficient and General Visual Foundation Model for Remote Sensing Image Understanding](https://arxiv.org/abs/2503.16426)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16426.png)

Vote: 0

Authors: Zhenwei Shi, Keyan Chen, Chenyang Liu, Wenyuan Li, Zhengxia Zou, Bowen Chen

- ***What's New***: DynamicVis는 새롭게 제안된 원격 감지 이미지 이해를 위한 비전 기반 모델로, 인간 시각 시스템에서 영감을 받은 선택적 주의 메커니즘을 통해 멀티테스크 비전 모델의 성능을 혁신적으로 향상시킵니다. 이 모델은 대량의 영역 레벨 주석이 포함된 데이터셋을 활용하여 사전 학습되어 다양한 원격 감지 응용 프로그램에서 그 적응성을 보여줍니다.
- ***Technical Details***: DynamicVis는 선택적 상태 공간 모델(State Space Model; SSM)을 기반으로 하는 새로운 백본을 통합하여 지역적 상세 추출과 글로벌 문맥 통합을 정교하게 조정하며, 대규모 데이터의 효율적 코딩이 가능하도록 설계되었습니다. 또한, 메타-임베딩 다중 인스턴스 학습(Meta-embedding Multi-instance Learning; MIL) 패러다임을 사용하여 다양한 작업 간 지식 학습을 강화하고 있습니다.
- ***Performance Highlights***: DynamicVis는 1024 × 1024픽셀 이미지 처리 시 97ms의 지연 속도와 833MB의 GPU 메모리 소비로, ViT(Vision Transformer)에 비해 6%의 지연, 3%의 메모리 소비로 효율성을 크게 증가시켰습니다. 각종 다운 스트림 원격 감지 태스크에서 당초 성능을 초과하는 성능을 보여주었으며, 상담 수평 분석을 통해 새로운 기준선을 제시합니다.

