## Daily Papers (2025-03-24)

### [When Less is Enough: Adaptive Token Reduction for Efficient Image Representation](https://arxiv.org/abs/2503.16660)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16660.png)

Vote: 54

Authors: Elizaveta Goncharova, Andrey Kuznetsov, Eduard Allakhverdov

- ***What's New***: 이 연구는 시각 인코더가 생성하는 비주얼 토큰의 효율성을 최적화하는 방법론을 제안합니다. Vision Transformer(ViT)에서 Gumbel-Softmax 메커니즘을 활용하여 가장 정보가 많은 특성만을 식별하고 유지하는 새로운 Autoencoder 기반 방식을 도입하였습니다. 이렇게 선택된 특성을 통해 처리를 간소화하고 비용을 절감하면서도 성능을 유지하는 것을 목표로 합니다.
- ***Technical Details***: 이 논문에서 제안한 방법론은 Autoencoder 구조에 Gumbel-Softmax 선택 메커니즘을 결합하여, 시각 인코더가 출력한 토큰 중 가장 유용한 토큰을 식별 및 선택합니다. 이 방법은 Autoencoder 학습 절차를 통해 원본 특성을 정확한 수준으로 재구성하는 것이 가능합니다. Feature Selector는 Gumbel-Softmax 기반의 헤드를 통해 비주얼 토큰을 마스크 처리하였으며, Feature Reconstructor는 손실된 정보를 복원하는 역할을 맡습니다.
- ***Performance Highlights***: 실험 결과, LLaVA-NeXT 및 LLaVA-OneVision 모델에서 시각 문맥의 최대 50%를 제거해도 성능 저하가 거의 없음을 확인했습니다. OCR 기반 작업에서는 랜덤 선택 대비 더 높은 성능을 보여주며, 대형 멀티모달 모델에서도 이러한 접근이 효과적임을 입증하였습니다.

### [MAPS: A Multi-Agent Framework Based on Big Seven Personality and Socratic Guidance for Multimodal Scientific Problem Solving](https://arxiv.org/abs/2503.16905)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16905.png)

Vote: 45

Authors: Jian Zhang, Rui Mao, Erik Cambria, Qika Lin, Zhangqi Wang, Xinyu Zhang, Zhiyuan Wang, Jun Liu, Fangzhi Xu

- ***What's New***: MAPS는 Big Seven 성격 이론과 소크라테스 가이던스를 기반으로 하는 혁신적인 멀티 에이전트 프레임워크로, 이를 통해 멀티모달 과학 문제 해결 능력을 강화합니다. 각 에이전트는 특정 문제 해결 단계에 중점을 두고 협력하여 이전 모델보다 15.84% 향상된 성능을 달성했습니다.
- ***Technical Details***: MAPS 프레임워크는 신중한 문제 해결을 위해 Big Seven 성격 이론에 기반을 두어 설계된 7개의 독립 에이전트를 사용합니다. 첫 번째 문제를 해결하기 위해 순차적으로 해석자(Interpreter), 정렬자(Aligner), 학자(Scholar), 해결자(Solver) 등의 에이전트를 사용하는 4단계 전략을 제안합니다. 소크라테스 질문법에서 영감을 받은 비평가(Critic) 에이전트는 반복적인 피드백을 통해 자율 학습을 촉진합니다.
- ***Performance Highlights***: MAPS는 EMMA, Olympiad, MathVista 데이터셋에 대한 실험에서 모든 작업에서 이전 SOTA 모델을 15.84% 앞서며 뛰어난 성과를 보였습니다. 이는 수학, 물리, 화학 등 여러 영역에서 탁월한 문제 해결 능력을 보여주는 것으로, 협력적인 멀티에이전트 시스템의 우수성을 입증합니다.

### [MARS: A Multi-Agent Framework Incorporating Socratic Guidance for Automated Prompt Optimization](https://arxiv.org/abs/2503.16874)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16874.png)

Vote: 38

Authors: Jian Zhang, Haiping Zhu, Qika Lin, Zhangqi Wang, Jun Liu, Erik Cambria

- ***What's New***: MARS는 소크라테스식 가이드를 결합한 다중 에이전트 프레임워크(Multi-Agent Framework Incorporating Socratic Guidance)를 제공하여 자동 프롬프트 최적화(Automated Prompt Optimization; APO)를 향상시킵니다. 이는 프롬프트 설계에 있어 기존의 한정된 템플릿 문제를 해결하고, 효율적인 검색을 가능하게 합니다.
- ***Technical Details***: MARS는 계획자 에이전트(Planner Agent)와 여섯 개의 유저 정의 에이전트를 포함하여 복잡한 다중 에이전트 아키텍처를 구성합니다. 교육자-비평가-학생(TCS; Teacher-Critic-Student) 소크라테스식 대화 패턴을 채용하여 프롬프트를 반복적으로 최적화하고 효과적이고 전체적인 탐색을 수행합니다.
- ***Performance Highlights***: MARS는 12 일반 작업과 5 도메인 특수 데이터셋에서 테스트되어 모든 기존의 기준 모델을 능가하는 성능을 보였습니다. 특히, 일반 작업에서의 평균 성능이 이전의 SOTA 모델보다 6.04% 높으며, 도메인 특수 작업에서는 최고 SOTA 모델을 6.42% 초과했습니다.

### [RoboFactory: Exploring Embodied Agent Collaboration with Compositional Constraints](https://arxiv.org/abs/2503.16408)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16408.png)

Vote: 32

Authors: Xiaohong Liu, Ruimao Zhang, Li Kang, Lei Bai, Xihui Liu, Yiran Qin, Xiufeng Song, Zhenfei Yin

- ***What's New***: 이 논문은 RoboFactory라는 다에이전트(embodied multi-agent) 시스템을 위한 새로운 자동화 데이터 수집 프레임워크를 제안합니다. 이 시스템은 compositional constraints라는 개념을 도입하여 에이전트 간 협업의 도전과제를 다루고자 합니다. Compositional constraints는 논리적, 공간적, 시간적 제약을 통해 에이전트 간의 안전하고 효율적인 협력을 가능하게 합니다.
- ***Technical Details***: RoboFactory 프레임워크는 두 가지 주요 모듈, RoboBrain과 RoboChecker로 구성됩니다. RoboBrain은 주어진 전역 과제 설명과 관찰 데이터를 바탕으로 각 에이전트의 다음 하위 목표를 생성하고, 텍스트 기반의 compositional constraints를 출력하여 사전 정의된 모션 프리미티브를 호출함으로써 하위 목표를 달성합니다. RoboChecker는 이러한 텍스트기반 제약을 물리적 세계와 상호 작용할 수 있는 구체적인 인터페이스로 변환하여 에이전트의 경로가 제약을 위반하지 않도록 합니다.
- ***Performance Highlights***: Diffusion policy와 같은 모범 학습(imitation learning) 기법을 여러 다중 에이전트 협업 작업에 배치하여 평가를 진행하였습니다. 각 작업에 대한 성공률의 증가가 RoboFactory에서 생성된 고품질 데이터셋의 필요성을 강조하며, 복잡한 다중 에이전트 환경에서 데이터의 충분한 양이 수행 성능을 높이는 데 중요하다는 점을 보여주고 있습니다. 그러나 에이전트 수가 증가할수록 성공률이 감소하는 과제가 존재해, 현재 방법론이 갖고 있는 한계를 드러냈습니다.

### [Bridging Continuous and Discrete Tokens for Autoregressive Visual Generation](https://arxiv.org/abs/2503.16430)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16430.png)

Vote: 28

Authors: Yao Teng, Zhijie Lin, Jiashi Feng, Yuanzhi Zhu, Shuhuai Ren, Xihui Liu, Yuqing Wang

- ***What's New***: TokenBridge는 비주얼 생성의 연속적(Continuous) 및 불연속적(Discrete) 토큰 표현 사이의 격차를 연결하는 새로운 접근법입니다. 연속적 토큰의 고품질 표현력을 유지하면서 불연속적 토큰의 단순한 모델링을 가능하게 합니다.
- ***Technical Details***: TokenBridge는 사후 훈련 양자화(Post-training Quantization)를 통해 훈련이 완료된 연속적 VAE(Variational AutoEncoder) 특징을 불연속적 토큰으로 변환합니다. 이러한 프로세스는 차원별 양자화(Dimension-wise Quantization) 전략을 통해 각 특징 차원을 독립적으로 불연속화하며, 경량의 자가회귀적(Autoregressive) 예측 메커니즘을 사용해 큰 토큰 공간을 효과적으로 모델링 합니다.
- ***Performance Highlights***: TokenBridge는 ImageNet 256x256 벤치마크에서 연속적 접근 방식에 필적하는 시각적 품질을 달성하고, 기존의 자가회귀적 방법보다 5.94배의 속도 향상을 보여줍니다. 또한, 일반적인 범주형 예측(Categorical Prediction)을 사용하여 복잡한 분포 모델링 없이도 높은 수준의 결과를 얻어내며, 생성 과정에서의 유연한 제어 가능성을 입증합니다.

### [OpenVLThinker: An Early Exploration to Complex Vision-Language Reasoning via Iterative Self-Improvement](https://arxiv.org/abs/2503.17352)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.17352.png)

Vote: 20

Authors: Hritik Bansal, Yihe Deng, Nanyun Peng, Wei Wang, Fan Yin, Kai-Wei Chang

- ***What's New***: OpenVLThinker는 대형 비전-언어 모델(Large Vision-Language Models; LVLMs)의 복잡한 추론 능력을 강화하는 데 중점을 두고 개발된 초기 탐구입니다. SFT(감독 학습)와 강화 학습(RL)을 교대로 적용하여 모델이 스스로 개선할 수 있도록 하는 방법론을 처음으로 제안하였습니다.
- ***Technical Details***: OpenVLThinker는 이미지 캡션을 텍스트화하여 순수 텍스트 기반 모델에서 R1 스타일의 추론을 증류하는 방법을 사용했습니다. 이 과정을 통해 생성된 데이터를 기반으로 SFT를 통해 초기 추론 구조를 형성하였고, 이후 GRPO(Group Relative Policy Optimization)와 같은 RL 기법으로 일반화 능력을 향상했습니다.
- ***Performance Highlights***: OpenVLThinker-7B는 MathVista와 같은 복잡한 멀티모달 추론 과제에서 기존 Qwen2.5-VL-7B 모델을 능가하는 성과를 보였습니다. 강화 학습을 통한 반복적인 개선을 통해 MathVista에서 정확도 69.4%를 달성하여, 원래의 Qwen2.5-VL-7B의 68.5%를 초과하는 결과를 보여주었습니다.

### [Modifying Large Language Model Post-Training for Diverse Creative Writing](https://arxiv.org/abs/2503.17126)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.17126.png)

Vote: 18

Authors: Melissa Roemmele, Vishakh Padmakumar, Max Kreminski, John Joon Young Chung, Yuqian Sun

- ***What's New***: 이 논문에서는 대규모 언어 모델(LLMs)의 출력 다양성과 품질을 동시에 향상시키기 위한 포스트 트레이닝(post-training) 접근법을 제안합니다. 기존의 방법들이 주로 출력 품질 향상에 집중했던 것과는 달리, 본 연구에서는 훈련 샘플 간 편차(deviation)를 학습 목표로 포함시켜 희귀하고 고품질의 인스턴스로부터 학습을 가능하게 하였습니다.
- ***Technical Details***: 제안된 방법은 직접 선호 최적화(Direct Preference Optimization; DPO)와 확률비 선호 최적화(Odds Ratio Preference Optimization; ORPO)에 편차 측정을 접목하여 다양한 출력을 생성할 수 있도록 합니다. 편차는 같은 프롬프트로부터 생성된 후보 샘플과의 차이를 측정하는 척도로 사용되며, 모델의 훈련 목표 함수에 통합되어 있습니다.
- ***Performance Highlights***: 8B 파라미터를 가진 최적의 모델은 인간이 생성한 데이터셋과 유사한 출력 다양성을 달성하였으며, GPT-4o 및 DeepSeek-R1과 같은 기존 최고 품질 모델에 필적하는 출력 품질을 유지했습니다. 사람 평가 및 다양한 조건에서의 변동성을 고려한 실험을 통해 제안된 접근법이 다양한 환경에서도 견고한 성능을 보임을 확인했습니다.

### [TaoAvatar: Real-Time Lifelike Full-Body Talking Avatars for Augmented Reality via 3D Gaussian Splatting](https://arxiv.org/abs/2503.17032)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.17032.png)

Vote: 12

Authors: Zhonghua Jiang, Chengfei Lv, Jianchuan Chen, Jingchuan Hu, Zhiwen Chen, Tiansong Zhou, Gaige Wang

- ***What's New***: TaoAvatar는 3D Gaussian Splatting(3DGS)을 기반으로 한 풀바디 아바타를 증강 현실(AR) 장치에서 실시간으로 실행할 수 있도록 설계되었습니다. 이전 방법론들과는 달리, TaoAvatar는 맞춤형 의류 파라메트릭 템플릿을 사용하여 본연의 SMPLX의 얼굴 표현과 제스처 제어 기능을 보존합니다.
- ***Technical Details***: TaoAvatar는 복잡한 자세 종속 비강체 변형을 처리할 수 있는 StyleUnet 기반 매개 변수를 사전 학습한 후, 경량 MLP 네트워크로 이 변형을 '베이킹'(baking)하여 미세한 디테일을 보완합니다. 또한, 다양한 모바일 및 AR 장치에서 실시간 렌더링이 가능하며, Apple Vision Pro와 같은 고해상도 스테레오 장치에서 90 FPS를 유지합니다.
- ***Performance Highlights***: 실험 결과 TaoAvatar는 고주파 외관 세부 정보를 효과적으로 캡처하면서 2K 해상도에서 150+ FPS의 렌더링 성능을 달성했습니다. 또한, TaoAvatar는 얼굴 표정, 손 제스처, 신체 자세로 구동될 수 있으며 최첨단 방법보다 뛰어난 품질과 성능을 제공합니다.

### [MathFlow: Enhancing the Perceptual Flow of MLLMs for Visual Mathematical Problems](https://arxiv.org/abs/2503.16549)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16549.png)

Vote: 10

Authors: Tao Feng, Zeying Huang, Yi Yang, Hangjie Yuan, Yunqiu Xu, Pengwei Liu, Felix Chen, Jun Cen

- ***What's New***: MathFlow는 멀티모달 대형 언어 모델(Multimodal Large Language Models; MLLMs)의 시각적 수학 문제 해결 성능을 개선하는 새로운 모듈형 문제 해결 파이프라인입니다. 이는 지각 단계와 추론 단계를 별도로 최적화하여 MLLMs의 시각적 콘텐츠 이해를 지원하고자 합니다.
- ***Technical Details***: FlowVerse 벤치마크는 기본적인 정보를 네 개의 구성 요소로 분류하여 다양한 수학적 문제 해결 능력을 평가합니다. MathFlow의 발전된 지각 역량을 발휘하기 위해 MathFlow-P-7B 모델을 개발했습니다. 이는 시각적 다이어그램에서 필수 정보를 추출하는 데 초점을 두며, 다단계 사전 훈련과 지도형 미세 조정 단계를 통해 최적화됩니다.
- ***Performance Highlights***: FlowVerse 실험 결과, MathFlow는 기존의 다양한 MLLMs보다 상당히 향상된 성능을 보여주었습니다. 특히 고급 인퍼런스 모델과 결합될 때 우수한 성능을 발휘하였으며, 오픈소스 및 클로즈드소스 모델 전반에서 높은 정확도를 기록했습니다. 이러한 결과는 MathFlow가 시각적 수학 문제를 해결하는 데 필요한 유연성과 강력한 통합 성능을 제공한다는 것을 증명합니다.

### [Enabling Versatile Controls for Video Diffusion Models](https://arxiv.org/abs/2503.16983)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16983.png)

Vote: 9

Authors: Jiaxing Yan, Hao Zhou, Haoming Qin, Xu Zhang, Zeyu Chen, Guanzhong Wang, Yi Liu, Xiaobin Lu

- ***What's New***: VCtrl(또는 PP-VCtrl)은 사전 훈련된 비디오 확산 모델(Pre-trained Video Diffusion Models)의 세밀한 제어를 가능하게 하는 새로운 프레임워크입니다. 사용자가 지정한 다양한 제어 신호를 통합하여 기존 생성기를 수정하지 않고도 다양한 조건 신호(Canny edges, segmentation masks, human keypoints)에 대한 제어를 통합할 수 있습니다.
- ***Technical Details***: VCtrl은 다양한 조건 입력을 통합된 표현으로 변환하는 단일 제어 신호 인코딩 프로세스를 특징으로 하며, 이는 비디오 확산 모델과의 통합을 위해 제안된 희소 잔여 연결 메커니즘(Sparse Residual Connection Mechanism)을 사용하여 효율적인 통제 표현 통합을 수행합니다. PaddlePaddle 프레임워크를 사용하여 구현되었으며, 전체 소스 코드와 사전 훈련된 모델이 공개되었습니다.
- ***Performance Highlights***: VCtrl 및 VCtrl-I2V 모델은 Canny-to-Video, Mask-to-Video, Pose-to-Video 생성 작업에서 경쟁 모델보다 월등한 성능을 보였습니다. 예를 들어, Canny Matching 지표에서 Text2Video-Zero 모델보다 0.04와 0.08의 향상을 보여 주었으며, FVD 점수는 Text2Video-Zero에 비해 292~313포인트 감소하였습니다. 또한, Masked Subject Consistency 및 FVD 점수를 기존 방법보다 향상시키는 성과를 거두었습니다.

### [Single Image Iterative Subject-driven Generation and Editing](https://arxiv.org/abs/2503.16025)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16025.png)

Vote: 9

Authors: Idan Schwartz, Gal Chechik, Yair Shpitzer

- ***What's New***: SISO (Single Image Subject Optimization)는 단일 주제 이미지를 사용하여 개인화된 이미지 생성 및 편집을 가능하게 하는 새로운 방법입니다. 이 방법은 사전 훈련된 이미지 유사성 점수 모델을 활용하며, 기존의 확산 모델 내에서 훈련 없이도 단일 주제 이미지로 작동합니다.
- ***Technical Details***: SISO는 이미지 생성 모델을 추론 시간 동안 최적화하여 손실을 계산합니다. 손실은 생성된 이미지와 입력된 주제 이미지 간의 유사도를 측정하는 고품질 사전 훈련 모델을 사용하여 픽셀 공간에서 정의됩니다. 이 방법은 DINO와 IR 임베딩 공간의 거리를 포함한 유사성 손실을 사용하는데, 이로 인해 모드 붕괴를 방지하고 성능을 향상시킵니다.
- ***Performance Highlights***: 실험 결과, SISO는 이미지 자연스러움 및 주제 보존에 있어 기존 기법들을 능가하였으며, 배경 일관성을 유지하면서도 높은 주제, 정체성 보존을 보여주었습니다. 사용자 연구에서는 이미지 편집의 자연스러움과 배경 보존에서 58%, 60%의 승률을 기록하였으며, 이미지 생성의 경우 65%, 69%로 높은 자연스러움과 프롬프트 일치를 확인했습니다.

### [FastCuRL: Curriculum Reinforcement Learning with Progressive Context Extension for Efficient Training R1-like Reasoning Models](https://arxiv.org/abs/2503.17287)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.17287.png)

Vote: 8

Authors: Yue Pan, Mao Zheng, Xuan Luo, Zheng Li, Feng Zhang, Mingyang Song, Wenjie Yang

- ***What's New***: FASTCURL는 R1과 유사한 추론 모델의 효율적인 훈련을 위한 Curriculum Reinforcement Learning 접근법을 제안하며, 특히 1.5B 파라미터 언어 모델을 통해 복잡한 추론 작업을 효과적으로 처리하도록 설계되었습니다. 이를 통해 복잡한 이유화 작업에서 DeepScaleR을 능가하는 성능을 나타냅니다.
- ***Technical Details***: FASTCURL은 두 가지 주요 프로세스로 구성됩니다: 길이 인지 훈련 데이터 분할과 문맥 윈도우 확장 훈련입니다. 원본 훈련 데이터를 입력 프롬프트 길이에 따라 세 가지 수준으로 분할하고, 각 분할된 데이터셋을 점진적으로 증가하는 문맥 윈도우 길이를 사용하여 훈련합니다.
- ***Performance Highlights***: FASTCURL-1.5B-Preview는 MATH 500, AIME 2024, AMC 2023, Minerva Math, OlympiadBench의 모든 데이터셋에서 DeepScaleR-1.5B-Preview를 뛰어넘는 성능을 보였으며, 훈련 단계는 단일 노드와 8개 GPU를 사용하여 50%의 훈련 스텝만으로 완료되었습니다.

### [ETVA: Evaluation of Text-to-Video Alignment via Fine-grained Question Generation and Answering](https://arxiv.org/abs/2503.16867)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16867.png)

Vote: 8

Authors: Peng Zhang, Kaisi Guan, Yuchong Sun, Ruihua Song, Wei Liu, Meng Cao, Kieran Liu, Zhengfeng Lai

- ***What's New***: ETVA는 텍스트-비디오 정렬(Text-to-Video Alignment)을 미세한 질문 생성 및 답변을 통해 평가하는 새로운 방법을 제안합니다. 이 접근 방식은 기존의 정렬 평가 지표가 수치만 제공하고 세밀한 정렬 정보를 제공하지 못하는 문제를 해결하고자 합니다.
- ***Technical Details***: ETVA는 다중 에이전트 시스템이 텍스트 프롬프트를 의미론적 장면 그래프로 파싱하여 원자적 질문을 생성합니다. 그리고 지식 보강 다단계 추론 프레임워크를 설계하여 질문에 답합니다. 보조 LLM은 물리법칙 등 상식 지식을 검색하고, 비디오 LLM은 다단계 추론을 통해 생성된 질문에 답변합니다.
- ***Performance Highlights***: ETVA는 스피어맨 상관계수 58.47을 달성하여 기존 지표보다 인간 판단과 높은 상관성을 보입니다(기존 지표는 31.0). 15개의 텍스트-비디오 모델을 체계적으로 평가하여 다음 세대의 T2V 생성의 발전 방향을 제시하였습니다.

### [From Head to Tail: Towards Balanced Representation in Large Vision-Language Models through Adaptive Data Calibration](https://arxiv.org/abs/2503.12821)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12821.png)

Vote: 7

Authors: Jiawei Zhou, Mingyang Song, Xiaoye Qu, Yu Cheng

- **What's New**: 이 연구는 대형 시각-언어 모델(Large Vision-Language Models; LVLMs)에서 데이터의 불균형성을 해결하기 위해 적응적 데이터 보정 프레임워크(Adaptive Data Refinement Framework; ADR)를 제안합니다. 이는 데이터 재조정(Data Rebalancing)과 데이터 합성(Data Synthesis)이라는 두 단계를 통해 장기 꼬리(Long-Tail) 문제를 효과적으로 완화합니다.
- **Technical Details**: ADR의 데이터 재조정 단계에서는 엔티티 분포를 기반으로 잉여 데이터를 적응적으로 제거하고, 데이터 합성 단계에서는 결핍된 부분을 보완하기 위해 Denoising Diffusion Probabilistic Models (DDPMs) 및 희소 이미지를 활용합니다. 이 프레임워크는 LLaVA와 같은 오픈소스 LVLM에 통합 가능하며, 데이터 양을 늘리지 않고도 모델의 성능을 향상시킵니다.
- **Performance Highlights**: 제안된 프레임워크는 11개의 벤치마크에 대한 평가에서 LLaVA 1.5 모델의 평균 성능을 4.36% 상대적으로 향상시켰습니다. 특히 꼬리부분의 개념에 대한 모델의 성능을 크게 개선하여 전반적인 성능을 저해하지 않으면서도 꼬리 개념에 대한 이해를 높일 수 있었습니다.

### [PVChat: Personalized Video Chat with One-Shot Learning](https://arxiv.org/abs/2503.17069)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.17069.png)

Vote: 6

Authors: Yumeng Li, Yuchen Li, Gang Xu, Ming Li, Yufei Shi, Fei Richard Yu, Weilong Yan, Zhenxi Li, Si Yong Yeo

- ***What's New***: PVChat는 개인화된 비디오 이해를 위한 첫 번째 대형 언어 모델로, 단일 참조 비디오에서 개인의 특성을 학습하고 주제를 인식하여 질의응답을 수행할 수 있습니다. 또한 기존의 비디오 LLMs가 일반적인 이해에 주로 제한된 문제를 해결합니다.
- ***Technical Details***: PVChat는 정체성을 보존하고 유사한 하드 네거티브를 포함하는 데이터 증강 파이프라인을 통해 개인화된 샘플을 생성합니다. ReLU Routing Mixture-of-Heads (ReMoH) 주의 메커니즘을 도입하여 효율적이고 적응적인 학습을 지원하며, Smooth Proximity Regularization과 Head Activation Enhancement 목표를 통해 학습의 안정성과 주의 헤드의 활성화를 향상시킵니다. 또한, 이미지 사전 학습과 비디오 초세분화의 두 단계에 걸친 학습 전략을 통해 정적인 속성에서 역동적인 표현으로의 점진적인 학습을 추진합니다.
- ***Performance Highlights***: 다양한 데이터셋을 기반으로 한 실험 결과 PVChat는 주제 존재, 외형, 행동, 위치 질의에 대한 SOTA 모델들보다 우수한 성능을 보여줌으로써 개인화된 비디오 이해와 주제 인식의 정확성을 높였습니다.

### [Implicit Bias-Like Patterns in Reasoning Models](https://arxiv.org/abs/2503.11572)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.11572.png)

Vote: 6

Authors: Calvin K. Lai, Messi H. J. Lee

- ***What's New***: 이 논문은 Reasoning Model Implicit Association Test (RM-IAT)라는 새로운 방법을 제안하여 AI의 Reasoning Models에서 '암묵적 편향' 패턴을 연구합니다. RM-IAT는 인간의 암묵적 편향을 측정하는 IAT를 기반으로 하여 모델 내 정보 처리 방식을 평가하고, 사회적 범주와의 관련성에서 더 많은 'Reasoning Tokens'를 필요로 하는지를 측정합니다. 이는 AI 시스템이 인간의 암묵적 편향과 유사한 정보 처리 패턴을 가질 수 있음을 시사합니다.
- ***Technical Details***: 연구는 OpenAI의 o3-mini Reasoning Model을 사용하여 각각의 프로세스에서 출력되는 Reasoning Tokens의 수를 측정하는 실험을 진행했습니다. 실험에서 모델은 10개의 RM-IATs를 통해 'association-incompatible' 정보를 처리할 때 더 많은 Reasoning Tokens를 필요로 한다는 것을 발견했습니다. 실험은 유연한 API 호출을 통해 진행되었고, 다양한 Prompt 변형을 사용하여 데이터의 다양성을 확보했습니다.
- ***Performance Highlights***: 실험 결과, 10개의 RM-IAT 중 9개는 'association-incompatible' 조건에서 'association-compatible' 조건보다 평균적으로 더 많은 Reasoning Tokens가 필요했습니다. 가장 강한 편향은 Instruments/Weapons + Pleasant/Unpleasant 테스트에서 발견되었으며, 일관되게 높은 토큰 사용량을 보여주었습니다. 이러한 결과는 Reasoning Models가 연관된 정보를 실제로 처리하는데 있어서 인간과 유사한 Biased 패턴을 보임을 나타냅니다.

### [Generalized Few-shot 3D Point Cloud Segmentation with Vision-Language Model](https://arxiv.org/abs/2503.16282)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16282.png)

Vote: 5

Authors: Ender Konukoglu, Yun Liu, Zhaochong An, Guolei Sun, Junlin Han, Runjia Li, Serge Belongie

- ***What's New***: 이 논문은 3D 비전-언어 모델(3D Vision-Language Models)를 활용하여 새로운 클래스에 적응하는 일반화된 소수 샷 3D 포인트 클라우드 세그먼테이션(Generalized Few-shot 3D Point Cloud Segmentation, GFS-PCS)을 제안합니다. 이는 구조적 대조와 적응적 채움 전략을 통해 3D VLM에서 얻은 노이즈가 있는 조밀한 가짜 레이블과 정확하지만 희소한 소수 샷 샘플을 결합하여 새로운 클래스의 학습을 극대화합니다.
- ***Technical Details***: 제안된 GFS-VL 프레임워크는 프로토타입 기반의 가짜 레이블 선택 메커니즘을 도입하여 저품질 영역을 걸러내며, 가짜 레이블 문맥과 소수 샷 샘플의 정보를 결합하여 필터링된 비레이블된 영역에 적응적인 라벨을 할당합니다. 또한, 소수 샷 샘플을 훈련 장면에 내장시켜 중요 컨텍스트를 보존하며 개선된 새로운 클래스 학습을 지원합니다.
- ***Performance Highlights***: 새로운 ScanNet200 및 ScanNet++ 벤치마크에서 GFS-VL은 모든 주요 메트릭에서 기존의 방법을 능가하며, 특히 5-shot 설정에서 28.57%의 Harmonic Mean (HM) 향상을 보여줍니다. 이로 인해 GFS-PCS의 실질적인 일반화 성능을 확인할 수 있으며, 제안된 벤치마크는 더욱 다양한 새로운 클래스의 평가를 가능케 합니다.

### [When Preferences Diverge: Aligning Diffusion Models with Minority-Aware Adaptive DPO](https://arxiv.org/abs/2503.16921)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16921.png)

Vote: 5

Authors: Yanwei Fu, Donghao Luo, Chengming Xu, Yuan Yao, Lingfan Zhang, Chen Liu, Kai Hu, Chengjie Wang

- ***What's New***: 이 논문에서는 소수 집단의 선호를 고려한 새로운 적응형 DPO(Adaptive-DPO)를 통해 확산 모델(Diffusion Models)의 정렬을 개선하는 방법을 제안합니다. 기존 모델들이 일반적인 인간 선호에 맞추는 것에 집중했던 것과 달리, 이 연구는 선호 데이터의 소수 집단 표본이 모델 성능에 미치는 부정적 영향을 탐구하고 있으며, 이를 해결하기 위한 DPO 목표에 소수 표본을 인식하는 메트릭을 통합합니다.
- ***Technical Details***: Adaptive-DPO는 DPO 목표에 소수 인스턴스 인식 메트릭을 통합하여 설계되었습니다. 이 메트릭은 '검토자 내부 확신도(intra-annotator confidence)'와 '검토자 간 안정성(inter-annotator stability)'을 포함하여 다수 표본과 소수 표본을 구별합니다. 이를 통해 소수 표본의 부정적 영향을 줄이고 다수 레이블의 학습을 향상시킬 수 있습니다.
- ***Performance Highlights***: 다양한 벤치마크와 모델 아키텍처를 통해 Adaptive-DPO의 효과가 입증되었습니다. SD1.5 및 SDXL을 백본으로 사용한 벤치마크 평가에서 전통적인 방법들을 능가하는 성과를 보였습니다. 특히, 실험을 통해 소수 데이터가 포함된 경우에도 보다 신뢰할 수 있는 다수 레이블 학습을 통해 학습의 일반화가 잘 이루어짐을 확인할 수 있었습니다.

### [GAEA: A Geolocation Aware Conversational Model](https://arxiv.org/abs/2503.16423)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16423.png)

Vote: 5

Authors: Mubarak Shah, Ron Campos, Ashmal Vayani, Parth Parag Kulkarni, Rohit Gupta, Aritra Dutta

- ***What's New***: GAEA는 이미지의 위치 지리정보를 제공하고 대화를 통해 사용자가 원하는 정보를 제공할 수 있는 지리적 대화 모델(Geolocation Aware Conversational; GAEA)을 소개합니다. 이 연구는 OpenStreetMap 데이터와 지리적 문맥 힌트를 이용한 데이터셋인 GAEA-1.6M과 대화적 평가를 위한 벤치마크 GAEA-Bench를 처음으로 제안합니다.
- ***Technical Details***: GAEA-1.6M은 다양한 지리적 맥락을 가진 800K 이상의 이미지와 약 1.6M의 질문-답변 쌍을 포함하는 포괄적인 데이터셋으로, 전 세계적으로 다양한 장소를 아우릅니다. OpenStreetMap(OSM) 데이터를 활용하여 주변 지역, 랜드마크, 이용 가능한 서비스 등에 대한 메타데이터를 확보하고 있습니다. GAEA 모델은 장단형 시각적 질문-답변(SVQA, LVQA), 다지선다형(MCQ), 참/거짓(T/F) 문항으로 제공되는 질문 유형을 통해 대화 능력을 평가합니다.
- ***Performance Highlights***: GAEA는 최적의 독점 모델인 GPT-4o를 MCQs 및 T/Fs에서 능가하며 평균 정확도 66.06%로, GPT-4o 대비 8.28%, 오픈소스 모델 LLaVA-OneVision 대비 25.69% 더 높은 성능을 보여 주었습니다. IM2GPS 등 기존 벤치마크에서 고급 모델들과 비교해 경쟁력 있는 성능을 발휘하며, 실제 지리적 위치로의 분류 정확도에서도 뛰어난 결과를 보였습니다.

### [Can Large Vision Language Models Read Maps Like a Human?](https://arxiv.org/abs/2503.14607)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.14607.png)

Vote: 3

Authors: Shuo Xing, Yuping Wang, Dezhen Song, Kaiyuan Chen, Zezhou Sun, Jiachen Li, Yanjia Huang, Zhengzhong Tu, Shuangyu Xie

- ***What's New***: 이 논문에서는 인간 판독 가능한 지도와 복잡한 경로 찾기 시나리오에서 대형 시각-언어 모델(LVLMs)의 내비게이션 지침 생성을 평가하기 위해 고안된 최초의 데이터셋인 MapBench를 소개합니다. 이 데이터셋은 100개의 다양한 지도에서 유래된 1600개 이상의 경로 찾기 문제로 구성되어 있으며, 특히 LVLMs의 공간 추론 및 구조적 의사결정에 대한 한계를 드러냅니다.
- ***Technical Details***: MapBench는 4가지 시각적 스타일 요소와 9가지 시나리오(도시, 대학, 테마파크 등)로 분류된 100개의 지도에서 1,600개 이상의 시각 경로 계획 쿼리를 포함하며, 각 지도에는 시작 및 종료 랜드마크와 관련된 질의가 수동으로 주석이 달려 있습니다. 평가된 LVLMs는 zero-shot 및 Chain-of-Thought (CoT) 프로토콜 하에서 테스트 되었습니다.
- ***Performance Highlights***: 이 데이터셋으로 평가한 결과, LVLMs는 인간 수준의 내비게이션 능력에 상당한 격차가 있음을 보여주며, CoT 추론 방식이 zero-shot에 비해 더 나은 성능을 보였지만, 경우에 따라 불필요한 정보가 추가되는 문제가 있었습니다. 특히 폐쇄형 LVLMs가 개방형 LVLMs보다 전반적으로 우수한 성능을 보였습니다.

### [FFaceNeRF: Few-shot Face Editing in Neural Radiance Fields](https://arxiv.org/abs/2503.17095)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.17095.png)

Vote: 2

Authors: Kwan Yun, Chaelin Kim, Hangyeul Shin, Junyong Noh

- ***What's New***: FFaceNeRF는 소수의 학습 샘플(few-shot training)을 기반으로 Neural Radiance Fields(NeRF)에서 3D 페이스 이미지를 자유롭게 편집할 수 있는 혁신적인 방법입니다. 이는 기존에 고정된 세분화 마스크를 사용하는 방식의 제약을 극복하고, 사용자 정의된 레이아웃을 수용하는 유연한 편집을 가능하게 합니다.
- ***Technical Details***: FFaceNeRF는 기하 어댑터(geometry adapter)와 특징 주입(feature injection)을 통해 기하 속성을 효과적으로 조작할 수 있습니다. 또한, LMTA(Latent Mixing for Triplane Augmentation)를 도입해 소수의 샘플로도 다양한 마스크 레이아웃에 맞춰 기하 디코더를 적응시킵니다. 이는 맞춤형 의료 영상이나 창의적인 얼굴 편집 분야에서 빠른 모델 적응을 가능하게 합니다. 새로운 오버랩 기반 최적화 방법을 통해 작은 영역의 편집에서도 정확성을 높였습니다.
- ***Performance Highlights***: FFaceNeRF는 기존의 마스크 기반 얼굴 편집 방법들보다 유연성, 제어력, 생성 이미지 품질 면에서 우수한 성능을 보였습니다. 특히 10개의 학습 이미지로 세그먼트 마스크 적응을 빠르게 수행해 기존 방법들보다 편집 세부 사항을 더 잘 반영하는 결과를 제공합니다.

