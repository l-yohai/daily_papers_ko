## Daily Papers (2025-03-07)

### [START: Self-taught Reasoner with Tools](https://arxiv.org/abs/2503.04625)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.04625.png)

Vote: 48

Authors: Mingfeng Xue, Junyang Lin, Xiang Wang, Chengpeng Li, Dayiheng Liu, Jiaxi Yang, Beichen Zhang, Bowen Yu, Zhenru Zhang, Binyuan Hui

- ***What's New***: START(Self-Taught Reasoner with Tools)은 외부 도구를 활용하여 추론 능력을 크게 향상시키고, 코드 실행을 통해 복잡한 계산 수행, 셀프 체크, 다양한 방법 탐색 및 셀프 디버깅을 가능하게 하는 매우 혁신적인 도구 통합 긴 Chain-of-Thought(CoT) 추론 LLM입니다.
- ***Technical Details***: START의 핵심 혁신은 Hint-infer와 Hint Rejection Sampling Fine-Tuning(Hint-RFT)라는 두 가지 주요 기법으로 구성된 자기 학습 프레임워크입니다. Hint-infer는 LRM 추론 과정에서 인위적으로 설계된 힌트를 삽입하여 외부 도구를 사용할 수 있는 능력을 자극합니다. 또한, Hint-RFT는 Hint-infer와 RFT를 결합하여 힌트를 통해 생성된 도구 호출 추론 경로를 평가, 필터링 및 수정한 후, LRM 파인 튜닝을 통해 그 자체로 도구 사용을 학습하는 모델을 만듭니다.
- ***Performance Highlights***: START는 PhD급 과학 QA, 경쟁수준의 수학 벤치마크(AMC23, AIME24, AIME25), 그리고 경쟁수준 코드 벤치마크(LiveCodeBench)에서 각각 63.6%, 95.0%, 66.7%, 47.1% 및 47.3%의 정확성을 목표로 하여 뛰어난 성능을 입증하였습니다. 이는 기본 QwQ-32B보다 훨씬 뛰어난 성능을 발휘하며, 오픈 소스 R1-Distill-Qwen-32B 및 독점 모델인 o1-Preview와 비교할 만한 성능을 보여줍니다.

### [Token-Efficient Long Video Understanding for Multimodal LLMs](https://arxiv.org/abs/2503.04130)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.04130.png)

Vote: 39

Authors: Jindong Jiang, Xiuyu Li, Muyang Li, Guo Chen, Yao Lu, Song Han, Hongxu Yin, Zhiding Yu, Zhiqi Li, Jan Kautz, Wonmin Byeon, Kurt Keutzer, Sungjin Ahn, De-An Huang, Zhijian Liu, Guilin Liu

- ***What's New***: STORM(Spatiotemporal TOken Reduction for Multimodal LLMs)은 비디오에 페어별 시공간 정보를 통합하는 새로운 아키텍처로, 기존의 영상 프레임만 독립적으로 처리하던 문제점을 개선하여 장기간 비디오 이해를 향상시킵니다. 이는 비디오 시퀀스 전체에서 프레임 간 동적 패턴을 통합하여 더 효율적인 토큰 감소 전략을 가능하게 합니다.
- ***Technical Details***: STORM은 영상 인코더와 LLM 사이에 Mamba State Space Model 기반의 시공간 엔코더를 도입하며, 입출력 시퀀스 처리를 병렬화하여 시간 효율성을 극대화합니다. 토큰 압축은 훈련 시 공간 및 시간 풀링, 테스트 시 토큰 샘플링을 통해 이루어집니다. 이러한 기법은 모든 프레임의 세밀한 정보 손실 없이 LLM의 계산 요구를 줄이고 성능을 향상시킵니다.
- ***Performance Highlights***: STORM은 MLVU 및 LongVideoBench와 같은 여러 벤치마크에서 5% 넘는 성능 향상을 기록하며, 최대 8배의 연산 비용 절감과 2.4-2.9배의 디코딩 지연시간 감소를 달성하였습니다. 이로 인해 STORM은 광범위한 비디오 이해 작업에서 기존 메서드를 능가하는 결과를 보였습니다.

### [LLMVoX: Autoregressive Streaming Text-to-Speech Model for Any LLM](https://arxiv.org/abs/2503.04724)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.04724.png)

Vote: 33

Authors: Sambal Shikhar, Fahad Khan, Sahal Shaji Mullappilly, Salman Khan, Mohammed Irfan Kurpath, Jean Lahoud, Rao Muhammad Anwer, Hisham Cholakkal

- ***What's New***: LLMVoX는 어떠한 대형 언어 모델(LLM)과도 통합할 수 있는 경량의 자동회귀 스트리밍 텍스트-음성 변환(Text-to-Speech; TTS) 시스템으로, 베이스 LLM의 언어적 능력을 온전히 보존하면서 낮은 대기 시간으로 고품질의 음성을 생성합니다. 또한, 멀티큐 토큰 스트리밍 시스템을 통해 무제한 길이의 대화를 지원하며, 새로운 언어로의 일반화도 가능합니다.
- ***Technical Details***: LLMVoX는 30M 파라미터로 구성된 경량 Transformer 구조를 활용하여 디스크리트의 오디오 토큰을 생성합니다. 또한, 스트리밍 LLM 텍스트에서 자동회귀 방식으로 음성을 생성하며, 모델 재교육이나 파인튜닝 없이 기존 LLM 파이프라인에 쉽게 플러그앤플레이 방식으로 사용 가능합니다. 이 시스템은 다중 큐 스트리밍 접근법을 채택해 연속적이고 잠재적으로 무한히 긴 음성 생성이 가능하며, 475 밀리초의 낮은 레이턴시를 유지합니다.
- ***Performance Highlights***: LLMVoX는 높은 음질과 짧은 대기 시간으로 최첨단 음성 기반 LLM들과 비교해 뛰어난 성능을 보여줍니다. 특히, Word Error Rate(WER)는 3.70%로 가장 낮고, UTMOS는 4.05입니다. 아랍어로의 확장에서도 Character Error Rate(CER) 8%를 기록하며, 비스트리밍 아랍어 TTS 기법과 거의 동등한 성능을 발휘합니다.

### [EgoLife: Towards Egocentric Life Assistant](https://arxiv.org/abs/2503.03803)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.03803.png)

Vote: 21

Authors: Ziwei Liu, Yuanhan Zhang, Marco Cominelli, Bo Li, Fangzhou Hong, Xiamengwei Zhang, Shuai Liu, Francesco Gringoli, Bei Ouyang, Sicheng Zhang, Yuhao Dong, Hongming Guo, Zhongang Cai, Ziyue Wang, Zitang Zhou, Zhengyu Lin, Pengyun Wang, Peiyuan Zhang, Joerg Widmer, Jingkang Yang, Lei Yang, Binzhu Xie

- ***What's New***: EgoLife 프로젝트는 AI 기반의 웨어러블 안경을 통해 개인의 효율성을 높이는 자아중심적 생활 보조 장치를 개발하는 것을 목표로 합니다. 이를 위해 6명의 참가자가 일주일간 함께 생활하며, 그들의 일상 활동을 지속적으로 기록한 EgoLife 데이터셋을 구축했습니다. 이 데이터셋은 자아중심적이고 대인관계적이며 멀티뷰 및 멀티모달 일상생활 데이터를 포함합니다.
- ***Technical Details***: EgoLife는 자아중심 데이터의 강력한 시각-오디오 모델 개발, 신원 인식 가능, 광범위한 시간 정보를 기반으로 하는 장기 맥락 질문 답변을 가능하게 하는 주요 기술적 도전을 해결합니다. 이를 위해 EgoGPT와 EgoRAG로 구성된 통합 시스템인 EgoButler를 도입합니다. EgoGPT는 자아중심 데이터셋에 대해 훈련된 만능 모달 모델로, 자아중심 비디오 이해에서 최첨단 성능을 달성합니다. EgoRAG는 초장기 맥락 질문 답변을 지원하는 검색 기반 컴포넌트입니다.
- ***Performance Highlights***: EgoButler 시스템은 EgoGPT와 EgoRAG의 결합으로 구성되어 있으며, 이를 통해 개인화된 자아중심 데이터를 지속적으로 수집하고, 관련 단서를 회수하여 정확하고 맥락에 맞는 답변을 제공합니다. 이는 현재의 모델들이 자아중심 비디오 이해와 초장기 맥락 질문 답변에서 상당한 도전 과제를 안고 있음을 보여주며, 향후 연구의 방향성을 제시합니다.

### [LINGOLY-TOO: Disentangling Memorisation from Reasoning with Linguistic Templatisation and Orthographic Obfuscation](https://arxiv.org/abs/2503.02972)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.02972.png)

Vote: 19

Authors: Andrew Bean, Harry Mayne, Adam Mahdi, Simi Hellsten, Karolina Korgul, Vlad Neacs, Ryan Kearns, Jude Khouja, Lingyi Yang

- ***What's New***: LINGOLY-TOO는 대형 언어 모델(Large Language Models; LLMs)의 reasoning 평가에서 데이터 노출의 영향을 줄이기 위해 개발된 새로운 프레임워크에 기반을 두고 설계된 복잡한 언어 추론 벤치마크입니다. 오소그래피적 템플릿(Orthographic Templatisation)을 통해 실제 언어의 쓰기 체계를 동적으로 난독화하여 다양한 문제 변형을 생성하며, 모델 학습 데이터에 이 특정 문제 인스턴스가 등장할 가능성을 줄입니다.
- ***Technical Details***: 오소그래피적 규칙셋을 통해 UKLO 과거 문제에서 표준화된 82개의 문제에서 언어 문제를 난독화하여 LINGOLY-TOO를 개발했습니다. 각 문제는 공인 언어학자가 연구하여 설정됩니다. 이러한 변형은 문제의 논리적 단계를 유지하면서도 데이터를 노출시키지 않도록 설계되어 있습니다. 평균 정확한 매치 점수(Mean Exact Match Score)를 모형에 대한 주요 성능 메트릭으로 사용했습니다.
- ***Performance Highlights***: CLAUDE 3.7 Sonnet 모델은 난독화된 문제에서 평균 Mobf = 0.43의 점수를 기록하며 가장 높은 점수를 받았습니다. GPT-4o와 같은 독점 모델은 obfuscated 문제에서 퍼포먼스 저하를 경험하며, 특히 고자원 언어에 대한 문제에서는 큰 성능 저하가 관찰되었습니다(예: 핀란드어 문제에서 -0.36). 사람 평가 결과, obfuscation은 인간 평가 참가자의 점수에 약간의 영향을 미치는 것으로 나타났으며, 이는 기계와 인간 모두에게 제기되는 추가적 난관을 시사합니다.

### [LLM as a Broken Telephone: Iterative Generation Distorts Information](https://arxiv.org/abs/2502.20258)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.20258.png)

Vote: 15

Authors: Amr Mohamed, Michalis Vazirgiannis, Guokan Shang, Mingmeng Geng

- ***What's New***: 이 연구는 대형 언어 모델(Large Language Models; LLMs)이 인간의 '고장난 전화' 효과와 유사하게 정보 왜곡을 유발하는지를 조사합니다. 반복적인 세대 과정에서 정보 왜곡이 누적될 수 있으며, 이러한 왜곡은 언어 선택과 체인 복잡성에 따라 영향을 받습니다.
- ***Technical Details***: 이 연구는 번역 기반 실험을 통해 이러한 왜곡이 시간이 지남에 따라 어떻게 누적되는지 조사했습니다. 실험은 한 언어에서 다른 언어로 문서를 번역한 후 다시 원래 언어로 번역하는 방식으로 진행되었으며, 이를 통해 LLMs의 정보 왜곡 문제를 분석했습니다.
- ***Performance Highlights***: 번역 체인의 정보 왜곡은 중간 언어의 선택에 크게 의존하며, 이는 소스 언어와의 유사도 및 모델의 사전 및 후학습 코퍼스에서의 빈도에 의해 영향을 받습니다. 체인에 언어나 모델이 추가되면 왜곡이 증폭되며, 긴 체인은 반복적인 체인의 유형과 관계없이 더 많은 악화를 초래합니다. 그러나 이 왜곡은 온도 제어와 제한적인 프롬프트를 통해 완화될 수 있습니다.

### [HybridNorm: Towards Stable and Efficient Transformer Training via Hybrid Normalization](https://arxiv.org/abs/2503.04598)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.04598.png)

Vote: 12

Authors: Ya Wang, Xun Zhou, Jinwen Ma, Yutao Zeng, Xiaoqing Li, Zhijian Zhuo, Jian Yang, Sijun Zhang

- ***What's New***: HybridNorm은 프리-노름(Pre-Norm)과 포스트-노름(Post-Norm)을 결합한 새로운 하이브리드 정규화 전략으로, 트랜스포머(Transformer) 모델의 안정성과 성능을 향상시킵니다. 특히, LLMs(대형 언어 모델들)에서 안정된 학습과 높은 성능을 달성하며, 이는 과거의 프리-노름과 포스트-노름 접근 방식 모두를 능가합니다.
- ***Technical Details***: HybridNorm은 주의 메커니즘에서 QKV 정규화(QKV Normalization)를 적용하고 각 트랜스포머 블록의 피드포워드 네트워크(FFN)에는 포스트-노름을 사용합니다. 이 디자인은 각 층 사이의 정보 흐름을 안정화하고, 딥러닝 모델에서의 학습 안정성을 강화하며, 최종 모델의 성능을 향상시킵니다.
- ***Performance Highlights***: 실험에서는 HybridNorm이 프리-노름과 포스트-노름 각각을 지속적으로 초과하는 결과를 보였으며, 다양한 벤치마크에서 최첨단의 결과를 얻었습니다. 특히, HellaSwag와 ARC-Easy 같은 다운스트림 작업에서 가장 높은 성과를 기록하며, 하이브리드 접근 방식의 이점을 입증했습니다.

### [IFIR: A Comprehensive Benchmark for Evaluating Instruction-Following in Expert-Domain Information Retrieval](https://arxiv.org/abs/2503.04644)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.04644.png)

Vote: 10

Authors: Guo Gan, Mingsheng Shang, Tingyu Song, Yilun Zhao

- ***What's New***: IFIR는 전문 도메인 정보 검색에서 명령-따르기(instruction-following) 능력을 평가하는 최초의 종합 벤치마크입니다. 금융, 법률, 의료, 과학 문헌 등 4가지 전문 분야에서 8개의 하위 집합으로 구성된 2,426개의 고품질 예제를 포함하고 있습니다. IFIR은 다양한 복잡도의 명령을 포함하여 데이터 검색 모델의 명령-따르기 능력을 상세히 분석할 수 있게 합니다. 또한 모델 성능을 더 정확하고 신뢰성 있게 평가하기 위해 새로운 LLM 기반 평가 메서드인 INSTFOL을 제안합니다.
- ***Technical Details***: IFIR 벤치마크는 금융, 과학 문헌, 법률, 의료의 4가지 전문 분야를 다루며, 각 분야는 실제 정보 검색 시나리오를 대표하는 3단계로 구성되어 있습니다. 데이터셋의 높은 품질을 보장하기 위해 데이터 구축 과정에서 인간 전문가 검증을 실시했습니다. 또한 전통적인 평가 방법의 한계를 넘어 설명-따르기 성능을 더 정확하게 측정하기 위해 LLM 기반 메트릭인 INSTFOL을 도입했습니다.
- ***Performance Highlights***: BM25는 전문 분야에서 어휘 용어가 더 많아 상대적으로 좋은 성능을 보였으며, 현재의 명령-튜닝된 검색기들은 복잡한 설명에 대해 그다지 개선된 성능을 보여주지 않았습니다. 대부분의 모델이 설명의 복잡도가 증가할수록 성능 저하를 경험했습니다. LLM 기반 검색기들은 nDCG와 INSTFOL에서 더 견고한 성능을 나타내어, 전문 분야의 더 복잡한 검색 작업을 처리할 잠재력을 강조합니다.

### [FuseChat-3.0: Preference Optimization Meets Heterogeneous Model Fusion](https://arxiv.org/abs/2503.04222)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.04222.png)

Vote: 9

Authors: Canbin Huang, Guosheng Liang, Longguang Zhong, Ziyi Yang, Fanqi Wan, Xiaojun Quan

- ***What's New***: FuseChat-3.0은 이질적인 소스 대형 언어 모델(Large Language Models; LLMs)의 강점을 결합하여 더 컴팩트한 타겟 모델로 개발된 LLMs 제품군입니다. 이 연구는 다양한 소스 모델에서 생성된 출력을 통해 타겟 LLM이 학습할 수 있도록 하는 암묵적 모델 융합(Imlicit Model Fusion; IMF)을 소개합니다.
- ***Technical Details***: FuseChat-3.0의 학습 파이프라인은 두 가지 주요 단계로 구성됩니다. 첫 번째는 지도 세부 조정(Supervised Fine-Tuning; SFT)으로, 이는 소스 모델과 타겟 모델의 분포를 맞추는 것이 목적입니다. 두 번째는 직접 선호 최적화(Direct Preference Optimization; DPO) 단계로서, 같은 소스 모델에서 생성된 응답 중 최고와 최악의 것을 쌍(pair)으로 구성하여 타겟 모델의 성능을 미세 조정합니다.
- ***Performance Highlights***: FuseChat-3.0은 14개의 벤치마크에서 평균 6.8점의 개선을 이루었고, 특히 AlpacaEval-2에서 37.1점, Arena-Hard에서 30.1점의 두드러진 상승을 보여주었습니다. 이는 타겟 모델이 원래의 성능을 능가하면서도 작은 모델에서도 대형 모델에 근접한 성능을 낼 수 있음을 시사합니다.

### [L^2M: Mutual Information Scaling Law for Long-Context Language Modeling](https://arxiv.org/abs/2503.04725)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.04725.png)

Vote: 9

Authors: Oriol Mayné i Comas, Zhuotao Jin, Di Luo, Zhuo Chen, Marin Soljačić

- ***What's New***: 이 논문은 자연어 내의 장거리 의존성을 지배하는 이분 상호 정보 확장 법칙(Bipartite Mutual Information Scaling Law)을 공식화하고, 장문맥 언어 모델링(Long-context Language Modeling)의 이해를 돕습니다. 이를 기반으로 모델의 과거 정보 저장 능력이 장문맥 길이 모델링에 어떻게 영향을 미치는지를 설명하는 L2M 조건을 소개하며, 이를 실험으로 검증합니다.
- ***Technical Details***: 이 연구는 완화된 힐베르크 추측(Relaxed Hilberg Conjecture)을 기반으로 하여, 자연어 데이터셋에서 이분 상호 정보를 측정하고, 이를 통해 장문맥 언어 모델링에 대한 L2M 조건을 수립하였습니다. 이 조건은 모델의 숨겨진 상태 크기가 상태 간 상호 정보(Bipartite Mutual Information)의 성장보다 빠르게 확장되어야 함을 증명합니다. 실험적으로는 state-of-the-art LLMs와 변환자(Transformer) 및 상태 공간 모델(State Space Models)을 사용하여 이를 검증하였습니다.
- ***Performance Highlights***: 실험 결과, 제안한 상호 정보 법칙은 LLaMA와 DeepSeek 모델을 사용하는 다양한 자연어 데이터셋에서 일관된 척도 성장 패턴을 관찰하였습니다. 특히, GPT2 모델은 Mamba 모델과 비교할 때 긴 문맥 길이를 효과적으로 처리할 수 있음을 보여주었으며, 이는 L2M 조건이 예측한 바와 일치합니다.

### [PokéChamp: an Expert-level Minimax Language Agent](https://arxiv.org/abs/2503.04094)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.04094.png)

Vote: 8

Authors: Andy Luu Nguyen, Seth Karten, Chi Jin

- ***What's New***: PokéChamp는 포켓몬 배틀을 위해 개발된 대형 언어 모델(LLM) 기반의 새로운 미니맥스 에이전트입니다. 이 에이전트는 플레이어 행동 샘플링, 상대 모델링, 그리고 가치 함수 추정을 위한 세 가지 주요 모듈을 LLM으로 대체하여 포켓몬 배틀의 복잡성을 효과적으로 처리합니다.
- ***Technical Details***: PokéChamp는 LLM을 활용하여 미니맥스 트리 탐색 알고리즘을 강화합니다. 구체적으로는 플레이어 행동 샘플링, 상대방 행동 예측, 그리고 게임 상태 가치 평가에 LLM을 사용합니다. LLM은 블랙박스로 기능하며, 추가적인 학습이 필요 없습니다. 이 접근 방식은 포켓몬의 게임 전환을 근사하여 부분 관측성 문제를 해결하고, 실제 플레이어 게임 데이터를 활용한 세계 모델을 구축합니다.
- ***Performance Highlights***: PokéChamp는 현재까지 가장 강력한 LLM 기반 봇과 상관없이 76%의 승률을 기록했으며, 가장 강력한 규칙 기반 봇에 대해서는 84%의 승률을 기록했습니다. 오픈 소스 8억 파라미터 Llama 3.1 모델을 사용할 때, 이전의 최고 LLM 기반 봇에 비해서도 지속적으로 더 높은 성과를 보였습니다. PokéChamp는 온라인 래더에서 1300-1500의 Elo 등급을 달성하여 상위 30%-10%의 인간 플레이어와 경쟁하는 수준에 도달했습니다.

### [Audio Flamingo 2: An Audio-Language Model with Long-Audio Understanding and Expert Reasoning Abilities](https://arxiv.org/abs/2503.03983)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.03983.png)

Vote: 7

Authors: S Sakshi, Dinesh Manocha, Sreyan Ghosh, Zhifeng Kong, Jaehyeon Kim, Bryan Catanzaro, Wei Ping, Rafael Valle, Sonal Kumar

- ***What's New***: Audio Flamingo 2 (AF2)는 오디오 이해와 추론 능력을 향상한 새로운 오디오-언어 모델(Audio-Language Model; ALM)입니다. 최신 성과를 자랑하며, 3억 파라미터의 소형 모델로도 20개 이상의 벤치마크에서 대형 오픈 소스 및 독점 모델을 능가했습니다. 그리고 처음으로 긴 오디오 구간(30초~5분)의 이해와 관련된 LongAudio 데이터셋을 제안했습니다.
- ***Technical Details***: AF2는 커스텀 CLAP 모델과 합성 오디오 질문-답변(Audio QA) 데이터를 이용해 정교한 오디오 추론을 수행하며, 3단계 커리큘럼 학습 전략을 채택합니다. LongAudio란 새로운 데이터셋을 통해 긴 오디오 캡션과 QA 작업에 대한 모델 훈련을 돕고, 이로 인해 긴 오디오 이해 평가를 위한 LongAudioBench를 제안했습니다.
- ***Performance Highlights***: AF2는 이전 SOTA 모델과 비교하여 20개 이상의 벤치마크에서 모든 기준 모델을 능가했습니다. 특히, 긴 오디오에 대한 이해를 평가하는 LongAudioBench에서는 경쟁 모델에 비해 18.9%의 성능 향상을 보였습니다. 실험 결과는 AF2가 오디오 추론 능력을 획기적으로 강화했음을 입증해 줍니다.

### [Dedicated Feedback and Edit Models Empower Inference-Time Scaling for Open-Ended General-Domain Tasks](https://arxiv.org/abs/2503.04378)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.04378.png)

Vote: 6

Authors: Jiaqi Zeng, Ellie Evans, Zhilin Wang, Felipe Soares, Olivier Delalleau, Oleksii Kuchaiev, Hoo-Chang Shin, Yi Dong, Daniel Egert

- ***What's New***: 이 논문에서는 전용 Feedback 모델과 Edit 모델을 도입하여 Inference-Time Scaling을 개방형 일반 도메인 작업에 적용했습니다. 이를 통해 첫 번째 모델이 응답을 생성하고, 두 번째 모델이 피드백을 제공하며, 세 번째 모델이 피드백을 바탕으로 응답을 수정하는 새로운 접근법을 제안합니다.
- ***Technical Details***: Feedback와 Edit 모델은 다양한 도메인의 복합적이고 개방형 과업을 다루기 위해 80개 이상의 지역에서 수집된 데이터를 기반으로 학습되었습니다. 이 시스템은 Llama 3 모델군의 70B 모델을 기반으로 하며, 각 프로세스의 최적화를 통해 응답을 생성합니다.
- ***Performance Highlights***: 이 접근법은 Arena Hard 벤치마크에서 92.7의 성과를 기록하며 SoTA(State of the Art) 성능을 달성했습니다. 이는 OpenAI o1-preview-2024-09-12의 90.4와 DeepSeek R1의 92.3을 상회하는 결과입니다. Llama 3 모델을 활용한 피드백과 수정 루프를 통해 모델 성능이 크게 향상되었습니다.

### [How to Steer LLM Latents for Hallucination Detection?](https://arxiv.org/abs/2503.01917)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01917.png)

Vote: 6

Authors: Xuefeng Du, Min-Hsuan Yeh, Seongheon Park, Haobo Wang, Yixuan Li

- **What's New**: 이 연구에서는 LLM(Large Language Models)의 생성 공간에서 할루시네이션을 탐지하기 위한 새로운 방법인 Truthfulness Separator Vector (TSV)를 제안합니다. TSV는 모델 매개변수를 변경하지 않고 추론 중에 LLM의 표현 공간을 조정함으로써 진실한 출력과 할루시네이션 출력 간의 구분을 향상시킵니다.
- **Technical Details**: TSV는 두 단계 프레임워크로 구성됩니다. 첫 단계에서는 소량의 레이블 제공 예제 집합을 사용하여 TSV를 훈련시켜 잘 분리된 클러스터를 형성합니다. 다음 단계에서는 unlabeled LLM 생성물을 활용하여 최적 운송 기반 알고리즘으로 pseudo-labeling을 수행하고 이를 통해 데이터 세트를 확장합니다. 이 과정에서 confidence 기반의 샘플 필터링 방법이 사용됩니다.
- **Performance Highlights**: TSV는 다양한 데이터세트에서 뛰어난 성능을 발휘하며, 특히 TruthfulQA 벤치마크에서 경쟁 방법들에 비해 할루시네이션 탐지 정확도 (AUROC) 지표에서 12.8%의 향상을 달성했습니다. 이 방법은 적은 수의 레이블로도 강력한 일반화 능력을 보여주며, 실용적인 LLM 응용에 대한 실질적인 해법을 제공합니다.

### [Identifying Sensitive Weights via Post-quantization Integral](https://arxiv.org/abs/2503.01901)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01901.png)

Vote: 5

Authors: Jianfei Chen, Zichen Liang, Jun Zhu, Weiyu Huang, Chang Chen, Jintao Zhang, Yuezhou Hu

- ***What's New***: 이 연구는 LLMs(Large Language Models)의 효율적인 서빙 문제를 해결하기 위해 PQI(Post-quantization Integral)를 제안합니다. 기존의 민감도 메트릭이 정확하지 않다는 문제점을 밝히고, PQI를 통해 정량화된 가중치의 영향을 보다 정확하게 예측할 수 있도록 했습니다.
- ***Technical Details***: PQI는 정량화 후 모델의 민감도를 정확하게 예측하는 새로운 메트릭입니다. 기존의 2차 근사법이 가지고 있는 수렴 반경의 제약을 극복하기 위해, PQI는 w와 ˜w 사이의 경로를 작은 조각으로 나누어 각 조각의 테일러 근사를 사용합니다. 이러한 자세한 예측 메트릭을 활용하여 ReQuant를 제안하였으며, 이는 크게 Dense-and-Sparse 분해로 구성되어 있습니다. 이 과정은 자가 적응형 outlier 선택과 단계별 중요한 가중치 분리를 포함합니다.
- ***Performance Highlights***: ReQuant는 최신 상태의 LLM을 4-bit로 압축할 때, Llama 3.2 1B 모델에서 일반 perplexity 감소를 2.6까지 줄일 수 있으며, 기반선 모델인 SqueezeLLM과 QTIP와 비교하여 수학적 문제 해결에서 거의 3% 향상을 보여주었습니다. 이는 저비트 정량화와 정확도 유지 사이의 개선을 의미합니다.

### [Union of Experts: Adapting Hierarchical Routing to Equivalently Decomposed Transformer](https://arxiv.org/abs/2503.02495)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.02495.png)

Vote: 4

Authors: Linhui Li, Yujiao Yang, Jing Lian

- ***What's New***: 이 논문에서는 Transformers를 효율적으로 분해하여 여러 전문가 그룹으로 구성하는 새로운 MoE 프레임워크, Union-of-Experts(UoE)를 제안합니다. 이 방법은 복잡한 네트워크 모델을 유지하면서 효율적인 전문가 통합과 협업을 촉진하여 성능 향상을 목표로 합니다.
- ***Technical Details***: UoE는 MLP와 Attention 블록을 등에 기반하여 전문가로 분해하는 구조를 가지며, 선택적 라우팅(Selective Routing) 메커니즘을 통해 입력 데이터를 최적으로 할당합니다. 여기에는 선택적 다중 헤드 어텐션(Selective Multi-Head Attention, SMHA)과 MLP 전문가의 결합체인 Union-of-MLP-Experts(UoME)가 포함됩니다. 병렬 처리 최적화를 통해 효율성을 향상시켰습니다.
- ***Performance Highlights***: UoE 모델은 언어 모델링 및 이미지 분류 등 여러 작업에서 동시에 여러 최첨단 MoE 및 효율적인 Transformer 모델을 능가하는 성능을 보였습니다. 특히 Wikitext-103 및 One Billion Word 데이터셋에서 Perplexity(성능 지표)를 24.09까지 낮추고, Long Range Arena 벤치마크에서는 다른 비교 모델보다 평균 정확도가 0.68% 더 높았습니다. 이 모든 것을 50%의 FLOPs로 달성하며 효율성 측면에서도 뛰어납니다.

### [The Best of Both Worlds: Integrating Language Models and Diffusion Models for Video Generation](https://arxiv.org/abs/2503.04606)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.04606.png)

Vote: 4

Authors: Juncheng Li, Kai Shen, Yichong Leng, Xu Tan, Xinyu Zhou, Aoxiong Yin, Siliang Tang

- ***What's New***: LanDiff는 언어 모델(Language Models; LLMs)과 확산 모델(Diffusion Models)의 장점을 결합한 비디오 생성 프레임워크로, 텍스트-비디오(T2V) 생성 분야에서 전례 없는 성능을 보여줍니다. 이 연구는 약 14,000배의 압축 비율을 달성하는 효율적인 의미 압축을 통해 3D 시각적 특징을 1D 이산 표현으로 변환하는 시맨틱 토크나이저(Semantic Tokenizer)를 도입합니다.
- ***Technical Details***: LanDiff의 구조는 LLM이 높은 수준의 시맨틱 정보를 기반으로 시맨틱 토큰을 생성한 후, 스트리밍 확산 모델이 이를 고화질 비디오로 완성하는 이단계 절차를 따릅니다. 시맨틱 토크나이저에 의해 1D 시각적 특징으로 압축된 3D 비디오 특징이 사용되며, Theia 모델로 혁신적인 토큰화 메커니즘을 설계하여 MP4 비디오 코딩 알고리즘에서 영감을 얻은 프레임 그룹핑 전략을 통해 시계열 중복성을 최소화했습니다.
- ***Performance Highlights***: LanDiff는 VBench T2V 벤치마크에서 85.43점을 기록하여, 13B 모델인 Hunyuan Video와 같은 최신 오픈 소스 모델보다 높은 성능을 보였습니다. 또한, 장시간 비디오 생성에서도 다른 오픈 소스 모델을 능가하며, 전반적인 일관성과 미적 품질에서 우수성을 입증했습니다.

### [Understanding and Predicting Derailment in Toxic Conversations on GitHub](https://arxiv.org/abs/2503.02191)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.02191.png)

Vote: 3

Authors: Robert Zita, Rahat Rizvi Rahman, Mia Mohammad Imran, Kostadin Damevski, Preetha Chatterjee, Rebekah Copeland

- ***What's New***: 이 연구는 GitHub에서의 유독한 대화가 어떻게 탈선(Derail)하여 독성(Toxicity)으로 전환되는지를 이해하고 이를 예측하는 방법에 관한 것입니다. 대화 탈선을 조기에 탐지하고 문제를 방지하는 새로운 모델을 제안하여, 잠재적으로 유해한 대화를 자동으로 감지하고 대처하는 방법을 제시합니다.
- ***Technical Details***: 연구진은 GitHub에서 202개의 유독한 대화를 포함한 새로운 데이터셋을 생성하여 분석을 수행했습니다. 이를 통해 대화 탈선 지점과 각 유독한 대화의 특징을 파악했습니다. LLM(대형 언어 모델)을 활용하여 Summaries of Conversation Dynamics (SCD)를 생성함으로써, 대화 진행 상황을 요약하고 탈선의 초기 징후를 포착하는 방법을 개발했습니다. LLaMA-3.1-70B 모델을 사용하여 이 요약을 기반으로 대화의 독성으로의 전환 가능성을 예측했습니다.
- ***Performance Highlights***: 제안된 방법론은 대화의 탈선 예측에서 69%의 F1-score를 기록하며, 이는 기존의 CRAFT 모델과 다른 기본 접근 방식에 비해 크게 개선된 결과입니다. 특히 새롭게 제안된 'Least-to-Most SCD' 프롬프트는 높은 정밀도(precision)를 보여주었으며, 이는 실제 응용 상황에서 비독성 대화가 독성으로 잘못 예측되는 비율을 줄이는데 중요한 역할을 합니다.

### [Lost in Literalism: How Supervised Training Shapes Translationese in LLMs](https://arxiv.org/abs/2503.04369)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.04369.png)

Vote: 3

Authors: Ronghao Zhang, Yue Zhang, Huajian Zhang, Zhilin Wang, Tong Xiao, Yafu Li, Yongjing Yin, Leyang Cui

- ***What's New***: 이 연구는 번역 모델에서 발생하는 '번역체(Translationese)' 문제를 대형 언어 모델(LLM)을 대상으로 체계적으로 분석하며, 감독된 미세조정(Supervised Fine-Tuning; SFT) 과정에서 발생하는 번역체 오류를 진단하고 해결 방안을 제시합니다. 연구진은 번역체 문제를 완화할 수 있는 방법으로 '황금 참조(Golden References)' 다듬기와 비자연스러운 학습 데이터 필터링을 제안합니다.
- ***Technical Details***: LLM 기반 번역 시스템에서 번역체 오류를 평가하기 위해 전문가 번역자를 활용하여 번역 오류 유형을 식별, 분석하도록 하였습니다. 그런 다음 번역 오류 비율(Translationese Span Ratio; TSR)을 측정하고, 비자연스러운 번역을 포함하는 구간을 특정하여 평균값을 제공하는 방식으로 번역체의 유례를 측정했습니다. 이 연구에서는 GPT-4를 사용해 기존 번역을 다듬어서 학습 데이터를 정제하고, 이를 통해 번역 자연스러움을 개선시키기 위한 방법을 제안했습니다.
- ***Performance Highlights***: 모든 LLM들이 영어-중국어 및 독일어-영어 번역에서 대체로 높은 번역체 오류를 보였으며, 특히 GPT-4의 번역 결과 중 40% 이상이 번역체 양상을 나타냈습니다. 번역체 비율(TSR)과 언어 모델이 예측한 당혹도(Perplexity) 간 긍정적인 상관관계가 있음을 확인했고, 이는 번역체 오류가 존재하면 당혹도가 높아진다는 것을 의미합니다. LLM을 통해 다듬은 번역은 자연스러움이 향상됨을 여러 자동 및 사람 평가를 통해 확인했습니다. 이 연구는 번역 자연스러움을 높이는 데 있어서 중요한 기여를 할 것으로 보입니다.

### [Combining Flow Matching and Transformers for Efficient Solution of Bayesian Inverse Problems](https://arxiv.org/abs/2503.01375)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01375.png)

Vote: 2

Authors: Daniil Sherki, Ivan Oseledets, Ekaterina Muravleva

- ***What's New***: 이 연구는 Conditional Flow Matching (CFM)와 Transformer 아키텍처를 결합하여 Bayesian Inverse Problems의 효율적인 해결 방안을 제시합니다. 다양한 관측 수를 처리할 수 있는 CFM 기반의 Transformer 아키텍처는 전통적 샘플링 방식에 비해 빠르고 효율적으로 후방 확률 분포를 추정할 수 있게 합니다.
- ***Technical Details***: 이 연구는 CFM-Tr(Conditional Flow Matching과 Transformer) 아키텍처를 제안하여 구체적이지 않은 관측 수에도 적응할 수 있도록 설계되었습니다. 이를 위해 각 샘플이 가진 관측 수에 맞추어 데이터셋을 생성하고 배치 훈련을 통해 적절한 속도 필드를 학습합니다. Transformer의 다방향 주의집중 기법을 활용하여 자유롭게 모델을 확장할 수 있으며, RoPE(Rotary Position Embedding)를 활용하여 학습하지 않은 길이의 연속 데이터에 일반화할 수 있는 능력을 갖췄습니다.
- ***Performance Highlights***: CFM-Tr는 SEIR 및 유체 투과성 필드 역문제에서 기존의 MCMC와 비교하여 상당한 성능과 효율성을 보여주었습니다. MCMC가 유사한 정확도를 달성하는데 많은 시간이 소요되는 반면, CFM-Tr는 상대적으로 짧은 시간에 8% 이하의 오류율을 기록하며 빠른 추론을 가능케 했습니다. 특히, CFM-Tr는 관측 수가 증가함에 따라 오류율이 꾸준히 감소하는 모습을 보여줍니다.

### [On the Acquisition of Shared Grammatical Representations in Bilingual Language Models](https://arxiv.org/abs/2503.03962)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.03962.png)

Vote: 0

Authors: Catherine Arnett, Benjamin K. Bergen, Tyler A. Chang, James A. Michaelov

- ***What's New***: 이 연구는 다국어 언어 모델(Multilingual Language Models)이 다국어 전이(crosslingual transfer)를 가능하게 하는 공유된 문법적 표현(shared grammatical representations)을 어떻게 획득하는지를 조사합니다. 이를 위해 이중 언어 모델(bilingual models)을 훈련하여 언어 데이터의 양과 노출 순서를 제어하고, 인간의 문법적 표현을 연구하는데 사용되는 구조적 프라이밍(structural priming)을 활용합니다.
- ***Technical Details***: 이중 언어 모델은 오토회귀 GPT-2 Transformer 언어 모델로, 124M 파라미터를 가집니다. 각각의 언어에 대해 별도의 SentencePiece tokenizer를 사용하며, 각 언어별 2억 개의 토큰을 샘플링하여 훈련합니다. 훈련 세션 중간에 두 번째 언어(L2)를 도입하여 구조적 프라이밍 효과의 시간적 연관성을 조사합니다.
- ***Performance Highlights***: 훈련 결과, 두 언어가 서로 매우 유사할 때(Dutch-English, Spanish-English) 구조적 프라이밍 효과가 더 강하게 나타났습니다. 대조적으로, 언어적 유사성이 낮은 Polish-English 및 Greek-English 쌍에서는 효과가 덜 두드러졌습니다. 이는 유사한 언어들 사이에서 다국어 전이가 더욱 효과적임을 시사합니다.

