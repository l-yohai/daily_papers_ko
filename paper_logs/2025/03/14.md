## Daily Papers (2025-03-14)

### [CoSTAast: Cost-Sensitive Toolpath Agent for Multi-turn Image Editing](https://arxiv.org/abs/2503.10613)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10613.png)

Vote: 51

Authors: Advait Gupta, NandaKiran Velaga, Dang Nguyen, Tianyi Zhou

- ***What's New***: CoSTA∗는 멀티턴 이미지 편집(Multi-turn Image Editing)을 위한 비용 민감형 툴패스 에이전트(Cost-Sensitive Toolpath Agent)로서, 대규모 언어 모델(LLM)과 A* 탐색 알고리즘의 장점을 결합하여 복합적인 이미지 편집 작업에서 최적의 툴패스를 찾아내고, 비용과 품질의 균형을 조절합니다.
- ***Technical Details***: CoSTA∗는 먼저 LLM을 사용하여 주어진 작업을 위해 하위 작업 트리(Subtask Tree)를 생성하여 이를 바탕으로 AI 도구의 그래프에서 불필요한 부분을 가지치기하고, 사용자가 필요로 하는 품질-비용 균형을 위한 작은 서브그래프에서 A* 탐색을 실행합니다. 각 하위 작업의 출력은 비전-언어 모델(VLM)로 평가되며, 실패 시 툴의 비용과 품질이 업데이트되어 A* 탐색이 다른 경로를 신속하게 탐색할 수 있도록 합니다. CoSTA∗는 또한 자동으로 하위 작업 동안 모드 간 전환을 통해 비용 및 품질의 균형을 조절합니다.
- ***Performance Highlights***: CoSTA∗는 새로운 멀티턴 이미지 편집 벤치마크에서 최첨단 이미지 편집 모델들보다 비용 및 품질 면에서 더 우수한 성능을 보였으며, 사용자 선호에 따른 다양한 트레이드오프를 수행할 수 있습니다. 다양한 복잡성을 갖는 작업에 대한 CoSTA∗의 전반적인 정확도는 이미지 작업에서 94%, 텍스트+이미지 작업에서 93%를 기록하여, 기존 방법론 대비 우수한 효율성을 증명했습니다.

### [Transformers without Normalization](https://arxiv.org/abs/2503.10622)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10622.png)

Vote: 42

Authors: Xinlei Chen, Yann LeCun, Zhuang Liu, Kaiming He, Jiachen Zhu

- ***What's New***: 이 논문은 정규화(Normalization) 층 없이도 Transformer 모델이 높은 성능을 달성할 수 있음을 보여줍니다. 이를 위해 Dynamic Tanh(DyT)라는 간단한 대안을 제안하였으며, 이는 정규화 층을 대신하여 적용됩니다. DyT는 Tanh 함수 형태의 입력-출력 매핑으로 동작하여, 정규화의 역할을 필요 없이 수행할 수 있도록 설계되었습니다.
- ***Technical Details***: Dynamic Tanh(DyT)는 요소 단위 연산으로 DyT(x) = tanh(αx)로 정의되고, α는 학습 가능한 파라미터입니다. 이는 Transformer의 정규화 층을 대체하여 다양한 환경에서 높은 성능을 보여줍니다. DyT는 시각 및 언어 모델 모두에 적용 가능하며, 특정 조건에서는 하이퍼파라미터 튜닝 없이도 안정적인 학습 성능을 보입니다. DyT는 정규화 층과 달리 활성화 통계를 계산할 필요 없이 효과를 달성합니다.
- ***Performance Highlights***: 여러 실험에서 DyT는 정규화 층을 이용한 일반적인 모델들과 유사하거나 그 이상의 성능을 보였습니다. 또한, DyT는 향상된 학습 및 추론 속도를 제공하였습니다. LLaMA 모델에서 DyT를 사용한 경우 RMSNorm 대비 유사한 손실 값과 제로샷 성능을 기록하였습니다. ViT 및 ConvNeXt 모델 실험에서도 DyT는 정규화 대신 사용되어 안정적인 학습과 우수한 성능을 발휘하였습니다.

### [Charting and Navigating Hugging Face's Model Atlas](https://arxiv.org/abs/2503.10633)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10633.png)

Vote: 35

Authors: Yedid Hoshen, Nitzan Kurer, Jonathan Kahana, Eliahu Horwitz, Liel Amar

- ***What's New***: 이 논문에서는 Hugging Face의 모델 아틀라스를 차트화하고 탐색하는 새로운 방법을 제안합니다. 이 아틀라스는 대형 모델 저장소의 구조적 연결성을 시각화하며, 특히 문서화되지 않은 구역을 추적하여 모델의 잠재력을 최대한 활용할 수 있는 기반을 제공합니다.
- ***Technical Details***: Hugging Face 모델 아틀라스는 Directed Acyclic Graph (DAG)를 사용하여 모델 간의 구조적 관계를 담고 있으며, 노드를 개별 모델로, 방향성 있는 에지(edge)를 모델 변환(예: 파인튜닝, 양자화)의 표현으로 활용합니다. 이 논문은 기존의 트리 기반 접근이 아닌 DAG 구조를 기반으로 하여, 실제 세계의 데이터를 반영한 새로운 모델 아틀라스 구축 방법론을 소개합니다.
- ***Performance Highlights***: 이 시스템은 Hugging Face에서 약 40만 개의 모델을 분석하며, 90% 이상의 정확도를 기록했습니다. 특히, 이 시스템은 기존 방법 대비 상당한 성능 향상을 보였으며, 노드 및 엣지 예측 정확도에서 앞섰습니다. 제시된 방법론은 실제 데이터 세트에서의 유용성을 증명합니다.

### [World Modeling Makes a Better Planner: Dual Preference Optimization for Embodied Task Planning](https://arxiv.org/abs/2503.10480)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10480.png)

Vote: 33

Authors: Qinyuan Cheng, Panpan Cai, Xipeng Qiu, Jinlan Fu, Shiduo Zhang, Siyin Wang, Zhaoye Fei

- ***What's New***: 이 논문에서는 현재의 대형 비전-언어 모델(Large Vision-Language Models; LVLMs)에서 발생하는 계획의 비효율성과 환경 동적성을 이해하는 데 필요한 한계를 극복하기 위한 새로운 학습 프레임워크, Dual Preference Optimization (D²PO)를 제안합니다. D²PO는 상태 예측과 행동 선택을 함께 최적화하여 세계 모델링 세계 모델링을 학습함으로써 LVLMs의 계획 능력을 강화합니다.
- ***Technical Details***: Dual Preference Optimization (D²PO) 프레임워크는 상태 예측과 행동 선택을 선호 학습(preference learning)을 통해 함께 최적화합니다. 이를 위해 시뮬레이션 환경 내에서 시도와 오류를 통해 자동으로 궤적과 단계우선 선호 데이터를 수집하는 트리 탐색 메커니즘을 소개합니다. 이러한 접근은 인간 주석 없이 다양한 신체적 상호작용 경험을 효율적으로 수집합니다.
- ***Performance Highlights***: VoTa-Bench에서의 실험에서, 7B-파라미터 모델이 성공률(success rate) 31.4%와 계획 효율성 33.0%의 상대적 향상을 보이며, 이 접근 방식의 효과와 잠재력을 강조했습니다. D²PO 기반 방법은 기존의 방식과 GPT-4o를 능가하며, 다양한 테스트 유형에서 탁월한 성과를 거두었습니다.

### [Silent Branding Attack: Trigger-free Data Poisoning Attack on Text-to-Image Diffusion Models](https://arxiv.org/abs/2503.09669)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.09669.png)

Vote: 28

Authors: June Suk Choi, Sung Ju Hwang, Kimin Lee, Sangwon Jang, Jaehyeong Jo

- ***What's New***: 이 논문은 이미지 생성 모델(Text-to-Image Diffusion Models)에 특정 브랜드 로고를 추가하는 새로운 데이터 중독 공격(Silent Branding Attack)을 소개합니다. 이 방법은 텍스트 트리거 없이 로고를 자연스럽게 삽입할 수 있는 기법을 제안합니다.
- ***Technical Details***: 공격은 원본 이미지에 로고를 섬세하게 삽입하여 데이터셋을 중독시킵니다. 이 과정은 마스크 생성, 로고 감지 및 감염 이미지 생성을 포함한 단계로 구성됩니다. 모델은 손상된 데이터셋으로 훈련되어, 트리거 없는 텍스트 프롬프트로도 대상 로고를 포함한 이미지를 생성합니다.
- ***Performance Highlights***: 실험 결과, 이 방법은 별도의 텍스트 트리거 없이 두 현장 설정에서 높은 성공률을 달성했습니다. 사람 평가와 수치 지표는 이 방법이 로고를 이미지에 매끄럽게 삽입하면서도 이미지 품질이나 텍스트 정렬을 해치지 않음을 보여줍니다.

### [CoRe^2: Collect, Reflect and Refine to Generate Better and Faster](https://arxiv.org/abs/2503.09662)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.09662.png)

Vote: 27

Authors: Lichen Bai, Dian Xie, Yuetong Fang, Zeke Xie, Tian Ye, Zikai Zhou, Shitong Shao

- ***What's New***: CoRe²는 최적의 데이터 수집(Collect), 반영(Reflect), 개선(Refine)을 통해 더 나은 텍스트-이미지 생성 모델을 구현하는 최초의 플러그-앤-플레이 방식의 추론 프레임워크입니다. 다양한 모델과 함께 동작하며, 특히 기존의 큰 시각적 자기회귀 모델(ARMs)과 확산 모델(DMs) 모두에서 뛰어난 효율성과 성능을 보여주고 있습니다.
- ***Technical Details***: CoRe²는 세 가지 주요 단계를 거칩니다: 데이터 수집 단계에서 '분류기 없는 가이드' (Classifier-Free Guidance; CFG)를 활용해 쉽게 학습 가능한 내용의 데이터셋을 구축하고, 이를 기반으로 하여 '약한 모델'을 훈련시킵니다. 두 번째 반영 단계에서는 '약한 모델'을 통해 제한된 용량 내에서 쉽게 학습 가능한 데이터만을 반영하게 하고, 마지막 개선 단계에서는 W2S 가이드를 통해 고주파수와 실제적인 이미지를 생성, 모델 성능을 개선합니다.
- ***Performance Highlights***: CoRe²는 SDXL과 SD3.5에서 각각 2.89초, 1.31초의 시간 단축을 통해 성능을 발휘하였으며, 동일한 GPU 대기 시간에서 HPS v2 점수가 각각 0.47과 0.39 만큼 향상되었습니다. 또한 플럭스(FLUX) 모델에서는 '약한'과 '강한' 모델을 동시에 사용하여 W2S 가이드를 활용한 뛰어난 성능을 선보였습니다.

### [GoT: Unleashing Reasoning Capability of Multimodal Large Language Model for Visual Generation and Editing](https://arxiv.org/abs/2503.10639)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10639.png)

Vote: 24

Authors: Kun Wang, Shilin Yan, Linjiang Huang, Xingyu Zeng, Hao Tian, Chengqi Duan, Xihui Liu, Jifeng Dai, Hongsheng Li, Hao Li, Rui Zhao, Rongyao Fang

- ***What's New***: GoT는 시각적 생성 및 편집을 위한 다중 모달 대형 언어 모델(Multimodal Large Language Model)의 추론 능력을 활용하도록 설계된 새로운 패러다임을 제시합니다. 이 접근 방식을 통해 이미지를 출력하기 전에 명시적인 언어 추론 과정을 거쳐 시각적 생성 및 편집을 수행할 수 있습니다.
- ***Technical Details***: GoT는 시맨틱-공간적(Semantic-Spatial) 추론 체인을 활용하는 멀티모달 추론 체인으로, 이미지 생성과 편집 작업을 강화합니다. 이 프레임워크는 Qwen2.5-VL의 추론 체인 생성을 통합한 통합 엔드 투 엔드(diffusion model) 모델을 구현합니다. 이를 통해 사용자 상호작용의 정확한 이미지 조정을 가능케 합니다.
- ***Performance Highlights***: GoT 프레임워크는 생성 및 편집 작업 모두에서 우수한 성능을 발휘하며, 기존의 베이스라인을 능가하는 성능 개선을 보여줍니다. 우리의 실험은 GoT가 텍스트에서 이미지로의 생성 및 이미지 편집 정확성에서 큰 개선을 이뤘음을 나타냅니다.

### [VisualPRM: An Effective Process Reward Model for Multimodal Reasoning](https://arxiv.org/abs/2503.10291)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10291.png)

Vote: 20

Authors: Weiyun Wang, Jinguo Zhu, Wenhai Wang, Xiangyu Zhao, Zhe Chen, Zhangwei Gao, Yue Cao, Lewei Lu, Lianjie Chen, Jifeng Dai, Yangzhou Liu, Xizhou Zhu, Haodong Duan, Shenglong Ye, Yu Qiao

- ***What's New***: VisualPRM은 멀티모달 추론을 위한 진보된 프로세스 리워드 모델(Process Reward Model; PRM)로, 기존의 멀티모달 대형 언어 모델(Multimodal Large Language Models; MLLMs)의 추론 능력을 다양한 모델 규모와 모델군에 걸쳐 개선합니다. 이는 Best-of-N 평가 전략을 통해 최적의 응답을 선택하여 향상된 추론 성능을 제공합니다.
- ***Technical Details***: VisualPRM은 약 400K개의 멀티모달 프로세스 수퍼비전 데이터로 구성된 VisualPRM400K 데이터셋을 기반으로 훈련됩니다. 이 데이터셋은 이미지, 질문, 단계별 솔루션과 각 단계의 정확성 주석으로 이루어져 있습니다. 모델은 주어진 단계의 정확성을 예측하도록 훈련되었습니다. 평가를 위해 VisualProcessBench라는 벤치마크를 구축하여 PRM과 MLLM의 단계별 오류 탐지 능력을 측정합니다.
- ***Performance Highlights***: VisualPRM은 7개의 주요 멀티모달 추론 벤치마크에서 MLLM의 성능을 크게 향상시켰습니다. 예를 들어, VisualPRM을 비평 모델로 사용했을 때 InternVL2.5-78B 모델은 5.9점, MiniCPM-V2.6-8B 모델은 8.0점의 성능 향상을 보였습니다. 또한, VisualProcessBench 실험에서 대부분의 오픈 소스 MLLM은 단계별 정확성을 판단하는데 어려움을 겪었지만, VisualPRM은 경쟁력 있는 성능을 보였습니다.

### [OmniPaint: Mastering Object-Oriented Editing via Disentangled Insertion-Removal Inpainting](https://arxiv.org/abs/2503.08677)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.08677.png)

Vote: 18

Authors: Haitian Zheng, Ziyun Zeng, Jiebo Luo, Yongsheng Yu

- ***What's New***: OmniPaint는 객체 제거 및 삽입을 상호 의존적인 프로세스로 재개념화한 혁신적인 프레임워크입니다. 새로운 CFD(metric)는 레퍼런스 없이도 컨텍스트 일관성 및 객체 환상을 평가하여 높은 충실도의 이미지 편집을 위한 새로운 벤치마크를 설정합니다.
- ***Technical Details***: OmniPaint는 FLUX(prior)를 활용하여 처음에는 작은 규모의 짝을 이루는 샘플 최적화와 그 후에는 CycleFlow를 통한 대규모 비패어드(unpaired) 정제를 통해 진행하는 점진적(training pipeline) 트레이닝 파이프라인을 포함합니다. 이 모델은 개체 제거와 삽입을 위한 물리적 및 기하학적 일관성을 성공적으로 유지합니다.
- ***Performance Highlights***: OmniPaint는 다른 기법들보다 월등히 낮은 FID와 CFD 점수를 달성하며, 구조적 및 지각적 충실도를 유지하면서 객체 환상을 효과적으로 억제합니다. 다양한 벤치마크 시험에서 OmniPaint는 날카로운 객체 제거 및 무결점 객체 삽입을 보여줍니다.

### [Shifting Long-Context LLMs Research from Input to Output](https://arxiv.org/abs/2503.04723)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.04723.png)

Vote: 17

Authors: Juanzi Li, Shangqing Tu, Roy Ka-Wei Lee, Zhiqing Hu, Ming Shan Hee, Yushi Bai, Yuhao Wu

- ***What's New***: 이 논문은 기존의 장문 이해와는 달리 장문 생성에 중점을 두고 연구 패러다임의 전환을 제안합니다. 소설 쓰기, 장기 계획 및 복잡한 추론 작업과 같은 실제 응용 분야에서 장문 텍스트 생성의 중요성을 강조하며, 이를 위해 장문 생성을 위한 기본적이고 고품질의 LLMs를 개발할 필요가 있음을 역설합니다.
- ***Technical Details***: 논문은 장문 생성의 중요성을 조사하고, 장문 LLMs (Long-Output LLMs)의 정의와 실제 응용에서 필요한 요구 사항을 제시합니다. 이러한 모델은 광범위한 문맥을 처리하고 생성된 텍스트가 일관성을 유지하며 논리적으로 연결될 수 있도록 설계되어야 합니다. 데이터 제한, 작업 실행의 복잡성, 계산 비용 제한과 같은 장문 생성의 주요 도전 과제를 설명합니다.
- ***Performance Highlights***: 현존하는 모델들이 수천 단어에 이르는 장문 텍스트를 생성하는 데 어려움을 겪고 있다는 연구 결과를 제시합니다. 이는 현재 LLMs가 장문 생성에서 상당한 한계를 가지고 있음을 드러내며, 향후 연구에서 개선 방향을 모색하는 데 중요한 지표가 될 것입니다.

### [GroundingSuite: Measuring Complex Multi-Granular Pixel Grounding](https://arxiv.org/abs/2503.10596)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10596.png)

Vote: 16

Authors: Xinggang Wang, Xiaoxin Chen, Rui Hu, Heng Liu, Lianghui Zhu, Wenyu Liu, Yuxuan Zhang, Lei Liu, Longjin Ran, Tianheng Cheng

- ***What's New***: GroundingSuite는 시각 및 언어 모델(Vision-Language Models; VLMs)을 활용한 자동 주석(annotation) 프레임워크입니다. 새로운 대규모 훈련 데이터세트와 평가 기준을 제안하여, 기존의 데이터셋이 가진 한계를 넘어서고자 합니다. 특히, Referring Expression Segmentation(RES) 등 픽셀 그라운딩 작업의 성능을 유의미하게 향상시킵니다.
- ***Technical Details***: GroundingSuite는 세 가지 주요 구성 요소로 이루어진 GSSculpt라는 주석 프레임워크를 도입하여 픽셀 그라운딩 데이터를 자동으로 생성합니다. 1) 실체 공간 로컬라이제이션(Entity Spatial Localization): 이미지 내 의미 있는 객체나 영역을 인식하고 고품질 마스크(mask)를 생성합니다. 2) 텍스트 생성(Grounding Text Generation): 세밀하고 명확한 언어적 설명을 생성합니다. 3) 노이즈 필터링(Noise Filtering): 모호하거나 잘못된 주석을 제거하여 데이터 품질을 보장합니다.
- ***Performance Highlights***: GSSculpt을 통해 훈련된 모델은 기존의 최고 성능을 뛰어넘는 결과를 여러 지표에서 달성했습니다. 특히 RefCOCOm에서 gIoU 55.3, gRefCOCO에서는 cIoU 68.9라는 성과를 기록했습니다. 추가로, 제안된 GSEval 평가 기준에서는 gIoU 기반으로 각 부문에서 강력한 성능을 보였습니다.

### [New Trends for Modern Machine Translation with Large Reasoning Models](https://arxiv.org/abs/2503.10351)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10351.png)

Vote: 15

Authors: Chenyang Lyu, Minghao Wu, Kaifu Zhang, Sinuo Liu, Longyue Wang, Weihua Luo

- ***What's New***: 이 연구 논문은 대형 추론 모델(Large Reasoning Models; LRMs)이 머신 번역(MT)에 새로운 가능성을 열어준다고 주장하며, 전통적인 신경망 기반의 MT와 대형 언어 모델(LLMs) 기반의 MT 패러다임을 역동적 추론 작업으로 재구성합니다. 특히, CoT(Chain-of-Thought) 추론을 활용하여 번역을 문맥적, 문화적, 언어적 이해와 추론이 필요한 작업으로 재정의합니다.
- ***Technical Details***: LRMs는 세 가지의 근본적 전환을 제안합니다: 1) 문맥적 일관성: 문장 간의 복잡한 문맥을 명시적으로 추론하여 담론 구조를 보존; 2) 문화적 의도성: 모델이 발화자의 의도, 청중의 기대, 사회-언어적 규범을 유도하여 적응적 출력을 생성; 3) 자기 반성: 추론 과정 중 자가 수정 기능을 통해 노이즈가 심한 상황에서도 번역 오류를 수정하는 기능을 수행합니다. 이 모델을 문체 번역, 문서 수준 번역, 멀티모달 번역 시나리오를 통해 검증합니다.
- ***Performance Highlights***: LRMs는 전통적인 번역 시스템과 LLMs 기반 접근법을 능가하기 위해 스타일을 유지하고, 긴 문서 전반에 걸친 일관성을 유지하며, 멀티모달 입력의 시각적 맥락을 통합하는 강력한 능력을 발휘합니다. 그러나 오토-피봇 번역(auto-pivot translation) 현상이나 번역의 과현지화(over-localization)와 같은 새로운 문제점을 드러내기도 합니다.

### [4D LangSplat: 4D Language Gaussian Splatting via Multimodal Large Language Models](https://arxiv.org/abs/2503.10437)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10437.png)

Vote: 14

Authors: Wanhua Li, Johannes Herter, Gao Huang, Hanspeter Pfister, Yingwei Song, Renping Zhou, Minghan Qin, Jiawei Zhou

- ***What's New***: 4D LangSplat은 동적 장면의 시공간에서 개방형 질문을 지원하는 4D 언어 필드를 구축하는 혁신적인 방법입니다. 이는 동영상 데이터를 객체 중심의 자막으로 변환하여, 다중 모달 대형 언어 모델(Multimodal Large Language Models; MLLMs)을 활용하여, 시공간적 일관성을 유지하며 객체별로 고품질의 자막을 생성하는 최초의 접근입니다.
- ***Technical Details***: 4D LangSplat은 다차원 가우시안 스플래팅(4D Gaussian Splatting; 4D-GS)을 사용하여 RGB 장면을 복원하고, 학습된 각 가우시안 포인트에 두 개의 언어 필드를 확장하여 시공간적 세부 사항을 포착합니다. 정적 장면의 시공간 변화에 강건한 정적 언어 임베딩을 학습하고, MLLMs를 통해 생성된 객체별 자막에서 텍스트 특징을 학습하여 동적 장면의 시공간적 세맨틱 변화를 포착합니다. 상태 변형 네트워크를 사용하여 객체 상태의 지속적인 변화를 효과적으로 모델링합니다.
- ***Performance Highlights***: 다양한 벤치마크에서의 실험은 4D LangSplat가 동적 실질 세계 환경의 개방형 질문에서 높은 정확도와 효율성을 달성함을 보여줍니다. 타임-어그노스틱과 타임-센스티브 쿼리링 모두에서 상당한 성능 향상이 관찰되었습니다.

### [VisualWebInstruct: Scaling up Multimodal Instruction Data through Web Search](https://arxiv.org/abs/2503.10582)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10582.png)

Vote: 13

Authors: Wenhu Chen, Yiming Jia, Xiang Yue, Ping Nie, Kai Zou, Bo Li, Jiachen Li

- ***What's New***: VisualWebInstruct는 웹 검색을 활용하여 다분야의 고품질 멀티모달 인스트럭션 데이터세트를 생성하는 새로운 접근 방법을 제안합니다. 이는 부족했던 고난이도 멀티모달 데이터의 다양성과 품질을 높이기 위한 시도로서, 검색엔진을 통해 수집한 이미지와 HTML 콘텐츠를 통해 데이터세트를 구성합니다.
- ***Technical Details***: VisualWebInstruct는 Google 이미지 검색을 활용하여 30,000개의 시드 이미지에서 출발, 700K 이상의 고유 URL에서 HTML을 추출하여 약 900K의 질문-답변 쌍을 생성합니다. 이 중 40%는 시각적 질문-답변 쌍입니다. 데이터세트는 컨텐츠 추출과 필터링, 합성을 통한 파이프라인을 통해 품질을 보장하며, GPT-4o를 사용하여 답변의 일관성을 확인합니다.
- ***Performance Highlights***: VisualWebInstruct로 학습된 MAmmoTH-VL2 모델은 MMMU-Pro-std (40.7%), MathVerse (42.6%), Dyna-Math (55.7%) 벤치마크에서 10B 파라미터 클래스 내에서 최첨단 성능을 보여주었습니다. Llava-OV-mid에서 훈련한 결과 다양한 벤치마크에서 10~20%의 성능 향상을 보였으며, MAmmoTH-VL에서 훈련한 경우에도 5%의 향상을 보였습니다.

### [DiT-Air: Revisiting the Efficiency of Diffusion Model Architecture Design in Text to Image Generation](https://arxiv.org/abs/2503.10618)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10618.png)

Vote: 13

Authors: Wenze Hu, Rui Qian, Chen Chen, Alex Schwing, Tsu-Jui Fu, Wei Liu, Lezhi Li, Yinfei Yang, Bowen Zhang

- ***What's New***: DiT-Air는 텍스트-이미지 생성에서의 효율성을 재고한 Diffusion Transformers(DiTs)를 제안하여, 텍스트 조건화 전략 및 훈련 프로토콜에 중점을 두고 새로운 아키텍처 설계를 제시합니다. DiT-Air와 더 경량화된 DiT-Air-Lite 모델을 도입하여, 최첨단 성능을 유지하면서도 모델 크기를 크게 줄이는 데 성공했습니다.
- ***Technical Details***: DiT-Air는 기존의 복잡한 MMDiT 구조와는 달리, 단일 스트림 설계로 입력된 텍스트와 노이즈를 직접 처리하여 더 효율적인 아키텍처를 제공합니다. 이 구조는, 모든 레이어에서 공유되는 Adaptive Layer Normalization(AdaLN) 파라미터를 통해 모델 크기를 66%까지 줄이며, 메모리 효율성을 높였습니다. 또한, text encoders로 CLIP, T5, LLMs를 사용하여 텍스트 조건화를 분석하고, 개선된 VAE를 도입하여 이미지의 질을 높였습니다.
- ***Performance Highlights***: DiT-Air는 GenEval과 T2I CompBench에서 각각 82.9, 59.5의 최고 성능 점수를 기록하며, 기존의 대규모 모델들보다 우수한 성능을 보였습니다. DiT-Air/XXL은 총 5.95B 파라미터로 SD3, FLUX 등보다 더 나은 효율성과 성능을 달성하였으며, DiT-Air/L-Lite는 제한적인 자원을 활용할 때에도 질 높은 결과를 유지합니다.

### [Light-R1: Curriculum SFT, DPO and RL for Long COT from Scratch and Beyond](https://arxiv.org/abs/2503.10460)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10460.png)

Vote: 11

Authors: Zhenyu Duan, Xiangzheng Zhang, Fenrui Xiao, Yimin Du, Haosheng Zou, Yongchao Deng, Lifu Tang, Liang Wen, Junchen Liu, Shousheng Jia, Xiaowei Lv, Xin He, Yunke Cai, Qi An

- ***What's New***: Light-R1는 처음부터 긴 사슬 사고(Long COT) 모델을 훈련하는 새로운 방식을 제안합니다. 커리큘럼 훈련을 통해 Qwen2.5-32B-Instruct 기반에서 시작하여 수학적 성능에서 DeepSeek-R1-Distill-Qwen-32B보다 우수한 성과를 보였습니다. 특히 강화 학습(Reinforcement Learning; RL)을 적용하여 14B 모델에서도 성공적인 개선을 이뤘습니다.
- ***Technical Details***: 이 연구는 커리큘럼 SFT(순서형 파인 튜닝), DPO(선호 최적화), GRPO(강화 학습)를 통한 포스트 트레이닝을 포함합니다. 두 단계의 SFT와 DPO를 거쳐 32B 모델을 훈련했으며, 두 번째 단계의 SFT를 위한 3k 데이터셋은 모델의 성능 향상을 가져왔습니다. 또한 RL을 통해 14B 모델에서는 응답 길이와 보상 점수가 동시에 증가하는 결과를 얻었습니다.
- ***Performance Highlights***: Light-R1-14B-DS 모델은 AIME24에서 74.0점, AIME25에서 60.2점을 기록하며, 이전의 여러 32B 모델과 DeepSeek-R1-Distill-Llama-70B를 능가하는 성능을 보여주었습니다. 이 연구는 적은 수의 파라미터로도 긴 사슬 사고 모델이 우수한 결과를 낼 수 있음을 입증했습니다.

### [Open-Sora 2.0: Training a Commercial-Level Video Generation Model in $200k](https://arxiv.org/abs/2503.09642)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.09642.png)

Vote: 11

Authors: Silan Hu, Qianran Ma, Xiaokang Wang, Xinying Guo, Anbang Ye, Leijun Cheng, Yang You, Yuting Zhong, Yuanheng Zhao, Wanying Liang, Chenhui Shen, Zhuangyan Li, Wenjun Li, Shijie Huang, Ruijie Zhang, Gang Ren, Yuqi Wang, Chaoyu Gong, Binluo Wang, Limin Zhang, Xiang Lian, Mingyan Jiang, Ziang Wei, Hongxin Liu, Zangwei Zheng, Minghao Li, Yuhui Wang, Xiwen Wu, Xiangyu Peng, Guojun Lei, Tom Young, Hang Xu

- ***What's New***: Open-Sora 2.0는 상업 수준의 비디오 생성 모델을 단 $200k로 훈련한 혁신적인 사례로, 비디오 생성 비용의 효율성을 획기적으로 개선했습니다. 이 모델은 HunyuanVideo 및 Runway Gen-3 Alpha와 같은 글로벌 주요 모델에 비해 비용 효과적으로 유사한 성능을 보입니다. Open-Sora 2.0는 완전히 오픈 소스로 제공되어, 콘텐츠 창작에서의 혁신과 창의성을 더욱 촉진하려는 목표를 가지고 있습니다.
- ***Technical Details***: Open-Sora 2.0는 효율적인 데이터 큐레이션, 모델 아키텍처, 훈련 전략, 그리고 시스템 최적화를 통해 비용 효율성을 최적화했습니다. 주요 기술로는 Video DC-AE 오토 인코더와 DiT 아키텍처 도입이 포함됩니다. 이 오토 인코더는 높은 복원 충실도를 유지하면서도 효율성을 향상시켜, 256px 영상에서의 정보 압축비를 높였습니다.
- ***Performance Highlights***: Open-Sora 2.0은 MovieGen 및 Step-Video-T2V와 같은 유사한 모델들에 비해 최대 10배 이상의 비용 효율성을 자랑합니다. VBench 평가 결과, Open-Sora 2.0은 Visual Quality, Prompt Adherence, Motion Quality 영역에서 강력한 성능을 보여주며, OpenAI Sora와의 성능 격차도 현저히 줄어듭니다. 이는 상업적 배포에 강력한 잠재력을 나타냅니다.

### [Long Context Tuning for Video Generation](https://arxiv.org/abs/2503.10589)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10589.png)

Vote: 10

Authors: Zhenheng Yang, Yuwei Guo, Zhibei Ma, Zhijie Lin, Ceyuan Yang, Lu Jiang, Dahua Lin, Ziyan Yang

- ***What's New***: 이 연구는 Long Context Tuning (LCT)을 통해 단일 샷 비디오 생성 모델을 장면 수준의 일관성을 학습하도록 확장하는 새로운 훈련 패러다임을 제안합니다. 이는 사전 훈련된 단일 샷 비디오 확산 모델의 컨텍스트 윈도우를 확장하여 장면 내 모든 샷을 포괄하는 풀 어텐션 메커니즘을 적용하는 것입니다. 새로운 인터리브드 3D 위치 임베딩과 비동기적 노이즈 전략을 통해 추가 매개변수 없이도 공동 및 오토리그레시브 샷 생성을 가능하게 합니다.
- ***Technical Details***: LCT는 각 샷에 독립적으로 샘플링된 확산 시간단계를 적용함으로써 모든 샷을 공동으로 디노이징하거나 일부를 조건으로 사용합니다. 또한, BIDIRECTIONAL ATTENTION을 CONTEXT-CAUSAL ATTENTION으로 추가 미세 조정하여 오토리그레시브 생성에 효율적인 KV-캐시(KV-cache)를 지원하고 컴퓨팅 오버헤드를 크게 줄였습니다. 실험 결과, Bidirectional Attention 모델은 LCT 후 장면 수준의 데이터에서 우수한 성능을 보였습니다.
- ***Performance Highlights***: 실험에서는 LCT 후 단일 샷 모델이 일관된 멀티샷 장면을 생성할 수 있음을 보여줍니다. 질적, 양적 평가에서 우리의 접근법은 기존 장면 수준 비디오 생성 접근법보다 성능이 뛰어나며, 구성적 생성 및 샷 확장의 새로운 가능성을 보여줍니다. 이 연구는 영상 생성 연구에 새로운 가능성을 제시할 것으로 기대됩니다.

### [Do I look like a `cat.n.01` to you? A Taxonomy Image Generation Benchmark](https://arxiv.org/abs/2503.10357)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10357.png)

Vote: 10

Authors: Chris Biemann, Irina Nikishina, Alina Lobanova, Viktor Moskvoretskii, Ekaterina Neminova, Alexander Panchenko

- ***What's New***: 이 논문은 분류학적 개념(Taxonomy Concepts)을 위한 이미지 생성의 가능성을 탐색하는 '분류학 이미지 생성 벤치마크(Taxonomy Image Generation Benchmark)'를 제안합니다. 이 벤치마크는 모델이 분류학 개념을 이해하고 관련 있는 고품질 이미지를 생성할 수 있는 능력을 평가하는 것입니다. 9개의 새로운 텍스트-이미지 평가 기준(Text-to-Image Metrics)을 제시하며, GPT-4의 피드백을 활용한 쌍대 평가(Pairwise Evaluation)를 통해 이미지 생성 성능을 측정합니다.
- ***Technical Details***: 벤치마크는 WordNet 개념과 LLM이 생성한 예측값을 포함하여 12개의 모델을 평가합니다. 평가를 위해 9개의 새로운 분류학 관련 텍스트-이미지 메트릭스가 사용되며, 인간의 피드백과 결합되어 모델의 성능을 종합적으로 평가합니다. 또한, GPT-4로 페어 평가를 수행하여 인간의 선호와의 정렬 정도를 분석합니다.
- ***Performance Highlights***: 플레이그라운드-v2와 FLUX는 메트릭스 및 세트 전반에서 일관되게 우수한 성능을 보여줬으며, 검색 기반 접근법은 부진한 성과를 보였습니다. 이는 구조적 데이터 자원의 자동 큐레이션 가능성을 강조합니다.

### [R1-Onevision: Advancing Generalized Multimodal Reasoning through Cross-Modal Formalization](https://arxiv.org/abs/2503.10615)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10615.png)

Vote: 9

Authors: Bo Zhang, Xiaoxuan He, Xiyan Jiang, Haoyu Lu, Wei Chen, Yan Deng, Xingtao Yang, Yi Yang, Minfeng Zhu, Hongkun Pan, Dacheng Yin, Fengyun Rao

- ***What's New***: R1-Onevision은 시각적 인식과 심층적 추론 사이의 격차를 해소하기 위해 설계된 멀티모달 추론 모델입니다. 이미지의 정형적 텍스트 표현으로 변환하는 크로스 모달 추론 파이프라인(Cross-Modal Reasoning Pipeline)을 제안하여, 언어 기반의 정밀한 추론을 가능하게 합니다.
- ***Technical Details***: R1-Onevision 데이터셋은 다양한 도메인에서 세부적이고 단계별로 진행되는 멀티모달 추론 프로세스를 제공합니다. 슈퍼바이즈드 파인튜닝(Supervised Fine-Tuning; SFT)과 강화 학습(Reinforcement Learning; RL) 단계를 통해 모델의 고급 추론 능력과 강력한 일반화 능력을 개발합니다. 또한, 인간 교육 시스템의 과목 범위를 커버하는 R1-Onevision-Bench라는 포괄적인 벤치마크를 도입하여 멀티모달 추론 성능을 평가합니다.
- ***Performance Highlights***: R1-Onevision은 수많은 어려운 멀티모달 추론 벤치마크에서 GPT-4o와 Qwen2.5-VL을 능가하며, 최첨단 성능을 달성합니다. 특히, 수학과 같은 복잡한 도메인에서의 정밀한 시각-텍스트 정렬과 체계적인 추론 방식을 통해 성능 향상을 이끌어냅니다.

### [SANA-Sprint: One-Step Diffusion with Continuous-Time Consistency Distillation](https://arxiv.org/abs/2503.09641)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.09641.png)

Vote: 9

Authors: Yuyang Zhao, Enze Xie, Sayak Paul, Junyu Chen, Han Cai, Shuchen Xue, Song Han, Jincheng Yu, Junsong Chen

- ***What's New***: SANA-Sprint는 초고속 텍스트-이미지(T2I; Text-to-Image) 생성의 효율성을 높이기 위한 새로운 확산 모델입니다. 이 모델은 사전 학습된 기반 모델을 사용하여 하이브리드 디스틸레이션(hybrid distillation) 전략을 결합하여, 20단계의 추론 과정을 1~4단계로 크게 줄였습니다. ControlNet과의 통합으로 실시간 상호작용 이미지 생성이 가능하며, 사용자는 즉각적인 시각적 피드백을 받을 수 있습니다.
- ***Technical Details***: SANA-Sprint는 이전의 연속시간 일관성 모델들(sCM; continuous-time consistency models)의 이점을 보존하며 트리그플로우 모델로의 변환을 통해 손실 없는 수학적 변환을 수행하였습니다. 또한, LADD(잠재적 적대적 디스틸레이션)와 일관성 모델을 결합하여 디스틸레이션의 불안정을 완화하며 빠른 수렴을 지원합니다. 교사 모델 학습 없이 사전 학습된 모델의 지식을 효율적으로 전송할 수 있도록 설계되었습니다.
- ***Performance Highlights***: SANA-Sprint는 1회 추론으로만 7.59 FID와 0.74 GenEval을 달성하며 Pareto 전선을 구축하고 있습니다. 이는 FLUX-schnell(7.94 FID/0.71 GenEval)보다 높은 성능을 제공하면서도 10배 빠른 속도로 결과를 생성합니다. 또한, NVIDIA RTX 4090에서 1024x1024 이미지 생성을 0.31초, H100에서 0.1초 만에 완료함으로써, 실시간 응용 프로그램에서의 강력한 활용 가능성을 보여줍니다.

### [Communication-Efficient Language Model Training Scales Reliably and Robustly: Scaling Laws for DiLoCo](https://arxiv.org/abs/2503.09799)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.09799.png)

Vote: 9

Authors: Lucio Dery, Zachary Garrett, Nova Fallen, Zachary Charles, Arthur Szlam, Arthur Douillard, Keith Rush, Gabriel Teston

- ***What's New***: 이 논문에서는 DiLoCo(Distributed Low-Communication)라는 접근법을 대형 언어 모델(LLM) 훈련에 적용하여 모델 크기에 따른 스케일링 법칙을 제시합니다. 기존의 데이터 병렬(Data-Parallel) 훈련보다 더 적은 통신 비용으로 훈련할 수 있으며, 특히 DiLoCo는 모델 크기가 커질수록 데이터 병렬 훈련보다 더 나은 성능을 보입니다.
- ***Technical Details***: DiLoCo는 주어진 고정 계산 예산 내에서 모델 복제 수, 하이퍼파라미터, 토큰 예산 등 알고리즘적 요소가 훈련에 미치는 영향을 예측하기 위해 스케일링 법칙을 사용합니다. 각 모델 복제본은 독립적으로 훈련된 후 주기적으로 동기화되며, 주기적 동기화 사이에는 'outer optimization'이 적용됩니다. 이 방법은 데이터 병렬 훈련에서의 글로벌 배치 크기와 내적 학습률(learning rate)을 다르게 설정할 수 있는 가능성을 제공합니다.
- ***Performance Highlights***: DiLoCo는 데이터 병렬 훈련보다 월등한 '평가 손실(evaluation loss)'을 보였으며, 특히 모델 크기가 증가할수록 그 격차는 더 커졌습니다. 실험 결과, DiLoCo는 더 큰 배치 크기를 수용할 수 있으며, 이를 통해 훈련 시간을 대폭 단축시킬 수 있음을 보여주었습니다.

### [Distilling Diversity and Control in Diffusion Models](https://arxiv.org/abs/2503.10637)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10637.png)

Vote: 8

Authors: David Bau, Rohit Gandikota

- ***What's New***: 이 논문에서 저자들은 확산 모델(Diffusion Models)의 증류 과정(distillation)에서 발생하는 샘플 다양성 손실 문제를 해결하기 위한 새로운 방법인 '다양성 증류(diversity distillation)'를 소개합니다. 이 접근법은 초기 타임스텝에서만 기본 모델을 사용하고 이후 효율적인 증류 모델로 전환하여 다양성을 회복하고 심지어는 초과하는 성과를 발휘합니다.
- ***Technical Details***: 논문에서는 증류된 확산 모델이 초기에 개념 표현을 유지하고 있지만 샘플 다양성이 감소하는 문제를 지적하고, 이를 해결하기 위해 DT 시각화(DT-Visualization)라는 새로운 분석 도구를 제안합니다. 이를 통해 모델의 초기 타임스텝에서의 구조적인 다양성이 최종 출력에 미치는 영향을 시각적으로 분석할 수 있습니다. 또한, 이걸 기반으로 하이브리드 추론 접근법을 사용하는 '다양성 증류'를 구현했습니다.
- ***Performance Highlights***: 다양성 증류 방법은 기존의 증류 모델보다 더 높은 다양성을 보이며, FID (프레쳅셔 녹합 분산)를 기준으로 측정된 결과에서 향상된 다양성을 달성하고, 또한 증류된 모델의 빠른 추론 속도를 유지합니다. 본 연구는 기존에 존재하던 계산 효율성과 생성 다양성 간의 상충 관계를 완화할 수 있음을 보여줍니다.

### [CINEMA: Coherent Multi-Subject Video Generation via MLLM-Based Guidance](https://arxiv.org/abs/2503.10391)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10391.png)

Vote: 6

Authors: Yiding Yang, Shenghai Yuan, Yizhi Wang, Angtian Wang, Chongyang Ma, Jacob Zhiyuan Fang, Bo Liu, Xun Guo, Yufan Deng, Haibin Huang

- ***What's New***: CINEMA는 멀티모달 대형 언어 모델(Multimodal Large Language Model; MLLM)을 활용하여 여러 주제가 포함된 비디오를 일관적으로 생성하는 새로운 프레임워크입니다. 기존의 주제 이미지와 텍스트 코드 사이에 명시적인 상관관계의 필요성을 제거하여 모호성을 감소시키고, 주제 관계를 더 잘 해석할 수 있도록 하여 개인화된 비디오 콘텐츠 생성의 유연성을 향상시킵니다.
- ***Technical Details***: CINEMA는 사전 학습된 오픈 소스 비디오 생성 모델 위에 구축된 모델비종속 방식입니다. 이를 위해 AlignerNet 모듈을 도입하여 MLLM의 출력을 기존 T5 텍스트 기능과 맞추고 Variational Autoencoder (VAE) 기능을 사용하여 참조 이미지의 세부사항을 보존합니다. 이외에도 MM-DiT(Multimodal Diffusion Transformer) 백본을 통해 텍스트와 비주얼 참조를 통합하여 멀티모달 조합된 기능을 생성합니다.
- ***Performance Highlights***: CINEMA를 통해 다중 주제 비디오 생성 시 주제의 일관성과 총체적인 비디오의 일관성을 상당히 개선할 수 있음을 광범위한 평가를 통해 입증하였습니다. 모델은 여러 참조 이미지의 시각적 속성을 정확히 보존하면서 텍스트 프롬프트에 따라 비디오를 생성합니다.

### [Quantization for OpenAI's Whisper Models: A Comparative Analysis](https://arxiv.org/abs/2503.09905)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.09905.png)

Vote: 5

Authors: Allison Andreyev

- ***What's New***: 이 논문은 OpenAI의 Whisper 모델에 대한 정량화(Quantization) 전략을 비교 분석합니다. 이 연구에서는 다양한 Whisper 변종 모델들이 갖는 유사점과 차이점을 평가하고, 정량화가 모델 지연 시간과 크기에 미치는 영향을 수량화하여 엣지 디바이스 배치 가능성을 평가합니다.
- ***Technical Details***: Whisper 모델의 기본 기능과 Whisper_Streaming 및 whisper-timestamped 변종 모델들을 비교 분석했습니다. 이 연구는 열린 소스 LibriSpeech 데이터셋을 사용하여 INT4, INT5, INT8 등 3가지 정량화 방법을 적용해 대역이 제한된 장치에서의 모델 배치 가능성을 평가했습니다.
- ***Performance Highlights***: 정량화 결과, 모델 크기는 45% 줄어들었고 지연 시간은 19% 감소하며, 음성 인식 정확성은 유지되었습니다. Whisper 모델은 엣지 디바이스에서 실시간 음성 변환 서비스를 제공하고, 정확성을 유지하면서도 메모리 제한을 극복할 수 있는 가능성을 보여주었습니다.

### [Discovering Influential Neuron Path in Vision Transformers](https://arxiv.org/abs/2503.09046)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.09046.png)

Vote: 5

Authors: Anqi Pang, Yifan Wang, Changming Li, Jingyi Yu, Kan Ren, Yifei Liu, Sibei Yang, Yingdong Shi

- ***What's New***: 본 논문은 비전 트랜스포머(Vision Transformer) 모델에서 정보 흐름에 가장 큰 영향을 미치는 중요한 뉴런 경로(influential neuron path)를 발견하는 방법을 제안합니다. 기존 연구들은 주로 입력 속성 또는 뉴런의 역할 분석에 초점을 맞췄으나, 이 연구는 레이어 수준의 정보 및 레이어 간 정보 흐름 경로를 종합적으로 고려하였습니다.
- ***Technical Details***: 이 연구는 합동 영향 측정(joint influence measure)을 제안하여 뉴런 세트가 모델 결과에 기여하는 방법을 평가하였습니다. 또한, 계층 진행 뉴런 파악(layer-progressive neuron locating) 접근법을 통해 각 레이어에서 가장 영향력 있는 뉴런을 효율적으로 선택하여 입력에서 출력까지 중요한 뉴런 경로를 발견하고자 합니다.
- ***Performance Highlights***: 실험 결과, 제안된 방법은 기존의 기준 솔루션에 비해 뉴런 경로를 효과적으로 찾는 데 우수함을 보여주었습니다. 특히, 발견된 뉴런 경로는 비전 트랜스포머 모델이 같은 이미지 카테고리 내의 시각 정보를 처리하는 특정 내부 작동 메커니즘을 가지고 있음을 보여주었습니다. 이러한 뉴런 경로는 모델 추론에서 중요한 역할을 하며, 모델 가지치기(model pruning)와 같은 실제 응용 분야에 대한 통찰을 제공할 수 있습니다.

### [UniGoal: Towards Universal Zero-shot Goal-oriented Navigation](https://arxiv.org/abs/2503.10630)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10630.png)

Vote: 5

Authors: Jiwen Lu, Xiuwei Xu, Lingqing Zhao, Hang Yin, Ziwei Wang, Jie Zhou

- ***What's New***: UniGoal는 다양한 목표 유형을 다룰 수 있는 일반적인 제로샷 목표 지향 내비게이션(Zero-shot Goal-oriented Navigation)을 위한 새로운 프레임워크를 제안합니다. 기존의 방법들은 특정 작업 수행에 제한되지만, UniGoal은 오브젝트 카테고리, 인스턴스 이미지 및 텍스트 설명을 포함하는 목표를 통합적으로 표현하여, 이를 통해 다양한 목표 유형을 위한 일반적인 추론 프레임워크를 제공합니다.
- ***Technical Details***: UniGoal은 모든 목표를 유니폼 그래프 표현(Uniform Graph Representation)으로 변환하여 다른 목표를 통합하며, 에이전트의 관찰을 온라인 장면 그래프(Scene Graph)로 변환합니다. 매 시점에서 장면 그래프와 목표 그래프 간의 그래프 매칭을 수행하며, 서로 다른 매칭 상태에 따라 장기 탐색 목표를 생성하기 위한 다단계 장면 탐색 정책을 제안합니다. 제로 매칭 상태에서는 목표 부분 그래프를 순차적으로 탐색하고, 부분 매칭 상태에서는 좌표 투영(Coordinate Projection)과 앵커 페어 정렬(Anchor Pair Alignment)을 사용하여 목표 위치를 추론합니다. 마지막으로 장면 그래프 수정 및 목표 검증 과정을 통해 완벽한 매칭을 달성할 수 있습니다.
- ***Performance Highlights***: MatterPort3D, HM3D 및 RoboTHOR 벤치마크에서 수행한 실험 결과에 따르면, UniGoal은 세 가지 목표 지향 내비게이션 작업에서 단일 모델로 state-of-the-art 제로샷 성능을 달성했으며, 기존의 작업별 제로샷 방법 및 학습 기반의 다양성 방법들을 초과하는 성능을 보였습니다. 예를 들어, 인스턴스 이미지-목표(In-instance Image-goal Navigation; IIN)에서 SynGeo와 Mod-IIN을 각각 0.8% 및 4.1% 뛰어넘었습니다.

### [Autoregressive Image Generation with Randomized Parallel Decoding](https://arxiv.org/abs/2503.10568)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10568.png)

Vote: 5

Authors: Guoqi Li, Huan Wang, Haopeng Li, Jinyue Yang

- ***What's New***: 이 논문에서는 ARPG(Autoregressive Image Generation with Randomized Parallel Decoding)라는 혁신적인 시각적 오토리그레시브 모델을 소개합니다. 이 모델은 랜덤 토큰 순서를 사용하여 병렬 이미지를 생성할 수 있으며, 기존의 순차적인 토큰 생성이 가진 비효율성과 제로 샷 일반화의 한계를 극복합니다.
- ***Technical Details***: ARPG는 'guided decoding' 프레임워크를 통해 랜덤한 순서로 학습과 추론을 지원합니다. 기존의 self-attention을 사용하는 디코더-온리 트랜스포머와는 달리, 데이터와 독립적인 쿼리와 데이터에 의존적인 키-밸류 쌍을 각각 독립적으로 처리하여 임의의 순서로 큐레이션이 가능합니다. 이미지를 포함한 여러 시각적 작업에서 활용될 수 있으며, 특히 이미지 인페인팅, 아웃페인팅, 해상도 확장이 가능합니다.
- ***Performance Highlights***: ARPG는 ImageNet-1K 256×256 벤치마크에서 64 샘플링 단계만으로 FID 1.94를 달성하며, 기존 오토리그레시브 모델들보다 20배 이상의 처리량을 보이고, 메모리 소비는 75% 이상 절감되었습니다. 이러한 성과는 ARPG가 고성능, 고효율 이미지 생성에 있어서 새로운 기준을 제시할 수 있음을 보여줍니다.

### [On the Limitations of Vision-Language Models in Understanding Image Transforms](https://arxiv.org/abs/2503.09837)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.09837.png)

Vote: 3

Authors: Saquib Sarfraz, Hasnain Ali, Ahmad Mustafa Anis

- ***What's New***: 이번 논문은 비전-언어 모델(Vision-Language Models; VLMs), 특히 CLIP와 SigLIP가 이미지 변환에 대해 제대로 이해하지 못하는 한계를 탐구하였습니다. 이미지 수정이 필요한 다운스트림 작업에서 이러한 모델의 한계가 어떤 영향을 미치는지를 파악하기 위해, Flickr8k 데이터셋을 변형하여 각 이미지와 변환에 대한 설명을 짝지어 조사했습니다.
- ***Technical Details***: 연구진은 프리트레인된 대형 데이터셋으로 학습된 VLMs가 필요시 변환을 명시적으로 이해할 수 있어야 한다고 주장합니다. 'Flickr8k' 데이터셋을 선택하여 다채로운 이미지와 이에 상응하는 설명을 제공하며, PyTorch의 torchvision.transforms 라이브러리를 통해 총 24개의 이미지 변형을 6개 범주로 구현했습니다. 변형은 기하학적, 색상, 선명도, 왜곡, 크기, 이미지 처리 효과 등입니다.
- ***Performance Highlights***: 다양한 VLMs 변형 이해도를 평가한 결과, 대부분이 50% 이하의 낮은 정확도를 기록했습니다. SigLIP의 Base 256 Multilingual 모델이 비교적 높은 평균 정확도(47.21%)를 나타내었지만, 여전히 명확한 변형 인지를 실패하는 경우가 많았습니다. 이는 VLMs가 현재의 이미지 구조와 공간 관계를 명확하게 이해하는 데 한계가 있음을 보여줍니다.

### [ConsisLoRA: Enhancing Content and Style Consistency for LoRA-based Style Transfer](https://arxiv.org/abs/2503.10614)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10614.png)

Vote: 3

Authors: Baoquan Zhao, Xudong Mao, Qing Li, Bolin Chen, Haoran Xie, Yi Cai

- ***What's New***: ConsisLoRA는 LoRA 기반 스타일 변환에서 콘텐츠와 스타일의 일관성을 개선하기 위해 제안된 새로운 방법입니다. 이 방법은 스타일 참조 이미지의 스타일을 대상 이미지의 콘텐츠에 전이하는 과정에서 발생하는 콘텐츠 누출, 스타일 불일치, 콘텐츠 불일치 문제를 해결하기 위해 개발되었습니다.
- ***Technical Details***: ConsisLoRA는 ϵ-예측(ϵ-prediction) 대신 x0-예측(x0-prediction)을 사용하여 LoRA 가중치를 최적화함으로써 원본 이미지를 예측하도록 설계되었습니다. 이 방법은 콘텐츠와 스타일의 학습을 두 단계로 나누어 실행하여 효율적으로 분리하며, 단계별 손실 전환 전략을 통해 콘텐츠 이미지의 전체 구조와 세부 사항 모두를 효과적으로 캡처합니다. 또한, 추론 가이던스(inference guidance) 방법을 통해 추론 중에 콘텐츠와 스타일 강도를 지속적으로 조절할 수 있습니다.
- ***Performance Highlights***: 정성적 및 정량적 평가 결과, ConsisLoRA는 콘텐츠 보존 및 스타일 정렬에서 현존하는 최신 방법들보다 뛰어난 성능을 보였습니다. 특히, B-LoRA와 비교했을 때 콘텐츠 일치에서 디노 점수(DINO score)가 현저히 개선되었습니다. 사용자 연구에서도 ConsisLoRA가 다른 기법들보다 선호되는 경향을 보였습니다.

### [Piece it Together: Part-Based Concepting with IP-Priors](https://arxiv.org/abs/2503.10365)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10365.png)

Vote: 3

Authors: Daniel Cohen-Or, Elad Richardson, Kfir Goldberg, Yuval Alaluf

- ***What's New***: 이 논문은 이미지 생성 모델들이 주로 텍스트 기반 조건에 의존하는 한계를 넘어서, 시각적 요소를 기반으로 이미지를 생성하는 프레임워크, ‘Piece it Together’ (PiT)를 소개합니다. 사용자가 제공한 부분적 시각 요소들을 기반으로 전체 구성물을 만들어내고, 부족한 부분을 보완하여 완전한 개념을 생성하는 새로운 접근법을 제안합니다.
- ***Technical Details***: PiT는 IP-Adapter+의 내부 표현을 활용하여 학습한 경량 플로우 매칭 모델인 IP-Prior를 기반으로 하여, 도메인별 사전 지식을 사용한 다양한 생성 작업을 수행합니다. 이 모델은 여러 이미지 부분을 결합하여 하나의 일관된 개념을 만들며, LoRA 기반의 세밀 조정 전략을 사용하여 텍스트 프롬프트에 대한 모델의 응답성을 높였습니다.
- ***Performance Highlights***: PiT는 IP+ 공간에서의 개선된 재구성과 더불어 의미 있는 시맨틱 편집을 가능하게 하여 창의적인 아이디어 생성에 기여합니다. IP-Prior를 통해 학습된 표현들은 생성된 모델이 주어진 도메인 내에서 다양한 변주를 제공할 수 있도록 합니다.

### [The Curse of Conditions: Analyzing and Improving Optimal Transport for Conditional Flow-Based Generation](https://arxiv.org/abs/2503.10636)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10636.png)

Vote: 3

Authors: Ho Kei Cheng, Alexander Schwing

- ***What's New***: 이 논문에서는 조건부 흐름 기반 생성(Conditional Flow-Based Generation)에서 최적 수송(Optimal Transport)을 분석하고 개선하는 방법을 제안합니다. 새로운 방식인 조건부 최적 수송(Conditional Optimal Transport; C2OT)을 통해 조건이 도입되었을 때 성능이 저하되는 기존 기법의 문제를 해결하고, 다양한 조건(이산 및 연속)에 대해서도 향상된 결과를 제공합니다.
- ***Technical Details***: C2OT는 최적 수송(Optimal Transport) 할당을 계산할 때 비용 행렬에 조건부 가중치 항을 추가하여 조건을 무시하는 기존의 최적 수송 방식의 한계를 극복합니다. 이 버전은 이산 조건에서는 정확하게 적용되며, 연속 조건에서는 근사적으로 적용 가능합니다. 실험 결과, C2OT는 8gaussians→moons, CIFAR-10, ImageNet-32×32, ImageNet-256×256 데이터셋 전반에서 기저 기법들보다 우수한 성능을 보였습니다.
- ***Performance Highlights***: 실험 결과에서는 2차원 및 고차원 이미지 데이터에 대한 성능을 평가하였으며, C2OT는 기존 최적 수송 방법을 대체하여 조건부 생성에서의 갭을 줄이는 데 성공했습니다. 특히 CIFAR-10에서는 FID 및 조건-adherence 평가에서 더 나은 결과를 보였고, 고차원 이미지 데이터셋에서도 안정적인 결과를 보이며 성능 향상을 달성했습니다.

### ["Silent Is Not Actually Silent": An Investigation of Toxicity on Bug Report Discussion](https://arxiv.org/abs/2503.10072)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10072.png)

Vote: 2

Authors: Jaydeb Sarker, Mia Mohammad Imran

- ***What's New***: 이번 연구는 오픈 소스 소프트웨어 개발에서 벌레 리포트(Bug Report) 토론에 대한 독성(Toxicity)을 조사하여 독성이 생산적인 토론을 어떻게 방해하고 프로젝트 해결 확률을 감소시키는지를 제시합니다. 독성은 사용자와 유지관리자 간의 벌레 심각도와 우선순위에 대한 인식 불일치에서 종종 발생하며, 도구에 대한 불만과 프로페셔널한 커뮤니케이션의 부족에서도 비롯됩니다.
- ***Technical Details***: 이 연구는 203개의 벌레 쓰레드(Thread)에서 수집된 데이터를 사용하여 실시되었으며, 여기에는 81개의 독성이 포함된 쓰레드가 있습니다. 독성 탐지를 위해 ToxiCR와 LLaMA 모델을 사용하였으며, 각 쓰레드는 수동으로 라벨링하여 독성 여부를 확인하였습니다. 연구 대상은 Github의 1000개 이상의 별을 받은 활발한 저장소들로 제한하였고, 이는 벌레 보고서에서 독성이 어떻게 발현되는지를 분석하기 위함입니다.
- ***Performance Highlights***: 결과적으로, 81개의 독성 쓰레드 중 36개(45%)만이 해결되어, 독성이 있는 토론이 문제 해결에 부정적인 영향을 미친다는 것을 보여줍니다. 특히, 욕설, 모욕, 권리 주장 및 도구에 대한 불만 요소가 있는 쓰레드에서 해결비율은 약 1/3로 낮았습니다. 독성 있는 벌레 보고서는 풀 리퀘스트(PR)와 연결될 가능성이 낮아 해결책으로 이어지기 어렵다는 것이 관찰되었습니다.

### [Studying Classifier(-Free) Guidance From a Classifier-Centric Perspective](https://arxiv.org/abs/2503.10638)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10638.png)

Vote: 2

Authors: Alexander G. Schwing, Xiaoming Zhao

- ***What's New***: 이번 연구는 분류기 기반 관점에서 분류기 제거 지침(Classifier-Free Guidance)을 탐구하고, 기존 접근 방식에 대한 이해도를 높이려는 시도입니다. 이를 통해, 분류기와 분류기 제거 지침 모두 노이즈 제거(diffusion) 경로를 데이터의 결정 경계에서 멀어지도록 하여 조건부 생성의 질을 높이는 기법으로 작용함을 밝혀냈습니다.
- ***Technical Details***: 이 연구에서는 분류기 지침을 다시 고찰하여, 조건부 생성이 어떻게 무조건부 생성과 분류기 예측의 조합으로 분해되는지에 대한 체계적인 연구를 수행했습니다. 그 결과, 분류기의 정확도가 분류기 지침의 생성 품질에 큰 영향을 미친다는 사실을 발견했습니다. 이를 바탕으로, 플로우 매칭(flow-matching) 기반 후처리 과정을 제안하여, 사전 학습된 노이즈 제거(diffusion) 모델의 학습된 분포와 실제 데이터 분포 간의 차이를 줄였습니다.
- ***Performance Highlights***: 제안된 접근 방식을 사용하여 여러 데이터셋에 대한 실험을 수행한 결과, 결정 경계 주변에서 생성되는 저품질 샘플들이 후처리 과정을 통해 실질적으로 개선되었습니다. 이는 특히 저품질 생성이 주로 발생하는 경계 근처에서 효과적인 향상을 제공함을 보여줍니다.

### [MinorBench: A hand-built benchmark for content-based risks for children](https://arxiv.org/abs/2503.10242)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10242.png)

Vote: 2

Authors: Rachel Shong, Shaun Khoo, Gabriel Chua

- ***What's New***: 이 논문은 아동을 위한 AI 시스템의 내용 기반 위험을 평가하기 위한 새로운 벤치마크인 MinorBench를 소개합니다. MinorBench는 대형 언어 모델(LLMs)이 아동으로부터의 부적절한 또는 안전하지 않은 질문을 거부하는 능력을 평가합니다. 이를 통해 아동의 안전성을 우선적으로 고려하는 AI 시스템을 개발하기 위한 실질적인 단계를 제시합니다.
- ***Technical Details***: MinorBench는 육체적 위험, 성적 콘텐츠, 욕설, 혐오, 자해, 약물 사용 등 6가지 주요 위험 범주를 포함하는 299개의 질문으로 구성된 데이터셋을 구축하여 다양한 LLM을 평가합니다. 연구팀은 네 가지 시스템 프롬프트 변형을 통해 LLM의 응답을 테스트하였습니다. 각 버전은 어린이에게 적합한 반응을 이끌어 내는 데 필요한 다양한 수준의 지침을 포함합니다.
- ***Performance Highlights***: GPT-4o-mini와 Gemini 2.0 Flash 모델은 위험한 프롬프트를 거부하는 데 있어 탁월한 성능을 보였으며, Llama 3.3 70B 모델도 그 뒤를 이었습니다. 반면, 논리 기반 모델(o3-mini와 R1 Distilled)은 표준 모델에 비해 아동 안전성을 유지하는 데 일관성이 부족했습니다. '성적 컨텐츠'와 '혐오' 카테고리에서 상당한 개선을 보였지만, '물질 사용'과 '위험' 범주에서는 여전히 일관적인 성능을 보여주지 못했습니다.

### [PerCoV2: Improved Ultra-Low Bit-Rate Perceptual Image Compression with Implicit Hierarchical Masked Image Modeling](https://arxiv.org/abs/2503.09368)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.09368.png)

Vote: 1

Authors: Nikolai Körber, Andreas Siebert, Sascha Hauke, Eduard Kromer, Björn Schuller, Daniel Mueller-Gritschneder

- ***What's New***: PerCoV2는 새로운 초저비트율 지각 이미지 압축 시스템으로, 안정적인 확산(Stable Diffusion) 3 체계를 기반으로 하며 대역폭과 저장공간이 제한된 응용에서 더 높은 이미지 충실도를 저비트율로 달성합니다. 이번 연구는 지각 품질을 유지하면서 더욱 효율적인 엔트로피 코딩을 위한 전용 엔트로피 모델을 도입합니다.
- ***Technical Details***: PerCoV2는 Vector Quantized(VQ) 기반 이미지 인코딩을 사용하고 Implicit Hierarchical Masked Image Model을 통해 이산 하이퍼-라텐트 이미지 분포를 명시적으로 모델링하여 엔트로피 코딩 효율성을 향상시킵니다. 또한, 최근 오토레그레시브 방법인 VAR(Visual Autoregressive Models) 및 MaskGIT(Masked Generative Image Transformer)을 비교하고, MSCOCO-30k 벤치마크에서 이 방법들을 평가합니다.
- ***Performance Highlights***: PerCoV2는 초저비트율(0.003−0.03 bpp)에서 뛰어난 성능을 보이며 이미지 충실도를 유지합니다. 반면, 높은 비트율에서는 상대적으로 덜 효과적이며, 이는 높은 용량의 라텐트 공간이 더 효율적인 오토엔코더 재구성 능력을 가져다주지 않을 수 있음을 시사합니다. MSCOCO-30k 벤치마크에서 강력한 베이스라인들을 초과하는 성능을 보여주었습니다.

### [A Frustratingly Simple Yet Highly Effective Attack Baseline: Over 90% Success Rate Against the Strong Black-box Models of GPT-4.5/4o/o1](https://arxiv.org/abs/2503.10635)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10635.png)

Vote: 0

Authors: Zhiqiang Shen, Dong-Dong Wu, Jiacheng Cui, Zhaoyi Li, Xiaohan Zhao

- ***What's New***: 이 연구는 GPT-4.5 및 기타 대형 비디오-언어 모델(LVLMs)을 대상으로 하는 강력한 블랙박스 환경에서 90% 이상의 성공률을 자랑하는 새로운 공격 기법을 제시합니다. 이 공격 기법은 텍스처 기반의 적대적 변형을 통해 전이 가능성을 획기적으로 개선합니다.
- ***Technical Details***: 이 논문은 전통적 적대적 공격 기법의 약점을 분석하고, 의미론적 세부 사항을 국부적으로 인코딩하여 전이 가능성을 높이는 방법을 제안합니다. 공격 이미지의 국부 영역을 무작위로 크롭하여 배율 조정 후, 타겟 이미지와 임베딩 공간에서 정렬합니다. 모델 앙상블(Ensemble)을 사용하여 일관된 의미론적 세부 사항을 유지하며, 이러한 선택적 지역적 변형이 적대적 공격의 전이 가능성을 상당히 향상시킵니다.
- ***Performance Highlights***: 이 새로운 공격 기법은 GPT-4.5, GPT-o1, Gemini-2.0과 같은 상용 LVLMs에 90% 이상의 성공률을 기록했습니다. 이는 기존의 최첨단 공격 방법들을 초월하는 성과로, 상용 블랙박스 LVLMs에 대한 적대적 공격의 한계를 극복하는 데 큰 기여를 하였습니다.

### [TruthPrInt: Mitigating LVLM Object Hallucination Via Latent Truthful-Guided Pre-Intervention](https://arxiv.org/abs/2503.10602)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10602.png)

Vote: 0

Authors: Hao Cheng, Jinhao Duan, Kaidi Xu, James Diffenderfer, Fei Kong, Bhavya Kailkhura, Xiaofeng Zhu, Lichao Sun, Xiaoshuang Shi

- ***What's New***: TruthPrInt는 대형 비전-언어 모델(Large Vision-Language Models, LVLMs)에서 발생하는 객체 환각(Object Hallucination; OH)을 줄이기 위해 설계된 새로운 프레임워크입니다. 이 연구는 LVLM의 내부 상태가 각 토큰 단위의 환각 행위를 식별하는 높은 특이성의 지표로 사용될 수 있음을 발견했고, 이를 바탕으로 진실된 방향으로 유도하는 사전 개입 기법을 도입하였습니다.
- ***Technical Details***: TruthPrInt는 LVLM의 디코딩 과정에서 진실된 방향 진학을 배우고 이를 바탕으로 추론 시 간섭을 통해 OH를 완화하는 방법론입니다. 특히, 공통 잠재 서브스페이스에서 환각의 패턴을 맞춰 교차-모델 및 교차-데이터 전송 가능성을 높이는 ComnHallu를 제안하였습니다. 이를 통해 다양한 LVLM과 OH 벤치마크에서 실험을 진행하여 그 효과를 검증하였습니다.
- ***Performance Highlights***: TruthPrInt는 MiniGPT-4, Llava-1.5, mPLUG-Owl2 등 여러 LVLM에서 기존의 최첨단 방법들을 능가하는 성능을 보였습니다. 실험 결과, CHAIRS와 CHAIRI 지표에서 다른 방법들보다 12%에서 14% 낮았으며, BLEU 지표에서 약 2% 높은 성능을 보여, 객체 환각 문제를 효과적으로 완화함과 동시에 고품질의 캡션을 생성할 수 있음을 입증했습니다.

