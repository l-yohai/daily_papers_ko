## Daily Papers (2025-03-27)

### [Qwen2.5-Omni Technical Report](https://arxiv.org/abs/2503.20215)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.20215.png)

Vote: 56

Authors: Jinzheng He, Keqin Chen, Kai Dang, Zhifang Guo, Shuai Bai, Yunfei Chu, Yang Fan, Ting He, Jin Xu, Hangrui Hu, Junyang Lin, Xiong Wang, Bin Zhang, Jialin Wang

- ***What's New***: Qwen2.5-Omni는 텍스트, 이미지, 오디오, 비디오 등 다양한 모달리티를 인식하고 스트리밍 방식으로 텍스트 및 자연 음성 응답을 생성할 수 있는 엔드 투 엔드 멀티모달 모델(Multimodal Model)입니다. 특히 새로운 위치 임베딩 알고리즘인 TMRoPE(Time-aligned Multimodal RoPE)를 도입해 오디오와 비디오의 시간 동기화를 개선하였습니다. 또한, Thinker-Talker 아키텍처를 제안하여 실시간으로 텍스트와 음성을 생성하며 상호 간섭을 최소화합니다.
- ***Technical Details***: Qwen2.5-Omni는 블록 단위로 멀티모달 데이터의 긴 시퀀스를 처리하며, Thinker는 텍스트 생성 역할을, Talker는 고수준의 표현에서 직접 오디오 토큰을 생성하는 이중 트랙 모형입니다. 슬라이딩-윈도우 DiT는 스트리밍 오디오 토큰 디코딩에서 지연 시간을 줄이기 위해 사용됩니다. 또한 멀티모달 인코더들은 블록 단위 스트리밍 처리 방식으로 수정되었습니다.
- ***Performance Highlights***: Qwen2.5-Omni는 Omni-Bench 등의 멀티모달 벤치마크에서 최첨단 성능을 기록하며 Qwen2.5-VL과 비교할 만한 성능을 보입니다. 특히, 종단 간 음성 지시 사항 따라하기에서는 텍스트 입력과 비슷한 성능을 보이며, 스트리밍 음성 생성에서 기존 대다수 스트리밍 및 비스트리밍 대안보다 우수한 강건성과 자연스러움을 나타냅니다. 구체적으로, SEED-tts-eval 시험에서는 WER(Word Error Rate)가 test-zh에서 1.42%, test-en에서 2.33%, test-hard에서 6.54%를 기록하여 MaskGCT와 CosyVoice 2를 웃돌았습니다.

### [Dita: Scaling Diffusion Transformer for Generalist Vision-Language-Action Policy](https://arxiv.org/abs/2503.19757)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19757.png)

Vote: 39

Authors: Chengyang Zhao, Yuwen Xiong, Xizhou Zhu, Yuntao Chen, Tianyi Zhang, Haonan Duan, Hengjun Pu, Ronglei Tong, Jifeng Dai, Yu Qiao, Zhi Hou

- ***What's New***: Dita는 대규모 변형 데이터셋(Cross-Embodiment Datasets)을 활용하여 일반적인 로봇 정책(Generalist Robot Policy)을 학습할 수 있는 확장 가능한 Diffusion Transformer 프레임워크입니다. 이 모델은 긴 시나리오와 다양한 환경 변화에 강인하며, 10-shot 학습(finetuning)으로 새로운 로봇 환경에 빠르게 적응할 수 있습니다.
- ***Technical Details***: Dita는 변형 데이터셋에 기반한 Transformer 아키텍처를 활용한 다중 모달 확산(Diffusion) 과정을 통해 연속적인 행동 시퀀스를 직접 비핵화(denoise)합니다. 네트워크는 CLIP 모델로부터 언어 지시를 토큰화하고 DINOv2로 이미지 패치 특징을 추출합니다. In-Context Conditioning 메커니즘을 통해 행동 시퀀스를 비핵화하며, 시공간 내의 미세한 변화들을 감지할 수 있도록 설계되었습니다. 이 모델은 334M 파라미터로 구성된 경량 오픈소스 베이스라인 모델이며, 다양한 입력을 유연하게 통합할 수 있습니다.
- ***Performance Highlights***: Dita는 SimplerEnv, LIBERO, CALVIN, ManiSkill2와 같은 시뮬레이션 벤치마크에서 최첨단 성능을 달성했습니다. 특히, LIBERO-LONG에서 이전 모델들보다 10% 개선된 성과를 보였으며, long-horizon 작업에서 우수한 일반화 성능을 보여줍니다. 실제 로봇 실험에서는 배경, 비타겟 객체 배열, 조명 조건 등 다양한 변동에도 높은 강인성을 발휘했습니다.

### [Wan: Open and Advanced Large-Scale Video Generative Models](https://arxiv.org/abs/2503.20314)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.20314.png)

Vote: 29

Authors: Chen-Wei Xie, Siyang Sun, Ruihang Chu, Lianghua Huang, Zeyinzi Jiang, Zhi-Fan Wu, Mengyang Feng, Yong Li, Yiming Wang, Tingyu Weng, Yun Zheng, Xin Xu, Ningyi Zhang, Shiwei Zhang, Haiming Zhao, Kang Zhao, Chaojie Mao, Yijing Liu, Jingren Zhou, Wenting Shen, Yangyu Lv, Zhen Han, Wente Wang, Wei Wang, Jianyuan Zeng, Yulin Pan, Yupeng Shi, WanTeam, Tianxing Wang, Ruili Feng, Wenyuan Yu, Pandeng Li, Jixuan Chen, Yan Kou, Yingya Zhang, Yu Liu, Yitong Huang, Yuntao Hong, Tong Shen, Bin Wen, Wenmeng Zhou, Jinkai Wang, Di Chen, Ziyu Liu, Xiaoming Huang, Tao Fang, Feiwu Yu, Yutong Feng, Yifei Li, Baole Ai, Wei Lin, Keyu Yan, You Wu, Jiayu Wang, Xianzhong Shi, Kai Zhu, Ang Wang, Pingyu Wu, Jianxiao Yang, Jingfeng Zhang, Tianyi Gui

- ***What's New***: Wan은 대규모 비디오 생성 모델을 향한 혁신적인 오픈 소스 솔루션으로, 비디오 생성의 경계를 확장하기 위해 설계되었습니다. Wan은 새로운 시공간 변이 오토인코더(spatio-temporal VAE), 확장 가능한 사전 학습 전략, 대규모 데이터 큐레이션 및 자동 평가 지표를 포함한 다양한 혁신을 통해 생성 기능을 크게 향상시켰습니다. 특히, Wan 모델은 오픈 소스와 상업적 솔루션을 능가하는 탁월한 성능을 여러 벤치마크에서 보여줍니다.
- ***Technical Details***: Wan은 변형 기반 확산(transformer-based diffusion) 모델을 사용하며, 14억 파라미터로 구성된 모델은 수십억의 이미지와 비디오 데이터로 학습됩니다. 이 모델은 시간적 및 공간적 주목 메커니즘을 활용하여 강력한 텍스트-비디오(Text-to-Video; T2V) 생성 기능을 제공합니다. 또한, Wan은 이미지-비디오 변환(Image-to-Video; I2V), 지시 기반 비디오 편집, 개인화 비디오 생성 등 다양한 응용 프로그램을 지원합니다. 이는 한국어와 영어로 시각적 텍스트를 생성할 수 있는 최초의 모델 중 하나로, 고객용 GPU에서도 실행 가능한 효율성을 자랑합니다.
- ***Performance Highlights***: Wan 모델은 여러 상업적 모델 및 오픈 소스 모델과 비교했을 때 성능 면에서 선두에 서 있습니다. 예를 들어, Wan2.1-14B 모델은 추론 속도가 기존의 SOTA 방법보다 2.5배 빠르며, 높은 부하에서도 여전히 95%의 효율성을 유지합니다. 이러한 성능 개선은 대규모 데이터를 사용하여 학습된 Wan의 설계가 효과적임을 입증하고, 미래의 VAE 기술 발전에 관한 통찰을 제공합니다.

### [LEGO-Puzzles: How Good Are MLLMs at Multi-Step Spatial Reasoning?](https://arxiv.org/abs/2503.19990)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19990.png)

Vote: 26

Authors: Kaifeng Lyu, Kexian Tang, Kai Chen, Haodong Duan, Wenran Liu, Yanhong Zeng, Junyao Gao, Zhening Xing, Yanan Sun

- ***What's New***: LEGO-Puzzles는 다중단계 공간 추론(multi-step spatial reasoning) 능력을 평가하는 새로운 벤치마크입니다. 이는 레고 기반의 작업을 통해 멀티모달 대형 언어 모델들(MLLMs)의 공간적 이해와 순서적 추론을 평가하는데 중점을 두고 있습니다. 총 1,100개의 VQA 샘플로 구성되어 있으며, 11개의 다양한 작업을 포함합니다.
- ***Technical Details***: LEGO-Puzzles는 LEGO 빌딩 과정의 단계별 조립을 기반으로 설계된 벤치마크로, 3가지 주요 작업 카테고리로 나누어집니다. 첫 번째는 기본 공간 이해, 두 번째는 단일 단계 순서적 추론, 세 번째는 다단계 순서적 추론입니다. 데이터셋은 공개적으로 이용 가능한 LEGO 프로젝트의 조립 설명서를 바탕으로 하며, 데이터 수집, 질문-답변 생성, 품질 관리의 3단계 절차를 거칩니다.
- ***Performance Highlights***: LEGO-Puzzles에서 사람들은 약 93.6%의 정확도를 기록하며 탁월한 성능을 보인 반면, 최첨단 MLLMs는 여전히 30% 이상의 성능 격차를 보이며 인간을 따라잡지 못했습니다. Gemini-2.0-Flash와 GPT-4o는 각각 54.0%와 57.7%의 전체 정확도를 기록했으며, 공간 이해 및 순차적 추론에서 기존 모델들의 한계를 드러냈습니다.

### [Open Deep Search: Democratizing Search with Open-source Reasoning Agents](https://arxiv.org/abs/2503.20201)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.20201.png)

Vote: 22

Authors: Windsor Nguyen, Salaheddin Alzubi, Edoardo Contente, Sewoong Oh, Creston Brooks, Purva Chiniya, Yihan Jiang, Pramod Viswanath, Lucas Irwin, Chiara von Gerlach, Himanshu Tyagi, Arda Kaz

- ***What's New***: Open Deep Search (ODS)는 최신 오픈소스 대형 언어 모델(LLMs)의 추론 능력을 향상시키기 위해 웹 검색 도구를 통합한 오픈소스 AI 검색 솔루션입니다. 사용자가 선택한 기본 LLM과 함께 작동하는 ODS는 Open Search Tool과 Open Reasoning Agent라는 두 가지 구성 요소로 구성되어 있습니다.
- ***Technical Details***: Open Reasoning Agent는 주어진 작업을 해석하고 도구를 호출하여 완료하며, Open Search Tool은 독자적인 솔루션보다 우수한 성능을 발휘하는 새로운 웹 검색 도구입니다. ODS는 다양한 도구와 기본 LLM을 통합하여 최첨단 성능을 달성합니다. 예를 들어, ODS-v1은 ReAct agent를, ODS-v2는 CodeAct agent를 사용합니다.
- ***Performance Highlights***: ODS는 SimpleQA 벤치마크에서는 88.3%, FRAMES에서는 75.3%의 정확도를 기록하며, 최신 독점 솔루션과 맞먹거나 이를 초과하는 성능을 보여주었습니다. 특히 FRAMES 벤치마크에서 OpenAI의 GPT-4o Search Preview를 9.7% 개선한 정확도를 기록했습니다.

### [Unconditional Priors Matter! Improving Conditional Generation of Fine-Tuned Diffusion Models](https://arxiv.org/abs/2503.20240)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.20240.png)

Vote: 19

Authors: Prin Phunyaphibarn, Phillip Y. Lee, Minhyuk Sung, Jaihoon Kim

- ***What's New***: 이 연구는 조건부 생성(Conditional Generation) 성능을 향상시키기 위해 미세 조정된 확산 모델(Fine-Tuned Diffusion Models)에 비조건부 사전(Unconditional Priors)의 중요성을 강조합니다. 이 논문은 기존의 비조건부 소음을 기초 모델(Base Model)의 예측치로 대체함으로써 생성 품질을 크게 향상시킬 수 있음을 보여줍니다.
- ***Technical Details***: Classifier-Free Guidance(CFG)는 일반적으로 조건부 확산 모델의 학습에 사용됩니다. 그러나 비조건부 소음 예측의 저품질이 조건부 생성 품질의 저하로 이어질 수 있습니다. 저자들은 미세 조정된 모델의 비조건부 예측을 기초 모델의 예측으로 대체하면 성능이 향상된다는 점을 발견했습니다. 이는 추가적인 학습이나 신경망의 수정이 필요하지 않은 방법입니다.
- ***Performance Highlights***: 새로운 방법은 다양한 조건부 확산 모델에서 높은 생성품질 향상을 보였습니다. Zero-1-to-3 모델에서는 LPIPS, PSNR, SSIM과 같은 이미지 품질 평가 지표가 개선되었고, Versatile Diffusion에서는 FID, FDDINOv2, CLIP-I, DINOv2 지표 모두에서 성능이 향상되었습니다. DynamiCrafter와 InstructPix2Pix에서도 보다 높은 일관성과 이미지 질을 보여 주었습니다.

### [GenHancer: Imperfect Generative Models are Secretly Strong Vision-Centric Enhancers](https://arxiv.org/abs/2503.19480)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19480.png)

Vote: 14

Authors: Yuxin Guo, Teng Wang, Shijie Ma, Yuying Ge, Ying Shan, Yixiao Ge

- ***What's New***: GenHancer는 시각 중심의 향상을 위한 새로운 접근법으로, 불완전한 생성 모델을 통해 CLIP의 세밀한 시각 표현력을 강화합니다. 우리는 최적의 재구성을 달성하는 것이 항상 바람직한 시각적 표현을 보장하지 않는다는 중요한 발견을 혁신적으로 밝혀냈습니다.
- ***Technical Details***: GenHancer는 시각 정보를 조건으로 이용하여 재구성을 수행하는 가벼운 디퓨저와의 두 단계 후처리 방법입니다. 주로 글로벌 비주얼 토큰(예: [CLS] 토큰)을 사용하여 정보 누출을 최소화하며, 시각적 표현을 향상시키기 위해 노이즈 제거기(denoiser)와 조건 설정 구조를 설계하였습니다. 이 모델은 학습의 첫 단계에서는 노이즈 제거기와 투사기를 훈련하고, 두 번째 단계에서 CLIP ViT를 미세 조정하여 세밀한 시각적 표현력을 강화합니다.
- ***Performance Highlights***: GenHancer는 DIVA와 같은 기존의 무거운 사전 훈련된 생성 모델을 사용하는 방법과 비교할 때, 다양한 CLIP 백본에서 4.5% 이상의 성능 향상을 보여주었습니다. 특히, 색상 인식과 시점 이해를 포함한 시각적 뉘앙스를 효과적으로 강화하였습니다.

### [Gemini Robotics: Bringing AI into the Physical World](https://arxiv.org/abs/2503.20020)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.20020.png)

Vote: 13

Authors: Jacky Liang, Anirudha Majumdar, Krzysztof Choromanski, Zhuo Xu, Antoine Laurens, Ashwin Balakrishna, Laura Graesser, Fei Xia, Jan Humplik, Brandon Hernaez, Leonard Hasenclever, Peng Xu, Ying Xu, Joshua Ainslie, Erik Frey, Rui Yao, Atil Iscen, Coline Devin, Razvan Surdulescu, Xi Chen, Michael Neunert, Marissa Giustina, Travis Armstrong, Claudio Fantacci, Wentao Yuan, Montserrat Gonzalez Arenas, Jake Varley, Robert Baruch, Robert Moreno, Vincent Vanhoucke, Dmitry Kalashnikov, Cody Fong, Francesco Nori, Isabel Leal, Sichun Xu, Emilio Parisotto, Gemini Robotics Team, Todor Davchev, Yixin Lin, Yilun Du, Norman Di Palo, Yuheng Kuang, Oriol Vinyals, Pannag Sanketi, Deepali Jain, Tsang-Wei Edward Lee, Jost Tobias Springenberg, Sharath Maddineni, Konstantinos Bousmalis, Maria Bauza, Rachel Sterneck, Ted Xiao, Pierre Sermanet, Ayzaan Wahid, Jingwei Zhang, Stefano Saliceti, Michiel Blokzijl, Stefan Welker, Jonathan Tompson, Federico Casarini, Kathryn Shea, Jose Enrique Chen, Debidatta Dwibedi, Tingnan Zhang, Yuxiang Zhou, Nicolas Heess, Paul Wohlhart, Grace Vesom, Wenhao Yu, Adil Dostmohamed, Keerthana Gopalakrishnan, Assaf Hurwitz Michaely, Jinyu Xie, Sergey Yaroshenko, R. Alex Hofer, Annie Xie, Danny Driess, Sean Kirmani, Jie Tan, Dorsa Sadigh, Saminda Abeyruwan, Mithun George Jacob, Charles Shu, Krista Reymann, Allan Zhou, Steven Bohez, Tianli Ding, Giulia Vezzani, Vikas Sindhwani, Jerad Kirkland, Oscar Chang, Carolina Parada, Sumeet Singh, Yuxiang Yang, Serkan Cabi, Alexander Herzog, Kanishka Rao, David D'Ambrosio, Chase Kew, Ryan Julian, Ken Caluwaerts, Michael Elabd, Peter Pastor, Jean-Baptiste Alayrac, Stefani Karp, Hao-Tien Lewis Chiang, Arunkumar Byravan, Alex X. Lee, Thomas Buschmann, Mohit Sharma, Radu Soricut, Thomas Lampe, Anthony Brohan, Dhruv Shah, Acorn Pooley, Sudeep Dasari, M. Emre Karagozler, Chuyuan Fu

- ***What's New***: Gemini Robotics는 AI 기술을 물리적 세계로 확장하기 위해 구글 딥마인드 팀이 고안한 새로운 AI 모델 패밀리입니다. 이 모델은 Vision-Language-Action(VLA) 일반 모델로, 로봇을 직접 제어할 수 있으며, 물체 유형과 위치에 대한 변동에 강하고, 다양한 개방형 어휘 명령을 준수할 수 있습니다. Gemini Robotics-ER(Embodied Reasoning) 모델을 기반으로 구축되어 공간 및 시간에 대한 이해를 강화하여 복잡한 조작 작업을 해결합니다.
- ***Technical Details***: Gemini Robotics-ER은 시각적 요소와 조작 예측을 포함하여 다양한 로봇 응용에 적합하게 설계되었으며, 3D 이해 및 멀티뷰 대응, 3D 바운딩 박스 예측 기능을 갖추고 있습니다. 이 모델은 Gemini 2.0의 다중 모드 이해와 추론 능력을 바탕으로 로봇에 적합한 강화된 기능을 제공합니다. 세부적으로 Gemini Robotics는 클라우드에서 호스팅 되는 VLA 백본과 로컬 액션 디코더로 구성되어 있으며, 물리적 상호작용에서 추론 중간 단계를 행동 예측에 더 가깝게 가져오도록 후처리된 데이터셋으로 모델을 미세 조정합니다.
- ***Performance Highlights***: Gemini Robotics 모델은 다양한 복잡한 작업에서 79%의 평균 성공률을 달성하며, 특히 점심 도시락 포장 작업에서는 100%의 성공률을 기록했습니다. 이 모델은 손으로 그린 스케치와 같은 훈련되지 않은 상황에서도 4개 중 3개의 단어를 정확하게 맞추는 등 복잡한 과제 해결 능력을 보여줍니다. 더불어, Gemini Robotics-ER은 새로운 개념 및 추론이 요구되는 문제에서도 높은 정확도를 보이며, 다양한 안전 및 책임 고려사항을 포함하고 있습니다.

### [Gemma 3 Technical Report](https://arxiv.org/abs/2503.19786)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19786.png)

Vote: 12

Authors: Amit Vadi, Demis Hassabis, Derek Cheng, Hadi Hashemi, Francesco Visin, Alvin Abdagic, Uday Kalra, Hanna Klimczak-Plucińska, Jordi Orbay, Rohan Anil, Michael Moynihan, Alexei Bendebury, Evan Senter, Bilal Piot, Dee Cattle, Harman Singh, Sertan Girgin, Eli Collins, Zach Gleicher, Olivier Bachem, Pouya Tafti, Shruti Sheth, Alek Andreev, Yuvein Zhu, Bo Wu, CJ Carey, Kiran Vodrahalli, Nabila Babar, Xi Chen, Victor Cotruta, Kevin Hui, Clement Farabet, Rakesh Shivanna, Koray Kavukcuoglu, Nilay Chauhan, Jan-Thorsten Peter, Antoine Miech, Armand Joulin, Noam Shazeer, Rohith Vallu, Doug Reid, Alaa Saade, Vlad Kolesnikov, Hussein Hazimeh, Minh Giang, Danila Sinopalnikov, Renjie Wu, Emilio Parisotto, Shreya Pathak, Matthew Watson, Antoine Yang, Vedant Misra, Anil Das, Ravin Kumar, Tyler Liechty, Bobak Shahriari, Nathan Byrd, Woohyun Han, Lepikhin, Ayan Chakrabarti, Sindhu Raghuram Panyam, Ivo Penchev, Dimitris Paparas, John Wieting, Oriol Vinyals, Yossi Matias, Phoebe Kirk, Matt Hoffman, Nick Roy, Abe Friesen, Paul Kishan Rubenstein, Tatiana Matejovicova, Tianqi Liu, Sivan Eiger, Robert Busa-Fekete, Harsh Mehta, Jonathan Lai, Louis Rouillard, Marcella Valentine, Antonia Paterson, Abheesht Sharma, Ankur Bapna, Divyashree Shivakumar Sreepathihalli, Sarah Perrin, Kat Black, Paul Caron, Adi Mayrav Gilady, Philipp Schmid, Rishabh Agarwal, Robert Dadashi, D. Sculley, András György, Christopher A. Choquette-Choo, Siim Põder, Woosuk Kwon, Joe Stanton, David Tian, Sabela Ramos, Sammy Jerome, Trevor Yacovone, Alexander Kolesnikov, Erica Moreira, Alexandre Ramé, Marvin Ritter, Nino Vieillard, Lucas Gonzalez, Noveen Sachdeva, Harshal Tushar Lehri, Phil Culliton, Jean-bastien Grill, Charlie Chen, Dustin Tran, Danielle Eisenbud, Yinlam Chow, Gaël Liu, Colin Cherry, Andreas Steiner, Joelle Barral, Kathy Yu, Ian Ballantyne, Sebastian Borgeaud, Morgane Rivière, Daniel Deutsch, Charline Le Lan, Mehran Kazemi, Geoffrey Cideron, Michelle Casbon, Kathleen Kenealy, Pier Giuseppe Sessa, Abhanshu Sharma, Benjamin Coleman, Frederick Liu, Edouard Yvinec, Vlad Feinberg, Luiz Gustavo Martins, Alex Feng, Mayank Chaturvedi, Utku Evci, Anton Tsitsulin, Etienne Pot, Natasha Noy, Omar Sanseviero, Adrian Goedeckemeyer, Linhai Qiu, Vincent Roseberry, Zichuan Wei, Anand Rao, Xiaohai Zhai, Basil Mustafa, Rob Willoughby, Josh Newlan, Ramona Merhej, Aishwarya Kamath, Dan Malkin, Reza Rokni, Tris Warkentin, Gagik Amirkhanyan, Gemma Team, Jetha Chan, Jessica Lo, Noah Fiedel, Dustin Zelle, Nikola Momchev, Pingmei Xu, Lucas Beyer, Klaus Greff, Elena Buchatskaya, Bryce Petrini, Jiaming Luo, Oskar Bunyan, Yi Gao, André Susano Pinto, Joseph Fernandez, Cassidy Hardin, Matan Eyal, Léonard Hussenot, Shashir Reddy, Pankil Botarda, Thomas Mesnard, Piotr Stanczyk, Renke Pan, Raia Hadsell, Ryan Mullins, Susan Zhang, Sara Smoot, Shariq Iqbal, Jean-Baptiste Alayrac, Idan Brusilovsky, Sijal Bhatnagar, Jean Pouget-Abadie, Eugene Kharitonov, Iain Barr, Eric Noland, Cormac Brick, Zoltan Egyed, Dmitry, Ivan Nardini, Marina Coelho, Johan Ferret, Ju-yeong Ji, Slav Petrov, Vahab Mirrokni, Jeff Dean, Surya Bhupatiraju, Min Ma, Zoubin Ghahramani, Idan Szpektor, Glenn Cameron, Erwin Huizenga, David Vilar, Jyotinder Singh, Ashish Shenoy

- ***What's New***: Gemma 3는 이전 Gemma 버전보다 개선된 다중모달(multi-modal) 이해를 제공하며, 특히 시각적 이해 능력을 추가하고, 다양한 언어 지원을 확대하며, 긴 컨텍스트를 처리하는 데 유리한 구조로 설계되었습니다. KV 캐시 메모리를 줄이기 위해 로컬과 글로벌 주의(attention) 레이어의 비율을 조정하여 긴 컨텍스트에서도 성능을 유지합니다.
- ***Technical Details***: Gemma 3 모델은 1억에서 270억 파라미터에 이르는 경량 오픈모델(Lightweight open models)로, 시각적 요소를 위해 SigLIP 비전 인코더를 사용합니다. 이미지를 256개의 소프트 토큰으로 인식하여 처리하며, 일정한 해상도로 작동하면서 LLaVA에 영감을 받은 팬 앤 스캔(Pan and Scan; P&S) 방법을 사용하여 유연한 해상도를 지원합니다. 또한, 긴 컨텍스트 지원을 위해 128K 토큰까지 확장 가능하며, KV 캐시 증가 문제를 해결하기 위해 5개의 로컬 레이어당 1개의 글로벌 레이어를 배치하여 메모리 사용을 줄입니다.
- ***Performance Highlights***: Gemma 3는 새로 추가된 시각적 요소와 다양한 개선 덕분에 이전 버전보다 모든 벤치마크에서 우수한 성능을 보입니다. 특히, 학습 후 수학, 대화, 지시 따르기 및 다중언어 기능에서 개선된 성능을 보여주며, 다양한 평가 벤치마크에서 뛰어난 기량을 입증합니다. Elo 점수로 측정된 Chatbot Arena에서 1338점으로 상위 10위권 내에 들며, 기존의 다른 모델들보다 뛰어난 성능을 입증합니다.

### [BizGen: Advancing Article-level Visual Text Rendering for Infographics Generation](https://arxiv.org/abs/2503.20672)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.20672.png)

Vote: 11

Authors: Yuyang Peng, Shishi Xiao, Keming Wu, Qisheng Liao, Kevin Lin, Bohan Chen, Ji Li, Yuhui Yuan, Danqing Huang

- ***What's New***: BizGen은 사용자가 제공한 기사 수준의 설명 프롬프트와 초밀도 레이아웃을 기반으로 인포그래픽 및 슬라이드를 생성하는 새로운 기법입니다. 이 연구는 이전 모델들이 다루지 못했던 기사 수준의 긴 컨텍스트와 고품질 비즈니스 콘텐츠 데이터의 부족 문제를 해결합니다.
- ***Technical Details***: BizGen은 두 가지 기술 기여를 합니다. 첫째, 초밀도 레이아웃과 프롬프트가 포함된 65만 개 이상의 고품질 비즈니스 콘텐츠 데이터를 생성하는 인포그래픽 데이터셋을 구축했습니다. 둘째, 여러 하위 지역의 프롬프트를 초밀도 레이아웃에 따라 자른 잠재 공간으로 주입하고, 레이아웃 조건부 CFG를 사용하여 유연하게 하위 지역을 조정할 수 있는 레이아웃 가이드 크로스 어텐션 스키마를 제안했습니다.
- ***Performance Highlights***: BizGen은 최신 모델인 FLUX 및 SD3와 비교하여 프롬프트 세트인 BizEval에서 강력한 성과를 보여주었습니다. 사용자 연구 결과, BizGen은 작성한 인포그래픽과 슬라이드의 시각적 미학, 텍스트 정확성 및 프롬프트 일치 측면에서 다른 모델보다 높은 우위를 차지했습니다.

### [LogQuant: Log-Distributed 2-Bit Quantization of KV Cache with Superior Accuracy Preservation](https://arxiv.org/abs/2503.19950)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19950.png)

Vote: 7

Authors: Zining Zhang, Zicong Jiang, Mian Lu, Han Chen, Yuqiang Chen, Pingyi Luo, Bingsheng He

- ***What's New***: LogQuant는 대형 언어 모델(LLM)의 추론 중 KV Cache에 적용되는 혁신적인 2-bit quantization 기술로, 메모리 절감을 제공하면서도 성능을 우수하게 보존합니다. 로그 기반 필터링 메커니즘을 적용하여 기존 방법보다 메모리 사용을 줄이면서도 더 나은 성능을 유지합니다.
- ***Technical Details***: LogQuant는 다양한 모델과 하위 작업에서 높은 주의를 받는 위치가 로그 분포를 따른다는 관찰을 바탕으로, 특정 토큰의 중요성을 추정합니다. 이와 같이 추정된 중요성을 바탕으로 하여 2-bit quantization 기법을 도입하였으며, 이를 통해 KiVi, H2O와 같은 기존 방법들보다 더 중요한 토큰을 잘 보존하게 됩니다. 또한, KV Cache 항목의 절대 위치를 무시함으로써 quantization/dequantization 과정의 속도를 최적화하여, 처리량을 25% 증가시키고 배치 크기를 60% 늘릴 수 있습니다.
- ***Performance Highlights***: LogQuant는 Math와 Code Completion 같은 복잡한 작업들에서 기존 방법보다 40%에서 200% 더 높은 정확도를 달성하였으며, 2-비트 quantization 압축 비율에서도 탁월한 성능을 보임으로써 특히 작은 모델의 정확도 손실 문제를 효과적으로 해결합니다.

### [MCTS-RAG: Enhancing Retrieval-Augmented Generation with Monte Carlo Tree Search](https://arxiv.org/abs/2503.20757)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.20757.png)

Vote: 7

Authors: Yunhai Hu, Yilun Zhao, Chen Zhao, Arman Cohan

- ***What's New***: MCTS-RAG는 몬테카를로 트리 탐색(Monte Carlo Tree Search)과 검색 보강 생성(Retrieval-Augmented Generation; RAG)을 결합하여 작은 언어모델(Small Language Models)의 지식 집약적 과제에 대한 추론 능력을 향상시키는 혁신적인 접근법을 소개합니다. 이 방법은 검색과 추론을 동적으로 통합하여 의사 결정을 개선하고 환각(hallucinations)을 줄이며, 사실 정확도와 응답 일관성을 보장합니다.
- ***Technical Details***: MCTS-RAG 프레임워크는 검색과 추론을 반복적인 검색 기반 프로세스를 통해 개선합니다. MCTS-RAG는 검색 조치를 적절한 시점에 통합하며, 이후 역전파(backpropagation)를 통해 유익한 검색 경로를 강화합니다. 이 구조화된 검색 메커니즘은 모델이 더 정확한 추론을 위해 관련 정보를 효율적으로 획득하고 활용할 수 있도록 보장합니다.
- ***Performance Highlights***: 여러 지식 집약적 데이터셋에서의 실험 결과, MCTS-RAG는 소규모 언어모델을 사용하여 최첨단 대형 언어 모델(Large Language Model)과 유사한 성능을 달성했습니다. 특히, ComplexWebQA, GPQA, FoolMeTwice와 같은 벤치마크에서 Llama 3.1-8B와 Qwen2.5-7B 모델에 대해 각각 20% 이상, 15%, 10% 이상의 성능 향상을 보여주었습니다.

### [ViLBench: A Suite for Vision-Language Process Reward Modeling](https://arxiv.org/abs/2503.20271)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.20271.png)

Vote: 6

Authors: Xianfeng Tang, Weitao Feng, Hardy Chen, Cihang Xie, Haoqin Tu, Hui Liu

- ***What's New***: ViLBench는 시각-언어 PRM(가공피드백 모델) 평가를 위한 포괄적인 벤치마크로, 기존의 VLLM(비전 랭귀지 대형 모델)을 출력 보상 모델(ORM)과 프로세스 보상 모델(PRM)로 벤치마킹하며, 세밀한 단계별 평가를 요구합니다. ViLBench는 73,600개 이상의 시각-언어 프로세스 보상 데이터를 수집하여 기존 모델 대비 성능을 향상시킵니다.
- ***Technical Details***: ViLBench는 이미지와 관련된 문제를 해결하기 위해 단계별 보상을 필요로하는 PRM을 평가하는 벤치마크를 제공합니다. 이 벤치마크에는 다양한 모델과 과정의 세부 단계에 대한 점수를 할당하여 평가하는 체계가 포함됩니다. 또한, MCTS 기반의 검색 알고리즘을 이용해 각 단계에 대해 정밀한 보상 값을 할당합니다.
- ***Performance Highlights***: ViLBench에서 일부 최신 VLLM인 GPT-4o는 27.3%의 정확도를 기록하며 어려움을 나타냈습니다. ViLPRM은 3B 모델이 스텝 보상 데이터로 학습되어 약 3.3%의 성능 향상을 보여주며, 체계적인 스텝 보상 평가에서 기존 PRM을 능가합니다. 일관된 보상 평가를 통해 시각-언어 과제에서 향상된 성능을 보였습니다.

### [AccVideo: Accelerating Video Diffusion Model with Synthetic Dataset](https://arxiv.org/abs/2503.19462)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19462.png)

Vote: 5

Authors: Yunhong Wang, Yaohui Wang, Xihui Liu, Yu Qiao, Xinyuan Chen, Haiyu Zhang

- ***What's New***: AccVideo 연구에서는 합성 데이터셋(Synthetic Dataset)을 사용하여 비디오 생성의 확산 모델(Diffusion Model)을 가속화하는 새로운 방식을 제안했습니다. Diffusion Model은 고품질 비디오를 생성할 수 있지만 많은 추론 단계가 필요하여 시간이 많이 걸립니다. AccVideo는 이러한 추론 단계를 줄이고 비디오 생성 속도를 8.5배 개선하며 품질을 유지합니다.
- ***Technical Details***: 기존의 Diffusion Distillation 방법이 가진 쓸모없는 데이터 포인트를 제거하는 AccVideo라는 효율적인 Distillation Method가 제안되었습니다. 이를 위해 SynVid라는 110K의 고품질 합성 비디오와 디노이징 트랙터리를 포함한 합성 비디오 데이터셋이 생성되었습니다. Trajectory-based Few-step Guidance는 denoising trajectories의 핵심 데이터를 활용하여 노이즈에서 비디오로의 매핑을 학습합니다. 또한, Adversarial Training Strategy를 도입하여 합성 데이터셋과 학생 모델의 출력 분포를 정렬해 비디오 품질을 향상시킵니다.
- ***Performance Highlights***: AccVideo는 5초 길이의 720×1280 해상도 비디오를 생성하는 데 필요한 추론 시간을 기존 모델 대비 8.5배 줄였습니다. HunyuanVideo와 비교할 때, 이 방법은 텍스트-비디오 생성에서 높은 품질을 유지하면서도 더 빠른 속도를 제공합니다.

### [Attention IoU: Examining Biases in CelebA using Attention Maps](https://arxiv.org/abs/2503.19846)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19846.png)

Vote: 4

Authors: Vikram V. Ramaswamy, Tyler Zhu, Aaron Serianni, Olga Russakovsky

- ***What's New***: Attention-IoU는 주의 맵(Attention Map)을 활용하여 이미지 분류 모델의 내부 표현에서 발생하는 편향을 측정하는 새로운 지표입니다. 이를 통해 모델이 실제 사용하지 않아야 할 이미지의 특정 영역에 의존하는 비정상적 연관관계를 정량화합니다.
- ***Technical Details***: Attention-IoU는 GradCAM을 사용하여 속성에 대한 주의 맵을 얻고, 이를 바탕으로 두 개의 점수인 마스크 점수와 히트맵 점수를 제안합니다. 마스크 점수는 주의 맵과 실제 마스크를 비교하고, 히트맵 점수는 두 개의 속성의 주의 맵을 비교합니다. 이들 점수는 특정 속성이 다른 보호된 속성에 의해 얼마나 영향을 받는지 분석하는 데 사용됩니다.
- ***Performance Highlights***: Attention-IoU를 Waterbirds 데이터셋에서 검증한 결과, 데이터셋의 편향 정도에 따라 모델의 성능이 어떻게 달라지는지를 명확히 반영했습니다. 또한 CelebA 데이터셋에 대한 분석에서 각 속성이 남성(Male) 속성에 어떻게 영향을 받는지를 드러내면서, 특정 속성이 단순한 레이블의 상관관계 이상의 편향을 가질 수 있음을 보여주었습니다.

### [Mask^2DiT: Dual Mask-based Diffusion Transformer for Multi-Scene Long Video Generation](https://arxiv.org/abs/2503.19881)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19881.png)

Vote: 4

Authors: Shancheng Fang, Tianhao Qi, Hongtao Xie, Yongdong Zhang, Jiawei Liu, SiYu Zhou, Jianlong Yuan, Qian He, Wanquan Feng

- ***What's New***: Mask2DiT는 복수 장면의 긴 동영상 생성에 대한 혁신적인 접근법을 제안합니다. DiT(Diffusion Transformer) 아키텍처 내에서 각 비디오 세그먼트와 해당 텍스트 주석 간의 정밀한 1대1 정렬을 보장하는 이중 마스크 기반 주의 메커니즘을 도입하여 여러 장면에 걸친 시각적 일관성을 유지하고 의미적 정렬을 보장합니다.
- ***Technical Details***: Mask2DiT는 DiT 아키텍처의 각 주의 레이어에 대칭 이진 마스크(symmetric binary mask)를 도입하여 각 텍스트 주석이 해당 비디오 세그먼트에만 적용되도록 하여 정밀한 문맥 간 텍스트-시각적 일치(alignment)를 가능하게 합니다. 추가로 세그먼트 수준의 조건 마스크(segment-level conditional mask)를 도입하여 기존 장면을 기반으로 새 장면을 생성하는 오토레그레시브(autoregressive) 장면 확장을 지원합니다.
- ***Performance Highlights***: 정성적 및 정량적 실험은 Mask2DiT가 시각적 일관성을 유지하면서 각 세그먼트와 해당 텍스트 설명 간의 정확한 의미적 일치를 보장하는 데 탁월함을 입증했습니다. Visual Consistency(시각적 일관성)에서 70.95%, Sequence Consistency(순서 일관성)에서 47.45%를 기록하며 기존 SOTA(state-of-the-art) 방법들보다 우수한 성과를 보였습니다.

### [DINeMo: Learning Neural Mesh Models with no 3D Annotations](https://arxiv.org/abs/2503.20220)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.20220.png)

Vote: 3

Authors: Weijie Guo, Guofeng Zhang, Wufei Ma, Alan Yuille

- ***What's New***: DINeMo는 3D 주석 없이 신경망 메쉬 모델(Neural Mesh Model)을 학습하는 새로운 방식입니다. 대형 시각적 기초 모델(Visual Foundation Models)로부터 얻은 유사-대응(Pseudo-Correspondence)을 활용하여 3D 주석의 필요성을 제거하였습니다.
- ***Technical Details***: DINeMo는 SD-DINO를 활용하여 이미지와 메쉬 렌더링의 특징을 결합한 후, 코사인 유사도(Cosine Similarity)를 통해 유사-대응 매칭을 수행합니다. 이는 로컬(low-level) 및 글로벌(High-level) 정보 모두를 고려한 양방향 유사-대응 생성을 통해 일관된 키포인트(Consistent Keypoint) 유사-대응을 만듭니다.
- ***Performance Highlights***: 자동차 데이터 셋에서 DINeMo는 기존의 제로샷(Zero-Shot) 및 몇몇 샷(Few-Shot) 3D 자세 추정 방법을 넓게 앞서며, 완전 감독 방법과의 성능 간격을 67.3% 줄였습니다. 또한, 라벨이 없는 이미지(Unlabeled Images) 수를 늘려도 효과적으로 확장할 수 있어, 3D 주석 의존이 없는 장점을 보여주고 있습니다.

### [Efficient Model Development through Fine-tuning Transfer](https://arxiv.org/abs/2503.20110)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.20110.png)

Vote: 3

Authors: Fengyuan Liu, Tu Vu, Rishab Balasubramanian, Pin-Jie Lin, Nikhil Kandpal

- ***What's New***: 이 논문은 파인 튜닝(Fine-Tuning) 업데이트를 모델 버전 간에 전이하는 방법을 제안하여 LLMs의 개발 효율성을 향상시킵니다. 이는 새로운 버전의 사전 학습 모델이 출시될 때마다 반복적으로 고비용의 조정 과정을 필요로 하는 문제를 해결하는 전략입니다.
- ***Technical Details***: 파인 튜닝 전이(Fine-Tuning Transfer)는 소스 모델 버전에서 도출된 차이 벡터(diff vector)를 타겟 모델 버전에 적용하는 방식입니다. 이를 통해 새로운 베이스 모델(mt)에 차이 벡터(∆s)를 추가하여 사전 튜닝된 모델을 근사화합니다. 재활용 시나리오에서는 구형 모델 버전의 튜닝 업데이트를 신형 모델로 전이하고, 백포팅(Backporting) 시나리오에서는 신형 버전의 업데이트를 최적화된 특정 사용 사례로 적용합니다.
- ***Performance Highlights***: Llama 3.0 8B에서의 파인 튜닝 업데이트를 Llama 3.1 8B에 재활용한 결과, GPQA에서 절대 정확도가 10.7% 증가하여 추가 학습 없이 Llama 3.1 8B 지시 모델(Llama 3.1 Instruct)을 능가했습니다. 더욱이, 다국어 모델 개발 설정에서는 말라가시어 및 터키어에 대해 각각 4.7% 및 15.5%의 절대적인 개선을 이루었습니다. 이는 파인 튜닝 전이가 상당한 비용 절감을 이루면서도 경쟁력 있는 성능을 유지함을 나타냅니다.

### [Beyond Words: Advancing Long-Text Image Generation via Multimodal Autoregressive Models](https://arxiv.org/abs/2503.20198)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.20198.png)

Vote: 3

Authors: Zhengyuan Yang, Alex Jinpeng Wang, Linjie Li, Min Li, Lijuan Wang

- ***What's New***: 이 연구는 긴 문장을 포함한 텍스트 이미지 생성에서의 새로운 접근법을 소개합니다. LongTextAR은 다중 모달 자기회귀 모델(Multimodal Autoregressive Models)을 사용하여 고품질의 긴 텍스트 이미지를 생성하는 첫 번째 프레임워크로, 기존의 짧은 문장 처리에 머물던 방법들의 한계를 극복합니다.
- ***Technical Details***: 논문에서는 텍스트 중심의 바이너리 토크나이저(TextBinarizer)를 개발하여 텍스트의 세부 특징을 효과적으로 포착하도록 했으며, 이를 통해 자기회귀 모델 프레임워크에 통합했습니다. LongTextAR은 65,536의 어휘 크기를 가진 BPE 토크나이저를 사용하며, 다양한 서식 제어가 가능한 텍스트 렌더링을 지원합니다.
- ***Performance Highlights***: LongTextAR은 SD3.5 Large와 GPT-4o + DALL-E 3 대비 긴 텍스트 생성에서 더 높은 정확성과 유연성을 보여주었습니다. 실험 결과, 긴 텍스트 이미지 생성시 문서 및 슬라이드 생성과 같은 혁신적인 응용 가능성을 엽니다. OCR 정확도 측면에서도 가장 높은 성능을 기록했습니다.

### [ADS-Edit: A Multimodal Knowledge Editing Dataset for Autonomous Driving Systems](https://arxiv.org/abs/2503.20756)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.20756.png)

Vote: 3

Authors: Ziwen Xu, Bozhong Tian, Ningyu Zhang, Xiang Chen, Huajun Chen, Chenxi Wang, Jizhan Fang

- ***What's New***: ADS-Edit는 자율 주행 시스템(Autonomous Driving System; ADS)을 위한 다중모달 지식 편집 데이터셋입니다. 이번 연구에서는 ADS에서 발생하는 다양한 실제 시나리오와 데이터를 포함하여, 모델의 행동을 전체 재훈련 없이도 특정 대상을 조정할 수 있는 '지식 편집(knowledge editing)'을 소개합니다.
- ***Technical Details***: ADS-Edit 벤치마크는 인식(perception), 이해(understanding), 의사결정(decision making)이라는 세 가지 시나리오로 구성되어 있으며, 비디오, 다중 시각 이미지, 단일 이미지의 세 가지 데이터 타입으로 구성됩니다. 데이터 수집은 LingoQA, DriveLM, CODA-LM와 같은 기존 데이터셋을 활용했으며, 답변 간소화 및 QA 쌍 생성에 AI 모델을 사용하였습니다.
- ***Performance Highlights***: GRACE와 WISE와 같은 메모리 기반 편집 방법은 믿음이 높은 편이었으며, GRACE는 LLaVA-Onevision과 Qwen2-VL 모델에서 100% 수정율을 달성했습니다. 그러나 일반화에서는 GRACE의 성능이 떨어졌습니다. 연속적인 지식 업데이트가 필요한 실제 주행 시나리오에서, 다양한 방법들의 성능이 시간이 흐르면서 감소하는 경향을 보였으나, WISE는 주기억-보조기억 메커니즘으로 인해 상대적으로 안정적인 성능을 보여줬습니다.

### [Image as an IMU: Estimating Camera Motion from a Single Motion-Blurred Image](https://arxiv.org/abs/2503.17358)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.17358.png)

Vote: 3

Authors: Jerred Chen, Ronald Clark

- ***What's New***: 이 연구는 단일 모션 블러된 이미지로부터 카메라의 움직임을 추정하는 새로운 프레임워크를 제안합니다. 기존 방법들이 모션 블러를 불필요한 아티팩트로 간주하는 것과 달리, 이 연구는 모션 블러를 활용하여 카메라의 빠른 움직임을 측정하는 IMU(관성측정장치)와 유사한 측정을 가능하게 합니다.
- ***Technical Details***: 제안된 모델은 모션 블러된 이미지로부터 포화 흐름장(Dense Motion Flow Field)과 단안 깊이 지도(Monocular Depth Map)를 예측합니다. 이후 작은 움직임을 가정한 선형 최소 제곱 문제(Linear Least Squares Problem)를 해결하여 즉각적인 카메라 속도를 복구합니다. 이 연구를 위해 ScanNet++v2에서 추출한 대규모 합성 모션 블러 데이터세트를 구축하였고, 완전 기울기 역전파 가능한 파이프라인을 통해 실제 데이터로 모델을 정제합니다.
- ***Performance Highlights***: 실험 결과, 제안된 방법은 실제 모션 블러 벤치마크에서 최첨단 수준의 각속도 및 번역 속도 추정을 달성하며, 기존 방법들(MASt3R, COLMAP 등)을 능가하는 성능을 보입니다. 특히, 제안된 방법은 30 FPS에서 실시간으로 동작하면서 기존 방법보다 훨씬 정확한 속도 추정을 제공합니다.

### [Unlocking Efficient Long-to-Short LLM Reasoning with Model Merging](https://arxiv.org/abs/2503.20641)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.20641.png)

Vote: 2

Authors: Mingxuan Yuan, Shuqi Liu, Tao Zhong, Xiaojin Fu, Han Wu, Hui-Ling Zhen, Xiongwei Han, Xing Li, Zehua Liu, Yuxuan Yao

- ***What's New***: 이 연구에서는 System 1과 System 2 모델의 효율성을 결합하여 LLM의 Long-to-Short(L2S) 추론 문제를 효과적으로 해결하기 위해 모델 합병 방법을 도입했습니다. 이는 모델 파라미터에 직접 작용하여 추가 학습 없이 추론 길이를 최대 55%까지 줄이면서 성능을 유지하거나 향상시킵니다.
- ***Technical Details***: 모델 합병에서 사용되는 다양한 방법으로는 task-vector 기반, SVD 기반, 활성화 기반(Activation-based) 방법이 있습니다. Task-vector 기반 합병은 평균 합병(Average Merging), Task Arithmetic, Ties-Merging, DARE를 포함하며, 활성화 기반 방법으로 AIM과 Sens-Merging이 있습니다. 실험을 통해 작은 모델이 L2S 추론 능력을 강화하기 위해 파라미터 합병을 통해 학습하는 데 어려움을 겪는 반면, 큰 모델들은 길이 감소를 시도할 때 성능 유지가 어려움을 발견했습니다.
- ***Performance Highlights***: 실험에서 task-vector 기반의 TA와 Ties-Merging 방법은 기존 성능을 유지하면서 약 50%의 응답 길이 감소를 이뤄냈습니다. 반면 SVD 기반 합병 방법은 제한적인 효과를 보였으며, 활성화 기반 방법은 반응 길이 압축 비율 측면에서 뛰어난 성과를 보였으나, 캘리브레이션 데이터 선택에 민감했습니다.

### [Sparse Logit Sampling: Accelerating Knowledge Distillation in LLMs](https://arxiv.org/abs/2503.16870)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16870.png)

Vote: 2

Authors: Taehwak Kwon, Akhil Kedia, Kangwook Lee, Joohyung Lee, Jinwoo Ahn, Mohd Abbas Zaidi, Anshumann, Haejun Lee

- ***What's New***: Sparse Logit Sampling은 대형 언어 모델(LLM)에서 지식 증류(Knowledge Distillation)를 가속화하기 위한 방법을 제안합니다. 이 연구는 전통적인 상위-K 확률 저장 방식을 대신하여 중요성 표본 추출(Importance Sampling)을 이용한 새로운 접근 방식을 통해 편향 없는 추정치를 제공하는 방법론을 제시합니다.
- ***Technical Details***: 기존의 상위-K 확률 저장 방식은 교사 모델의 분포를 왜곡하여 성능 저하를 초래합니다. 이를 해결하기 위해 제안된 '무작위 샘플링 지식 증류(Random Sampling Knowledge Distillation)'는 교사 분포를 무작위로 샘플링하여 편향 없는 추정치를 제공하며, 그래디언트를 기대치 수준에서 보존합니다. 이렇게 하면 10% 미만의 오버헤드로 빠른 학습이 가능하고, 300M에서 3B에 이르는 다양한 모델 크기에서 경쟁력 있는 성능을 유지할 수 있습니다.
- ***Performance Highlights***: 제안된 방법은 적은 수의 로그값을 저장하면서도 온전한 증류의 성능을 유지합니다. 특히, 'Random Sampling KD' 방법은 기존의 상위-K 방법보다 그래디언트 유사성이 뛰어나고, 적은 샘플링 토큰으로도 높은 로스 성능과 스펙트럼 디코딩(LM Loss)에서 'FullKD'와 유사한 결과를 보여 주었습니다.

### [Self-Supervised Learning of Motion Concepts by Optimizing Counterfactuals](https://arxiv.org/abs/2503.19953)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19953.png)

Vote: 2

Authors: Daniel LK Yamins, Jiajun Wu, Seungwoo Kim, Stefan Stojanov, Rahul Venkatesh, Kevin Feigelis, David Wendt

- ***What's New***: 이 논문에서는 Opt-CWM이라는 자가 지도 학습(Self-Supervised Learning) 기법을 소개합니다. 이는 사전 훈련된 비디오 예측 모델을 기반으로 운동 추정(Motion Estimation)을 위한 최첨단 성능을 제공합니다. 기존의 고정된 경험적 규칙(Heuristics)을 대체하고 상황에 의해 제한받지 않는 비디오 입력에서 흘수(flow)와 폐영(occlusion)을 추정합니다.
- ***Technical Details***: Opt-CWM은 사전 학습된 다중 프레임 모델(Counterfactual World Modeling, CWM)을 기반으로 동작하며, 광류 및 폐영을 추정하기 위해 '흐름-조정 프레딕터(Flow-conditioned Predictor)'와 '교란 생성기(Perturbation Generator)'를 최적화합니다. 교란 생성기는 비디오의 로컬 외관에 특화된 교란을 학습된 인공신경망(Neural Network)을 통해 생성합니다. 이는 어떠한 지도 학습도 없이 비디오의 비대칭 마스킹 원리를 확장하여 학습이 진행됩니다.
- ***Performance Highlights***: 실제 월드 벤치마크에서 Opt-CWM은 최첨단 자가 지도 운동 추정 방법보다 성능이 우수했으며, 복잡한 운동을 포함하는 다양한 시나리오에서 감독된 방법을 능가할 수 있음을 보였습니다. 이 방법은 대규모 자가 지도 학습의 잠재력을 보여주며, 운동 추정을 위한 스케일 가능한 대안적 접근법을 제시합니다.

### [RONA: Pragmatically Diverse Image Captioning with Coherence Relations](https://arxiv.org/abs/2503.10997)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10997.png)

Vote: 1

Authors: Aashish Anantha Ramakrishnan, Aadarsh Anantha Ramakrishnan, Dongwon Lee

- ***What's New***: 해당 연구에서 제안된 RONA는 대형 멀티모달 언어 모델(MLLMs)을 위한 새로운 프롬프트 전략으로, Coherence Relations (CRs)을 활용하여 보다 실질적으로 다양한 이미지 캡션을 생성합니다. RONA는 다수의 도메인에서 MLLM 기존 기준선을 능가하는 전반적 다양성과 실제 유사성의 캡션을 생성함을 입증하였습니다.
- ***Technical Details***: RONA는 Coherence Relations (CRs)를 이용하여 MLLM 프롬프트에 변화를 줌으로써, 다기관 의미별 다양성을 지닌 캡션을 생성합니다. CRs는 이미지 텍스트 관계의 구조적 개요를 제공하며, RONA는 두 가지 수준에서 이를 분류합니다: 개체 수준 관계와 장면 수준 관계. 5가지 유형의 CRs에는 삽입, 구체화, 투영, 재진술, 확장이 포함되며, 이는 프롬프트 지침으로서 각기 다른 문맥적 역할을 수행합니다.
- ***Performance Highlights***: RONA와 결합된 GPT-4o 및 Claude 모델은 다양한 설정에서 기존 모델 기준선을 초과하는 성능을 보여 주었습니다. 특히, CLIPScore와 BLEURT 같은 유사성 메트릭에서 RONA 기반 모델이 8개의 실험 설정에서 각각 7번과 6번 기준선을 능가하였으며, Self-BLEURT와 Div-2 같은 다각적 다양성 메트릭에서도 개선된 성능을 보였습니다. 이는 전반적인 컨텍스트 일관성을 상실하지 않고도 캡션의 다양성을 향상시켰음을 나타냅니다.

### [Trajectory Balance with Asynchrony: Decoupling Exploration and Learning for Fast, Scalable LLM Post-Training](https://arxiv.org/abs/2503.18929)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18929.png)

Vote: 1

Authors: Brian R. Bartoldson, Moksh Jain, James Diffenderfer, Yoshua Bengio, Bhavya Kailkhura, Siddarth Venkatraman, Seanie Lee, Johan Obando-Ceron, Tal Ben-Nun, Minsu Kim

- ***What's New***: Trajectory Balance with Asynchrony (TBA)는 대형 언어 모델(LLM)의 후처리를 위한 새로운 강화 학습 프레임워크로, 탐색과 학습을 비동기적으로 분리하여 빠르고 확장 가능한 훈련을 제공합니다. TBA는 이력 균형(Trajectory Balance; TB)를 기반으로 하는 오프-정책 학습 목표를 사용하여, 분산된 탐색자 노드가 다양한 데이터 경로를 생성하고 이를 중앙 재생 버퍼에 저장함으로써 효율적인 데이터 활용을 가능하게 합니다.
- ***Technical Details***: TBA는 탐색 노드가 독립적으로 다양한 경로를 생성하고, 이를 재생 버퍼에 저장하는 구조입니다. 중앙 훈련 노드는 이 버퍼에서 보상 또는 최근성에 기반해 데이터를 샘플링하여 정책을 업데이트합니다. TBA는 GFlowNets의 이력 균형 목표를 사용하여 비동기적으로 훈련 시간을 4배 이상 단축시킬 수 있습니다.
- ***Performance Highlights***: TBA는 수학적 추론, 기호 조정, 자동화된 Red-Teaming 실험에서 뛰어난 속도와 성능을 보여주고 있습니다. 특히, GSM8K 테스트에서는 기존 베이스라인들보다 훈련 속도가 50배 빠른 것으로 나타났으며, 성능도 개선되었습니다. 이러한 결과는 TBA가 다양한 탐색과 희소 보상 설정에서 강력한 탐색 역량을 발휘함을 나타냅니다.

### [UniHDSA: A Unified Relation Prediction Approach for Hierarchical Document Structure Analysis](https://arxiv.org/abs/2503.15893)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.15893.png)

Vote: 1

Authors: Qiang Huo, Kai Hu, Jiawei Wang

- ***What's New***: UniHDSA는 문서 구조 분석에서 다양한 서브 태스크를 관계 예측 문제로 간주하고, 이를 통합 레이블 공간(Unified Label Space)에 병합하여 한 모듈로 여러 태스크를 동시에 처리할 수 있게 하는 새로운 접근법입니다. 이 방법은 페이지 수준 및 문서 수준 구조 분석을 두 가지 주요 단계로 단순화하여 시스템의 효율성과 확장성을 높입니다.
- ***Technical Details***: UniHDSA는 Transformer 아키텍처를 기반으로 한 멀티모달 종단 간 시스템을 개발하여 시각적 및 언어적 피처를 결합한 강력한 표현을 생성합니다. 페이지 수준에서는 텍스트 라인과 그래픽 객체를 연관하여 다양한 관계 예측 태스크를 통합 관계 예측 헤드로 처리합니다. 문서 수준에서는 문서의 전체적 계층 구조를 이해하기 위해 텍스트 블록과 그래픽 객체의 관계를 통해 논리적 연결성을 분석합니다.
- ***Performance Highlights***: UniHDSA는 Comp-HRDoc 벤치마크에서 최첨단 성능을 달성하였으며, DocLayNet의 대규모 문서 레이아웃 분석 데이터셋에서도 경쟁력 있는 결과를 보여주었습니다. 특히, 텍스트와 그래픽 요소를 통합하여 모델링함으로써 문서 구조 분석에서 일관되면서도 효율적인 관계 예측을 가능하게 했습니다.

### [Any6D: Model-free 6D Pose Estimation of Novel Objects](https://arxiv.org/abs/2503.18673)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18673.png)

Vote: 1

Authors: Bowen Wen, In So Kweon, Kuk-Jin Yoon, Minjun Kang, Taeyeop Lee, Gyuree Kang

- ***What's New***: Any6D는 새로운 방식의 모델-프리(model-free) 6D 물체 자세 추정 프레임워크로, 미지의 신환경에서도 단 하나의 RGB-D 앵커 이미지 만을 사용하여 물체의 6D 자세와 크기를 추정할 수 있습니다. 현재의 텍스처가 있는 3D 모델이나 다중 시점 대신 결합된 객체 정렬 및 렌더링 전략을 통해 2D-3D 정렬과 메트릭 크기 추정의 정확성을 높였습니다.
- ***Technical Details***: Any6D는 물체의 6D 자세를 추정하기 위해 단일 RGB-D 앵커 이미지와 쿼리 이미지를 활용하는 시스템으로, 참조 이미지에서 단일 이미지 to 3D 모델을 통해 초기 객체 형상을 생성하고, 객관적인 물체를 2D와 3D 공간에서 정렬하여 메트릭 크기 물체 형상을 추정합니다. 이후, 이 정보와 쿼리 이미지를 사용해 물체의 상대 자세를 추론합니다. 이 과정은 다양한 환경과 시점, 가려짐 등을 효과적으로 처리할 수 있도록 설계되었습니다.
- ***Performance Highlights***: Any6D는 REAL275, Toyota-Light, HO3D, YCBInEOAT, LM-O를 포함한 5개의 데이터셋에서 실험을 통해 기존 최첨단 방법들을 능가하는 성능을 입증했습니다. HO3D 데이터셋에서는 ADD-S, ADD, AR 메트릭에서 각각 98.7%, 40.4%, 38.3%의 우수한 결과를 기록했습니다. 이처럼, Any6D는 다양한 시나리오에서 높은 정확도의 물체 자세 추정 성능을 보여주고 있습니다.

### [RecTable: Fast Modeling Tabular Data with Rectified Flow](https://arxiv.org/abs/2503.20731)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.20731.png)

Vote: 0

Authors: Masane Fuchi, Tomohiro Takagi

- ***What's New***: RecTable은 정류 흐름(rectified flow) 접근을 활용하여 대규모 언어 모델이나 확산 모델(diffusion models) 기반의 방법들보다 더 빠른 학습 기간으로 표 데이터를 생성하는 새로운 방법입니다.
- ***Technical Details***: RecTable은 간단한 네트워크 아키텍처를 사용하며, 정류 흐름 모델링을 사용합니다. 이 모델은 Gated Linear Units (GLU)을 사용하여 비선형적인 특징 간의 관계를 잘 포착합니다. 또한, 로그-정규(logit-normal) 시간 간격 분포와 혼합형 데이터에 맞춘 노이즈 분포를 통해 효율적인 학습 전략을 채택하였으며, 코드 기반은 PyTorch로 구현되어 있습니다.
- ***Performance Highlights***: RecTable은 여섯 가지 실제 데이터셋에서 평가되었으며, 많은 데이터셋에서 기존 최첨단 방법들과 비교해 경쟁력 있는 성능을 보여주었습니다. 특히 데이터 학습 효율성(Machine Learning Efficiency; MLE)을 통해 고품질의 데이터를 보다 빠르게 생성할 수 있음을 입증하였습니다.

### [PathoHR: Breast Cancer Survival Prediction on High-Resolution Pathological Images](https://arxiv.org/abs/2503.17970)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.17970.png)

Vote: 0

Authors: Yutong Xie, Jun Liu, Yu Lu, Shiru Wang, Jiaxuan Xiao, Rundong Xue, Hao Zhang, Zeyu Zhang, Yang Zhao, Yang Luo

- ***What's New***: PathoHR는 유방암 생존 예측을 위한 새로운 파이프라인으로, 고해상도 병리 이미지를 활용하여 더 나은 특징 학습을 가능하게 합니다. 이 접근 방식은 직접적으로 플러그 앤 플레이 방식의 고해상도 비전 트랜스포머(Vision Transformer; ViT)를 통합하여 패치 단위로 병리 이미지를 처리하고, 여러 고급 유사도 메트릭(Similarity Metrics)을 평가하여 종양 특성을 효과적으로 포착합니다. 이를 통해 작은 이미지 패치가 더 큰 원본 패치와 동등하거나 더 높은 예측 정확도를 달성하며, 계산 오버헤드를 크게 줄일 수 있습니다.
- ***Technical Details***: PathoHR 파이프라인은 (1) 패치 단위의 특징 추출, (2) 토큰 합병을 위한 유사도 계산을 통한 향상된 표현학습, (3) 다중 해상도 패치 단위 병리 이미지를 처리하는 플러그 앤 플레이 ViT 모듈로 구성됩니다. Whole Slide Images (WSIs)를 처리하여 낮은 차원의 특징을 추출하며, Otsu 방법을 사용하여 조직 경계를 식별하고 분석에 적합한 조직만 유지합니다.
- ***Performance Highlights***: PathoHR는 1,036개의 Whole Slide Images (WSIs)를 사용하여 생존 예측을 수행하고, 16×16 픽셀 패치가 24×24 패치보다 더 높은 AUC 및 F1 점수를 기록하며 더 나은 예측 성능을 보여주었습니다. 이는 고해상도 특징 학습의 효율성을 나타내며, 이 방법은 상당한 계산 비용을 절약하면서도 진단적으로 중요한 특징을 강조할 수 있음을 보여줍니다.
- ***Ablation Study***: 다양한 설계 방안을 비교하는 실험에서, 코사인 유사성을 활용하거나 풀링을 사용하는 경우 잔여 연결을 사용했을 때 가장 높은 성능을 보였습니다. 이 실험 결과는 PathoHR 파이프라인이 고해상도 병리 이미지를 사용하여 더 작고 효율적인 패치에서도 우수한 성능을 보일 수 있음을 시사합니다.

