## Daily Papers (2025-03-20)

### [φ-Decoding: Adaptive Foresight Sampling for Balanced Inference-Time Exploration and Exploitation](https://arxiv.org/abs/2503.13288)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.13288.png)

Vote: 39

Authors: Chang Ma, Hang Yan, Haiteng Zhao, Jun Liu, Fangzhi Xu, Qika Lin, Zhiyong Wu

- ***What's New***: ϕ-Decoding은 LLMs(Large Language Models)의 추론 성능을 개선하기 위한 새로운 추론시간 최적화 알고리즘입니다. 이 알고리즘은 적응형 foresight 샘플링을 사용해 탐색과 개발의 균형을 맞추어 효율성을 향상시킵니다.
- ***Technical Details***: ϕ-Decoding은 foresight 경로에서 획득한 두 분포를 활용해 각 단계의 최적화 된 값을 추정합니다. 이 알고리즘은 in-width와 in-depth 가지치기(pruning) 전략을 도입하여 추가적인 계산 자원을 효율적으로 할당하도록 설계되었습니다. 이러한 전략은 추론 시간을 절감하면서도 성능을 극대화합니다.
- ***Performance Highlights***: ϕ-Decoding은 여러 추론 벤치마크에서 LLaMA3.1-8B-Instruct에 대해 _14%_ 이상의 평균 성능 향상을 이루었으며, Mistral-v0.3-7B-Instruct에서는 _6.92%_의 성능 개선이 나타났습니다. 특히 FLOPS 계산 비용에서 경제성을 유지하면서도 우수한 성능을 지속적으로 보여주었습니다.

### [DeepMesh: Auto-Regressive Artist-mesh Creation with Reinforcement Learning](https://arxiv.org/abs/2503.15265)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.15265.png)

Vote: 34

Authors: Zhengyi Wang, Guangce Liu, Yiwen Chen, Ruowen Zhao, Junliang Ye, Yikai Wang, Jun Zhu

- ***What's New***: DeepMesh는 새로운 토큰화 알고리즘(tokenization algorithm)과 강화학습(reinforcement learning; RL)을 결합한 3D 메시 생성(framework)입니다. Direct Preference Optimization(DPO)를 도입하여 인간의 선호도를 반영한 메시 생성 최적화가 가능해졌습니다.
- ***Technical Details***: DeepMesh는 고해상도 메시의 시퀀스 길이를 72% 줄이는 개선된 토큰화 알고리즘을 사용하며, 학습 효율성을 높이기 위해 데이터 큐레이션 및 패키징 전략을 사용합니다. RL을 활용하여 DPO를 통해 인간의 선호도와 메시 아웃풋을 정렬하고 있습니다. 이는 자오르윈과 같은 다양한 토폴로지 생성을 가능하게 합니다.
- ***Performance Highlights***: DeepMesh는 최대 30,000개의 얼굴로 이루어진 높은 품질의 예술적 메시를 생성하며, 다양한 메트릭에서 기존의 방법보다 우수한 성능을 보여줍니다. 실험 결과, 양적 및 질적 측면 모두에서 현저히 개선된 성능이 나타났습니다.

### [TULIP: Towards Unified Language-Image Pretraining](https://arxiv.org/abs/2503.15485)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.15485.png)

Vote: 32

Authors: Long Lian, Alane Suhr, Zineng Tang, XuDong Wang, Roei Herzig, Trevor Darrell, Seun Eisape, Adam Yala, David M. Chan

- ***What's New***: TULIP은 기존 CLIP과 유사한 모델들을 대체할 수 있는 이미지-텍스트 대비 학습(Image-Text Contrastive Learning) 모델로, 정교한 시각적 특징을 학습하면서도 전반적인 의미 정렬(Global Semantic Alignment)을 유지하는 데 중점을 둔 새로운 오픈소스 모델입니다. 이 접근 방식은 생성적 데이터 증강(Generative Data Augmentation), 이미지-이미지 및 텍스트-텍스트 대비 학습(Image-Image and Text-Text Contrastive Learning), 이미지/텍스트 재구성 규제를 통해 시각적 특징을 강화합니다.
- ***Technical Details***: TULIP은 이미지-텍스트 대비 학습의 글로벌 시맨틱 특징의 유지와 연결된 시각적 상세 정보를 학습하기 위해 패치 수준의 글로벌 및 로컬 멀티-크롭 증강(Global and Local Multi-Crop Augmentations) 및 목표를 도입합니다. TULIP은 iBOT과 DINO와 같은 방법에서 영감을 얻어 복잡한 시각적 이해를 위한 재구성 목표도 포함합니다. 또한, Diffusion Model을 기반으로 한 생성적 데이터 증강 전략을 제안하여 세밀한 시맨틱 정렬을 개선합니다.
- ***Performance Highlights***: TULIP은 여러 벤치마크에서 기존 최신 모델(SOTA)들을 능가하며, ImageNet-1K에서 새로운 SOTA 제로-샷 성능을 수립했습니다. RxRx1의 대화 상호 작용에서는 기존 모델 대비 최대 2배 향상된 성능을 보였으며, MMVP에서는 SigLIP 대비 3배 더 높은 점수를 기록했습니다. TULIP은 시각-언어 모델에 대한 다운스트림 작업에서 강력한 성능을 보여줍니다.

### [Cube: A Roblox View of 3D Intelligence](https://arxiv.org/abs/2503.15475)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.15475.png)

Vote: 21

Authors: Naveen Marri, Maurice Chu, Marcel van Workum, Kangle Deng, Haomiao Jiang, Jessica Wang, Liangjun Feng, Bryce Erickson, Thomas Lane, Jean-Philippe Fauconnier, Yinan Zhang, Skylar Litz, Anupam Singh, Alejandro Pelaez, Akash Garg, David Baszucki, Yiheng Zhu, Daiqing Li, Lukas Kuczynski, Alexander Weiss, Leon Liu, Nishchaie Khanna, David Harmon, Anirudh Sudarshan, Peiye Zhuang, Maneesh Agrawala, Derek Liu, Ravi Kiran Chirravuri, Michael Palleschi, Denis Goupil, Charles Shang, Xiaoxia Sun, Kayvon Fatahalian, Ravali Kandur, Anying Li, Salil Pathare, Kyle Price, Jihyun Yoon, Christian Reiser, Foundation AI Team, Tijmen Verhulsdonck, Brian Yin, Kiran Bhat, Tinghui Zhou, Karun Channa

- ***What's New***: 이 논문은 Roblox에서 3D 지능을 위한 퍼운데이션 모델을 구축하기 위한 첫 번째 단계로, 3D 형상 토큰화(Shape Tokenization) 기법을 제안합니다. 이 접근법은 3D 오브젝트와 장면을 생성하고, 캐릭터를 리깅하며, 오브젝트 행동을 설명하는 프로그래밍 스크립트를 생성하는 것을 목표로 합니다.
- ***Technical Details***: 3D 형상 토큰화는 형상을 불연속 토큰으로 변환하는 과정으로, 입력 메쉬를 샘플링하여 위상 변조 위치 인코딩(Phase-Modulated Positional Encoding) 방법으로 임베딩하고, Perceiver 기반의 Transformer를 통해 연속적인 잠재 벡터로 인코딩합니다. 최적 수송 벡터 양자화(Optimal Transport Vector Quantization)를 적용하여 불연속 형상 토큰으로 변환하며, 훈련의 안정성을 높이기 위해 확률적 그래디언트 숏컷(Stochastic Gradient Shortcut)을 도입하였습니다.
- ***Performance Highlights***: Ours-VQ 및 Ours-KL 모델이 CraftsMan보다 S-IoU 및 V-IoU 메트릭에서 우수한 성능을 보였습니다. 형상 재구성 품질은 Toys4K 데이터셋에서 검증되었으며, 특히 복잡한 형태의 세부 사항에 대해 높은 충실도를 보여줍니다. PMPE 기법을 통해 보다 나은 공간적 구별을 구현하여 임베딩 공간에서 유사한 형상을 더 유사하게 매핑할 수 있습니다.

### [Efficient Personalization of Quantized Diffusion Model without Backpropagation](https://arxiv.org/abs/2503.14868)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.14868.png)

Vote: 18

Authors: Kyungryeol Lee, Hoigi Seo, Wongi Jeong, Se Young Chun

- ***What's New***: 이 논문에서는 역전파(Backpropagation) 없이 정량화된 디퓨전 모델(Quantized Diffusion Model)을 개인화하는 새로운 기법 ZOODiP를 소개합니다. 이는 텍스트 인버전(Textual Inversion)과 0차 최적화(Zeroth-Order Optimization)를 활용하여 메모리 소모를 대폭 줄이면서 개인화가 가능합니다.
- ***Technical Details***: ZOODiP는 텍스트 인버전 기법을 통해 개인화 토큰을 최적화하며, 역전파 없이 메모리 효율성을 높인 0차 최적화를 활용합니다. '하위 공간 그래디언트(Subspace Gradient)'를 도입해 토큰의 주요 변화가 일어나는 차원에서 불필요한 노이즈를 제거하고 효율적 학습을 가능하게 합니다. 또한, 텍스트 임베딩이 효과적으로 작용하는 부분을 집중적으로 학습하기 위한 '부분 균일 시간 스텝 샘플링(Partial Uniform Timestep Sampling)' 기법을 제안합니다.
- ***Performance Highlights***: ZOODiP는 DreamBooth 데이터셋에서 기존 방법과 유사한 텍스트-이미지 정렬 점수 성능을 달성하면서도, 메모리 요구량을 최대 8.2배까지 줄이는 성과를 보였습니다. 특히 2.37GB의 VRAM만을 사용하여 병목 자원이 많은 환경에서도 개인화를 수행할 수 있음을 입증했습니다.

### [Temporal Regularization Makes Your Video Generator Stronger](https://arxiv.org/abs/2503.15417)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.15417.png)

Vote: 18

Authors: Yexin Liu, Xianfeng Wu, Harry Yang, Ser-Nam Lim, Yajing Bai, Haojian Huang, Wen-Jie Shu, Harold Haodong Chen

- ***What's New***: 이 연구에서는 비디오 생성 모델의 시간적 품질을 향상시키는 새로운 데이터 증강 전략인 FLUXFLOW를 소개합니다. FLUXFLOW는 모델 아키텍처의 변경 없이도 시간적 왜곡을 통제하며, 다양한 비디오 생성 모델에서 시간적 일관성과 다양성을 상당히 개선합니다.
- ***Technical Details***: FLUXFLOW는 두 가지 수준의 시간적 왜곡을 도입합니다: 프레임 수준에서 개별 프레임을 무작위로 섞어서 고정된 시간 순서를 방해하고, 블록 수준에서 연속 프레임 블록을 재정렬하여 현실적인 시간적 왜곡을 시뮬레이션합니다. 이러한 프로세스를 통해 모델이 다양한 움직임 및 광학 흐름(dynamic and optical flow dynamics)을 학습하도록 유도합니다.
- ***Performance Highlights***: UCF-101 및 VBench 벤치마크에서 실행한 광범위한 실험을 통해 FLUXFLOW가 여러 비디오 생성 모델(U-Net, DiT, AR 기반 아키텍처)에서 시간적 일관성을 개선하면서도 공간적 충실도를 유지한다는 것을 입증했습니다. 또한, 시간적 다양성도 크게 향상되었습니다.

### [VERIFY: A Benchmark of Visual Explanation and Reasoning for Investigating Multimodal Reasoning Fidelity](https://arxiv.org/abs/2503.11557)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.11557.png)

Vote: 18

Authors: Jing Bi, Guangyu Sun, Susan Liang, Ali Vosoughi, Chen Chen, Jinxi He, Luchuan Song, Chenliang Xu, Jiarui Wu, Yunlong Tang, Junjia Guo

- ***What's New***: VERIFY는 최첨단 멀티모달 대형 언어 모델(Multimodal Large Language Models; MLLMs)의 시각적 추론 능력을 엄격하게 평가하기 위해 설계된 새로운 벤치마크입니다. 시각 정보에 주로 의존하여 모델이 텍스트 입력 없이도 추론할 수 있도록 하고 있으며, 최초로 인공지능의 의사결정 과정을 심층 평가하는 인간 주석 기반의 추론 경로(Reasoning Path)를 제공합니다.
- ***Technical Details***: VERIFY 벤치마크는 Raven’s Progressive Matrices에서 영감을 받아 디자인되었으며, 시각적 정보에 의해 주로 결정되는 문제들을 포함하고 있습니다. 각 문제에는 간결한 텍스트 질문과 이미지, 답변 옵션이 포함되어 있으며, 이로 인해 텍스트 입력과의 의존성을 최소화하여 비언어적 큐에 주로 의존하는 추론을 유도합니다. 새로운 평가 지표를 도입하여 단순 정확도 외에 시각적 추론 충실도를 평가합니다.
- ***Performance Highlights***: OpenAI, Google 등 주요 MLLM 모델을 포괄적으로 평가한 결과, 가장 진보된 모델조차도 정확도 21.7%에 불과하여 무작위 추정(25%) 보다 낮았습니다. 이는 현재의 MLLMs가 시각적 추론에서 상당한 도전 과제를 안고 있음을 나타내며, 추론 경로의 깊이와 질적 평가가 중요하다는 점을 강조합니다. 모델들이 인식, 추상화, 연역의 세 단계별로 얼마나 잘 전환되는지를 독립적으로 평가해 모델의 추론 프로세스를 더 정확하게 이해합니다.

### [Optimizing Decomposition for Optimal Claim Verification](https://arxiv.org/abs/2503.15354)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.15354.png)

Vote: 16

Authors: Noah Ziems, Hy Dang, Meng Jiang, Yining Lu

- ***What's New***: 이 논문은 새로운 동적 분해(dynamics decomposition) 프레임워크를 제안하여 사실 확인 시스템에서 분해-검증(decompose-then-verify) 패러다임의 최적화를 시도합니다. 이 접근 방식은 원본 주장을 검증자가 선호하는 정보 밀도(atomicity) 수준으로 분해하는 정책을 학습합니다.
- ***Technical Details***: 동적 분해는 강화 학습(reinforcement learning) 프레임워크로 표준 NP 하드 문제를 근사하는 방법으로 소개됩니다. 이 접근 방식은 상위 최적화 문제와 하위 최적화 문제로 구성된 이중 수준 최적화 문제(bilevel optimization problem)로 정의되며, 검증자로부터 피드백을 활용하여 주장을 검증자가 선호하는 정보 밀도로 분해하는 정책을 학습하여 최적의 분해 정책을 찾아냅니다.
- ***Performance Highlights***: 동적 분해 방법은 기존의 분해 정책보다 검증 신뢰도(verification confidence)를 평균 0.07, 정확도를 0.12 증가시켰습니다. 이러한 결과는 다양한 데이터 셋과 검증기(verifiers)에서 일관되게 나타납니다.

### [MetaLadder: Ascending Mathematical Solution Quality via Analogical-Problem Reasoning Transfer](https://arxiv.org/abs/2503.14891)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.14891.png)

Vote: 15

Authors: Mengzhang Cai, Qizhi Pei, Lijun Wu, Xin Gao, Honglin Lin, Yu Li, Zhuoshi Pan, Conghui He

- ***What's New***: MetaLadder는 대형 언어 모델(LLMs)의 수학적 문제 해결 능력을 강화하기 위해 인간과 유사한 유추 문제 해결 방법을 포함한 새로운 프레임워크입니다. 이 프레임워크는 LLMs가 주어진 문제를 해결하기 전에 구조적 또는 의미적으로 유사한 메타-문제를 회상하고 성찰하도록 명시적으로 유도하여, LLMs가 예시를 통해 학습하고 일반화하는 능력을 부여합니다.
- ***Technical Details***: MetaLadder 프레임워크는 문제의 이해를 향상시키기 위해 원래 문제를 재생성하는 문제-다시진술 메커니즘을 도입하여, 유추 문제와 그 해결 방법을 기반으로 한 유추적 추론을 수행합니다. 반영 데이터를 생성하고 모델의 유추적 추론을 활성화하는 데이터 구성을 통해, 모델이 자율적으로 데이터를 생성하여 자기발전을 할 수 있는 기회를 제공합니다.
- ***Performance Highlights***: MetaLadder는 GSM8K 및 MATH와 같은 수학적 벤치마크에서 성능을 크게 개선하여, 표준 Chain-of-Thought 기반 방법을 12.4% 및 11.3% 정확도 향상으로 상회합니다. 또한, MetaLadder로 학습된 모델은 구조적으로 새로운 문제에 더 강력한 일반화 성능을 보이며, 기존 CoT 모델에 비해 9.3% 더 많은 분포 외 테스트 케이스를 해결합니다.

### [STEVE: AStep Verification Pipeline for Computer-use Agent Training](https://arxiv.org/abs/2503.12532)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12532.png)

Vote: 12

Authors: Ziqin Wei, Fanbin Lu, Shu Liu, Chi-Wing Fu, Jiaya Jia, Zhisheng Zhong

- ***What's New***: STEVE는 컴퓨터 사용 에이전트를 훈련시키기 위한 단계 검증 파이프라인으로, 대규모 지침 세트를 구축하여 하위 최적 에이전트를 사용해 궤적 데이터를 수집하고, 각 단계의 올바름을 GPT-4o로 검증하여 이진 라벨을 부여합니다. 이는 복잡한 컴퓨터 환경에서 잘 정의되지 않은 보상 함수 없이 에이전트 훈련을 대규모로 확장할 수 있게 합니다.
- ***Technical Details***: STEVE의 단계 검증 파이프라인에서는 UI-설정 비전-언어 모델(UI-grounding Vision Language Model)을 먼저 학습하고, 제한된 궤적 데이터로 컴퓨터 사용 에이전트로 미세 조정한 후 Windows 실환경에서 많은 궤적 데이터를 수집합니다. 이 궤적에 대해 GPT-4o를 활용한 단계 검증기를 통해 각 행동의 품질을 평가하며, 칼 네만-트버스키 최적화(Kahneman-Tversky Optimization)를 통해 검증된 궤적을 사용하여 에이전트를 최적화합니다. 이는 계획과 실행의 복잡한 순서의 작업을 처리하기 위한 에이전트 훈련을 가능케 합니다.
- ***Performance Highlights***: STEVE를 통해 훈련된 에이전트는 전통적인 감독 미세 조정보다 더 나은 성능을 발휘하며, WinAgentArena와 같은 도전적인 실시간 데스크탑 환경에서 우수한 성과를 보였습니다. 특히 7B 비전-언어 모델을 컴퓨터 사용 에이전트로 훈련시키며 탁월한 효율성을 통해 비용을 절감하는 성과를 거두었습니다.

### [MusicInfuser: Making Video Diffusion Listen and Dance](https://arxiv.org/abs/2503.14505)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.14505.png)

Vote: 8

Authors: Ira Kemelmacher-Shlizerman, Brian Curless, Steven M. Seitz, Susung Hong

- ***What's New***: MusicInfuser는 기존의 비디오 생성 모델을 활용하여 음악에 맞춰 춤을 생성하는 새로운 접근 방식을 소개합니다. 이 모델은 음악-비디오 크로스-어텐션(music-video cross-attention)과 저랭크 어댑터(low-rank adapter)를 도입하여 경량 조정만으로 높은 품질의 음악-동기화된 비디오를 생성합니다.
- ***Technical Details***: MusicInfuser는 텍스트-투-비디오(Text-to-Video; T2V) 모델을 사용하여 사전 학습된 인프라를 음악 입력에 맞춰 조정합니다. 제안된 어댑터 네트워크는 음악을 비디오 토큰과 교차-어텐션을 통해 상호작용하여 오디오와 시각적 안무 사이의 상관관계를 설정합니다. 또한, 베타-유니폼(Beta-Uniform) 스케줄링 전략을 제안하여 학습 노이즈 분포가 베타 분포에서 균일 분포로 전환되도록 합니다.
- ***Performance Highlights***: MusicInfuser는 새로운 음악 트랙에 대해 기존의 무브먼트와 신뢰할 수 있는 춤 비디오를 생성합니다. 다른 모델과의 비교 실험에서 스타일 적합성 및 비트 적합성, 이동 현실감 및 안무 복잡성 평가에서 높은 점수를 기록하며, 다양한 프로세서를 통해 음악-구동 안무 합성을 위한 새로운 가능성을 제안했습니다.

### [LEGION: Learning to Ground and Explain for Synthetic Image Detection](https://arxiv.org/abs/2503.15264)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.15264.png)

Vote: 8

Authors: Siwei Wen, Hengrui Kang, Baichuan Zhou, Weijia Li, Junyan Ye, Bin Wang, Linfeng Zhang, Dahua Lin, Peilin Feng, Conghui He, Zichen Wen

- ***What's New***: 이 논문에서는 LEGION(LEarning to Ground and Explain for Synthetic Image detectiON)이라는 새로운 MLLM(Multimodal Large Language Model) 기반의 이미지 위조 분석 프레임워크를 소개합니다. LEGION은 아티팩트 탐지, 세그멘테이션 및 설명 기능을 통합하여 완전한 합성 이미지를 분석합니다. 또한, 이 프레임워크는 이미지 정제 파이프라인에 통합되어 더 높은 품질과 더욱 현실적인 이미지를 생성하도록 안내합니다.
- ***Technical Details***: SynthScars라는 데이터셋은 4가지 이미지 유형과 3가지 아티팩트 카테고리를 포함해 12,236개의 고품질 합성 이미지로 구성되어 있으며, 정밀한 픽셀 레벨 세그멘테이션, 텍스트 설명, 아티팩트 카테고리 레이블의 세밀한 주석을 제공합니다. LEGION은 Global Image Encoder, LLM, Grounding Image Encoder, Pixel Decoder의 네 가지 핵심 컴포넌트로 구성되어 있으며 아티팩트 세그멘테이션, 위조 탐치 및 자연어 설명을 수행합니다.
- ***Performance Highlights***: LEGION은 SynthScars 데이터셋에서 mIoU 기준으로 3.31%, F1 점수 기준으로 7.75% 기존의 전문가 모델보다 우수한 성능을 보입니다. 이 프레임워크로 생성된 정제 이미지는 인간 선호와 더 강한 일치를 보이며, 기존 방법론을 여러 벤치마크에서 앞서는 성능을 입증합니다.

### [ViSpeak: Visual Instruction Feedback in Streaming Videos](https://arxiv.org/abs/2503.12769)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12769.png)

Vote: 7

Authors: Jian-Fang Hu, Xihan Wei, Wei-Shi Zheng, Shenghao Fu, Yi-Xing Peng, Kun-Yu Lin, Qize Yang, Xiaohua Xie, Yuan-Ming Li

- ***What's New***: ViSpeak는 스트리밍 비디오 환경을 위한 새로운 과제인 Visual Instruction Feedback을 제안합니다. 이 과제는 모델이 비주얼 콘텐츠에 적극적으로 반응해야 하며, 이는 사용자-에이전트 상호작용을 크게 향상시킵니다.
- ***Technical Details***: ViSpeak 모델은 GPT-4o 수준의 성능을 가진 스트리밍 비디오 이해를 위한 SOTA LMM(Large Multi-modal Model)입니다. 세 가지 주요 단계로 세분화된 파인튜닝 프로세스를 통해 비주얼 명령어 피드백 능력을 갖추고 있습니다. ViSpeak-Bench 벤치마크는 1,000개의 비디오와 1,000개의 QA 쌍으로 이루어져 있으며, 7개의 하위 과제를 포함하고 있습니다: Visual Wake-Up, Anomaly Warning, Gesture Understanding, Visual Reference, Visual Interruption, Humor Reaction, Visual Termination.
- ***Performance Highlights***: ViSpeak는 StreamingBench에서 62.00, OVO-Bench에서 61.08의 점수를 기록하며, 이는 GPT-4o와 비슷한 수준의 성능입니다. ViSpeak-Bench에서의 성능 역시 높은 수준을 나타내면서 전반적인 스트리밍 비디오 이해 능력을 입증합니다.

### [GKG-LLM: A Unified Framework for Generalized Knowledge Graph Construction](https://arxiv.org/abs/2503.11227)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.11227.png)

Vote: 7

Authors: Jian Zhang, Jun Liu, Shihao Qi, haiping Zhu, Qika Lin, Bifan Wei

- ***What's New***: GKG-LLM는 통합된 프레임워크(unified framework)를 통해 다양한 유형의 지식 그래프(Generalized Knowledge Graph; GKG)를 구축하는 방법을 제안합니다. 이는 기존의 구체적인 지식 그래프(knowledge graph), 이벤트 지식 그래프(event knowledge graph), 상식 지식 그래프(commonsense knowledge graph)를 결합하여 자원 효율성을 증대시키고 더 나은 성능을 제공하려는 첫 시도입니다.
- ***Technical Details***: 이번 연구에서는 29개의 다양한 데이터셋에서 파생된 15개의 서브 태스크(sub-tasks) 데이터를 수집하여 수용하는 새로운 통합 프레임워크를 제안했습니다. 주요 학습 방식으로는 커리큘럼 학습(curriculum learning)와 3-스테이지 파인튜닝(fine-tuning) 기법을 채택하여, 대형 언어 모델(Large Language Models; LLMs)에 다양한 그래프 타입의 지식을 체계적으로 주입합니다.
- ***Performance Highlights***: GKG-LLM는 KG, EKG, CKG에 대한 데이터에서 모두 평균 67.90%의 퍼포먼스를 기록하며, 각 그래프 타입에 대해 최적화된 단일 학습 방법보다 우수한 결과를 보였습니다. 특히 OOD(Out-of-Distribution) 데이터에서 강력한 일반화 능력을 시연했으며, F1 점수에서 기존 모델들보다 최소 9.88% 향상된 결과를 나타냈습니다.

### [Mitigating Visual Forgetting via Take-along Visual Conditioning for Multi-modal Long CoT Reasoning](https://arxiv.org/abs/2503.13360)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.13360.png)

Vote: 5

Authors: Han-Jia Ye, Hai-Long Sun, Houwen Peng, Zhun Sun

- ***What's New***: 이 논문은 멀티모달 연쇄 추론(Multi-modal Long CoT Reasoning)에서 시각적 망각(Visual Forgetting) 문제를 해결하기 위한 새로운 전략인 동반 시각 조건화(Take-along Visual Conditioning; TVC)를 제안합니다. 이러한 전략은 이미지 입력을 추론의 중요한 단계로 전환하고 불필요한 시각적 토큰을 동적으로 가지치기하여 모델이 추론 단계 동안 시각적 정보에 대한 주의를 유지할 수 있게 돕습니다.
- ***Technical Details***: TVC는 동적 시각 재확인(Dynamic Visual Reaffirmation)이라는 학습 단계를 통해 동안 시각 증거를 지속적으로 모델이 추론에 통합할 수 있도록 합니다. 학습 단계에서는 주기적인 시각 보정(Periodic Visual Calibration)을 통해 시각 정보를 주기마다 재활성화하여, 추론의 결정적 순간에 시각적 증거가 다시 방문될 수 있도록 만듭니다. 이 방법론은 두 가지 핵심 단계로 구성됩니다: 훈련 단계는 시각 증거의 재확립을 도와주며, 테스트 현장에서는 주기적인 시각 보정을 통해 시각적 주의를 보정합니다.
- ***Performance Highlights***: TVC는 수학적 추론 벤치마크에서 평균 +3.4%의 성능 향상을 달성하여 멀티모달 추론 시스템에 대한 TVC의 효과를 입증했습니다. 이 연구의 결과는 긴 연쇄 추론 작업에서 시각적 망각 문제를 해결하기 위한 견고한 솔루션을 제공합니다.

### [Unlock Pose Diversity: Accurate and Efficient Implicit Keypoint-based Spatiotemporal Diffusion for Audio-driven Talking Portrait](https://arxiv.org/abs/2503.12963)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12963.png)

Vote: 5

Authors: Yuyao Yan, Yifei Zhang, Chenru Jiang, Kaizhu Huang, Chaolong Yang, Guangliang Cheng, Jie Sun, Weiguang Zhao, Bin Dong, Kai Yao

- ***What's New***: KDTalker는 오디오 기반 토킹 포트레이트 생성에서 첫 번째로 비지도 임플리시트(Inplicit) 3D 키포인트를 통한 시공간(Diffusion Spatiotemporal) 확산 모델을 통합한 프레임워크로, 다채로운 머리 자세와 섬세한 얼굴 표정을 유연하게 포착하여 생생하고 일관된 애니메이션을 만들어냅니다.
- ***Technical Details***: KDTalker는 비지도 임플리시트 3D 키포인트로 얼굴의 정보 밀도를 조정하여 다양한 머리 포즈와 정밀한 얼굴 세부 사항을 캡처할 수 있습니다. 사용자 정의된 시공간 주의 메커니즘(Spatiotemporal Attention Mechanism)을 통합하여 정확한 입술 동기화를 보장하고, 생성된 애니메이션이 시간적으로 일관되게 고품질을 유지하며, 컴퓨팅 효율성을 높입니다.
- ***Performance Highlights***: KDTalker는 실험 결과 입술 동기화 정확성, 머리 자세의 다양성, 실행 효율성 측면에서 최첨단 성능을 달성했음을 보여줍니다. 특히, LSE-D(입술 동기화 오차 거리)가 작아 더 나은 입술 동기화 성능을 나타내며, FID(프리셰 인셉션 거리)와 CSIM(코사인 유사성) 지표에서 뛰어난 시각적 품질과 구조적 정확성을 보여줍니다.

### [PyGDA: A Python Library for Graph Domain Adaptation](https://arxiv.org/abs/2503.10284)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10284.png)

Vote: 3

Authors: Meihan Liu, Zhen Zhang, Bingsheng He

- ***What's New***: PyGDA는 그래프 도메인 적응(Graph Domain Adaptation)을 위한 최초의 포괄적인 파이썬 라이브러리로, 20개가 넘는 그래프 도메인 적응 모델과 다양한 그래프 데이터셋을 통합하여 제공합니다. 이를 통해 사용자들이 커스텀 모델을 쉽게 구축하고, 대규모 그래프 데이터를 효율적으로 처리할 수 있는 모듈화된 구성 요소를 제공합니다.
- ***Technical Details***: PyGDA는 파이토치(Pytorch) 및 파이토치 지오메트릭(PyTorch Geometric)을 기반으로 하며, 그래프 데이터셋을 위한 자동 전처리 기능을 갖추고 있어 다양한 실험 환경에서도 일관성을 유지합니다. 또한, 소스 기반, 소스프리, 다중 소스프리 그래프 도메인 적응과 같은 다양한 설정을 위한 알고리즘을 지원하며, 노드 및 그래프 수준의 분류 작업을 처리할 수 있습니다.
- ***Performance Highlights***: PyGDA는 AUC, 정확도(Accuracy), 마이크로-F1(Micro-F1), 매크로-F1(Macro-F1) 등의 평가 지표를 통해 알고리즘 성능을 평가할 수 있는 기능을 제공합니다. 다양한 그래프 도메인 적응 알고리즘의 성능을 일관되게 평가할 수 있어 연구자와 실무자 모두에게 유용한 도구로 작용합니다.

### [Decompositional Neural Scene Reconstruction with Generative Diffusion Prior](https://arxiv.org/abs/2503.14830)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.14830.png)

Vote: 3

Authors: Yu Liu, Ruijie Lu, Yixin Chen, Siyuan Huang, Song-Chun Zhu, Junfeng Ni, Zirui Zhou

- ***What's New***: DP-RECON은 디퓨전 프리어(Diffusion Prior)를 이용하여 새로운 뷰에서도 정확하게 네트워크로부터 물체의 기하학적 구조와 색상을 최적화하는 새로운 방식의 3D 장면 복원 기술을 제안합니다. 기존의 방법들이 간헐적이거나 가려진 영역에서는 정보 부족으로 인해 정확도 감소 속에 움직였던 반면, DP-RECON은 이러한 문제를 해결하고자 가시성 기반 접근법을 도입하여 이러한 영역에서 부족한 정보를 보충합니다.
- ***Technical Details***: DP-RECON은 Score Distillation Sampling(SDS)을 사용하여 개별 물체에 대한 뉴럴 임플리시트 리프레젠테이션을 최적화합니다. 이는 프리트레인된 Stable Diffusion 모델을 비판자로 사용하여 구성 요소를 통합하고 기존의 방법에서 발견되던 일관성 손실 문제를 해결하기 위해 가시성에 따라 다이내믹하게 각 픽셀의 손실 가중치를 조정합니다. 물체의 기하학적 구조와 색상 회복을 동시에 수행하며, 특히 가려진 영역에서 색상 맵과 상세한 UV 맵을 생성하여 3D 소프트웨어에서 포토리얼리스틱 (Photorealistic) 효과를 내는 데 사용됩니다.
- ***Performance Highlights***: Replica와 ScanNet++ 데이터 세트를 사용한 다양한 실험 결과, DP-RECON은 기존 최첨단 기법보다 훨씬 나은 기하학적 구조와 색상 복원 효과를 보였습니다. 특히 10개의 뷰만으로도 100개의 뷰를 사용하는 기존 방법보다 더 우수한 성과를 기록했습니다. 이는 현재의 기술로 해결하기 어려운 가려진 영역에서도 우수한 성능을 보임을 나타냅니다.

### [LLM-FE: Automated Feature Engineering for Tabular Data with LLMs as Evolutionary Optimizers](https://arxiv.org/abs/2503.14434)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.14434.png)

Vote: 2

Authors: Nikhil Abhyankar, Chandan K. Reddy, Parshin Shojaee

- ***What's New***: LLM-FE는 대형 언어 모델(Large Language Models; LLMs)을 진화적 최적화 기법으로 활용하여 자동으로 효과적인 피처를 발견하는 새로운 프레임워크를 제안합니다. 이 프레임워크는 표형 데이터 학습 과제를 위해 도메인 지식과 LLM의 추론 능력을 결합하여 피처 엔지니어링을 프로그래밍 탐색 문제로 공식화합니다.
- ***Technical Details***: LLM-FE는 LLM이 표형 데이터셋에 대해 피처 변형 프로그램을 제안하고, 데이터 기반 피드백을 사용하여 탐색 과정을 안내하는 진화적 검색을 따릅니다. 피처 엔지니어링은 초기 피처 변형 프로그램에서 시작하며, 완료된 프로그램을 갖춘 도메인별 세부 정보 및 데이터 샘플을 통합하여 새로운 피처 발견 가설을 생성합니다.
- ***Performance Highlights***: LLM-FE는 다양한 분류 및 회귀 벤치마크에서 최신 기준보다 일관되게 우수한 성능을 발휘하며, 각종 예측 모델(XGBoost, TabPFN, MLP)의 성능을 일정하게 향상시킵니다. LLM-FE는 특히 데이터 기반 피드백과 도메인 지식을 사용한 피처 최적화에서 진화적 검색의 중요성을 강조합니다.

### [SkyLadder: Better and Faster Pretraining via Context Window Scheduling](https://arxiv.org/abs/2503.15450)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.15450.png)

Vote: 2

Authors: Shiqi Chen, Min-Yen Kan, Haonan Wang, Qian Liu, Tongyao Zhu, Xiangming Gu, Tianyu Pang

- ***What's New***: SkyLadder는 컨텍스트 윈도우 스케줄링(Context Window Scheduling)을 통해 보다 효율적이고 효과적인 사전 훈련(Pretraining)을 가능하게 하는 새로운 접근 방식을 제안합니다. 이 방법은 짧은 컨텍스트에서 긴 컨텍스트로의 전환을 통해 모델의 학습 효율성을 향상시키고, 기존의 긴 컨텍스트 윈도우를 사용하는 모델 대비 우수한 성능을 보입니다.
- ***Technical Details***: SkyLadder는 사전 훈련 동안 짧은 컨텍스트 윈도우로 시작하여 점진적으로 길이를 확장하는 전략을 채택합니다. 이를 위해 각 훈련 단계에서 컨텍스트 윈도우의 길이를 일정 비율로 증가시키는 방법을 사용하며, 최종적으로 원하는 긴 컨텍스트 길이에 도달하게 됩니다. 이 방법은 여러 로컬 '미니' 인과 관계 마스크(Causal Masks)를 긴 패킹 시퀀스에 적용함으로써 구현됩니다.
- ***Performance Highlights***: SkyLadder를 사용한 1B-parameter 모델은 기본 긴 컨텍스트 사전 훈련 베이스라인 대비 표준 벤치마크(예: HellaSwag)에서 최대 3.7%의 성능 향상을 보였으며, 최대 22%까지 빠른 훈련 속도를 달성했습니다. 긴 컨텍스트 평가 작업에서도 SkyLadder 모델이 일관되게 베이스라인과 맞먹거나 이를 초과하는 성능을 보여주었습니다.

### [ELTEX: A Framework for Domain-Driven Synthetic Data Generation](https://arxiv.org/abs/2503.15055)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.15055.png)

Vote: 1

Authors: Arina Razmyslovich, Julien Capitaine, Eugene Dmitriev, Sofia Sedlova, Kseniia Murasheva

- ***What's New***: ELTEX는 특정 분야에 맞춘 고품질의 합성 데이터 생성을 지원하는 도메인 기반 프레임워크입니다. 특히, 블록체인 관련 사이버 공격 탐지에서의 효과를 보였으며, 이를 통해 GPT-4와 경쟁 가능한 성능을 발휘하면서도 적은 컴퓨팅 리소스를 필요로 합니다.
- ***Technical Details***: ELTEX는 명시적인 도메인 인디케이터 추출과 동적 프롬프트(dynamtic prompt; DP) 생성을 통합하여 중요한 도메인 정보를 보존합니다. 블록체인 관련 사이버 공격 논의의 텍스트 분류 맥락에서 ELTEX의 효과를 입증하였습니다.
- ***Performance Highlights***: ELTEX로 강화된 모델은 표준 분류 메트릭 및 불확실성 보정에서 GPT-4의 성능과 경쟁하며, 특히 사이버 보안 도메인에서 리소스 효율적인 모델도 대형 아키텍처에 필적하는 성능을 발휘할 수 있음을 보여줍니다.

### [CURIE: Evaluating LLMs On Multitask Scientific Long Context Understanding and Reasoning](https://arxiv.org/abs/2503.13517)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.13517.png)

Vote: 1

Authors: Hao Cui, Michael P Brenner, Chenfei Jiang, Shutong Li, Eun-Ah Kim, Gowoon Cheon, Elise Kleeman, Haining Pan, Peter Norgaard, Ean Phing VanLee, Yasaman Bahri, Michael J. Statt, Victor V. Albert, Subhashini Venugopalan, Muqthar Mohammad, Viren Jain, Philippe Faist, Drew Purves, Brian Rohr, Dan Morris, Sameera Ponda, Matthew Abraham, Elizabeth Dorfman, Zahra Shamsi, Martyna Plomecka, Pranesh Srinivasan, Paul Raccuglia, Nayantara Mudur, Maria Tikhanovskaya, Ruth Alcantara, Xuejian Ma

- ***What's New***: CURIE 벤치마크는 과학적 문제 해결에서 대형 언어 모델(Large Language Models; LLMs)의 잠재력을 평가하기 위해 설계된 새로운 벤치마크입니다. 6개의 과학 분야에 걸쳐 실제 연구 문서를 기반으로 하는 580개의 문제를 제공하여 긴 문맥 이해와 추론을 요구하는 복잡한 과제를 제시합니다.
- ***Technical Details***: CURIE는 재료과학, 응축물질물리학, 양자 컴퓨팅, 지리공간 분석, 생물다양성, 단백질 등 다양한 분야에 걸쳐 429개의 연구 문서에서 추출한 10개의 과제로 구성됩니다. 각 과제는 도메인 전문 지식과 긴 문맥의 정보를 이해하고 다단계 추론을 필요로 합니다. 평가에 사용된 모델로는 GPT-4o, Claude-3, Gemini Flash 2.0 등이 있으며, 이들은 긴 문맥 창을 지원합니다.
- ***Performance Highlights***: 가장 높은 성능을 보인 모델은 Claude-3와 Gemini Flash 2.0으로, 다양한 과제에서 일관된 높은 이해력을 보였습니다. 반면, GPT-4o는 몇 가지 과제, 특히 단백질 시퀀싱에서 성능이 미흡했으며, 최대 성능이 32%에 머물렀습니다. 이는 현재 LLMs가 과학적 추론과 문제 해결에서 여전히 많은 도전 과제를 안고 있음을 시사합니다.

### [SWEET-RL: Training Multi-Turn LLM Agents on Collaborative Reasoning Tasks](https://arxiv.org/abs/2503.15478)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.15478.png)

Vote: 1

Authors: Sainbayar Sukhbaatar, Yifei Zhou, Xian Li, Yuandong Tian, Sergey Levine, Song Jiang, Jason Weston

- ***What's New***: SWEET-RL는 다중 턴 상호작용이 필요한 실질적 작업에서 대형 언어 모델(LLM) 에이전트를 훈련하는 새로운 강화 학습 알고리즘으로, ColBench라는 벤치마크를 도입하여 LLM 에이전트가 인간과 협력하여 백엔드 프로그래밍과 프론트엔드 디자인 작업을 해결하도록 합니다.
- ***Technical Details***: SWEET-RL(강화학습의 단계 단위 평가법)은 훈련 중 획득한 추가 정보를 활용하여 평가 모델을 훈련하고, 이 평가자가 정책 모델의 보상을 개선하도록 설계됐습니다. 이를 통해 LLM 에이전트는 부분적 관찰이 가능한 환경에서 정보 탐색 행동에 대한 보상을 효과적으로 받을 수 있습니다.
- ***Performance Highlights***: SWEET-RL을 적용한 실험에서 ColBench 작업의 성공률과 승률이 기존 최첨단 다중 턴 RL 알고리즘과 비교하여 6% 절대적 증가를 보였으며, 이로 인해 Llama-3.1-8B는 GPT-4o와 같은 고성능 독점 모델과 맞먹거나 이를 초과하는 성능을 발휘할 수 있습니다.

