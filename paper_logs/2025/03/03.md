## Daily Papers (2025-03-03)

### [DeepSolution: Boosting Complex Engineering Solution Design via Tree-based Exploration and Bi-point Thinking](https://arxiv.org/abs/2502.20730)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.20730.png)

Vote: 23

Authors: Yaojie Lu, Xianpei Han, Fei Huang, Le Sun, Haiyang Yu, Yongbin Li, Zhuoqun Li, Xuanang Chen, Hongyu Lin

- ***What's New***: 복잡한 공학적 솔루션 설계를 자동화하고 신뢰성을 향상시키기 위해 새로운 시스템, SolutionRAG가 개발되었습니다. 이 시스템은 트리 기반의 탐색과 양방향 사고 방식(bi-point thinking)을 활용하므로 이전의 해결책보다 개선된 성능을 제공합니다.
- ***Technical Details***: SolutionRAG는 솔루션 노드와 코멘트 노드가 번갈아 가며 연결되는 '양방향 사고 트리'를 통해 신뢰할 수 있는 해결책을 생성합니다. 이는 각 입력 요구사항에 대한 가장 효과적인 개선 방향을 탐색합니다. 트리 기반의 탐색을 수행하며, 다중 제약을 수반한 요구 사항을 해결하기 위해 제안-검토를 반복합니다. 이 과정에서 노드 평가를 통해 트리를 가지치기하여 효율성과 성능을 균형 있게 조율합니다.
- ***Performance Highlights***: SolutionRAG는 SolutionBench에서의 실험 결과, 기존의 다양한 복합 문제 해결 접근 방식들보다 뛰어난 성능을 발휘했습니다. mining 분야에서의 테크니컬 점수는 Naive-RAG보다 10.4만큼, Self-RAG보다 8.9만큼 향상되었습니다. 이는 다양한 실제 공학적 시나리오에서 SolutionRAG가 효과적인 해결책을 제시할 수 있음을 확인해줍니다.

### [Chain of Draft: Thinking Faster by Writing Less](https://arxiv.org/abs/2502.18600)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.18600.png)

Vote: 22

Authors: Pengcheng He, Lingxiao Zhao, Wenhao Xie, Silei Xu

- ***What's New***: Chain of Draft(CoD)는 대형 언어 모델(LLMs)의 연산 속도와 비용 절감을 위해 제안된 새로운 전략입니다. 기존의 Chain-of-Thought(CoT) 프롬팅이 복잡한 문제 해결에 효과적임에도 불구하고 매우 장황한 출력을 생성하여 비용과 지연이 증가한다는 문제를 해결하고자 고안되었습니다. CoD는 인간의 인지 과정을 본떠 주요 정보를 함축하는 최소한의 중간 출력을 생성하도록 LLMs를 유도하여, 정확성을 유지하면서도 토큰 사용량과 지연을 대폭 줄였습니다.
- ***Technical Details***: Chain of Draft(CoD)는 각 중간 단계에서 최소한의 정보로 구성된 간결한 출력을 생성하도록 합니다. 이 전략은 각 추론 단계를 5단어 이내로 제한하여 긴 설명 없이도 문제 해결에 필요한 핵심 내용을 잘 잡아내도록 설계되었습니다. 실험에서는 CoD가 기존의 CoT 프롬팅을 통해 얻어진 정확성을 유지하거나 개선하면서도, 토큰 사용량을 7.6%까지 줄여 비용 절감 효과를 입증하였습니다.
- ***Performance Highlights***: 기존의 Chain-of-Thought(CoT) 전략 대비 Chain of Draft(CoD)는 지연 시간과 토큰 사용량을 현저히 줄이면서도 다양한 추론 작업에서 유사하거나 더 높은 정확성을 기록했습니다. 예를 들어, GSM8k 데이터세트에 대한 실험에서는 CoD가 평균 43.9개의 토큰으로 91.1%의 정확도를 달성하여, CoT의 평균 205.1개의 토큰에 비해 약 80% 토큰을 줄였습니다.

### [Multi-Turn Code Generation Through Single-Step Rewards](https://arxiv.org/abs/2502.20380)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.20380.png)

Vote: 20

Authors: Wenting Zhao, Sanjiban Choudhury, Arnav Kumar Jain, Alexander M Rush, Wayne Chen, Gonzalo Gonzalez-Pumariega

- ***What's New***: 이 논문에서는 다중 회귀 실행 피드백(Multi-Turn Execution Feedback)만을 바탕으로 코드 생성 문제를 해결하는 µCODE라는 방식이 소개되었습니다. 기존의 복잡한 강화학습(parametric reinforcement learning)을 사용하지 않고, 단일 단계 보상(Single-Step Rewards)을 활용하여 코드 생성 문제를 더 효율적으로 해결합니다.
- ***Technical Details***: µCODE는 코드 생성 문제를 단일 단계로 회복 가능한 마르코프 결정 과정(One-Step Recoverable Markov Decision Process; MDP)으로 모델링하여, 복잡한 다중 단계 보상 최적화 대신 간단한 모방 학습(Imitation Learning)을 통한 안정적인 훈련을 가능하게 합니다. 이 접근법은 생성기(Generator)와 검증기(Verifier)가 피드백을 기반으로 서로 개선될 수 있도록 반복적으로 훈련됩니다.
- ***Performance Highlights***: µCODE는 MBPP와 HumanEval 벤치마크에서 기존의 다중 회귀 접근 방식보다 우수한 성능을 보였습니다. 특히 1B 파라미터 모델에서 HumanEval 데이터셋에서 Multi-STaR 방법에 비해 1.9% 이상의 성능 향상을 이뤄냈습니다. 또한, µCODE는 학습된 검증기를 활용한 Best-of-N 검색에서 최대 12.8%의 성능 향상을 보여줍니다.

### [How far can we go with ImageNet for Text-to-Image generation?](https://arxiv.org/abs/2502.21318)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.21318.png)

Vote: 13

Authors: N. Dufour, L. Degeorge, A. Ghosh, D. Picard, V. Kalogeiton

- ***What's New***: 이미지넷(ImageNet) 데이터셋만을 사용해 텍스트-이미지 생성(Text-to-Image; T2I) 모델을 훈련할 수 있다는 가능성을 입증한 연구입니다. 대규모 웹스크래핑 데이터셋에 의존하지 않고도 전략적인 데이터 증강(Data Augmentation)을 통해 동등하거나 우수한 퍼포먼스를 달성할 수 있음을 보여줍니다.
- ***Technical Details***: 본 연구에서 제안한 방법은 이미지넷에서 직접 훈련된 작은 디퓨전 모델들(Diffusion Models)이 포함됩니다. 텍스트와 이미지 증강을 통한 전략적 개선을 통해 이미지넷의 제한점을 극복하였으며, 주로 LLaVA를 사용하여 장문의 설명적 캡션을 생성하고, CutMix를 사용하여 이미지 간의 독특한 개념 결합을 도입하여 다양한 데이터셋을 만드는 방식입니다.
- ***Performance Highlights***: 제안된 접근법을 통해 이미지넷만을 사용하여 훈련된 작은 모델들이 GenEval 및 DPG 벤치마크에서 스테이블 디퓨전-XL(Stable Diffusion-XL)을 초과하는 성능을 보였습니다. 특히 1/10의 파라미터와 1/1000의 훈련 이미지를 사용하여 SD-XL 대비 각각 +2 및 +5 점수 상승을 달성했습니다.

### [SoS1: O1 and R1-Like Reasoning LLMs are Sum-of-Square Solvers](https://arxiv.org/abs/2502.20545)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.20545.png)

Vote: 10

Authors: Coralia Cartis, Kechen Li, Shiwei Liu, Tianbo Ji, Wenqi Zhu

- ***What's New***: SoS1 연구는 대형 언어 모델(LLMs)의 수학적 추론 능력을 테스트하면서 여러 NP-hard 문제를 해결할 수 있는 잠재력을 보여줍니다. 연구자들은 sum-of-squares(SoS) 문제 해결을 위한 SoS-1K라는 1,000개의 다항식과 이에 대한 전문가가 설계한 추론 지침을 도입했습니다.
- ***Technical Details***: SoS1은 대형 언어 모델이 다변 수 다항식이 비음수인지를 판단하는 문제를 다루었으며, 이는 Hilbert의 17번째 문제와 밀접한 연관이 있습니다. 주어진 1,000개의 다항식 데이터셋인 SoS-1K에 대해 최고 수준의 모델들이 구조화된 가이던스 없이 무작위 추측 수준(50%) 이상의 약간의 성능만을 보여주었으나, 고품질의 추론 지침을 사용하여 정확도를 최대 81%까지 개선했습니다.
- ***Performance Highlights***: 7B 모델인 SoS-7B는 SoS-1K에 대해 단 4시간의 미세 조정을 통해 671B DeepSeek-V3와 GPT-4o-mini보다 높은 정확력을 달성했습니다. SoS-7B는 DeepSeek-V3와 GPT-4o-mini 대비 각각 1.8% 및 5%의 계산 시간만 필요로 하며 뛰어난 성능을 보여줍니다.

### [ViDoRAG: Visual Document Retrieval-Augmented Generation via Dynamic Iterative Reasoning Agents](https://arxiv.org/abs/2502.18017)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.18017.png)

Vote: 10

Authors: Pengjun Xie, Weiqi Wu, Shihang Wang, Qiuchen Wang, Feng Zhao, Zehui Chen, Ruixue Ding

- ***What's New***: ViDoRAG는 복잡한 시각 문서에서의 복잡한 추론을 위해 설계된 새로운 멀티 에이전트 RAG(Retrieval-Augmented Generation) 프레임워크입니다. 이 연구는 시각적 및 텍스트 특징을 통합하는 데 어려움을 겪는 순수 시각적 검색 방법의 한계와 토큰 제한으로 인해 충분한 추론을 활성화하지 못하는 기존 접근법의 문제를 해결하기 위해 개발되었습니다.
- ***Technical Details***: ViDoRAG는 복잡한 시각 문서 전반에 걸쳐 복합 추론을 수행하기 위해 멀티모달 하이브리드 검색(Multi-Modal Hybrid Retrieval) 전략을 채택하며, 이는 Gaussian Mixture Model(GMM)을 기반으로 한 하이브리드 전략을 사용합니다. 생성 과정에서 모형의 추론 능력을 최대한 발휘하기 위해 탐색, 요약, 반성을 포함한 반복적 에이전트 작업 흐름을 도입했습니다. 이 프레임워크 하에서 시각 및 텍스트 파이프라인 검색 결과를 통합한 후 탐색(Fusion)합니다. 생성 과정에서는 시커(Seeker), 검사자(Inspector), 답변 에이전트(Answer Agent)로 구성된 세 가지 에이전트가 다양한 스케일을 통해 단서 추출, 반성, 정답 제공을 수행합니다.
- ***Performance Highlights***: ViDoRAG는 새롭게 제안된 벤치마크 ViDoSeek에서 최신 방법을 크게 앞서는 성과를 거두며 10% 이상의 성능 증가를 기록했습니다. 하이브리드 검색과 동적으로 조절되는 토큰 길이를 활용한 ViDoRAG는 기존의 잘 알려진 모델들과 비교하여 더 나은 정밀도와 검색 효율성을 보였습니다.

### [Sim-to-Real Reinforcement Learning for Vision-Based Dexterous Manipulation on Humanoids](https://arxiv.org/abs/2502.20396)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.20396.png)

Vote: 8

Authors: Toru Lin, Yuke Zhu, Linxi Fan, Kartik Sachdev, Jitendra Malik

- ***What's New***: 이 연구는 시뮬레이션에서 현실 세계로의 전이(Sim-to-Real) 강화학습을 활용하여 비전 기반의 인간 형태 로봇이 다양한 접촉이 많은 조작 작업을 수행할 수 있도록 하는 새로운 기술을 소개합니다. 주요 기여는 자동화된 현실-시뮬레이션 튜닝 모듈, 일반화 가능한 보상 설계 방식, 샘플 효율성을 개선하는 분할 및 정복 해법, 그리고 희소 및 밀집 객체 표현을 혼합하여 지각 격차를 줄이는 것입니다.
- ***Technical Details***: 이 연구에서는 자동화된 현실-시뮬레이션 튜닝 모듈이 환경 모델링 갭을 줄이는 데 사용됩니다. 또한, 제안된 보상 설계는 '접촉 목표'와 '객체 목표'로 작업을 분리하여 복잡한 조작 작업에 적용 가능합니다. 초기작업 태스크 인식 손 자세(task-aware hand poses) 및 분할-정복(distillation) 방법을 활용하여 학습 효율성을 높이고, 현실 세계로의 정책 전이에서 희소 및 밀집 객체 표현의 혼합을 사용하여 시각적 인지 격차를 줄입니다.
- ***Performance Highlights***: 제안된 방식은 잡기, 상자 들어올리기 및 양손 전달과 같은 인간 형태의 조작 작업에서 유망한 결과를 보여줍니다. 각 작업의 성공률은 각각 62.3%, 80%, 52.5%로 다양한 객체에 대한 일반화 능력이 우수하며, 힘 교란에도 견고성을 보입니다.

### [Tell me why: Visual foundation models as self-explainable classifiers](https://arxiv.org/abs/2502.19577)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.19577.png)

Vote: 7

Authors: Gianmarco Mengaldo, Hugues Turbé, Mina Bjelogrlic, Christian Lovis

- ***What's New***: 이 논문에서는 Visual Foundation Models (VFMs)을 개선하여 효율적이고 해석 가능한 새로운 프로토타입 기반 아키텍처를 소개합니다. 이 접근법은 고정된 VFMs 위에 가벼운 헤드(약 100만 매개변수)만을 훈련함으로써 기존 해석 가능성을 뛰어넘고 다양한 지표에서 경쟁력을 입증합니다.
- ***Technical Details***: 제안된 아키텍처 ProtoFM은 고정된 VFM을 시각적 특징 추출기로 사용하여, 이미지와 그 변형을 프로토타입 공간에 매핑하고, Cosine Similarity를 통해 학습된 프로토타입과 비교합니다. 최종 분류 헤드는 양의 가중치로 제한된 선형 분류기를 사용하여 해석 가능성을 높이고, 학습 목표는 패치 할당의 일관성과 지역성을 촉진하도록 설계되었습니다.
- ***Performance Highlights***: ProtoFM은 CUB-200-2011과 Stanford Cars 같은 일반적인 이미지 분류 데이터셋에서 SOTA (최신 기술) 성능을 기록하며, 해석 가능성 지표에서는 기존 모델을 능가합니다. RSNA 데이터셋에서는 AUROC 86.1을 기록하여, 본 아키텍처가 전문화된 데이터셋에도 잘 적응함을 보여주었습니다.

### [Optimal Brain Apoptosis](https://arxiv.org/abs/2502.17941)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.17941.png)

Vote: 5

Authors: Zheng Fang, Renjing Xu, Mingyuan Sun, Junjie Jiang, Jiaxu Wang, Yuetong Fang, Chenming Hu, Delei Kong

- ***What's New***: Optimal Brain Apoptosis (OBA)는 경량화 중요성 추정을 위한 새로운 가지치기(pruning) 방법론으로, Hessian 행렬의 개별 요소를 직접적으로 계산하여 Convolutional Neural Networks(CNNs)와 Transformers의 계산 효율성을 향상시킵니다.
- ***Technical Details***: 기존의 근사화 방법 대신, OBA는 각 매개변수에 대해 Hessian-벡터 곱 객체를 계산하여 두 번째 차수 Taylor 전개를 활용합니다. 이 방법은 계층 간의 비-영 Hessian 부분 행렬을 식별하여 네트워크 계층을 통해 Hessian 행렬을 분해함으로써, CNNs와 Transformers의 구조화된 가지치기와 비구조화된 가지치기 모두에 적용됩니다.
- ***Performance Highlights***: VGG19, ResNet32, ResNet50, ViT-B/16을 포함한 다양한 데이터셋(CIFAR10, CIFAR100, ImageNet)에서 실험을 통해 OBA의 효과가 입증되었습니다. 특히 ImageNet에서 ResNet50의 성능을 2배 속도 향상으로 감소시키지 않고도 0.53%의 정확도 감소만을 가져오는 데 성공했으며, ViT-B/16에서는 1.43%의 정확도 감소로 1.30배 속도 향상을 달성하였습니다.

### [TeleRAG: Efficient Retrieval-Augmented Generation Inference with Lookahead Retrieval](https://arxiv.org/abs/2502.20969)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.20969.png)

Vote: 5

Authors: Madhav Kashyap, Kan Zhu, Zihao Ye, Xiaoxiang Shi, Luis Ceze, Rulin Shao, Baris Kasikci, Arvind Krishnamurthy, Yile Gu, Yiyu Liu, Rohan Kadekodi, Keisuke Kamahori, Stephanie Wang, Chien-Yu Lin

- ***What's New***: TELERAG는 RAG(Retrieval-Augmented Generation) 추론 시스템을 위한 새로운 'Lookahead Retrieval' 기법을 도입하여 GPU 메모리 사용을 최소화하면서도 RAG의 지연 시간을 줄이는 혁신적인 시스템입니다. 이 기법은 CPU에서 GPU로 데이터를 예측적으로 전송하여 LLM(대형 언어 모델) 생성을 병렬로 수행함으로써 메모리와 계산의 동시성을 최적화합니다.
- ***Technical Details***: TELERAG는 RAG 파이프라인의 모듈성을 활용하여 예상되는 데이터를 미리 가져오는 'Lookahead Retrieval' 메커니즘을 도입합니다. 이 시스템은 Inverted File Index (IVF) 검색 알고리즘을 활용하여 데이터 이동과 계산을 최적화하며, 실험적으로 기존의 최첨단 시스템과 비교하여 최대 1.72배까지 RAG 추론 지연을 줄입니다. 또한 TELERAG는 프로파일 기반 접근법을 통해 이상적인 데이터를 미리 가져오는 양을 동적으로 결정합니다.
- ***Performance Highlights***: TELERAG는 61GB의 위키피디아 기반 데이터 저장소와 Llama 모델(3B, 8B, 13B)을 활용하여 실험한 결과, 단일 RTX 4090 GPU에서 최대 2.68배, 평균 1.70배의 속도 향상을 보여주었습니다. 이러한 결과는 TELERAG가 제한된 메모리 환경에서 대규모 RAG 추론 작업을 효율적으로 처리할 수 있음을 입증합니다.

### [LiteASR: Efficient Automatic Speech Recognition with Low-Rank Approximation](https://arxiv.org/abs/2502.20583)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.20583.png)

Vote: 4

Authors: Jungo Kasai, Baris Kasikci, Keisuke Kamahori, Noriyuki Kojima

- ***What's New***: LITEASR는 자동 음성 인식(ASR) 모델의 효율성을 크게 개선하기 위해 ASR 인코더를 저랭크 근사법(Low-Rank Approximation)으로 압축하는 새로운 접근법을 소개합니다. 이 방법은 기존 모델에서 주로 수집된 중간 활성 값의 저랭크 특성을 활용하여 계산 비용을 줄이면서도 높은 전사 정확도를 유지합니다.
- ***Technical Details***: LITEASR는 주성분 분석(PCA)을 사용하여 작은 양의 보정 데이터셋으로 ASR 인코더의 활성화를 분석합니다. 이를 통해 선형 변환을 저랭크 행렬 곱셈 체인으로 근사화하고, 자기 주의 알고리즘(Self-Attention Algorithm)이 축소된 차원에서 작동하도록 최적화합니다. 이를 위해 FlashAttention을 기반으로 한 특수 GPU 커널이 개발되었습니다.
- ***Performance Highlights***: LITEASR는 Whisper large-v3 모델의 인코더 크기를 약 50% 압축했으며, Whisper medium보다 더 나은 전사 정확도를 확보했습니다. 이로써 속도와 정확도 간의 새로운 파레토 최적(Pareto-optimal) 경계를 설정했습니다. RTX 4090 GPU에서는 인코더 실행 속도가 원본 모델 대비 최대 1.57배로 증가했습니다.

### [Preference Learning Unlocks LLMs' Psycho-Counseling Skills](https://arxiv.org/abs/2502.19731)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.19731.png)

Vote: 4

Authors: Zhiyu Zoey Chen, Shaun M. Eack, Mian Zhang

- ***What's New***: 이 논문은 선호 학습(Preference Learning)을 통해 대형 언어 모델(LLMs)의 심리 상담 능력을 강화하는 방법을 탐구합니다. 연구자들은 전문 심리치료사들과의 협력을 통해 심리 상담 세션에서의 치료사 응답을 평가하기 위한 포괄적 원칙을 제안하고, 이를 바탕으로 PsychoCounsel-Preference라는 선호 데이터셋을 생성했습니다.
- ***Technical Details***: PsychoCounsel-Preference 데이터셋은 36,000개의 고품질 선호 비교 쌍으로 구성되어 있으며, 7가지 원칙(공감, 관련성 개인화, 명료성, 유해 언어 회피, 자기 탐색 촉진, 자율성 및 자신감 증진, 변화 단계에 대한 민감도)을 바탕으로 LLM들을 훈련시킵니다. 그리고 Generative AI 모델은 Llama3-8B 기반으로 하며, 다양한 크기의 20개의 인기 있는 LLM을 사용하여 심리 상담 응답을 생성하고 평가합니다.
- ***Performance Highlights***: PsychoCounsel-Llama3-8B 모델은 테스트 세트에서 GPT-4o 대비 87%의 승률을 기록하며, 잘 훈련된 상태에서 높은 수준의 응답을 제공합니다. 이는 LLM들이 심리 상담 세션에서 고객 발화에 효과적으로 반응할 수 있도록 훈련될 수 있음을 보여줍니다. 데이터 검증 결과, 전문가 평가와의 높은 일치를 보여 dataset의 높은 신뢰성을 뒷받침합니다.

### [MIGE: A Unified Framework for Multimodal Instruction-Based Image Generation and Editing](https://arxiv.org/abs/2502.21291)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.21291.png)

Vote: 4

Authors: Xueyun Tian, Bingbing Xu, Huawei Shen, Yige Yuan, Yuanzhuo Wang, Wei Li

- ***What's New***: MIGE는 멀티모달 인스트럭션(Multimodal Instructions) 기반의 이미지 생성과 편집을 통합하는 새로운 프레임워크를 제안합니다. 이 프레임워크는 주제 기반 생성(Subject-driven Generation)과 지시 기반 편집(Instruction-based Editing)을 혼합하여 새로운 조합 과제를 수행할 수 있으며, 지시 기반 주제 기반 이미지 편집(Instruction-based Subject-driven Image Editing)의 새로운 국가 최첨단 수준을 설정합니다.
- ***Technical Details***: MIGE는 멀티모달 인스트럭션을 통합 비전-언어 공간(Unified Vision-Language Space)으로 매핑하는 독창적인 멀티모달 엔코더(Multimodal Encoder)를 도입합니다. 그리고 채널 차원에서 연결된 잠재적 노이즈(latent noise)와 조건 입력(conditional input)을 통해 변환 기반 확산 모델을 사용하여 입력-출력 관계를 모델링합니다. 통합적인 데이터 생성 파이프라인을 제공하여 멀티모달 라지 랭귀지 모델(MLLM)이 다양한 멀티모달 인스트럭션을 자율적으로 생성하여 독립적 평가 벤치마크인 MIGEBench를 통해 평가합니다.
- ***Performance Highlights***: MIGE는 지시를 따르는 능력과 시각적 일관성을 강화함으로써 주제 기반 생성과 지시 기반 편집에서 뛰어난 성능을 발휘합니다. 실험 결과, MIGE는 DreamBench와 EmuEdit에서 주제 일관성을 높이고, MagicBrush 테스트 세트에서 텍스트와 이미지 일치율에서 최고 성과를 기록했습니다. 또한 MIGE는_instruction-based subject-driven image generation_의 새로운 조합 과제에서 우수한 결과를 달성하여 기존의 모델을 능가합니다.

### [DexGraspVLA: A Vision-Language-Action Framework Towards General Dexterous Grasping](https://arxiv.org/abs/2502.20900)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.20900.png)

Vote: 3

Authors: Yitao Liang, Yaodong Yang, Ceyao Zhang, Yuanpei Chen, Yifan Zhong, Xuchuan Huang, Ruochong Li

- ***What's New***: DexGraspVLA는 일반적인 손재주 있는 그리핑을 향한 최초의 계층적 비전-언어-행동 프레임워크(Vision-Language-Action Framework; VLA)로, 수천 개의 보지 못한 객체, 조명, 배경 조합에서 90% 이상의 성공률을 달성합니다. 이는 제로 샷(real-world zero-shot) 환경에서 시험되었습니다.
- ***Technical Details***: DexGraspVLA는 두 부분으로 구성됩니다. 고수준에서는 미리 학습된 비전-언어 모델(Vision-Language Model; VLM)을 태스크 플래너로 사용하며, 저수준에서는 확산 기반 정책(Diffusion-based Policy)을 행동 컨트롤러로 사용합니다. 이 접근법은 다양한 언어와 시각적 입력을 도메인 비변이 표현으로 변환하여 모방 학습(Imitation Learning)을 효과적으로 적용하고자 합니다. 이러한 구조로 인해 소량의 단일 도메인 인간 시연에서 학습하며 다양한 실제 상황에 신뢰성 있게 일반화할 수 있습니다.
- ***Performance Highlights***: DexGraspVLA는 1,287개의 보지 못한 물체, 조명 및 배경 조합에서 90.8%의 성공률을 달성했습니다. 이는 단일 객체 그리핑 벤치마크에서도 98.6%의 성공률을 기록하며, 기존의 비동결(frozen) 기능 추출기를 사용하지 않는 대안 설계보다 최소 48% 높은 성과를 가져옵니다. 실험적 분석은 환경 변이에 걸쳐 내부 모델 행동의 일관성을 입증하며, 프레임워크 설계의 적절성을 설명합니다.

### [LettuceDetect: A Hallucination Detection Framework for RAG Applications](https://arxiv.org/abs/2502.17125)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.17125.png)

Vote: 3

Authors: Ádám Kovács, Gábor Recski

- ***What's New***: LettuceDetect는 RAG(Retrieval-Augmented Generation) 시스템을 위한 새로운 환각 감지 프레임워크를 소개합니다. 이 프레임워크는 기존의 인코더 기반 방법에서의 문맥 제한과 LLM 기반 접근법의 비효율성을 해결합니다. ModernBERT를 기반으로 개발되었으며, 최대 8k 토큰까지의 확장된 문맥을 처리할 수 있습니다. RAGTruth 벤치마크 데이터셋에서 학습한 이 방법은 이전의 모든 인코더 기반 모델을 능가하며, 대부분의 프롬프트 기반 모델보다도 우수한 성능을 보입니다.
- ***Technical Details***: LettuceDetect는 ModernBERT를 기반으로 한 토큰 분류 모델로, 문맥-질문-답변 삼중지를 처리하여 지원되지 않는 주장(hallucination)을 토큰 수준에서 식별합니다. RAGTruth 데이터셋을 이용하여 인코더 기반 모델을 훈련했으며, 이는 이전의 상태-최신 기술로 인정받았던 Luna 아키텍처보다 14.8% 향상된 79.22%의 Example-level F1 점수를 달성합니다. LettuceDetect는 최대 8,192 토큰의 긴 문맥을 효율적으로 관리하는 조정된 로컬-글로벌 주의 메커니즘을 활용합니다.
- ***Performance Highlights***: LettuceDetect는 단일 GPU에서 30~60개의 예제를 초당 처리할 수 있는 실용성을 제공합니다. Large 모델(lettucedetect-large-v1)은 프롬프트 기반 방법(GPT-4-turbo의 63.4% F1과 비교해 79.22%)을 모두 능가하며, 이전의 최고 성능이었던 강화된 Llama-2-13B 모델을 초과 성능을 보입니다. Span-level 작업에서는 58.93%의 최고 F1 점수를 기록해 현재의 모든 스팬 수준 평가를 능가합니다.

### [EgoNormia: Benchmarking Physical Social Norm Understanding](https://arxiv.org/abs/2502.20490)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.20490.png)

Vote: 2

Authors: Phil Cuvin, Yanzhe Zhang, Hao Zhu, Caleb Ziems, Diyi Yang, Yicheng Fu, MohammadHossein Rezaei

- **What's New**: EgoNormia는 물리적 사회 규범 이해를 평가하기 위해 개발된 새로운 비디오 질문응답(VQA) 벤치마크입니다. 이 벤치마크는 인간의 행동과 사회적 상호작용에 관한 1,853개의 자아 중심 비디오로 구성되어 있으며, 각 비디오는 규범적 행동의 예측과 정당화에 관한 두 가지 질문을 포함합니다.
- **Technical Details**: EgoNormia는 Ego4D 데이터셋에서 1,077개의 비디오를 샘플링하여 만들어졌으며, 각 비디오는 독특한 사회적, 물리적 상황을 나타냅니다. 각 비디오에는 규범적 사건을 평가하기 위한 5개의 후보 행동과 그에 따른 정당화를 포함합니다. 철저한 사람 검증과 필터링 과정을 통해 데이터셋의 다양성과 정확성을 보장합니다.
- **Performance Highlights**: EgoNormia에서 최고의 모델인 Gemini 1.5 Pro는 45.3%의 정확도를 기록한 반면, 인간 기준 성능은 92.4%로 모델 성능과 큰 차이를 보였습니다. 이는 Vision-Language Models(VLMs)가 규범적 추론에 있어 여전히 많은 도전과제를 안고 있음을 시사합니다. 이러한 제한된 규범 이해 능력을 개선하기 위해 데이터 기반 검색-생성 방법이 약 9.4%의 성능 개선을 보여줍니다.

### [HAIC: Improving Human Action Understanding and Generation with Better Captions for Multi-modal Large Language Models](https://arxiv.org/abs/2502.20811)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.20811.png)

Vote: 1

Authors: Weihong Lin, Jianlong Wu, Jingyun Hua, Fuzheng Zhang, Yuanxing Zhang, Di Zhang, Xiao Wang, Liqiang Nie

- ***What's New***: 이 연구는 대규모 멀티모달 언어 모델(MLLMs)의 인간 행동 이해 능력을 향상시키기 위해 HAIC(인간 행동 이해 및 생성 향상)라는 새로운 데이터 주석 파이프라인을 도입합니다. 이 파이프라인은 인터넷에서 명확한 행동을 가진 비디오를 수집하고, 인간의 속성을 사용하여 개인을 구별하고 행동과 상호작용을 시간 순서대로 상세히 묘사하는 표준화된 캡션 형식을 사용하여 주석을 달아줍니다.
- ***Technical Details***: HAIC는 두 가지 데이터세트를 만들어냅니다. HAICTrain은 웹에서 수집된 126K 비디오와 Gemini-1.5-Pro를 이용해 생성된 캡션을 포함하며, HAICBench는 500 수작업으로 주석된 비디오와 1,400 QA 쌍을 포함하여 MLLMs의 인간 행동 이해 능력을 평가합니다. 캡션 주석 단계에서는 인간 속성을 사용하여 개인을 구별하고, 각 개인의 상세한 신체 동작과 상호작용을 시간 순서대로 강조합니다.
- ***Performance Highlights***: 전문가들은 HAICTrain으로 학습한 모델이 MVBench, PerceptionTest, ActivityNet-QA 및 HAICBench와 같은 4가지 벤치마크에서 인간 행동 이해 능력을 1.4%-2.1% 개선시킨다고 평가하였습니다. 또한, MovieGenBench에서 향상된 모델은 GSB 점수로 2.15와 6.81을 기록하여 원래 모델보다 향상된 성능을 보였습니다. 이 결과는 HAICTrain이 실제로 인간 행동의 이해와 캡션 품질 향상에 기여한다는 것을 보여줍니다.

