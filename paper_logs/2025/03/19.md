## Daily Papers (2025-03-19)

### [RWKV-7 "Goose" with Expressive Dynamic State Evolution](https://arxiv.org/abs/2503.14456)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.14456.png)

Vote: 93

Authors: Nathan Wilce, Ruichong Zhang, Daniel Wuttke, Bo Peng, Tianyi Wu, Christian Zhou-Zheng, William Merrill, Eric Alcaide, Janna Lu, Johan S. Wind, Saiteja Utpala, Haowen Hou, Guangyu Song, Daniel Goldstein, Kaifeng Tan

- ***What's New***: RWKV-7 'Goose'는 새로운 시퀀스 모델링 아키텍처를 도입하며, 3억 개의 파라미터 규모에서 멀티링구얼(다국어) 작업에서 새로운 최첨단 성능을 달성합니다. 이는 다른 최상위 3B 모델에 비해 훨씬 적은 유효 작업으로 훈련되었음에도 불구하고 현재의 영어 언어 성능을 충족시킵니다.
- ***Technical Details***: RWKV-7는 델타 규칙의 일반화된 형태를 소개하여 벡터 값 게이팅과 문맥 내 학습률을 포함하며, 이를 통해 상태 추적과 모든 정규 언어를 인식할 수 있습니다. 이 모델은 일정한 기억 사용과 일정한 추론 시간을 요구하여 입증적인 훈련 병렬성을 유지합니다. 3.1조 텍스트 이상의 멀티링구얼 코퍼스를 통해 훈련된 세 가지 뉴럴네트워크 모델을 공개합니다.
- ***Performance Highlights***: RWKV-7 모델은 동일한 파라미터 수규로 훈련된 다른 공개 모델과 비교했을 때 영어 및 멀티링구얼 벤치마크에서 높은 성능을 보였습니다. 또한 RWKV-7 Goose는 RWKV-6와 비교하여 MMLU 성능이 대폭 상승했으며, 일부 벤치마크에서 SmolLM2, Llama-3.2, Qwen-2.5와 같은 모델을 성능에서 초과했습니다.

### [Impossible Videos](https://arxiv.org/abs/2503.14378)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.14378.png)

Vote: 47

Authors: Hai Ci, Mike Zheng Shou, Zechen Bai

- ***What's New***: Impossible Videos는 현실 세계에서 불가능한, 반사실적 및 반현실적 장면을 포함하는 비디오를 생성하고 이해하는 새로운 접근법입니다. 이 연구는 이러한 불가능한 영상을 통해 현재의 비디오 생성 모델과 이해 모델을 평가하고, 그들의 발전을 촉진하기 위해 IPV-BENCH라는 벤치마크를 소개합니다.
- ***Technical Details***: IPV-BENCH는 물리적, 생물학적, 지리적, 사회적 법칙을 위반하는 다양한 장면을 포함하는 종합적인 분류 체계를 가지고 있습니다. 이 체계에 기반하여, 불가능한 장면을 묘사하는 260개의 텍스트 프롬프트(IPV-TXT)와 902개의 고품질 비디오 세트(IPV-VID)가 구성되었습니다. 이 벤치마크는 비디오 LLM의 시간 동역학 및 세계 지식 추론 능력을 평가합니다.
- ***Performance Highlights***: 주요 비디오 모델들은 불가능한 비디오의 생성과 이해에서 한계를 드러냈습니다. 예를 들어, GPT-4o는 OpenQA 과제에서 가장 높은 점수를 얻었지만 모든 모델이 불가능한 영상을 충분히 이해하는 데 어려움을 겪었습니다. IPV-BENCH는 현재 비디오 모델들의 한계를 밝혀내고, 향후 연구를 위한 방향을 제시합니다.

### [DAPO: An Open-Source LLM Reinforcement Learning System at Scale](https://arxiv.org/abs/2503.14476)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.14476.png)

Vote: 43

Authors: Wei-Ying Ma, Mofan Zhang, Lin Yan, Chengyi Wang, Guangming Sheng, Jinhua Zhu, Jiaze Chen, Mingxuan Wang, Gaohong Liu, Xiaochen Zuo, Lingjun Liu, Yu Yue, Tiantian Fan, Bole Ma, Yuxuan Tong, Haibin Lin, Zhiqi Lin, Mu Qiao, Jingjing Liu, Ruofei Zhu, Yufeng Yuan, Yonghui Wu, Wang Zhang, Hang Zhu, Jiangjie Chen, Chi Zhang, Weinan Dai, Xiangpeng Wei, Yuxuan Song, Ya-Qin Zhang, Xin Liu, Qiying Yu, Hongli Yu, Zheng Zhang, Hao Zhou

- ***What's New***: 이 논문에서는 대규모 언어 모델(LLM)의 강화 학습 시스템을 공개하며, Decoupled Clip and Dynamic sAmpling Policy Optimization (DAPO) 알고리즘을 제안합니다. DAPO는 강화 학습을 통해 복잡한 추론을 유도하며, AIME 2024에서 Qwen2.5-32B 모델 기반으로 50점을 달성하여 기존의 최첨단 모델보다 향상된 성과를 보입니다.
- ***Technical Details***: DAPO 알고리즘은 클립-하이어(Clip-Higher), 동적 샘플링(Dynamic Sampling), 토큰 레벨 정책 그래디언트 손실(Token-Level Policy Gradient Loss), 그리고 과장된 보상 형성(Overlong Reward Shaping) 등 네 가지 주요 기술을 사용하여 확장 가능한 강화 학습 시스템을 구현합니다. 이를 통해 모델의 훈련 안정성을 향상시키고 복잡한 추론 시나리오에 효과적으로 대응할 수 있습니다.
- ***Performance Highlights***: DAPO는 DeepSeek-R1-Zero-Qwen-32B 모델보다 50% 적은 학습 단계로 AIME 2024에서 50점을 기록하며, 기존 47점이었던 성능을 초과 달성하였습니다. 이러한 결과는 DAPO 알고리즘이 대규모 언어 모델의 강화 학습에 효과적이며 효율적이라는 것을 입증합니다.

### [Creation-MMBench: Assessing Context-Aware Creative Intelligence in MLLM](https://arxiv.org/abs/2503.14478)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.14478.png)

Vote: 37

Authors: Xiangyu Zhao, Zhijian Chen, Kai Lan, Yingji Liang, Haodong Duan, Kai Chen, Guofeng Zhang, Zicheng Zhang, Farong Wen, Dahua Lin, Shengyuan Ding, Xinyu Fang

- ***What's New***: Creation-MMBench는 MLLMs의 창의적 역량을 평가하기 위한 새로운 멀티모달 벤치마크로, 실제 이미지 기반 작업에 대한 창의적 능력을 측정하기 위한 총 51개의 세밀한 작업과 765개의 테스트 케이스로 구성되어 있습니다. 각 테스트 케이스에 대해 사례별 평가 기준을 정의하여, 일반 응답 품질과 시각 정보와의 사실적 일관성을 평가할 수 있도록 설계되었습니다.
- ***Technical Details***: Creation-MMBench는 문학 작문, 일반 기능적 작문, 전문 기능적 작문, 창의적 멀티모달 이해의 네 가지 주요 그룹으로 나뉘어 있습니다. MLLMs는 주어진 이미지와 상세한 컨텍스트를 활용해 다양한 창의적 작업을 수행해야 하며, 각 작업에는 인스턴스별로 평가 기준이 설정되어 있습니다. 평가 프레임워크에서는 MLLM-as-a-Judge 방법론을 채택하여 모델 생성 응답의 품질을 GPT-4o를 이용해 평가합니다. 시각적 사실성 점수는 모델의 응답이 시각적 입력의 주요 사실과 맞는지를 평가하는 데 사용됩니다.
- ***Performance Highlights***: 현재의 오픈 소스 MLLMs는 창의적 작업에서 독점 모델들에 비해 성과가 낮음이 나타났습니다. 또한, 시각적 미세조정(Visual Fine-tuning)이 기본 LLM의 창의적 능력에 부정적인 영향을 미칠 수 있음을 실험적으로 확인했습니다. Creation-MMBench를 바탕으로 대부분의 오픈 소스 MLLMs는 시각적 창의성에서 여전히 주요한 도전 과제를 안고 있으며, 이는 향후 연구개발 방안에 중요한 통찰을 제공합니다.

### [DeepPerception: Advancing R1-like Cognitive Visual Perception in MLLMs for Knowledge-Intensive Visual Grounding](https://arxiv.org/abs/2503.12797)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12797.png)

Vote: 24

Authors: Xinyu Ma, Chi Chen, Zonghao Guo, Zhicong Luo, Maosong Sun, Derek F. Wong, Xiaoyi Feng, Ziyang Ding

- ***What's New***: DeepPerception은 MLLMs가 도메인 지식에 기반하여 정확한 시각적 분석을 수행할 수 있도록 도와주는 새로운 KVG(Knowledge-Intensive Visual Grounding) 작업을 도입했습니다. 이 작업은 기존의 시각적 그라운딩에서 더 나아가, 세부적인 시각 인식과 도메인 특화 지식의 통합을 요구합니다.
- ***Technical Details***: DeepPerception은 자동화된 데이터 합성 파이프라인과 인지적 추론 스캐폴딩을 위한 지도 학습 기반의 두 단계 학습 프레임워크를 통해 모델의 인지적 시각 인식 능력을 강화합니다. 이 모델은 KVG-Bench라는 10개 도메인과 1.3K 개의 수작업으로 큐레이션된 테스트 사례를 포함한 포괄적인 데이터셋에서 평가됩니다.
- ***Performance Highlights***: DeepPerception은 KVG-Bench에서 기존의 단순한 파인 튜닝 방식보다 정확도를 8.08% 향상시켰고, 교차 도메인 일반화 측면에서 4.60% 더 뛰어난 성능을 보여주었습니다. 이는 지식 결합적 인지 과정이 MLLMs의 인간과 유사한 시각 인식에 필수적임을 강조합니다.

### [CapArena: Benchmarking and Analyzing Detailed Image Captioning in the LLM Era](https://arxiv.org/abs/2503.12329)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12329.png)

Vote: 19

Authors: Jiaxin Fan, Nuo Chen, Kanzhi Cheng, Chenyang Yan, Wenpo Song, Jianbing Zhang, Zheng Ma, Fangzhi Xu, Qiushi Sun, Jiajun Chen

- ***What's New***: CapArena는 대형 언어 모델(LLMs) 시대에 맞춰 세부적 이미지 캡셔닝의 성능을 벤치마크하고 분석하는 새로운 플랫폼입니다. 6,000개 이상의 인간 선호도를 반영한 캡션 배틀을 통해 모델을 평가하며, 최신 모델들은 점점 인간 수준의 성능을 달성하고 있는 반면, 대부분의 오픈 소스 모델은 여전히 뒤처지는 모습을 보입니다.
- ***Technical Details***: CapArena 플랫폼은 14개의 첨단 비전-언어 모델(Vision-Language Models; VLMs)과 인간의 이미지를 평가하기 위해 6,000개 이상의 고품질 인간 주석을 포함합니다. 또한, 캡션 평가를 위한 제3자 메트릭과 VLM-as-a-Judge를 활용하여 캡션 품질을 평가한 결과 기존 메트릭의 체계적 편향이 있음을 발견하였습니다.
- ***Performance Highlights***: 최신 VLMs, 특히 GPT-4o는 인간 수준의 성능을 달성했으며, 일부 경우에서는 이를 초과하기도 합니다. 반면, 대부분의 오픈 소스 모델은 섬세한 이미지 인지 능력에서 상업적 모델과 비교해 여전히 격차가 큽니다. 새로운 VLM-as-a-Judge 메트릭은 인간 판단과의 일치를 높이며, CapArena-Auto 플랫폼은 신속하고 정확한 평가를 제공하여 인간 순위와 94.3%의 상관관계를 보입니다.

### [Infinite Mobility: Scalable High-Fidelity Synthesis of Articulated Objects via Procedural Generation](https://arxiv.org/abs/2503.13424)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.13424.png)

Vote: 18

Authors: Zhaoyang Lyu, Zichao Yu, Kaixu Chen, Yitong Wang, Jiangmiao Pang, Li Ray Luo, Qihong Tang, Xinyu Lian, Yuanzhen Zhou, Xudong Xu, Bo Dai, Ruiming Liang

- ***What's New***: Infinite Mobility는 절차적 생성(Procedural Generation)을 통해 고품질의 대규모 관절 객체(Articulated Objects)를 합성하는 새로운 파이프라인을 제안합니다. 이는 물리적 특성과 메쉬 품질 모두에서 인간 주석 데이터셋과 비교할 수 있는 결과를 제공하고, 향후 생성 모델(Generative Models)에서의 데이터 확장에 기여할 수 있습니다.
- ***Technical Details***: Infinite Mobility 파이프라인은 각 객체의 관절을 URDF의 트리 구조(Tree Structure)와 유사하게 표현하며, 노드는 링크, 간선은 관절을 나타냅니다. 우리는 관절 구조 생성을 위한 트리 성장 전략(Tree-Growing Strategy)을 사용하며,루트 노드에서 시작해 점진적으로 트리를 성장시킵니다. 이를 통해 모든 관절과 파트의 물리적 특성의 정확성을 보장할 수 있는 절차적 파라다임(Procedural Paradigm)을 제공합니다.
- ***Performance Highlights***: 우리의 데이터는 Sapien 시뮬레이션을 통해 관절의 물리적 특성을 평가하며, 인간 평가자와 비전-언어 모델(Vision Language Models)을 사용하여 메쉬 품질을 비교 평가하였습니다. 결과적으로 Infinite Mobility는 기존 데이터셋 및 최첨단 생성 모델을 모든 측면에서 능가하는 성능을 보여주었습니다. 이를 통해 생성된 합성 데이터가 효과적으로 사용될 수 있음을 입증하였습니다.

### [Frac-Connections: Fractional Extension of Hyper-Connections](https://arxiv.org/abs/2503.14125)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.14125.png)

Vote: 13

Authors: Yutao Zeng, Xun Zhou, Banggu Wu, Zihao Huang, Qiyang Min, Defa Zhu, Jundong Zhou, Hongzhi Huang

- ***What's New***: Frac-Connections는 Hyper-Connections의 확장을 개선한 새로운 방법입니다. 이 방법은 숨겨진 상태(Hidden States)를 여러 부분으로 나누어 메모리 소비를 줄이면서 Hyper-Connections의 일부 이점을 유지합니다. 대규모 언어 모델 실험을 통해 Frac-Connections가 잔여 연결보다 뛰어난 성능을 발휘함을 입증했습니다.
- ***Technical Details***: Frac-Connections는 숨겨진 상태를 복사하여 폭을 늘리는 대신 여러 조각으로 나누어 처리합니다. 이는 Hyper-Connections의 확장률을 분수 영역으로 확장하여 구현되며, 확장률 n이 1일 때 Frac-Connections와 Hyper-Connections가 동일해집니다. 이 방법으로 메모리 사용량을 줄이면서 다양한 연결 강도를 모델링할 수 있습니다. Frac-Connections는 큰 규모의 언어 모델과 Mixture-of-Experts(MoE) 아키텍처에 대한 실험을 통해 모델의 학습 안정성과 다양한 자연어 처리 벤치마크에서의 성능 향상을 제공합니다.
- ***Performance Highlights***: Frac-Connections는 OLMoE-7B 및 OLMo2-1B2 모델에서 각각 3T 및 2T 토큰으로 훈련된 경우 대체 모델보다 높은 정확도를 기록했습니다. 특히, SciQ 및 Commonsense QA와 같은 대부분의 벤치마크에서 더 나은 성능을 보였으며, 예를 들어 WinoGrande에서 +0.95%의 성능 향상을 보였습니다. 이러한 결과는 Frac-Connections가 다양한 NLP 작업에서 더 나은 일반화 성능을 제공함을 시사합니다.

### [Cosmos-Transfer1: Conditional World Generation with Adaptive Multimodal Control](https://arxiv.org/abs/2503.14492)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.14492.png)

Vote: 12

Authors: Huan Ling, Shiyi Lan, NVIDIA, Fabio Ramos, Ting-Chun Wang, Mike Chen, Xian Liu, Xiaohui Zeng, Yu Zeng, Ming-Yu Liu, Joshua Chen, Hassan Abu Alhaija, Jay Wu, Jinwei Gu, Jose Alvarez, Yunhao Ge, Ali Hassani, Michael Isaev, Hanzi Mao, Tobias Lasser, Yuchong Ye, Maciej Bala, Dieter Fox, Xuanchi Ren, Yifan Lu, Alice Luo, Pooya Jannaty, Liz Cha, Tianchang Shen, Tiffany Cai, Shitao Tang, Sanja Fidler, Kevin Xie, Jiashu Xu, Qianli Ma, Tianshi Cao, Francesco Ferroni, Xiaodong Yang, Stella Xu

- ***What's New***: Cosmos-Transfer1는 조건부 세계 생성 모델로서 다양한 모달리티(spatial control inputs)를 기반으로 시뮬레이션을 생성할 수 있는 최초의 모델입니다. 특히 세분화(segmentation), 깊이(depth), 엣지(edge)와 같은 다양한 입력을 통해 세계를 생성할 수 있는 특성을 가집니다. 이 모델은 로봇 공학에서의 Sim2Real 변환 및 자율 주행 차량 데이터 강화와 같은 다양한 세계 간 전이(world transfer) 활용 사례에 사용됩니다.
- ***Technical Details***: Cosmos-Transfer1은 디퓨전 기반 모델(diffusion model)로서 Cosmos-Predict1(NVIDIA, 2025)의 확장판입니다. 각 모달리티에 대해 개별 제어 브랜치(control branches)를 적용하며, 제어 입력(conditional inputs)을 통해 세분화된 세계 생성 능력을 발휘합니다. 학습 시, 개별 컨트롤 브랜치를 별도로 학습하고, 추론 시 각기 다른 모달리티의 정보를 융합하여 활용합니다. 또한, NVIDIA GB200 NVL72 랙을 이용해 실시간 세계 생성을 실현할 수 있습니다.
- ***Performance Highlights***: TransferBench를 이용한 평가 결과, Cosmos-Transfer1은 제어 입력 신호와의 일관성, 생성 다양성, 전반적인 생성 품질에서 우수한 성능을 보여 주었습니다. 특히, 여러 제어 입력을 융합하여 모달리티 간 상호 보완적인 특성을 활용함으로써 높은 품질의 결과를 도출할 수 있음을 입증했습니다. 실험 결과 자율주행 및 로봇공학 시뮬레이션의 현실감을 향상시키는 잠재력을 보였습니다.

### [Aligning Multimodal LLM with Human Preference: A Survey](https://arxiv.org/abs/2503.14504)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.14504.png)

Vote: 10

Authors: Liang Wang, Chaoyou Fu, Tao Yu, Tianlong Xu, Junkang Wu, Zhang Zhang, Guibin Zhang, Yunhang Shen, Tieniu Tan, Dingjie Song, Yan Huang, Yi-Fan Zhang, Kun Wang, Yibo Yan, Qingsong Wen, Xingyu Lu, Jinda Lu

- ***What's New***: 이 논문은 대형 언어 모델(LLM)을 기반으로 한 멀티모달 대형 언어 모델(MLLM)을 인간의 선호와 정렬시키기 위한 알고리즘을 체계적으로 조사합니다. 특히, 이러한 정렬 알고리즘이 다양한 응용 시나리오와 데이터 생성 요인에 어떻게 적용되는지에 대한 포괄적이고 체계적인 검토를 제공합니다.
- ***Technical Details***: MLLM 정렬을 위한 데이터셋은 주로 데이터 소스, 모델 응답, 선호도 주석의 세 가지 핵심 요소를 통해 구축됩니다. 이 논문은 구체적으로 시나리오별 정렬 알고리즘, 데이터셋 구축, 알고리즘 평가 기준, 정렬 알고리즘의 미래 방향에 대한 심도 있는 분석을 포함합니다. 이러한 분석을 통해 연구자들은 연구 분야에서의 현재 발전 상황을 정리하고 데이터 품질과 다양성을 개선할 수 있는 방법을 찾을 수 있습니다.
- ***Performance Highlights***: 대부분의 MLLM은 최고 수준의 정렬 단계인 인간 선호에 기반한 강화 학습(RLHF)과 직접적인 선호 최적화(DPO) 단계를 거치지 않았으며, 이는 진실성, 안전성 등의 문제를 해결하는 데 불충분했습니다. 현재의 MLLM 정렬 알고리즘들은 벤치마크에서 다양한 역량을 평가하며, 시각 정보 통합의 가능성과 현재의 도전 과제를 논의합니다. 연구진은 이 논문이 학계와 산업계의 연구자들에게 유용한 지침이 되기를 기대합니다.

### [Measuring AI Ability to Complete Long Tasks](https://arxiv.org/abs/2503.14499)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.14499.png)

Vote: 9

Authors: Luke Harold Miles, Neev Parikh, Lawrence Chan, Sami Jawhar, Brian Goodrich, Hjalmar Wijk, Max Hasin, Tao Lin, David Rein, Ryan Bloom, Lucas Jun Koba Sato, Megan Kinniment, Thomas Broadley, Joel Becker, Nate Rush, Amy Deng, Daniel M. Ziegler, Seraphina Nix, Thomas Kwa, Haoxing Du, Nikola Jurkovic, Sydney Von Arx, Katharyn Garcia, Elizabeth Barnes, Ben West

- ***What's New***: AI의 장기 작업 수행 능력을 평가하는 새로운 방법론이 제안되었습니다. AI 모델의 50% 작업 완료 시간 지평선(50%-task-completion time horizon)을 사용하여 인간 전문가가 동일한 작업을 완료하는 데 걸리는 평균 시간을 기준으로 AI의 능력을 측정합니다.
- ***Technical Details***: 이 연구에서는 HCAST, RE-Bench, 그리고 66개의 새로운 짧은 작업을 포함한 170개의 다양한 작업 세트를 사용하여 AI 모델의 성능을 평가합니다. AI의 성능은 인간의 성공률 50%에 해당하는 작업 완료 시간을 통해 측정되며, 이를 통해 AI의 실제 능력을 평가하고 예측합니다.
- ***Performance Highlights***: 2024년 이후 AI 모델의 작업 완료 시간 지평선이 급격히 증가하여 o1 모델의 경우 약 39분의 50%-time horizon을 달성했습니다. 이는 2019년 이후 매 7개월마다 지평선이 두 배로 증가한 것을 보여줍니다. 이러한 성능 증가는 논리적 추론 개선과 도구 사용 역량 향상에 기인하며, AI의 자율성이 증가하고 있음을 시사합니다.

### [AudioX: Diffusion Transformer for Anything-to-Audio Generation](https://arxiv.org/abs/2503.10522)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10522.png)

Vote: 9

Authors: Xu Tan, Qifeng Chen, Wei Xue, Yike Guo, Zhaoyang Liu, Zeyue Tian, Ruibin Yuan, Yizhu Jin

- ***What's New***: AudioX는 최초로 다양한 멀티모달 입력(텍스트, 비디오, 이미지, 음악, 오디오)을 통합하여 고품질 오디오와 음악을 생성할 수 있는 통합 Diffusion Transformer 모델입니다. 이 모델은 입력 마스킹(multi-modal masking) 전략을 활용하여 강력한 교차 모달 표현(cross-modal representations)을 학습하도록 합니다.
- ***Technical Details***: AudioX는 텍스트, 비디오, 이미지, 음악, 오디오를 입력으로 받아들이며, 이들과의 조합을 통해 오디오를 생성할 수 있는 DiT (Diffusion Transformer) 기반의 프레임워크를 사용합니다. 입력 모달을 마스킹하여 강건한 표현 학습과 모달 간의 정렬을 보장하며, 두 개의 거대한 코퍼스(vggsound-caps와 V2M-caps)를 학습 데이터로 활용하여 데이터 부족 문제를 해결했습니다.
- ***Performance Highlights***: AudioX는 다수의 벤치마크 데이터셋에서 최첨단(SOTA) 성능을 달성하며, 다양한 멀티모달 입력의 통합 처리에서 뛰어난 성능을 보였습니다. 텍스트-오디오 변환, 비디오-오디오 변환 등의 작업에서 AudioX는 기존 전문 모델들을 능가하거나 유사한 성능을 보였습니다.

### [Atlas: Multi-Scale Attention Improves Long Context Image Modeling](https://arxiv.org/abs/2503.12355)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12355.png)

Vote: 9

Authors: Kumar Krishna Agrawal, Natalia Harguindeguy, Adam Yala, Long Lian, Boyi Li, Alexander Bick, Longchao Liu, Trevor Darrell, Maggie Chung

- ***What's New***: Atlas는 대규모 이미지 모델링 문제를 해결하기 위해 Multi-Scale Attention(MSA)이라는 새로운 신경망 아키텍처를 도입했습니다. 이는 O(logN)의 스케일에서 이미지 특징을 점진적으로 표현하는 방법을 사용하여 MSA를 통해 다양한 스케일의 정보 통신을 가능하게 합니다.
- ***Technical Details***: Atlas는 MSA를 기반으로 하여 각 스케일에서 느린 스케일 축소(macroscope sampling)을 통해 다중 스케일 표현을 활용합니다. MSA 블록은 O(logS N) 스케일을 유지하며 O(N log N) 시간 복잡도로 모든 스케일 간의 정보를 혼합합니다. MSA는 상향식(top-down) 및 하향식(bottom-up) 통신을 통해 조밀한 교차-주의(cross-attention)를 사용하여 각 스케일의 토큰들이 상호작용할 수 있게 설계되어 있습니다.
- ***Performance Highlights***: Atlas는 머신러닝의 ImageNet-100의 고해상도 변형에서 1024px 해상도에서 ConvNext-B와 유사한 91.04%의 정확도를 달성하면서도 4.3배 빠른 처리 속도를 보여줍니다. 다른 모델과 비교했을 때, FasterViT보다 2.95배 빠르고 7.38% 더 높은 정확도를 기록했으며, LongViT보다 2.25배 빠르고 4.96% 더 높은 정확도를 얻었습니다. 이는 Atlas가 높은 해상도에서 기존의 효율적 아키텍처를 능가하는 성능을 발휘함을 강조합니다.

### [MPBench: A Comprehensive Multimodal Reasoning Benchmark for Process Errors Identification](https://arxiv.org/abs/2503.12505)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12505.png)

Vote: 8

Authors: Kaipeng Zhang, Hongxun Yao, Pengfei Zhou, Wangbo Zhao, Jiaxin Ai, Kai Wang, Zhaopan Xu, Wenqi Shao, Xiaojiang Peng

- ***What's New***: MPBench는 대규모 언어 모델(LLMs)의 복잡한 추론 과제 및 프로세스 오류 식별 능력을 강화하기 위한 포괄적인 멀티모달 벤치마크로 처음 소개되었습니다. 기존 벤치마크가 텍스트 기반에 치중하고 있는 반면, MPBench는 멀티모달 콘텐츠를 포함하여 프로세스 보상 모델(PRMs)의 다양한 시나리오에서의 효과를 평가할 수 있도록 설계되었습니다.
- ***Technical Details***: MPBench는 세 가지 평가 패러다임을 통해 PRMs가 추론 과정에서 수행하는 특정 역할을 평가합니다: (1) 단계 정확성(Step Correctness)은 중간 추론 단계에서의 정확성을 평가합니다; (2) 답변 집계(Answer Aggregation)는 여러 해결책을 집계하여 최적의 해결책을 선택합니다; (3) 추론 프로세스 검색(Reasoning Process Search)은 추론 중 최적의 추론 단계를 탐색하는 것을 안내합니다. 이 벤치마크는 6개의 하위 카테고리에서 9,745개의 세밀한 데이터 인스턴스를 제공하여 다양한 도메인에서 PRM 성능을 종합적으로 평가할 수 있는 견고한 프레임워크를 제공합니다.
- ***Performance Highlights***: MPBench 테스트에서는 GPT-4o와 같은 독점 모델이 모든 평가 범주에서 두각을 나타냈으며, 특히 추론 프로세스 검색에서의 F1과 MCC에서 높은 성능을 보였습니다. 반면, Qwen2.5-VL과 같은 오픈 소스 모델은 이러한 복잡한 추론 과제에서 상반된 성능 특성을 보이며, 특히 수학 도메인에서 어려움을 겪었습니다. 이는 강력한 모델 용량이 복잡한 문제에 효과적으로 대응하기 위해 중요하다는 것을 시사합니다. 또한, 단계 정확성, 답변 집계, 추론 프로세스 검색 간의 긍정적인 상관관계를 발견하였으며, 이는 다양한 추론 능력 간의 상호 연관성을 나타냅니다.

### [FlexWorld: Progressively Expanding 3D Scenes for Flexiable-View Synthesis](https://arxiv.org/abs/2503.13265)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.13265.png)

Vote: 7

Authors: Hao Sun, Min Zhao, Chongxuan Li, Wenhao Huang, Ji-Rong Wen, Luxi Chen, Zihan Zhou, Yikai Wang, Ge Zhang

- ***What's New***: FlexWorld는 단일 이미지에서 유연한 시각 3D 장면을 생성하는 새로운 프레임워크로, 두 가지 주요 구성 요소를 포함합니다: 강력한 비디오에서 비디오(V2V) 확산 모델을 사용하여 불완전한 장면에서 고품질의 새로운 시점을 생성하고, 기하학 인식 장면 확장을 통해 모든 3D 콘텐츠를 통합하여 전체 3D 장면을 점진적으로 생성합니다.
- ***Technical Details***: FlexWorld는 사전 훈련된 비디오 모델을 기반으로 하며, 정확한 깊이 추정 훈련 쌍을 사용합니다. 이 프레임워크는 큰 카메라 자세 변경에도 새로운 시점을 생성할 수 있고, V2V 모델은 조밀한 스테레오(dense stereo) 모델을 통해 장면을 확장하고 통합함으로써 단일 이미지에서 3D 장면을 점진적으로 생성합니다. 비디오로부터 추출한 새로운 3D 콘텐츠를 전체 구조에 통합하며, 다양한 카메라 경로 계획(cameras trajectory planning)을 사용하여 장면의 시각적 품질을 개선하는 정제 프로세스(refinement process)를 포함합니다.
- ***Performance Highlights***: FlexWorld는 다양한 데이터셋에서 기존 최신 방법들에 비해 뛰어난 시각적 품질을 달성하였으며, 실험 결과에서 우수한 새로운 시점 생성 및 유연한 시각의 3D 장면 합성 능력을 입증했습니다. 특히, 고품질의 비디오와 장면 생성에서 기준 모델들과 비교할 때 우수한 성능을 보여줍니다.

### [Temporal Consistency for LLM Reasoning Process Error Identification](https://arxiv.org/abs/2503.14495)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.14495.png)

Vote: 6

Authors: Yue Wu, Mengdi Wang, Ling Yang, Kaixuan Huang, Jiacheng Guo, Jiahao Qiu, Xinzhe Juan

- ***What's New***: 이 논문은 LLM(대형 언어 모델)의 수학적 과정 오류 식별을 개선하기 위해 새로운 시간적 일관성(Temporal Consistency) 방법을 제안합니다. 이는 LLM이 일련의 자기 반영(Self-reflection) 행동을 통해 일관성을 활용하여 검증 정확성을 향상시키는 것입니다. 이 방법은 기존의 단일 라운드 검증이나 다중 모델 토론 접근 방식과는 차별화됩니다.
- ***Technical Details***: 시간적 일관성 방법은 각 LLM이 자기 자신을 검토하는 자기 확인(Self-checking) 메커니즘을 포함한 일련의 검증 단계를 통해, 최종적으로 여러 LLM이 시간 경과에 걸쳐 일관된 자기 확인을 표시할 때만 최종 출력을 생성합니다. 이는 초기 및 반복적인 자기 확인 단계를 포함하여 세 단계로 나뉘고, 다수결 투표(Majority Voting) 함수 및 다양한 합의 조건을 통해 수렴성을 확인합니다.
- ***Performance Highlights***: 이 방법은 Mathcheck, ProcessBench, PRM800K 등 다양한 수학적 과정 오류 식별 벤치마크에서 일관된 성능 향상을 보여주었습니다. 특히 Deepseek R1 모델에서 7B/8B 모델이 기존 70B/72B 모델 및 GPT-4o를 능가하는 성과를 보였으며, 14B 모델은 Deepseek-R1과 유사한 성능을 달성했습니다. MathCheck에서 Llama-8B 모델은 46.6% 향상, ProcessBench에서 37.9% 향상, PRM800K에서 29.0% 향상을 보였다.

### [Reflect-DiT: Inference-Time Scaling for Text-to-Image Diffusion Transformers via In-Context Reflection](https://arxiv.org/abs/2503.12271)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12271.png)

Vote: 6

Authors: Shufan Li, Aditya Grover, Akash Gokul, Konstantinos Kallidromitis, Arsh Koneru, Kazuki Kozuka, Yusuke Kato

- ***What's New***: Reflect-DiT는 텍스트-이미지 디퓨전 트랜스포머(Text-to-Image Diffusion Transformers; DiTs)에 컨텍스트 반영(In-Context Reflection) 기능을 추가하여, 이전에 생성된 이미지와 텍스트 피드백을 활용해 이미지 생성의 정확도를 높이는 신규 프레임워크입니다. 이는 단순 무작위 샘플링에 의존하기보다는, 특정 개선이 필요한 부분을 명시적으로 다루도록 설계되었습니다.
- ***Technical Details***: Reflect-DiT는 시각-언어 모델(Vision-Language Model; VLM)과 디퓨전 트랜스포머로 구성되며, 이전 생성된 이미지와 피드백을 시각 및 텍스트 인코더로 변환한 뒤 컨텍스트 트랜스포머를 거쳐 디퓨전 트랜스포머에 전달, 이미지 생성 과정에서 피드백을 반영합니다. 컨텍스트 길이는 고정되어 있으며, 이전 생성 결과가 컨텍스트 제한을 초과할 경우, 일부를 선택하여 반영합니다.
- ***Performance Highlights***: Reflect-DiT는 SANA-1.0-1.6B 모델을 기반으로 GenEval 벤치마크에서 +0.19점 향상을 기록하며, 20개 샘플만으로 0.81의 최고 점수를 달성하여 이전 기록을 경신했습니다. 이는 2048 샘플을 사용한 이전의 최고점 모델 SANA-1.5-4.8B의 성능을 뛰어넘는 결과입니다.

### [MM-Spatial: Exploring 3D Spatial Understanding in Multimodal LLMs](https://arxiv.org/abs/2503.13111)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.13111.png)

Vote: 6

Authors: Yinfei Yang, Gefen Kohavi, Marcin Eichner, Nina Wenzel, Peter Grasch, David Griffiths, Justin Lazarow, Haiming Gang, Kai Kang, Afshin Dehghan, Erik Daxberger

- ***What's New***: 이번 연구에서는 멀티모달 대형 언어 모델(MLLMs)의 3D 공간 이해를 향상시키기 위해 새로운 데이터셋과 벤치마크를 제안하였습니다. Cubify Anything VQA(CA-VQA)라는 데이터로 다양한 3D 공간 이해 작업을 수행할 수 있도록 하고, 이를 통해 MM-Spatial이라는 강력한 멀티모달 언어 모델을 학습하여 최신 성능을 달성하였습니다.
- ***Technical Details***: CA-VQA 데이터셋은 실내 장면을 중심으로 한 3D 공간 인식 작업을 포함하며, 여러 뷰 이미지, 센서 기반 깊이 및 단안(몬코큘러) 추정 깊이 데이터를 통해 다양한 과제를 다루고 있습니다. MM-Spatial 모델은 깊이 정보를 활용하여 체인-오프-솔루션(Chain-of-Thought) 스타일의 공간 추론을 수행하며, 단안 깊이 추정 모델과 비교할 수 있는 깊이 인식 능력을 확보했습니다.
- ***Performance Highlights***: MM-Spatial 모델은 3D 공간 인식 벤치마크에서 최첨단 성능을 보여주며, 특히 깊이 및 다양한 시각적 입력을 통합하여 3D 이해도를 크게 향상시켰습니다. 시각 쪽 능력을 검증하는 실험에서는 오직 언어 정보에 의존하는 모델보다 훨씬 뛰어난 성능을 발휘하여, 궁극적으로 멀티모달 대형 언어 모델의 공간 이해 측면에서 중요한 발전을 이루었습니다.

### [Concat-ID: Towards Universal Identity-Preserving Video Synthesis](https://arxiv.org/abs/2503.14151)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.14151.png)

Vote: 5

Authors: Xiaotao Gu, Chongxuan Li, Zhuoyi Yang, Yong Zhong, Jiayan Teng

- ***What's New***: Concat-ID는 새로운 통합 프레임워크로서, 정체성 유지 비디오 생성(identity-preserving video generation) 분야에서 혁신적인 접근법을 제시합니다. 이 시스템은 VAEs(Variational Autoencoders)를 활용하여 이미지 특징을 추출하고, 이를 비디오 잠재 요소와 연결하여 정체성을 유지하면서도 편집 가능성을 높입니다. 또한, 단일 및 다중 정체성 시나리오에 대한 새로운 벤치마크를 설정하였습니다.
- ***Technical Details***: Concat-ID는 정체성 유지 비디오 생성을 위해 3D 자체 주의 메커니즘(3D self-attention mechanisms)만을 사용합니다. 이 접근법은 추가 모듈이 필요 없이, 단일 및 다중 정체성 생성과 다양한 주제에 대해 간편하게 스케일링 됩니다. 크로스 비디오 페어링 전략(cross-video pairing strategy)과 다단계 훈련 레지멘(multi-stage training regimen)을 통해 정체성 일관성과 얼굴 편집 가능성의 균형을 맞추며, 비디오 자연스러움을 향상시킵니다.
- ***Performance Highlights***: Concat-ID는 단일 및 다중 정체성 생성에서 기존 방법들보다 뛰어난 정체성 일관성을 입증하였으며, 얼굴 편집 가능성 면에서도 우수하였습니다. 또한 다중 주제 시나리오에서도 무리 없이 확장될 수 있어, 다양한 응용 분야에서 일관된 높은 성능을 보장합니다.

### [Florenz: Scaling Laws for Systematic Generalization in Vision-Language Models](https://arxiv.org/abs/2503.09443)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.09443.png)

Vote: 4

Authors: Sven Behnke, Sebastian Houben, Julian Spravil

- ***What's New***: Florenz는 비전-언어 모델(Vision-Language Models; VLMs)의 체계적인 일반화 확장 법칙을 연구하며, 단언어 VLM을 이용하여 다중언어 작업에서의 전환을 시도합니다. 새로운 합성 데이터셋을 통해 다국어 이미지 캡션 작업에서의 일반화 잠재성을 평가합니다.
- ***Technical Details***: Florenz는 0.4B에서 11.2B 파라미터의 단언어 인코더-디코더 VLM으로, 사전 훈련된 VLM Florence-2와 대형 언어 모델 Gemma-2를 결합하여 만들어졌습니다. 이는 합성 다국어 번역 데이터셋을 사용해 훈련되었으며, 캡션 데이터가 일부 언어에만 제공되는 불완전한 데이터셋으로 일반화를 시험합니다.
- ***Performance Highlights***: 실험 결과, Florenz는 영어와 독일어에서 볼 수 없는 언어-작업 쌍에서도 효과적으로 적용 가능함을 보였습니다. Multi30K, CoMMuTE, XM3600, COCO Karpathy 벤치마크에서 더 일반적인 다중 모달 번역과 이미지 캡셔닝 작업에서 유망한 성능 확장 경향을 보였습니다.

### [PEBench: A Fictitious Dataset to Benchmark Machine Unlearning for Multimodal Large Language Models](https://arxiv.org/abs/2503.12545)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12545.png)

Vote: 4

Authors: Weidong Tang, Kaipeng Zhang, Hongxun Yao, Pengfei Zhou, Wangbo Zhao, Jiaxin Ai, Kai Wang, Yang You, Zhaopan Xu, Wenqi Shao, Xiaojiang Peng

- ***What's New***: PEBench는 다중 모달 대형 언어 모델들(Multimodal Large Language Models; MLLMs)에 대한 기계 학습 해제(Machine Unlearning; MU)를 평가하기 위해 가상의 개인 엔티티와 일반적인 이벤트 장면으로 구성된 데이터를 제공하는 최초의 벤치마크입니다. 이 벤치마크는 MLLMs의 보안과 프라이버시 보호 능력을 향상시키기 위한 표준화된 프레임워크를 제공합니다.
- ***Technical Details***: PEBench는 200명의 가상 인물 엔티티와 40개의 이벤트 장면을 포함하여 총 8,000 장의 이미지를 제공하는 합성 데이터셋을 포함합니다. 데이터는 텍스트 설명 생성과 이미지 생성을 통해 다양한 직업과 지리적 배경을 가진 인물들을 상세히 표현하며, 각 인물은 고유한 외모와 이름을 가집니다. 데이터 셋은 세부적인 이미지의 일관성과 정합성을 보장합니다. MU 방식은 6개의 서로 다른 방법으로 벤치마킹되며, 각 방법의 강점과 약점을 드러냅니다.
- ***Performance Highlights***: PEBench에서 평가된 결과, 인간 유닛 목표에서 대부분의 방법은 거의 100%의 효율성을 보였지만, 이벤트 유닛 목표에서는 방법에 따라 큰 차이를 보였습니다. 이러한 결과는 이벤트 삭제를 포함한 포괄적인 평가의 중요성을 강조합니다. 또, GD(Gradient Difference)와 KL 방법은 이벤트 유닛에서 더 나은 성과를 보였으며, 사람과 이벤트의 동시 유닛화에는 성능 저하가 나타났습니다. 그러나, BGD(Balance Gradient Difference)를 적용한 결과, 특히 사람 유닛화에서 성능이 크게 개선되었습니다.

### [Towards Self-Improving Systematic Cognition for Next-Generation Foundation MLLMs](https://arxiv.org/abs/2503.12303)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12303.png)

Vote: 4

Authors: Xiaoying Zhang, Yipeng Zhang, Chengyue Wu, Maosong Sun, Chi Chen, Zonghao Guo, Wei Ke, Da Peng, Helen Meng

- ***What's New***: 이 논문은 다음 세대의 기초 MLLMs(Foundation MLLMs)를 구축하기 위해 자기 학습 프레임워크인 SICOG(Self-Improving Cognition)를 도입합니다. 시각적 이해 능력을 단계적으로 향상시킬 수 있는 Chain-of-Description과 심도 깊은 추론을 가능케 하는 구조화된 Chain-of-Thought(CoT)을 통해 자기생성 데이터를 역할별로 활용하여 MLLMs의 체계적 인지 능력을 강화합니다.
- ***Technical Details***: SICOG는 최소한의 외부 주석을 통해 MLLM에 체계적 인지 능력을 부여합니다. 이는 단계적인 시각적 분석을 위한 Chain-of-Description과 통합된 멀티모달 추론을 위한 구조화된 CoT를 포함합니다. 그런 다음, 향상된 모델이 자체적으로 캡션과 논리적 추론 데이터를 생성하고, 자가 일관성을 통해 이를 선별하여 사용합니다. 궁극적으로 이 데이터를 멀티모달 사전 학습에 사용함으로써 다음 세대의 기초 MLLMs를 개발합니다.
- ***Performance Highlights***: SICOG는 두 가지 해상도 모델(고해상도 및 저해상도)에 걸쳐 다양한 벤치마크에서 멀티모달 인지를 유의미하게 향상시켜 기존 사전 학습 접근 방법들보다 더 높은 성능을 보여주었습니다. 예를 들어, MMStar에서 약 2-4%의 정확도 향상을 달성했으며, 이는 전반적으로 더 강력한 기초 MLLMs로 이끄는 결과를 가져왔습니다.

### [EvalTree: Profiling Language Model Weaknesses via Hierarchical Capability Trees](https://arxiv.org/abs/2503.08893)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.08893.png)

Vote: 2

Authors: Pang Wei Koh, Zhiyuan Zeng, Yizhong Wang, Hannaneh Hajishirzi

- ***What's New***: EvalTree는 다양한 능력을 평가하는 '계층적 능력 트리'를 통해 언어 모델(Language Model; LM)의 약점을 진단하는 새로운 프로파일링 방법을 제안합니다. 이 방법은 개별 벤치마크 인스턴스에서 모델 성능을 평가하여 자연어로 기술된 약점을 도출합니다.
- ***Technical Details***: EvalTree는 각 노드가 특정 능력을 나타내고 해당 능력을 평가하는 벤치마크 인스턴스를 연결하는 능력 트리를 자동으로 구성합니다. 그런 다음 성능이 저조한 노드를 추출하여 약점 프로파일을 생성합니다. 이러한 프로파일은 능력 기반의 데이터 수집을 통해 모델 성능 개선에 활용됩니다.
- ***Performance Highlights***: MATH 및 WildChat 벤치마크에서 EvalTree는 다른 약점 프로파일링 방법보다 약점을 더 정확하고 포괄적으로 식별했습니다. 이 방법을 통해 식별된 약점에 기반하여 데이터를 수집하면, 다른 수집 전략과 비교하여 LM의 성능을 더 효과적으로 향상시킬 수 있었습니다.

### [Learning to Inference Adaptively for Multimodal Large Language Models](https://arxiv.org/abs/2503.10905)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10905.png)

Vote: 2

Authors: Saurabh Bagchi, Yingyu Liang, Khoi Duc Nguyen, Zhuoyan Xu, Somali Chaterji, Preeti Mukherjee, Yin Li

- ***What's New***: 이 논문에서는 자원 제약적 환경에서의 멀티모달 대형 언어 모델(Multimodal Large Language Models; MLLMs)의 적응적 추론을 위한 AdaLLaVA라는 프레임워크를 소개합니다. AdaLLaVA는 입력 데이터와 지연 시간 예산(Latency Budget)을 반영하여 추론 시 모델의 작업을 동적으로 재구성할 수 있는 학습 기반 스케줄러를 활용합니다.
- ***Technical Details***: AdaLLaVA는 MLLM의 기본 모델의 추론 시에 학습된 스케줄러를 통해 모델의 일부 작업을 선택적으로 수행하도록 설계되었습니다. 이 스케줄러는 입력 콘텐츠와 지연 시간 제약 조건을 기반으로 하여 경제적이고 지능적인 실행 계획을 생성합니다. 또한, 확률 모델링과 전용 샘플링 전략을 사용하여 지연 시간 제약을 고려하면서 학습을 튼튼하게 만듭니다.
- ***Performance Highlights***: 다양한 벤치마크를 통한 실험 결과, AdaLLaVA는 MLLM의 성능 저하를 최소화하면서 지연 시간 제약을 준수합니다. 예를 들어, 다양한 벤치마크에서 AdaLLaVA는 지연 시간 예산의 80%와 65%를 사용할 때 각각 99.0%와 98.2%의 성능을 유지했습니다. 특히, 입력 지연 시간 및 콘텐츠에 적응하여 MLLM의 효율성을 높입니다.

### [KUDA: Keypoints to Unify Dynamics Learning and Visual Prompting for Open-Vocabulary Robotic Manipulation](https://arxiv.org/abs/2503.10546)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10546.png)

Vote: 2

Authors: Mingtong Zhang, Yunzhu Li, Zixian Liu

- ***What's New***: KUDA는 동적 학습(Dynamics Learning)과 시각적 프롬프팅(Visual Prompting)을 통합한 공개-어휘 로봇 조작 시스템입니다. 이 시스템은 비전 언어 모델(Vision Language Models; VLMs)과 학습 기반의 신경 동적 모델을 활용하여 키포인트(Keypoints)를 통해 대상을 명확하고 효율적으로 명시합니다.
- ***Technical Details***: KUDA는 RGB 이미지와 언어 명령어를 기반으로 키포인트를 환경에서 할당하고, VLM을 통해 목표 명세(target specification)를 생성합니다. 이러한 추상적 키포인트 기반의 표현을 비용 함수로 변환하여 학습된 동적 모델을 사용해 로봇의 궤적을 최적화합니다. 키포인트 기반의 목표 명세는 VLM을 통해 산출된 코드로 변환되며, 목표 명세는 이산적 목표 지점과의 공간적 관계로 표현됩니다. 두 단계의 닫힌 루프(closed-loop) 제어 메커니즘을 통해 모델 기반 계획의 효율성과 견고성을 높입니다.
- ***Performance Highlights***: KUDA 시스템은 다양한 물체 유형에 대해 실험을 통해 시스템의 효율성을 입증합니다. 총 6개의 작업에서 KUDA는 60번의 시도 중 80%의 성공률을 기록하며, 기존 방법인 MOKA와 VoxPoser를 크게 능가하는 결과를 보여주었습니다. 특히 많은 작업에서 VLM의 닫힌 루프 계획이 목표 명세의 불완전성과 실행 오류를 효과적으로 교정하는 데 기여했습니다.

### [RoCo-Sim: Enhancing Roadside Collaborative Perception through Foreground Simulation](https://arxiv.org/abs/2503.10410)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10410.png)

Vote: 2

Authors: Anning Hu, Zichen Chao, Genjia Liu, Yuwen Du, Yifan Lu, Siheng Chen, Junhao Ge, Lanjun Wang, Weitao Wu

- ***What's New***: RoCo-Sim은 최초의 시뮬레이션 프레임워크로, 도로변 협력 인식을 위해 설계되었습니다. 단일 이미지의 전체 장면 스타일 전환과 동적 전경 편집을 통해 다양한 멀티뷰 일관된 시뮬레이션 도로변 데이터를 생성할 수 있습니다. 이는 도로변에서 다양한 상황을 대비할 수 있도록 하여 차량 안전성 향상에 기여합니다.
- ***Technical Details***: RoCo-Sim은 네 가지 구성 요소로 구성되어 있습니다: (1) 카메라 외적 최적화(Camera Extrinsic Optimization)는 도로변 카메라의 정확한 3D-2D 투사를 보장합니다. (2) 고유한 멀티뷰 가려짐 인식 샘플러(Multi-View Occlusion-Aware Sampler; MOAS)는 다양한 디지털 자산을 3D 공간에 배치할 위치를 결정합니다. (3) DepthSAM은 단일 프레임 고정 뷰 이미지에서 전경-배경 관계를 모델링하여 멀티뷰 일관성을 보장합니다. (4) 확장 가능한 후처리 툴킷(Scalable Post-Processing Toolkit)은 스타일 전환을 통해 더 현실적이고 풍부한 장면을 생성합니다.
- ***Performance Highlights***: RoCo-Sim은 Rcooper-Intersection 및 TUMTraf-V2X에서 AP70 기준으로 각각 83.74% 및 83.12%의 성능을 보여주며, 이는 현재 SOTA(SOTA; State Of The Art) 방법들을 크게 초과합니다. 이는 도로변 협력 인식을 크게 개선하며, 시뮬레이션 데이터가 모델의 성능을 향상시킬 수 있음을 입증합니다.

### [Pensez: Less Data, Better Reasoning -- Rethinking French LLM](https://arxiv.org/abs/2503.13661)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.13661.png)

Vote: 1

Authors: Huy Hoang Ha

- ***What's New***: Pensez는 대규모 데이터셋 대신 소량의 고품질 이중 언어(영어-프랑스어) 데이터셋에 집중하여 대형 언어 모델(Large Language Model; LLM)의 추론 능력과 프랑스어 능력을 동시에 향상시키는 새로운 접근 방식을 제안합니다.
- ***Technical Details***: Pensez-2k 데이터셋은 질적 보증 및 언어 순도를 중시하며 2,000개의 예제(영어 1,000개, 프랑스어 1,000개)로 구성됩니다. 데이터 필터링은 Tokenization을 위해 FastText와 같은 도구를 활용하여 품질이 높은 샘플만을 선별합니다. 교육 데이터는 모델의 연산적 '사고 시간'을 늘리기 위해 체계적인 추론 체인을 포함합니다.
- ***Performance Highlights***: Pensez 7B는 영어 추론 벤치마크 AIME25에서 20포인트 향상, 프랑스어 MATH 레벨 5에서 12포인트 향상 등의 상당한 성능 향상을 보였으며, 대규모 데이터 사용이 필수적이라는 기존 가정을 도전합니다. 느린 데이터 사용으로 인한 효율성은 다중 언어 LLM의 성능을 효과적으로 향상시킬 수 있음을 입증했습니다.

### [CoLMDriver: LLM-based Negotiation Benefits Cooperative Autonomous Driving](https://arxiv.org/abs/2503.08683)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.08683.png)

Vote: 1

Authors: Genjia Liu, Changxing Liu, Zijun Wang, Jinchang Yang, Siheng Chen

- ***What's New***: CoLMDriver는 최초의 LLM 기반 차량 간 협업 자율주행 시스템으로, 자연어를 통한 협상과 실시간 주행 제어를 통해 기존 방법의 한계를 극복합니다. 이를 통해 정교한 협업 정책 수정과 실행 가능한 경로 생성이 가능합니다.
- ***Technical Details***: CoLMDriver는 두 가지 주요 구성 요소로 이루어져 있습니다. 첫째, 'Actor-Critic 기법'을 사용하는 LLM 기반 협상 모듈은 차량 간의 다중 라운드 협상을 통해 협업 정책을 지속적으로 개선합니다. 둘째, '의도 기반 웨이포인트 생성기'는 협상 결과를 실행 가능한 웨이포인트로 변환합니다. 또한 CARLA 기반의 InterDrive 벤치마크를 도입하여 10개의 도전적인 시나리오에서 V2V 협력을 평가합니다.
- ***Performance Highlights***: CoLMDriver는 다양한 V2V 주행 시나리오에서 기존 접근 방식보다 11% 더 높은 성공률을 달성하였습니다. 이는 CoLMDriver의 LLM 기반 협업 시스템이 다중 차량 주행 시나리오에서도 탁월한 성능을 보임을 입증하며, 코드도 GitHub에 공개될 예정입니다.

### [Hyperbolic Safety-Aware Vision-Language Models](https://arxiv.org/abs/2503.12127)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12127.png)

Vote: 1

Authors: Tobia Poppi, Lorenzo Baraldi, Tejaswi Kasarla, Rita Cucchiara, Pascal Mettes

- ***What's New***: 이 논문에서는 시각-언어 모델에서 안전하지 않은 콘텐츠를 제거하는 대신, 하이퍼볼릭 공간의 계층적 특성을 활용하여 안전 및 비안전 콘텐츠를 인식하는 새로운 접근 방식을 제안합니다. HySAC, Hyperbolic Safety-Aware CLIP는 안전과 비안전 이미지-텍스트 쌍 간의 계층 및 비대칭 관계를 모델링하여 모델이 안전하지 않은 콘텐츠에 대해 인식할 수 있게 합니다.
- ***Technical Details***: HySAC는 시각적 및 텍스트 데이터를 하이퍼볼릭 임베딩으로 표현하여, 안점 콘텐츠는 하이퍼볼릭 공간의 중심에 가깝게, 비안전 콘텐츠는 더 멀리 배치하는 구조를 취합니다. 안전과 비안전 콘텐츠 간의 관계는 각론적 손실 함수(entailment loss function)를 통해 모델링되며, 이에 따라 입력 쿼리가 안전한 영역으로 조정됩니다.
- ***Performance Highlights***: 여러 실험을 통해 HySAC는 안전 인식 및 NSFW 콘텐츠 처리에 있어 기존 모델보다 우수한 성능을 보였습니다. 특히, 모델은 안전 및 비안전 콘텐츠를 효과적으로 구분하고, 안전한 방향으로 쿼리를 동적으로 리디렉션 할 수 있습니다. 안정성과 해석이 가능한 프레임워크 제공을 통해, 시각-언어 모델에서의 콘텐츠 모더레이션을 더욱 유연하게 할 수 있음을 보여줍니다.

### [MeshFleet: Filtered and Annotated 3D Vehicle Dataset for Domain Specific Generative Modeling](https://arxiv.org/abs/2503.14002)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.14002.png)

Vote: 1

Authors: Phillip Mueller, Lars Mikelsons, Dominik Schmid, Damian Boborzi, Jonas Emrich, Sebastian Mueller

- ***What's New***: MeshFleet는 Objaverse-XL에서 추출된 3D 차량 데이터셋으로, 도메인 특화 생성 모델링을 위한 고품질 데이터셋입니다. 이 데이터셋은 자동차 설계를 위한 생성 모델을 미세 조정하기 위해 데이터 필터링을 자동화하는 파이프라인을 제안합니다.
- ***Technical Details***: MeshFleet 구축은 수동으로 레이블이 지정된 데이터 샘플을 사용하여 DINOv2와 SigLIP 임베딩을 기반으로 훈련 된 품질 분류기를 활용하여 고품질 객체를 자동으로 식별하고 필터링하는 과정을 포함합니다. 이 과정은 모델 불확실성 및 캡션 기반 분석을 통해 반복적으로 개선되었습니다. 데이터세트는 1,620개의 고품질 차량 모델을 포함하며, 각각의 차량에 대해 생성된 캡션 및 크기 추정치를 추가했습니다.
- ***Performance Highlights***: SV3D 멀티뷰 생성 모델을 MeshFleet 데이터셋을 사용하여 미세 조정한 결과, 텍스트와 미적 점수에만 의존한 다른 필터링 전략과 비교하여 생성된 객체의 품질과 일관성이 향상되었습니다. 특히, MeshFleet로 미세 조정된 모델은 높은 CLIP-Score를 기록하며, 이는 전체적인 시각적 품질을 개선합니다.

