## Daily Papers (2025-03-17)

### [ReCamMaster: Camera-Controlled Generative Rendering from A Single Video](https://arxiv.org/abs/2503.11647)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.11647.png)

Vote: 81

Authors: Xiang Bai, Jianhong Bai, Xintao Wang, Menghan Xia, Di Zhang, Zuozhu Liu, Jinwen Cao, Xiao Fu, Pengfei Wan, Haoji Hu, Lianrui Mu

- ***What's New***: ReCamMaster는 단일 영상으로부터 새로운 카메라 궤적을 따라 영상을 다시 생성할 수 있는 카메라 제어 기반의 생성적 비디오 렌더링 프레임워크입니다. 이 연구에서는 Unreal Engine 5로부터 다중 카메라 동기화 비디오 데이터세트를 새롭게 구축하여 다양한 장면과 카메라 움직임을 포함함으로써 모델이 야생 데이터에 일반화할 수 있도록 돕습니다.
- ***Technical Details***: ReCamMaster는 사전 학습된 텍스트-비디오 변환 모델의 생성 능력을 활용하여 목표 카메라 궤적과 소스 비디오의 동적 장면을 조건으로 한 영상을 생성합니다. 데이터 부족 문제를 해결하기 위해 Unreal Engine 5를 사용하여 136,000개의 현실적 비디오를 생성하고, 동적 장면 13,600개와 고품질 3D 환경 40개에서 다양한 카메라 궤적 122,000개를 포함하여 훈련 데이터로 활용합니다.
- ***Performance Highlights***: 실험 결과 ReCamMaster는 기존 최첨단 방법들보다 시각적 품질, 카메라 정확성, 뷰 동기화 측면에서 탁월한 성과를 보였습니다. 특히, 이 방법은 비디오 안정화, 초해상도, 테두리 확장과 같은 다양한 응용에서 유망한 가능성을 보여줍니다.

### [PLADIS: Pushing the Limits of Attention in Diffusion Models at Inference Time by Leveraging Sparsity](https://arxiv.org/abs/2503.07677)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07677.png)

Vote: 66

Authors: Kwanyoung Kim, Byeongsu Sim

- ***What's New***: PLADIS(PLADIS)는 스파시티(sparsity)를 활용해 확산 모델(Diffusion Models)에서의 어텐션(attention) 한계를 인퍼런스 시점에서 극복하는 새로운 방법을 제안합니다. 추가적인 훈련이나 신경망 평가(NFE) 없이 스파시티를 활용하여 기존의 훈련된 모델(U-Net/Transformer)을 향상시킵니다.
- ***Technical Details***: PLADIS는 클래스프리 가이던스(Classifier-Free Guidance) 등의 가이던스 샘플링 방법이나 가이던스 증류 모델에도 통합이 가능하며, 스파시티를 강조하기 위해 스파스(spare)와 덴스(dense) 어텐션의 차이에 가중치를 부여합니다. 이를 통해, 새로운 효과성과 더불어 텍스트-이미지 정렬에서의 성능을 크게 향상시킵니다.
- ***Performance Highlights***: PLADIS는 MS-COCO 데이터셋에서 텍스트-이미지 정렬과 사용자 선호도에서 현저한 성능 향상을 보여주며, 다양한 벤치마크 데이터셋에서 시각적 품질 또한 향상시킵니다. 실험 결과, 기존의 가이던스 샘플링 방법과 통합함으로써 시각적 품질 및 이미지-텍스트 정렬을 크게 향상시킵니다.

### [Adversarial Data Collection: Human-Collaborative Perturbations for Efficient and Robust Robotic Imitation Learning](https://arxiv.org/abs/2503.11646)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.11646.png)

Vote: 31

Authors: Shu Jiang, Guanghui Ren, Si Liu, Siyuan Feng, Siyuan Huang, Hongsheng Li, Yue Liao, Maoqing Yao

- ***What's New***: 이 논문에서는 로봇 모방 학습에서 데이터 효율성을 높이기 위한 Adversarial Data Collection (ADC)라는 새로운 방법을 제안합니다. 이는 인간의 협력적 파괴(Human-Collaborative Perturbations)를 통해 다양한 실패 회복 행동과 환경 변화를 최소한의 시연으로 압축하여 데이터를 수집합니다. 이를 통해 대량의 데이터셋 없이도 높은 성능을 달성할 수 있습니다.
- ***Technical Details***: ADC는 사람-루프(Human-in-the-Loop) 프레임워크로, 실시간으로 객체 상태, 환경 조건 및 언어 명령을 동적으로 변경하여 텔레-오퍼레이터가 이러한 변화에 적응하도록 합니다. 이러한 방식을 통해 다양한 시각적 및 언어적 교란이 한 번의 시연으로 압축되며, 이는 데이터의 정보 밀도를 높이고 다양성을 극대화하여 더욱 강력한 정책 학습을 가능하게 합니다.
- ***Performance Highlights***: 논문에서는 ADC로 수집된 20%의 데이터만으로도 전통적인 방법으로 수집된 전체 데이터셋 수준의 성능을 상회하는 결과를 얻었습니다. 특히 시각적 교란에 대한 강력한 내성을 보였으며, 새로운 환경과 물체 구성에 대해 우수한 일반화 성능을 보여주었습니다.

### [Technologies on Effectiveness and Efficiency: A Survey of State Spaces Models](https://arxiv.org/abs/2503.11224)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.11224.png)

Vote: 21

Authors: Xinwei Long, Xingtai Lv, Yi Wu, Ermo Hua, Bowen Zhou, Shang Qu, Ning Ding, Youbang Sun, Kaiyan Zhang, Xuekai Zhu, Yuchen Fan

- ***What's New***: 이 논문에서는 상태 공간 모델(State Space Models; SSMs)이 트랜스포머 기반 모델에 대한 효율적이고 효과적인 대안으로 주목받고 있으며, 특히 순차 데이터나 긴 문맥을 필요로 하는 작업에서 탁월한 성능을 보여준다고 설명합니다. 기존 모델 클래스와의 비교 및 다양한 응용 분야를 포함하는 SSM의 포괄적이고 체계적인 개요를 제공합니다.
- ***Technical Details***: SSM을 원형 SSM, S4로 대표되는 구조화된 SSM, Mamba로 대표되는 선택적 SSM으로 나누어 설명합니다. SSM의 수학적 공식화와 디스크리타이제이션을 통해 연속 시간 시스템을 컴퓨터에서 처리 가능한 형태로 전환하였으며, 다양한 수학적 기법들(Euler’s method 등)을 사용해 모델의 성능을 향상시켰습니다. 주요 기술로는 HiPPO와 LegS 구조가 있으며, 이는 긴 시퀀스 의존성을 효과적으로 처리할 수 있게 합니다.
- ***Performance Highlights***: SSM 기반 알고리즘은 특히 긴 텍스트 작업을 처리하는 데 있어서 계산 효율성이 크며, 다양한 데이터 형식(텍스트, 이미지, 오디오, 비디오 등)에서 트랜스포머와 유사한 성능을 보입니다. 이러한 모델들은 헬스케어, 대화형 어시스턴트, 영화 산업 등 여러 도메인에 널리 응용되고 있습니다. Mamba 모델은 성능과 효율을 더욱 개선하였으며, 다양한 실용적인 응용 시나리오에 맞춰 설계되었습니다.

### [API Agents vs. GUI Agents: Divergence and Convergence](https://arxiv.org/abs/2503.11069)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.11069.png)

Vote: 20

Authors: Dongmei Zhang, Yu Kang, Chaoyun Zhang, Liqun Li, Qingwei Lin, Si Qin, Shilin He

- ***What's New***: 이 논문은 API 기반 LLM 에이전트와 GUI 기반 LLM 에이전트를 체계적으로 비교한 첫 번째 연구로, 두 에이전트의 차이점과 잠재적인 융합 가능성을 분석합니다. 이를 통해 하이브리드 접근 방식의 장점을 활용하여 더 유연하고 적응력 있는 솔루션을 제안하고, 실무자와 연구자들이 이러한 패러다임을 선택, 결합 또는 전환하는 데 도움을 주고자 합니다.
- ***Technical Details***: API 기반 LLM 에이전트는 텍스트 명령을 API 엔드포인트와 연계하여 즉각적인 작업 실행이 가능하며, GUI 기반 LLM 에이전트는 시각적 요소와 인간과 유사한 인터페이스 상호작용을 통해 작업을 수행합니다. 두 모델은 구조적 복잡성, 개발 워크플로우, 사용자 상호작용 모델에서 상당한 차이를 보이며, 각각의 강점을 활용하는 하이브리드 접근 방식의 필요성을 제안합니다.
- ***Performance Highlights***: API 에이전트는 강력한 자동화 및 확장성을 제공하지만, 이는 정의된 엔드포인트에 의존하므로 유연성이 제한됩니다. 반면 GUI 에이전트는 광범위한 사용자 인터페이스와의 상호작용이 가능하여, 실시간으로 인터페이스를 통해 작업을 수행할 수 있습니다. 이러한 특성들은 다양한 시나리오에서 서로 다른 장점을 제공하며, 하이브리드 모델이 이러한 두 접근 방식의 장점을 최적화할 수 있도록 합니다.

### [SmolDocling: An ultra-compact vision-language model for end-to-end multi-modal document conversion](https://arxiv.org/abs/2503.11576)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.11576.png)

Vote: 13

Authors: Miquel Farré, Ahmed Nassar, Andres Marafioti, Yusik Kim, Rafael Teixeira de Lima, Peter W. J. Staar, Matteo Omenetti, Michele Dolfi, Nikolaos Livathinos, Maksym Lysak, A. Said Gurbuz, Christoph Auer, Lucas Morin

- ***What's New***: SmolDocling은 초소형 비전-언어 모델(Vision-Language Model; VLM)로, 문서의 콘텐츠, 구조 및 공간적 위치를 정확하게 포착하는 DocTags라는 새로운 마크업 형식을 생성하여 문서 변환을 수행합니다. 이는 기존의 대규모 모델이나 여러 전문화된 모델들의 조합보다 더 경제적인 리소스를 사용하여 비즈니스 문서, 학술 논문, 특허 등의 다양한 문서 유형에서의 성능을 입증합니다.
- ***Technical Details***: SmolDocling은 SmolVLM-256M 아키텍처를 기반으로 한 초소형 모델로, 각 페이지의 이미지를 비전 인코더를 통해 DocTags 시퀀스로 변환합니다. DocTags는 문서의 모든 요소를 유형, 위치, 콘텐츠로 나타내는 구조화된 형식으로, 독창적이고 일관된 문서 표현을 제공합니다. 이 모델은 다양한 문서 유형을 다룰 수 있도록 확장된 데이터셋을 포함하고 있으며, 가장 최근의 비전-언어 모델 기술을 채택하여 성능을 극대화합니다.
- ***Performance Highlights***: SmolDocling은 전체 페이지 텍스트 인식에서 평균 Edit Distance 0.48, F1-Score 0.80을 기록하며, Qwen2.5-VL과 같은 27배 더 큰 모델보다도 뛰어난 성능을 보였습니다. 코드 및 수식 인식 작업에서도 높은 정확도를 유지하며, 특히 코드 목록 인식에서는 마차와 유사한 선두적인 성과를 보여주고 있습니다.

### [Exploring the Vulnerabilities of Federated Learning: A Deep Dive into Gradient Inversion Attacks](https://arxiv.org/abs/2503.11514)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.11514.png)

Vote: 13

Authors: Feifei Wang, Shuang Zeng, Hui Xiong, Liangqiong Qu, Haoning Jiang, Pengxin Guo, Runxi Wang, Yanran Wang, Jinjing Zhu, Yuyin Zhou

- ***What's New***: 이 논문은 연합학습(Federated Learning; FL)의 취약점을 집중적으로 분석하며, 특히 Gradient Inversion Attacks(GIA)의 세부적인 분석과 평가를 통해 FL의 개인 정보 보호를 위협하는 요소를 심층적으로 체계적으로 고찰합니다.
- ***Technical Details***: 연구에서는 GIA를 최적화 기반(Optimization-based GIA; OP-GIA), 생성 기반(Generation-based GIA; GEN-GIA), 분석 기반(Analytics-based GIA; ANA-GIA) 세 가지 유형으로 분류하고 각각의 성능, 실용성 및 잠재적 위협 요인을 분석합니다. OP-GIA는 수신된 기울기와 더미 데이터의 기울기 간의 최소 거리를 기반으로, GEN-GIA는 입력 데이터를 재구성하기 위해 생성기를 활용하며, ANA-GIA는 선형 계층의 특징을 이용해 입력 데이터를 복구합니다.
- ***Performance Highlights***: OP-GIA는 실용성이 높지만 성능이 만족스럽지 못하고, GEN-GIA는 많은 의존성을 가지며, ANA-GIA는 공격 성능이 높지만 클라이언트 측에서 쉽게 탐지되고 방어할 수 있다는 결론을 도출합니다. FL에 대한 실질적인 위협을 제한하기 위해 배치 크기와 이미징 해상도를 증가시키고, 복잡한 네트워크 아키텍처를 선택하는 등의 판단을 권장합니다. 또한, PEFT 하에서 높은 해상도에서는 성공적인 공격이 어려움을 보여줍니다.

### [FlowTok: Flowing Seamlessly Across Text and Image Tokens](https://arxiv.org/abs/2503.10772)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10772.png)

Vote: 12

Authors: Qihao Liu, Liang-Chieh Chen, Ju He, Qihang Yu

- ***What's New***: FlowTok는 서로 다른 모달리티인 텍스트와 이미지를 통합된 1D 잠재 공간(unified 1D latent space)으로 프로젝트하여 직접적인 흐름 매칭(flow matching)을 구현하는 새로운 프레임워크입니다. 이를 통해 텍스트와 이미지 모달리티 간의 복잡한 조건 설정 및 노이즈 스케줄링 없이도 매끄러운 전환이 가능합니다.
- ***Technical Details***: FlowTok는 텍스트와 이미지를 1D 토큰화하여 통합된 낮은 차원의 잠재 공간으로 매핑합니다. 텍스트는 사전 훈련된 텍스트 인코더(pre-trained text encoder)를 사용하여 초기 1D 임베딩으로 추출되고, 이미지 토크나이저(image tokenizer)는 이미지 토큰을 압축된 1D 형태로 인코딩합니다. RoPE와 SwiGLU FFN을 통합하여 위치 정보 처리와 재구성 품질을 향상시켰습니다. FlowTok의 전체 설계는 복잡한 조건이나 추가 비용 없이 자원 효율적으로 훈련될 수 있습니다.
- ***Performance Highlights***: FlowTok는 훈련 및 추론 속도에서 현존하는 모델들보다 월등히 우수합니다. COCO 데이터셋에서 FlowTok-XL은 22.7 이미지/초, FlowTok-H는 18.2 이미지/초로, 기존 모델들의 속도를 크게 초과합니다. 훈련 효율성 면에서 FlyTok-XL은 20.4 8-A100 days, FlowTok-H는 26.1 8-A100 days로, 기존 모델들보다 최소 4배 더 적은 리소스를 필요로 합니다.

### [Large-scale Pre-training for Grounded Video Caption Generation](https://arxiv.org/abs/2503.10781)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10781.png)

Vote: 10

Authors: Josef Sivic, Cordelia Schmid, Evangelos Kazakos

- ***What's New***: 이 논문은 비디오 자막 생성 및 객체 그라운딩에서 'GROVE' 모델을 도입하며, 자동 주석 방법을 제안하여 대규모 사전 학습 데이터 세트인 HowToGround1M을 구성하고, 새로운 고품질 수동 주석 데이터 세트 iGround를 소개합니다.
- ***Technical Details***: GROVE 모델은 시공간적 어댑터(spatio-temporal adapters), 경계 상자 디코더(bounding box decoder), 일시적으로 프레임을 떠나거나 가려진 객체를 모델링하는 시간적 객체성 헤드(temporal objectness head)를 통합한 새로운 자동 주석 방법입니다. 이 방법을 HowTo100M 데이터 세트에 적용하여 1백만 개 이상의 비디오로 구성된 HowToGround1M을 생성하였고, 비디오 프레임 수준에서 개체를 지속적으로 라벨링하기 위해 LLM(대형 언어 모델)을 사용하였습니다.
- ***Performance Highlights***: GROVE 모델은 제안된 iGround 데이터 세트에서 이전 방법들보다 우수한 성능을 보였으며, VidSTG와 ActivityNet-Entities 데이터 세트에서도 우수한 결과를 달성하였습니다. 대규모 HowToGround1M 데이터 세트로 사전 학습한 후 소규모이지만 고품질인 iGround 데이터 세트에서 미세 조정을 거쳐 성능을 최적화하였습니다.

### [TxAgent: An AI Agent for Therapeutic Reasoning Across a Universe of Tools](https://arxiv.org/abs/2503.10970)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10970.png)

Vote: 10

Authors: Ayush Noori, Richard Zhu, Theodoros Tsiligkaridis, Shanghua Gao, Zhenglun Kong, Marinka Zitnik, Curtis Ginder, Xiaorui Su

- ***What's New***: TXAGENT는 개인 맞춤형 치료 권고를 생성하는 AI 에이전트로서, 211개의 다양한 도구들(TOOLUNIVERSE)을 활용하여 약물 상호작용, 금기사항 및 환자 특성에 기반한 치료 전략을 분석합니다. 이를 통해 분자, 약동학 및 임상 수준에서 약물 간의 상호작용을 평가하고, 환자 특성에 맞춘 치료 전략을 제공하는 방식을 제안합니다.
- ***Technical Details***: TXAGENT는 TOOLUNIVERSE라는 신뢰할 수 있는 출처로부터 211개의 도구가 통합된 시스템을 기반으로 합니다. 이 도구들에는 오픈FDA, Open Targets, 그리고 Human Phenotype Ontology 등이 포함되며, 각 도구들은 질병 및 약물 관련 정보를 제공하며, TXAGENT는 이를 통해 클리니컬 리즌닝(Clinical Reasoning) 작업을 수행합니다. TOOLRAG라는 모델을 사용하여 가장 관련성 높은 도구들을 선택하고, 분할된 함수 호출을 통해 치료 작업을 해결합니다.
- ***Performance Highlights***: TXAGENT는 DrugPC, BrandPC, GenericPC, TreatmentPC, 그리고 DescriptionPC 등 다섯 개의 벤치마크에서 기존 LLMs, 툴-유즈 모델 및 리즌닝 에이전트를 능가하는 성능을 보입니다. TXAGENT는 약리적 추론 작업에서 92.1%의 정확도를 달성하며, 이는 GPT-4o보다 최대 25.8% 높은 수치입니다. 또한, 약물 이름 변형과 설명을 처리하는 데 있어 기존의 툴-유즈 LLMs를 55% 이상 능가합니다.

### [Vamba: Understanding Hour-Long Videos with Hybrid Mamba-Transformers](https://arxiv.org/abs/2503.11579)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.11579.png)

Vote: 9

Authors: Cong Wei, Wenhu Chen, Wentao Ma, Ge Zhang, Weiming Ren, Huan Yang

- ***What's New***: VAMBA는 하이브리드 Mamba-Transformer 모델을 통해 시간 단위의 긴 비디오를 효율적으로 이해할 수 있도록 설계된 시스템입니다. 이 모델은 기존의 비디오 토큰을 압축하는 방법 대신, Mamba-2 블록을 사용하여 비디오 토큰을 선형 복잡도로 인코딩합니다.
- ***Technical Details***: VAMBA는 Mamba-2 블록을 사용하여 비디오 토큰을 선형 복잡도로 처리하며, 텍스트 토큰은 비디오 토큰을 기반으로 교차 주의(cross-attention)를 통해 업데이트됩니다. 이러한 접근 방식은 기존 Transformer 기반의 모델에 비해 O(d(M+N)^2)에서 O(dMN + d^2M)으로 복잡도를 줄입니다.
- ***Performance Highlights***: 실험 결과, VAMBA는 GPU 메모리 사용량을 50% 이상 감소시키고, LVBench에서 기존의 효율적인 비디오 LMMs에 비해 4.3% 정확도 향상을 보여줍니다. 또한, 다른 중단기 비디오 이해 벤치마크에서 우수한 성능을 기록하며, 1시간 이상 길이의 비디오를 효과적으로 처리하는 능력을 입증했습니다.

### [VGGT: Visual Geometry Grounded Transformer](https://arxiv.org/abs/2503.11651)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.11651.png)

Vote: 8

Authors: Christian Rupprecht, Nikita Karaev, Minghao Chen, Jianyuan Wang, David Novotny, Andrea Vedaldi

- ***What's New***: VGGT는 수십 개의 이미지에서 고유 3D 속성을 동시에 추론할 수 있는 대형 Feed-Forward Neural Network입니다. 이 네트워크는 카메라 파라미터, 포인트 맵, 깊이 맵 및 3D 포인트 트랙을 예측하며, 후처리 없이 최첨단 성능을 자랑합니다. 또한, VGGT는 다양한 3D 작업에서 기존의 최적화 기반 대안보다 우수한 결과를 보여왔습니다.
- ***Technical Details***: VGGT는 기본적으로 Transformer 아키텍처를 기반으로 하여, 최소한의 3D 귀납적 바이어스를 가집니다. 이 모델은 입출력 프레임 간의 관계 처리를 가능하게 하기 위해 프레임 간 및 전역 Self-Attention Layer를 번갈아 사용합니다. 그리고 프레임당 독립적인 밀도 예측을 위해 DPT(Depth Prediction Transformer) 헤드를 사용합니다. 또한, 코어 구조는 C-dimensional Feature를 통해 포인트 트랙킹을 지원합니다.
- ***Performance Highlights***: VGGT의 feed-forward 방식은 두 데이터셋 RealEstate10K와 CO3Dv2에서 카메라 포즈 추정에서 타의 추종을 불허하며, 동시에 최첨단 성능을 0.2초 내에 달성합니다. VGGT와 번들 조정(Bundle Adjustment) 기법을 결합하면 더욱 향상된 성능을 보이며, 이는 실시간 응용에 적합한 효율성과 단순성을 제공합니다.

### [Kolmogorov-Arnold Attention: Is Learnable Attention Better For Vision Transformers?](https://arxiv.org/abs/2503.10632)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10632.png)

Vote: 8

Authors: Subhajit Maity, Killian Hitsman, Xin Li, Aritra Dutta

- ***What's New***: Kolmogorov-Arnold Attention (KArAt)는 비전 트랜스포머(Vision Transformers; ViTs)에서 소프트맥스(Self-attention) 대신 학습 가능한 주의력(Learnable Attention)을 도입한 첫 번째 연구입니다. 이 연구는 이미지 분류 작업에서 사용될 수 있는 일반 변형 가능한 Kolmogorov-Arnold 다중헤드 셀프어텐션 모듈을 설계하였습니다.
- ***Technical Details***: KArAt는 ViTs의 인코더 블록 내부에서 사용할 수 있는 학습 가능한 다중헤드 셀프어텐션 모듈입니다. 이 모듈은 B-스플라인, 푸리에, 프랙탈 등의 다양한 기초를 선택하여 작동할 수 있습니다. CIFAR-10, CIFAR-100, ImageNet-1K 데이터셋에서 모듈의 성능을 벤치마킹하고, 손실 지형, 가중치 분포, 최적화 경로, 주의력 시각화 및 분광 행동을 분석하여 ViTs와 비교하였습니다.
- ***Performance Highlights***: VIiT-Tiny 모델의 Fourier-KArAt 변형이 CIFAR-10과 CIFAR-100에서 각각 5.40%와 7.40%의 향상된 성능을 보여주었고, ImageNet-1K에서는 유사한 정확도를 기록했습니다. ViT-Base의 경우, Fourier-KArAt 변형은 일반 ViTs 모델과 거의 유사한 성능을 보였습니다.

### [ARMOR v0.1: Empowering Autoregressive Multimodal Understanding Model with Interleaved Multimodal Generation via Asymmetric Synergy](https://arxiv.org/abs/2503.06542)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.06542.png)

Vote: 6

Authors: Shenglin Zhang, Jianwen Sun, Fanrui Zhang, Yukang Feng, Chuanhao Li, Jiaxin Ai, Sizhuo Zhou, Kaipeng Zhang, Zizhen Li, Yu Dai

- ***What's New***: ARMOR v0.1은 기존 다국적 대언어 모델들(Multimodal Large Language Models, MLLMs)을 유니파이드 모델들(Unified Models, UniMs)로 업그레이드 하여 상호 변조된 멀티모달 생성(Interleaved Multimodal Generation)을 가능하게 합니다. 이는 비대칭 시너지를 통한 자원 효율적인 오토레그레시브 프레임워크로, 추론 및 생성 능력을 한층 강화했습니다.
- ***Technical Details***: ARMOR는 비대칭 인코더-디코더 아키텍처를 도입하여, 텍스트와 이미지 모달리티를 통합하는 임베딩 공간을 통일시켰습니다. 또한, 세심하게 큐레이션된 고품질의 데이터셋을 수집하여 MLLMs의 세 단계를 거친 'What or How to Generate'(WoHG) 알고리즘을 제안합니다. 각 단계는 응답 모달리티 결정, 이미지 생성 능력 향상, 그리고 텍스트-이미지 조합 응답 통합을 목표로 합니다. 이러한 접근법을 통해, 최소한의 추가 파라미터로 멀티모달 생성과 이해를 조화롭게 할 수 있습니다.
- ***Performance Highlights***: ARMOR는 9개의 벤치마크 실험을 통해 기존의 UniMs 대비 멀티모달 이해에서 탁월한 성능을 보이며 상당한 이해 능력을 유지한 채 멀티모달 생성 능력을 강화했습니다. ARMOR는 약 7%의 파라미터만 추가하여 InternVL2.5를 미세 조정하였으며, 이는 기존 모델의 대대적인 파라미터 학습을 필요로 하는 것과 대조됩니다. 이미지 생성 벤치마크인 GenEval에서도 유망한 성능을 보여, 적은 훈련 리소스로 효과적인 결과를 달성했습니다.

### [Neighboring Autoregressive Modeling for Efficient Visual Generation](https://arxiv.org/abs/2503.10696)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10696.png)

Vote: 5

Authors: Yefei He, Bohan Zhuang, Yuanyu He, Feng Chen, Shaoxuan He, Kaipeng Zhang, Hong Zhou

- ***What's New***: Neighboring Autoregressive Modeling (NAR)는 시각적 내용의 공간 및 시간적 인접성을 활용하여 새로운 자율회귀 방식의 시각 생성 패러다임을 도입합니다. 이는 시각적 생성 작업을 초기 토큰에서 시작하는 진행적인 상상(outpainting) 절차로 구성하여, 생성 효율성을 향상시키고 더 높은 품질의 이미지를 생성하도록 합니다.
- ***Technical Details***: NAR 방식은 'next-neighbor prediction' 메커니즘을 활용하여 기존의 'next-token prediction' 패러다임과 달리, 초기 토큰으로부터 더 가까운 공간-시간적 거리에 따라 토큰을 점진적으로 생성합니다. 이를 위해 각기 다른 직교 차원에서 다음 토큰을 예측하는 dimension-oriented decoding heads를 도입하여 인접한 다수의 토큰을 병렬로 예측합니다.
- ***Performance Highlights***: ImageNet 256×256과 UCF101 데이터셋 실험 결과, NAR 모델은 각각 2.4배 및 8.6배의 더 높은 처리량을 보여주었으며, FID/FVD 점수가 기존의 PAR-4X 접근 방식보다 우수했습니다. 또한, NAR 0.8B 파라미터 모델은 Chameleon-7B를 넘어서는 성능을 보여주었으며, 학습 데이터의 0.4%만을 사용하면서도 text-to-image 생성 성능에서 앞섰습니다.

### [Cockatiel: Ensembling Synthetic and Human Preferenced Training for Detailed Video Caption](https://arxiv.org/abs/2503.09279)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.09279.png)

Vote: 5

Authors: Mengping Yang, Hao Li, Zhiyu Tan, Luozheng Qin, Xiaomeng Yang

- ***What's New***: Cockatiel은 비디오 상세 캡션(VDC)을 개선하기 위해 합성 훈련과 인간 선호도의 훈련을 결합한 세 단계 학습 파이프라인을 제안합니다. Cockatiel-13B는 다양한 VDC 모델의 강점과 인간의 선호도를 통합하여 새로운 상태의 변화를 이루며, 세부적인 비디오 캡션에서 뛰어난 성능을 보입니다.
- ***Technical Details***: Cockatiel은 세 단계의 학습 파이프라인으로 구성됩니다. 첫 번째 단계에서는 인간 정렬 데이터 큐레이션을 통해 고품질의 악세서리 캡션을 선택합니다. 두 번째 단계에서는 합성 훈련을 통해 VDC 모델의 강점과 인간의 선호도를 통합하여 Cockatiel-13B를 계발합니다. 마지막 단계에서는 Cockatiel-8B를 증류하여 보다 작은 모델 크기로 쉽게 사용할 수 있게 합니다.
- ***Performance Highlights***: Cockatiel-13B는 VDCSCORE에서 새로운 최고 성과를 달성하며, 기존의 VDC 모델보다 인간 선호도에서 크게 우세한 결과를 보였습니다. Cockatiel-8B도 Cockatiel-13B로부터 증류되어 현 상태의 최첨단 모델에 비추어 대등하거나 그 이상의 성능을 보입니다.

### [ProJudge: A Multi-Modal Multi-Discipline Benchmark and Instruction-Tuning Dataset for MLLM-based Process Judges](https://arxiv.org/abs/2503.06553)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.06553.png)

Vote: 5

Authors: Baojin Huang, Zhaopan Xu, Jianwen Sun, Fanrui Zhang, Pengfei Zhou, Ming Li, Zhongyuan Wang, Yukang Feng, Jiaxin Ai, Kaipeng Zhang, Zizhen Li

- ***What's New***: ProJudge는 MLLM(Multi-Modal Large Language Models) 기반의 프로세스 평가를 위한 최초의 포괄적 벤치마크인 ProJudgeBench와 이를 보완하기 위한 대규모 인스트럭션 튜닝(instruction-tuning) 데이터셋인 ProJudge-173k를 소개합니다. ProJudgeBench는 다양한 과학 분야와 난이도로 구성된 2,400개의 테스트 사례를 포함하여 모델의 문제 해결 과정을 체계적으로 평가합니다.
- ***Technical Details***: ProJudgeBench는 수학, 물리학, 화학, 생물학을 아우르는 다중 난이도 및 다중 모달 콘텐츠로 구성된 데이터셋으로, 50,118개의 단계별 레이블이 포함되어 있습니다. 각 단계는 인간 전문가에 의해 오류의 정확성, 오류 유형, 설명 등이 주석 처리되어 있으며, 7가지 오류 유형을 정의하여 모델의 오류 분석 능력을 평가합니다. ProJudge-173k는 두 가지 경로를 통해 구성된 대형 인스트럭션 튜닝 데이터셋으로, 정확성을 보장하기 위해 엄격한 필터링 과정을 거친 합성 주석이 특징입니다.
- ***Performance Highlights***: ProJudge-173k로 미세 조정된 오픈 소스 모델들은 ProJudgeBench 평가에서 상당한 성능 향상을 보였으며, 특히 단계의 정확성을 평가하는 데 있어서 프로프라이트리 모델과의 격차를 줄였습니다. 이 새로운 초점은 과학적 문제 해결에서 MLLM의 프로세스 평가 능력을 상당히 향상시킵니다.

### [Learning Few-Step Diffusion Models by Trajectory Distribution Matching](https://arxiv.org/abs/2503.06674)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.06674.png)

Vote: 4

Authors: Jing Tang, Tianyang Hu, Jiacheng Sun, Yihong Luo, Yujun Cai

- ***What's New***: 본 논문에서는 'Trajectory Distribution Matching' (TDM)이라는 새로운 프레임워크를 제안하여 소수의 단계만으로도 고품질 이미지를 생성할 수 있는 학습 방법을 도입했습니다. 이는 기존의 분포 매칭(distribution matching)과 궤적 매칭(trajectory matching)의 강점을 결합한 새로운 디스틸 방법으로, 데이터가 필요 없는 방식으로 모델 훈련을 가속화하고 효율적으로 활용할 수 있도록 합니다.
- ***Technical Details***: TDM은 데이터가 필요 없는 점수 디스틸 목표(data-free score distillation objective)를 도입하여 학생 모델의 궤적(trajectory)을 교사 모델의 분포 수준에서 정렬합니다. 또한, 각 샘플링 단계의 학습 목표를 분리하여 보다 조정 가능한 샘플링을 가능하게 하는 '샘플링 단계 인식 목표(sampling-steps-aware objective)'를 개발했습니다. 이는 SDXL 및 PixArt-α와 같은 다양한 백본에서 뛰어난 성능을 제공하며, 특히 학생 모델을 500회 반복 및 2 A800 시간 내에 교사 모델보다 우수한 성능을 구현하는 데 성공했습니다.
- ***Performance Highlights***: 제안된 TDM 모델은 SD-v1.5 및 SDXL과 같은 여러 모델에서 탁월한 성능을 보여주었으며, Human Preference Score (HPS), CLIP Score 등 다양한 평가 지표에서 기존 방법보다 우수한 결과를 나타냈습니다. SDXL의 경우, DMD2의 160 A100 일에 비해 2 A800일만으로 훈련이 가능하여 매우 효율적인 학습 능력을 보여주었습니다. 또한, CogVideoX-2B를 4-단계 생성기로 디스틸하여 VBench의 총 점수를 80.91에서 81.65로 개선했습니다.

### [ETCH: Generalizing Body Fitting to Clothed Humans via Equivariant Tightness](https://arxiv.org/abs/2503.10624)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10624.png)

Vote: 3

Authors: Michael J. Black, Zeyu Cai, Yuliang Xiu, Boqian Li, Haiwen Feng

- ***What's New***: ETCH는 의류 장착 인간의 본체 적합을 일반화하기 위한 새로운 방법론으로, 의류-신체 SE(3) 등가적 긴밀도를 모델링하여 옷 표면에서 인간 신체 표면으로의 맵핑을 예측합니다. 이는 기존 방법들이 갖는 포즈 다양성의 일반화 문제를 극복하여 의복의 변형과 상관없이 일관되게 신체에 맞는 결과를 도출합니다.
- ***Technical Details***: ETCH는 3D 인간 점군에서 신체 표지자(marker) 좌표를 회귀시키기 위해 SE(3) 등가적 및 불변적 포인트 특성을 활용합니다. 이 방법론은 의복 표면에서 본체 표면으로 향하는 'tightness vector'를 포함하며, 이 벡터는 의복 변형에도 불구하고 지리적 정보에 기반하여 신체 표면으로 일관되게 맵핑됩니다. 또한 희소 표지를 활용하여 포즈 불변 신체 특성을 회귀하여 최종 신체 모델을 최적화합니다.
- ***Performance Highlights***: ETCH는 느슨한 의복의 신체 적합도에서 16.7% ~ 69.5%, 형태 정확도에서 평균 49.9% 향상된 성능을 보여줍니다. 특히 방향 오차를 67.2% ~ 89.8% 줄여서 다양한 실험에서입증된 바와 같이, 도전적인 포즈, 증가된 변형력에 강건한 일반화 능력을 발휘합니다.

### [Open-World Skill Discovery from Unsegmented Demonstrations](https://arxiv.org/abs/2503.10684)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10684.png)

Vote: 2

Authors: Yitao Liang, Anji Liu, Jingwen Deng, Shaofei Cai, Zihao Wang

- ***What's New***: 이 연구는 비세분화된 시연(Unsegmented Demonstrations)에서 기술(skill)을 자동으로 발견하는 'Skill Boundary Detection'(SBD)라는 새로운 알고리즘을 소개했습니다. 이는 기존의 수동 레이블 없이도 자기 감독(self-supervised learning) 기반으로 비디오를 의미있는 기술 단계로 분할할 수 있는 새로운 접근 방식입니다.
- ***Technical Details***: SBD는 사전훈련된 무조건적인 행동 예측 모델(action-prediction model)을 활용하여 예측 오류가 증가할 때를 기술 경계로 인식, 이 시점을 중심으로 비디오 시퀀스를 분할합니다. 해당 알고리즘은 주어진 시점에서 예측 손실이 일정 임계치를 초과할 때 기술이 변환되는 것으로 간주하여 경계선을 식별합니다.
- ***Performance Highlights***: 마인크래프트에서 SBD는 단기 기술 과제에서 63.7%와 52.1%로 정책 성능을 크게 개선했으며, 장기간 과제에서는 11.3%와 20.8%의 성능 향상을 보여주었습니다. 이는 SBD가 비세분화 시연에서 기술을 효과적으로 학습할 수 있게 한다는 것을 의미합니다.

### [From TOWER to SPIRE: Adding the Speech Modality to a Text-Only LLM](https://arxiv.org/abs/2503.10620)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10620.png)

Vote: 2

Authors: Sonal Sannigrahi, Anil Keshwani, Tsz Kin Lam, Ben Peters, André F. T. Martins, Bruno Martins, Kshitij Ambilduke, Marcely Zanon Boito

- ***What's New***: 이 논문은 SPIRE라는 음성 지원 대형 언어 모델을 소개하여 기존의 다국어 모델 TOWER를 기반으로 확장했습니다. SPIRE는 음성 입력을 처리하여 자동 음성 인식(ASR) 및 음성 번역(ST)을 수행하면서도, TOWER의 기계 번역(MT) 관련 기존 성능을 유지합니다.
- ***Technical Details***: SPIRE는 기존의 텍스트 전용 LLM을 음성 모달리티로 확장하기 위해 음성 분산화(speech discretization)와 지속적인 사전 학습(continued pre-training; CPT)을 사용합니다. 이 과정에서는 HuBERT 기반의 k-평균 클러스터링을 통해 음성을 대표하는 '분산 음성 단위'(DSUs)를 생성하여 텍스트와 동일하게 처리합니다. обучен на 42.5K часовах аудиоматериалов. 훈련 과정은 두 단계로 나뉘며, 첫 번째는 ASR 데이터와 기존 텍스트 데이터 분수의 CPT, 두 번째는 특정 과제에 맞춘 영어 ASR 및 ST 데이터를 이용한 명령어 튜닝(instruction tuning; IT)을 포함합니다.
- ***Performance Highlights***: SPIRE는 ASR 및 ST에서 강력한 성능을 보여 Top 모델인 Whisper-large-v3와 SeamlessM4T와 견줄 만합니다. 음성 외 단위에만 의존함에도 불구하고, 여러 ASR 데이터셋에서 경쟁력 있는 성과를 거두었으며, TOWER 기반 성능을 유지하면서도 기계 번역 능력을 인정받았습니다. 특히, SPIREFULL 모델은 MT 관련 태스크에서 SeamlessM4T를 상회하는 성과를 보였습니다.

### [GoalFlow: Goal-Driven Flow Matching for Multimodal Trajectories Generation in End-to-End Autonomous Driving](https://arxiv.org/abs/2503.05689)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05689.png)

Vote: 2

Authors: Tong He, Zebin Xing, Xiaoxiao Long, Yang Hu, Xingyu Zhang, Wei Yin, Bo Jiang, Qian Zhang

- ***What's New***: GoalFlow는 자율주행에서 고품질의 다중모드 궤적을 생성하는 종단간(end-to-end) 메서드로, 고정밀 궤적을 생성하기 위한 새로운 스코어링 메커니즘을 도입했습니다. 이 메서드는 후보 목표점들 중 최적의 목표점을 선택하여 생성 과정의 궤적 일탈 문제를 해결함으로써 고품질 궤적을 만듭니다.
- ***Technical Details***: GoalFlow는 세 개의 주요 모듈로 구성됩니다. [1] 탐지 모듈(Perception Module): 이미지와 LiDAR를 활용한 BEV 특징을 생성합니다. [2] 목표점 구축 모듈(Goal Point Construction Module): 밀도 높은 목표점 어휘를 구성하고 최적의 목표점을 선택합니다. [3] 궤적 계획 모듈(Trajectory Planning Module): Scene 정보를 조건으로 하며, 선택한 목표점의 강한 지도를 포함하여 Flow Matching을 사용해 다중모드 궤적을 모델링합니다.
- ***Performance Highlights***: GoalFlow는 Navsim에서 PDMS 90.3을 달성하여 다른 방법들을 크게 초과하는 성능을 나타냈습니다. 기존 방법들과 비교할 때, 이 접근 방식은 단일 역확진(denoising) 단계만을 필요로 하여 뛰어난 성능을 얻을 수 있으며, 이는 현재 자율주행 시스템에서의 실질적인 활용 가능성을 크게 높입니다.

### [Can Large Reasoning Models do Analogical Reasoning under Perceptual Uncertainty?](https://arxiv.org/abs/2503.11207)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.11207.png)

Vote: 2

Authors: Abu Sebastian, Roger Wattenhofer, Michael Hersche, Giacomo Camposampiero, Abbas Rahimi

- ***What's New***: 이 연구는 최첨단 대형 추론 모델(Large Reasoning Models; LRMs)인 OpenAI의 o3-mini와 DeepSeek R1을 비언어 추론 테스트에서 평가하여, 시각적 불확실성을 도입함으로써 상징적 시각 추론의 성능을 종합적으로 분석합니다. I-RAVEN-X 데이터셋을 최신화하여 추론 과정에서 불확실성이 있는 시나리오를 실제와 유사하게 구현하려는 첫 시도입니다.
- ***Technical Details***: I-RAVEN-X는 기존 I-RAVEN 데이터셋의 확장 버전으로, 시각적 불확실성이 추가된 3x10 매트릭스의 비언어적 추론 문제를 제공합니다. 이 데이터셋은 추가적인 혼동 속성(Confounding Attributes)을 도입하고 속성 값의 분포를 부드럽게 하는 방식으로 불확실성을 시뮬레이션합니다. 모델 평가 시에는 분포가 부드럽게 처리된 입력 속성 값을 제공하여 추론의 안정성을 테스트합니다.
- ***Performance Highlights***: OpenAI o3-mini의 정확도는 I-RAVEN에서 86.6%였으나, 시각적 불확실성을 도입한 I-RAVEN-X에서는 17.0%로 급격히 낮아졌습니다. DeepSeek R1의 경우, 정확도가 80.6%에서 23.2%로 떨어졌습니다. 반면, ARLC 모델은 이러한 불확실성 하에서도 98.6%에서 88.0%로 정확도의 감소가 매우 적어, 시각적 불확실성에 대한 견고한 추론 능력을 보였습니다.

### [MaRI: Material Retrieval Integration across Domains](https://arxiv.org/abs/2503.08111)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.08111.png)

Vote: 1

Authors: Yangfan He, Jingwei Huang, Zhifei Yang, Yuxuan Chen, Jianhui Wang, Huixiong Zhang

- ***What's New***: MaRI는 새로운 프레임워크로, 시각적 표현과 물질적 속성을 조화시킨 하이브리드 공간을 구축하여 합성 및 실제 세계의 재료들 간의 격차를 줄이는 데 초점을 맞추었습니다. MaRI는 이미지와 물질 인코더를 공동으로 훈련하여 유사한 재료와 이미지를 가까이하고, 이질적인 쌍은 분리합니다.
- ***Technical Details***: MaRI는 이중 인코더(dual encoder) 아키텍처를 기반으로 하여 DINOv2 모델을 백본으로 활용하며, 마지막 Transformer 블록만 미세 조정하여 도메인 특화된 변이를 효과적으로 포착합니다. 새로운 데이터셋으로, 합성 데이터를 블렌더(Blender)로 생성하고 ZeST 기법을 사용하여 실제 데이터를 처리하여 구축되었습니다. 대조적 학습 전략을 사용하여 유사한 재료 쌍을 학습합니다.
- ***Performance Highlights***: MaRI는 새로운 재료 검색에서 탁월한 성능을 보였습니다. 'Trained' 데이터셋에서 인스턴스 레벨에서 90%의 정밀도를 기록하며 기존 방법을 능가하였고, 'Unseen' 데이터셋에서도 54%의 높은 정밀도를 유지하며 GPT-4V 기반 검색 방법보다 뛰어난 성능을 보였습니다.

### [TreeMeshGPT: Artistic Mesh Generation with Autoregressive Tree Sequencing](https://arxiv.org/abs/2503.11629)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.11629.png)

Vote: 1

Authors: Stefan Lionar, Gim Hee Lee, Jiabin Liang

- ***What's New***: TreeMeshGPT는 Autoregressive Tree Sequencing을 도입하여 예술적 메시(Artistic Mesh) 생성 시 입력 점구름(Point Clouds)과 일치하는 고품질 메시를 자동으로 생성하는 새로운 방법을 제안합니다. 이는 Autoregressive Transformer의 전통적인 순차적 토큰 예측을 대체하는 접근법으로, 삼각형의 인접성을 기반으로 동적으로 성장하는 트리 구조에서 다음 입력 토큰을 검색하는 방식을 채택했습니다.
- ***Technical Details***: TreeMeshGPT는 각 삼각형 얼굴을 두 개의 토큰으로 표현하여 기존의 단순 토큰화보다 약 22%의 압축률을 달성합니다. 7-bit 디스크리타이제이션(discretization)를 통해 최대 5,500개의 삼각형을 포함한 메시를 강력한 점구름 조건 하에 생성할 수 있습니다. 또한, 트리 구조 기반의 탐색 방식은 메시가 로컬에서 확장될 수 있도록 하여 훈련의 어려움을 줄이고 메시 품질을 향상시킵니다.
- ***Performance Highlights***: TreeMeshGPT는 이전의 메서드보다 높은 메시 품질을 생성할 수 있음을 광범위한 실험을 통해 보여줍니다. 정상적인 방향 제약을 강력하게 사용하여 이전 방법에서 흔히 나타나는 뒤집힌 노말의 발생을 최소화합니다.

### [Group-robust Machine Unlearning](https://arxiv.org/abs/2503.09330)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.09330.png)

Vote: 0

Authors: Massimiliano Mancini, Subhankar Roy, Thomas De Min, Stéphane Lathuilière, Elisa Ricci

- ***What's New***: 이 논문은 그룹 강건성(Group-Robustness)을 고려한 기계 학습 모델의 비학습(Machine Unlearning) 문제를 다룹니다. 이 논문에서는 비학습 데이터 집합이 균일하게 분포되어 있지 않을 때 발생하는 성능 저하 문제를 완화하기 위해 새롭고 효과적인 샘플 분포 재가중치 기술(Reweighting)을 제안했습니다.
- ***Technical Details***: 논문은 그룹 강건성을 유지하면서 비학습을 수행하는 최초의 방법인 MIU(MUTUAL INFORMATION-AWARE MACHINE UNLEARNING)를 소개합니다. MIU는 모델의 특성과 그룹 정보 사이의 상호 정보를 최소화하여, 비학습을 효과적으로 수행하고 비학습 집합의 지배적 그룹에서의 성능 저하를 줄입니다.
- ***Performance Highlights***: MIU는 CelebA, Waterbirds, FairFace 세 가지 데이터셋에서 기존의 비학습 기법보다 뛰어난 성과를 보였습니다. 이러한 실험 결과는 MIU가 기존 방법들보다 비학습 효율과 그룹 강건성 측면에서 우수함을 보여줍니다.

