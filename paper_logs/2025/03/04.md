## Daily Papers (2025-03-04)

### [Predictive Data Selection: The Data That Predicts Is the Data That Teaches](https://arxiv.org/abs/2503.00808)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.00808.png)

Vote: 45

Authors: Yuzhen Huang, Kashun Shum, Xiaoxin Chen, Yixuan Liao, Qian Liu, Ding Qi, Junxian He, Hongjian Zou

- ***What's New***: PRESELECT는 언어 모델(LLM)의 사전학습에 있어 데이터의 예측 능력을 활용하여 고품질과 저품질 데이터를 판별하는 새로운 데이터 선택 방법입니다. 이 방법은 데이터의 정규화된 손실을 통해 모델의 능력을 얼마나 잘 나타낼 수 있는지를 측정하여 보다 효율적이고 확장 가능한 데이터 선택을 지원합니다.
- ***Technical Details***: PRESELECT는 데이터의 예측 강도를 정의하여 데이터 선택의 기준으로 삼습니다. 각 문서의 예측 강도를 계산하기 위해 소규모의 샘플 데이터셋을 선정하고 fastText 기반 스코어러를 훈련하여 대규모 코퍼스에서 데이터를 선택합니다. 이는 기존의 도메인 수준이 아닌 문서 수준에서 세밀하게 작업하여 데이터 품질을 개선합니다.
- ***Performance Highlights***: 1B 모델을 30B 토큰으로 PRESELECT를 통해 훈련했을 때 난수 선택보다 평균 5.3%의 성능 개선을 이루었으며, DCLM 등 기존의 다른 데이터 선택 방법보다 2% 이상 뛰어났습니다. 또한, PRESELECT는 3B 모델에서도 최고 성능을 기록하며 데이터 선택과 효율성을 크게 향상시켰습니다.

### [Visual-RFT: Visual Reinforcement Fine-Tuning](https://arxiv.org/abs/2503.01785)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01785.png)

Vote: 43

Authors: Zeyi Sun, Ziyu Liu, Yuhang Zang, Jiaqi Wang, Yuhang Cao, Haodong Duan, Dahua Lin, Xiaoyi Dong

- ***What's New***: Visual-RFT는 LVLMs(Large Vision-Language Models)에 강화 학습 기반의 파인 튜닝 기술을 적용하여 시각 인식 및 추론 능력을 향상시키는 새로운 방법론을 제안합니다. 이를 통해 한정된 데이터로도 효과적인 파인 튜닝을 가능하게 합니다.
- ***Technical Details***: Visual-RFT는 입력마다 LVLMs가 여러 개의 응답을 생성하고, 각 응답에 대해 정의된 규칙 기반의 검증 가능한 보상 함수(verifiable reward functions)를 사용하여 정책 최적화 알고리즘(policy optimization algorithm)을 통해 모델을 업데이트합니다. 예를 들어, 객체 인식 과제에서는 IoU(Intersection over Union) 보상을 사용합니다.
- ***Performance Highlights***: Visual-RFT는 몇 가지 시각적 인식 작업에서 눈에 띄는 성능 향상을 보여줍니다. 예를 들어, one-shot 미세 분류에서는 24.3%의 정확도 향상을 보여 기존의 SFT(Supervised Fine-Tuning)를 크게 능가하며, COCO 및 LVIS의 Few-shot 객체 인식 작업에서도 상당한 성능 개선을 나타냅니다.

### [Phi-4-Mini Technical Report: Compact yet Powerful Multimodal Language Models via Mixture-of-LoRAs](https://arxiv.org/abs/2503.01743)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01743.png)

Vote: 39

Authors: Zeqi Lin, Yi-ling Chen, Weijian Xu, Donghan Yu, Zhenghao Wang, Li Lyna Zhang, Hany Awadalla, Atabak Ashfaq, Weizhu Chen, Xihui Lin, Amit Garg, Yifan Yang, Yen-Chun Chen, Jianmin Bao, Piyush Madan, Yang Liu, Bo Ren, Mei Gao, Qi Dai, Ishmam Zabir, Min Gao, Shuohang Wang, Ruchao Fan, Mengchen Liu, Dongdong Chen, Xiyang Dai, Chong Luo, Sambuddha Roy, Nguyen Bach, Praneetha Vaddamanu, Gina Lee, Dong Chen, Thomas Portet, Xia Song, Dongwoo Kim, Mahmoud Khademi, Xiren Zhou, Alon Benhaim, Martin Cai, Yunsheng Li, Kai Qiu, Jinyu Li, Adam Atkinson, Young Jin Kim, Saksham Singhal, Ali Mousavi, Yiming Wang, Abhishek Goswami, Vishrav Chaudhary, Liliang Ren, Gilsinia Lopez, Ning Shang, Jacob Platin, Haoran Xu, Junkun Chen, Abdelrahman Abouelenin, Anh Nguyen, Daniel Perez-Becker, Subhojit Som, Yunan Zhang, Congcong Chen, Yelong Shen, Amr Hendy, Jing Pan, Jianwen Zhang, Haibin Wu, Ziyi Yang, Yuxuan Hu, Chen Liang, Vadim Mazalov, Xin Jin, Tetyana Sych, Junheng Hao

- ***What's New***: Phi-4-Mini와 Phi-4-Multimodal은 최신의 컴팩트하지만 강력한 멀티모달 언어 모델(Multimodal Language Models)입니다. Phi-4-Mini는 3.8억 개의 파라미터를 가진 언어 모델로, 고품질 웹 및 합성 데이터로 학습되어 유사한 크기의 최신 오픈 소스 모델을 능가하며, 수학 및 코딩 작업에서도 복잡한 추론 능력을 발휘합니다. Phi-4-Multimodal은 텍스트, 비전, 음성/오디오 입력 모달리티를 통합하여 여러 모달리티를 조합해 수행할 수 있는 새로운 기술을 적용합니다.
- ***Technical Details***: Phi-4-Mini는 다국어 애플리케이션을 지원하기 위해 200K 토큰의 확장된 어휘 크기를 특징으로 하며, 더 효율적인 긴 시퀀스 생성(Group Query Attention)을 위한 주의 메커니즘을 포함합니다. Phi-4-Multimodal은 텍스트, 비전, 음성/오디오 입력 모달리티를 하나의 모델로 통합하며, LoRA 어댑터(Modality-Specific Routers)를 통해 다양한 모달리티를 조합하여 여러 추론 모드를 지원합니다. 예를 들어, 스피치/오디오 모달리티의 LoRA 구성 요소는 4억 6천만 파라미터 밖에 되지 않지만 OpenASR 리더보드에서 1위를 차지합니다.
- ***Performance Highlights***: Phi-4-Mini는 수학과 추론 관련 작업에서 동일한 크기와 두 배 큰 모델에 견줄 수 있는 성능을 보여줍니다. Phi-4-Multimodal은 (비전 + 언어), (비전 + 스피치), (스피치/오디오) 입력을 포함한 다양한 작업에서 더 큰 비전-언어 및 스피치-언어 모델을 능가합니다. 이 모델은 특히 다국어 음성 인식 및 번역 작업에서 강력한 성능을 보여주며, 초기 실험 결과 추론 최적화된 Phi-4-Mini는 훨씬 더 큰 최신 추론 시스템과 견줄 수 있는 성능을 발휘합니다.

### [Difix3D+: Improving 3D Reconstructions with Single-Step Diffusion Models](https://arxiv.org/abs/2503.01774)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01774.png)

Vote: 29

Authors: Zan Gojcic, Huan Ling, Mike Zheng Shou, Jun Gao, Jay Zhangjie Wu, Xuanchi Ren, Yuxuan Zhang, Haithem Turki, Sanja Fidler

- ***What's New***: Difix3D+는 단일 단계 디퓨전 모델(Single-Step Diffusion Models)을 활용하여 3D 재구성과 새로운 뷰 합성을 개선하는 혁신적인 파이프라인을 제안합니다. 이 시스템은 특히 NeRF 및 3D Gaussian Splatting(3DGS)와 같은 표현에서 발생하는 아티팩트를 제거하는 기능을 추가적으로 제공합니다.
- ***Technical Details***: Difix3D+의 핵심은 단일 단계 이미지 디퓨전 모델인 DIFIX로, 이는 훈련된 3D 표현으로부터 렌더링된 새로운 뷰에서 발생하는 결함을 개선하고 제거합니다. 이 모델은 참조 뷰와 생성된 뷰 사이의 교차 뷰 의존성을 포착하기 위해 자기-어텐션(Self-Attention) 레이어를 수정한 참조 혼합 레이어를 포함합니다. 훈련 과정에서 가중치의 손실은 L2 차이와 LPIPS 손실, 스타일 손실을 사용하여 도출됩니다.
- ***Performance Highlights***: 실험 결과, DIFIX3D+는 기존 방법들보다 PSNR이 약 1dB 향상되고, LPIPS는 거의 0.1 감소하며, FID는 거의 3배 개선되는 성능을 보였습니다. 특히, 이 모델은 Nerfacto 및 3DGS 기반 시스템의 백본을 사용했을 때도 뛰어난 성능을 발휘하여 앨리어싱과 같은 문제를 효과적으로 줄였습니다.

### [OneRec: Unifying Retrieve and Rank with Generative Recommender and Iterative Preference Alignment](https://arxiv.org/abs/2502.18965)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.18965.png)

Vote: 18

Authors: Shiyao Wang, Weifeng Ding, Lejian Ren, Qiang Luo, Jiaxin Deng, Kuo Cai, Qigen Hu, Guorui Zhou

- ***What's New***: OneRec은 현재 추천 시스템의 복잡한 다단계 구조를 통합적 생성 모델로 대체하며, 실세계에서의 성능을 크게 향상시킨 최초의 종단간(end-to-end) 생성 추천 시스템입니다. 이 시스템은 Kuaishou의 주 장면에 배치되어 일일 활성 사용자가 수백만 명에 이르는 플랫폼에서 시청 시간을 1.6% 증가시켰습니다.
- ***Technical Details***: OneRec은 사용자의 과거 행동 시퀀스를 인코딩하고 사용자가 관심을 가질 수 있는 비디오를 점진적으로 디코딩하는 인코더-디코더 구조를 사용합니다. 모델 용량을 확장하기 위해 희소한 전문가 집합(Mixture-of-Experts; MoE)이 사용되며, 이는 계산 FLOPs를 비례적으로 증가시키지 않고 모델 용량을 확장합니다. 기존의 다음 아이템 예측과 달리, OneRec은 세션-기반(session-wise) 생성을 통해 보다 우아하고 문맥적으로 일관된 추천을 지원합니다. 직접 선호 최적화(Direct Preference Optimization; DPO)와 결합된 iterative preference alignment(IPA) 모듈을 추가로 사용하여 생성 결과의 품질을 향상시킵니다.
- ***Performance Highlights***: OneRec은 Kuaishou 플랫폼에서 온라인으로 배포되었으며, 시청 시간이 1.6% 증가했습니다. 이는 플랫폼 성능에 큰 기여를 하는 개선입니다. 오프라인 실험에서는 OneRec이 기존의 다양한 랭킹 및 생성 모델보다 우수한 성능을 보였습니다.

### [DiffRhythm: Blazingly Fast and Embarrassingly Simple End-to-End Full-Length Song Generation with Latent Diffusion](https://arxiv.org/abs/2503.01183)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01183.png)

Vote: 18

Authors: Lei Xie, Ziqian Ning, Jixun Yao, Chunbo Hao, Shuai Wang, Huakang Chen, Guobin Ma, Yuepeng Jiang

- ***What's New***: DiffRhythm는 라텐트 확산(Latent Diffusion)을 기반으로 엄청난 속도와 단순함을 자랑하는 최초의 완전한 노래 생성 모델입니다. 이 모델은 음악성과 이해도를 유지하며 4분 45초 길이의 전체 곡을 불과 10초 만에 합성할 수 있습니다. DiffRhythm의 혁신적인 점은 복잡한 데이터 준비가 필요 없고, 간단한 모델 구조로 확장성이 뛰어난 점이며, 비자기회귀(Non-autoregressive) 구조로 빠른 추론 속도를 보장합니다.
- ***Technical Details***: DiffRhythm 모델은 두 단계의 모델로 구성됩니다: 1) 파형의 압축된 라텐트 표현을 학습하는 변이형 오토인코더(VAE)와 2) 라텐트 공간에서 노이즈를 점진적으로 제거하여 곡을 생성하는 확산 변환기(Diffusion Transformer; DiT)입니다. 새로운 문장 단위 가사 정렬 메커니즘을 도입하여 적은 감독하에 가사의 이해도를 높였으며, 라텐트 공간은 풍부한 음악적 디테일과 보컬 뉘앙스를 캡처합니다.
- ***Performance Highlights***: DiffRhythm는 노래 생성을 위한 다양한 비교 실험을 통해 이전의 SongLM보다 더 나은 품질과 이해도를 보여주며, PER(Phoneme Error Rate)에서는 18.2% 감소를 확인했습니다. SongLM과 비교해 ∼50배 빠른 실시간 비율(RTF)을 달성하여 계산 효율성을 높였습니다. 이러한 성과는 DiffRhythm의 확산 기반 접근 방식이 자기회귀 언어 모델보다 더 효율적임을 증명합니다.

### [When an LLM is apprehensive about its answers -- and when its uncertainty is justified](https://arxiv.org/abs/2503.01688)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01688.png)

Vote: 16

Authors: Edvard Khalafyan, Daniil Vyazhev, Andrey Goncharov, Alexey Zaytsev, Petr Sychev

- ***What's New***: 이 연구는 대형 언어 모델(Large Language Models; LLMs)의 불확실성 추정을 위한 새로운 접근 방식을 제안하고, 여러 다중 선택 질문-답변 과제에서 다양한 질문 주제에 대한 불확실성의 정당성을 평가합니다. 특히, 토큰 단위 엔트로피와 모델-판단(Model-as-Judge; MASJ)을 사용하여 불확실성을 추정합니다.
- ***Technical Details***: 이 연구는 다양한 도메인의 다중 선택 질문에 대해 모델 응답의 불확실성을 평가하는 파이프라인을 제안합니다. 주요 방법으로 엔트로피 기반 접근법과 MASJ를 활용하여 데이터와 모델 수준에서의 불확실성을 추정합니다. 실험은 Phi-4, Mistral, Qwen 모델(1.5B에서 72B까지의 크기)을 대상으로 진행되었습니다.
- ***Performance Highlights***: 생물학 영역에서는 엔트로피가 모델 오류를 잘 예측하여 ROC-AUC 0.73을 기록했으나, 수학 영역에서는 0.55로 상관관계가 사라졌습니다. MASJ는 랜덤 오차 예측기와 유사한 성능을 보여줘 개선이 필요합니다. 실험에서도 엔트로피 값이 작은 경우 정확한 응답과 잘 일치하였으며, 엔트로피 기반 불확실성 추정이 많은 추론을 요구하지 않는 경우, 높은 품질의 예측을 제공합니다.

### [Liger: Linearizing Large Language Models to Gated Recurrent Structures](https://arxiv.org/abs/2503.01496)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01496.png)

Vote: 13

Authors: Jusen Du, Weigao Sun, Disen Lan, Yu Cheng, Jiaxi Hu

- ***What's New***: Liger는 대형 언어 모델(LLMs)을 게이트된 선형 반복 구조로 변환하여 추가 매개변수 없이 더 효율적으로 배포할 수 있는 새로운 접근법을 제안합니다. 이 방법은 사전 훈련된 키 행렬 가중치를 재사용하여 다양한 게이팅 메커니즘을 구축하고, 선형화를 통해 성능을 원래의 LLMs와 동등하게 회복합니다.
- ***Technical Details***: Liger는 선형화 방법에서 기존의 세부적인 설계를 고려하여, 추가 트레이닝 없이 사전 훈련된 LLM의 가중치를 활용하여 게이트 모듈을 구성합니다. 또한, Liger Attention이라는 레이어 내 하이브리드 주의 메커니즘을 도입하여, 1B에서 8B 크기의 모델에 대해 경쟁력 있는 성능을 이끌어냅니다.
- ***Performance Highlights***: Liger는 사전 훈련 토큰의 0.02%로 트랜스포머 기반 LLM의 성능을 93% 회복하며, 다양한 벤치마크에서 우수한 결과를 보입니다. 이는 Liger가 Llama-3와 같은 모델과의 성능 차이를 줄이면서 선형 반복 구조로의 변환을 효과적으로 수행함을 시사합니다.

### [Cognitive Behaviors that Enable Self-Improving Reasoners, or, Four Habits of Highly Effective STaRs](https://arxiv.org/abs/2503.01307)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01307.png)

Vote: 13

Authors: Kanishk Gandhi, Anikait Singh, Nathan Lile, Ayush Chakravarthy, Noah D. Goodman

- ***What's New***: 이 논문에서는 언어 모델(LM)의 '자기 개선' 능력을 이해하기 위한 새로운 분석 프레임워크를 제안합니다. 특히 Qwen-2.5-3B와 Llama-3.2-3B 모델의 차별적인 성능을 바탕으로 4가지 주요 인지 행동인 검증(Verification), 역추적(Backtracking), 하위 목표 설정(Subgoal Setting), 역방향 연결(Backward Chaining)을 중점적으로 연구합니다. 이러한 행동들은 인간의 문제 해결 방식과 비슷하며, 적절히 모델 내에서 유도될 수 있습니다.
- ***Technical Details***: 논문에서는 테스트 시점의 추론(Test-time Inference) 기법이 언어 모델이 복잡한 문제를 해결하는 데 있어 깊이 있는 사고를 하도록 유도할 수 있음을 보여줍니다. RL(Reinforcement Learning)을 통해 모델을 훈련하면서, 언어 모델이 자기 개선하는 데 필요한 초기 인지 행동의 존재 여부를 분석했습니다. Qwen 모델은 자체적으로 이러한 인지 행동을 나타내는 반면, Llama는 초기에는 이 행동이 부족했으나 인위적인 행동 유도로 성능을 향상시켰습니다.
- ***Performance Highlights***: Qwen-2.5-3B는 RL 훈련 시 Llama-3.2-3B보다 더 많은 문제 해결 능력 향상을 보였습니다. 실제로 Qwen 모델은 60%의 정확도를 달성한 반면, Llama는 30%에 머물렀습니다. 그러나 Llama에 인지 행동 패턴을 주입하면 이 모델도 Qwen과 동등한 성능을 발휘할 수 있음을 확인했습니다. 특히, 잘못된 해답이라도 적절한 인지 패턴을 포함하면 자기 개선이 이루어질 수 있음을 보여주었습니다.

### [Qilin: A Multimodal Information Retrieval Dataset with APP-level User Sessions](https://arxiv.org/abs/2503.00501)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.00501.png)

Vote: 10

Authors: Jia Chen, Xiaohui He, Yao Hu, Yi Wu, Yan Gao, Shaosheng Cao, Ping Yang, Haitao Li, Qian Dong, Yiqun Liu, Chen Xu, Qingyao Ai

- ***What's New***: Qilin은 Xiaohongshu 플랫폼에서 수집된 사용자 세션과 다양한 이미지-텍스트 노트, 비디오 노트, 상업적 노트 및 직접 답변과 같은 이질적인 결과를 제공하는 최초의 실제 멀티모달 검색 및 추천(S&R) 데이터셋입니다. 이 데이터셋은 고급 멀티모달 신경 검색 모델의 개발을 촉진하고 사용자 행동에 대한 포괄적인 분석을 가능케 합니다.
- ***Technical Details***: Qilin 데이터셋은 약 15,482명의 사용자의 APP 레벨 세션으로 구성되어 있으며, 각 요청은 세션 ID, 요청 ID, 사용자 ID로 식별됩니다. 데이터 처리 파이프라인은 사용자 샘플링, 프론트엔드 로그 결합, 피처 수집 및 데이터 필터링으로 이루어져 있습니다. 특히 LLM을 이용하여 텍스트, 이미지 데이터의 안전성을 검토하여 불필요한 내용을 여과했습니다.
- ***Performance Highlights***: Qilin 데이터셋을 사용한 실험에서는 검색 및 추천 태스크 모두에서 멀티모달 정보를 고려하는 VLM이 뛰어난 성능을 보였습니다. 검색 태스크에서는 DCN-V2 모델이 가장 높은 성능을 기록했으며, 추천 태스크에서는 VLM이 다른 모델들에 비해 우수한 결과를 나타냈습니다. RAG 파이프라인을 통해서는 Qwen2.5가 ROUGE-L 점수에서 최고 성능을 보였습니다.

### [DuoDecoding: Hardware-aware Heterogeneous Speculative Decoding with Dynamic Multi-Sequence Drafting](https://arxiv.org/abs/2503.00784)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.00784.png)

Vote: 8

Authors: Xipeng Qiu, Honglin Guo, Qipeng Guo, Kai Lv

- ***What's New***: 이 논문은 듀오디코딩(DuoDecoding)이라는 새로운 하드웨어 인식 이종 추측 디코딩(speclative decoding) 방법을 제안합니다. 이 방법은 초안 모델(draft model)과 목표 모델(target model)을 각각 CPU와 GPU에 배치하여, 기존의 추측 디코딩에서 초안 모델이 병목으로 작용하는 문제를 해결하며, 병렬 디코딩을 가능하게 합니다.
- ***Technical Details***: 듀오디코딩은 하드웨어 인식 최적 초안 예산(hardware-aware optimal draft budget)을 사용하여 CPU와 GPU의 유휴 시간을 최소화하며, 초안 출력의 불확실성에 기반한 동적 다중 시퀀스 초안(dynamic multi-sequence drafting)을 도입하여 초안의 품질을 향상시킵니다. 이 프로세스는 초안 모델과 목표 모델을 병렬로 동작시켜 회귀 종속성을 제거하고 성능 면에서 병목을 해결합니다.
- ***Performance Highlights***: 7개 작업에서 광범위하게 실험한 결과 듀오디코딩은 생성 대기 시간을 최대 2.61배까지 단축하며, 기존의 추측 디코딩보다 첫 번째 토큰까지의 시간을 17% 감소시켰습니다. 이것은 특히 수학적 추론 및 질의 응답 같은 작업에서 탁월한 효율성을 보여줍니다.

### [Speculative Ad-hoc Querying](https://arxiv.org/abs/2503.00714)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.00714.png)

Vote: 8

Authors: Maria Angels de Luis Balaguer, Haoyu Li, Srikanth Kandula, Venkat Arun, Aditya Akella

- ***What's New***: 본 논문은 사용자가 완성하기 전에 SQL 쿼리를 실행하여 데이터 쿼리의 반응성을 높이는 'Speculative Ad-hoc Querying'을 제안합니다. 새로운 시스템, SpeQL을 통해 LLMs(Large Language Models)를 활용하여 데이터베이스 스키마와 사용자의 이전 쿼리 및 불완전한 쿼리를 기반으로 쿼리를 예측합니다.
- ***Technical Details***: SpeQL은 쿼리의 구조를 미리 예측해 쿼리를 컴파일하고 계획합니다. 또한, 원본 데이터베이스보다 훨씬 작은 임시 테이블을 미리 계산하여 최종 쿼리를 처리하기에 충분한 정보를 포함하고 있습니다. SpeQL은 쿼리의 실행 및 임시 테이블 작성 시 DAG(Directed Acyclic Graph) 형태로 쿼리를 구조화하여, 부분적으로 완성된 쿼리의 실행 계획을 단순화합니다.
- ***Performance Highlights***: SpeQL은 쿼리의 대기 시간을 최대 289배까지 줄이며, 100GB 규모의 데이터에 대해 P90 계획 지연 시간을 94.42%, 컴파일 지연 시간을 99.99%, 실행 지연 시간을 87.23% 줄이는 성능을 보였습니다. 이러한 성능 향상은 더 작은 데이터 세트(10GB)와 더 큰 데이터 세트(1000GB)에 대해서도 일관되게 유지됩니다.

### [SampleMix: A Sample-wise Pre-training Data Mixing Strategey by Coordinating Data Quality and Diversity](https://arxiv.org/abs/2503.01506)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01506.png)

Vote: 7

Authors: Jingang Wang, Deyang Kong, Zhengyu Chen, Xunliang Cai, Shikun Zhang, Xiangyu Xi, Wei Ye, Wei Wang, Jiawei Yang, Jian Yang

- ***What's New***: SampleMix는 데이터 품질과 다양성을 조절하여 샘플 기반의 새로운 프리트레이닝 데이터 믹싱(Sample-wise Pre-training Data Mixing) 전략을 제안합니다. 기존 도메인 기반이 아닌 샘플 별 접근 방식으로, 프리트레이닝 데이터의 글로벌 다양성을 더 잘 통제하려고 합니다.
- ***Technical Details***: SampleMix는 데이터셋 전반에 걸쳐 샘플 품질과 다양성을 개별적으로 평가하여 글로벌 크로스 도메인 샘플링을 수행합니다. 이를 통해 각 샘플 별로 최적의 도메인 분포를 동적으로 결정하고 각 샘플의 품질과 다양성 평가를 기반으로 샘플링 가중치를 할당합니다. 그런 다음 주어진 토큰 예산에 따라 각 예제를 샘플링 하여 최적의 학습 데이터셋을 구성합니다. 또한, 동적으로 다양하게 변하는 토큰 예산에 적응하여 최적의 데이터 비율을 결정할 수 있도록 설계되었습니다.
- ***Performance Highlights***: SampleMix는 기존 도메인 기반 방법들을 능가했으며, 특히 8개의 다운스트림 작업에서 평균 정확도 47.77%로 가장 우수한 성능을 발휘했습니다. 또한 perplexity 평가에서도 Pile(25.63) 및 xP3(46.38) 데이터셋에서 가장 낮은 perplexity 점수를 기록하여 언어 모델링 작업에서의 이점을 강조합니다. 이 방법은 평균 베이스라인 정확도에 1.9배 더 빠른 수렴 속도를 보였습니다.

### [Kiss3DGen: Repurposing Image Diffusion Models for 3D Asset Generation](https://arxiv.org/abs/2503.01370)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01370.png)

Vote: 7

Authors: Lie XU, Xinli Xu, Jiantao Lin, Ying-Cong Chen, Leyi Wu, Shunsi Zhang, Yingjie Xu, Meixi Chen, Xin Yang, Dongyu Yan

- ***What's New***: Kiss3DGen은 2D 이미지 확산 모델(2D Image Diffusion Model)을 3D 자산 생성(3D Asset Generation)에 재사용하는 효율적인 프레임워크입니다. 이 방법은 '3D 번들 이미지(3D Bundle Image)'라는 다중 뷰 이미지와 이를 통해 3D 메시를 재구성할 수 있는 매우 간단한 방법을 사용하여 3D 객체 생성 문제를 2D 이미지 생성 문제로 변환합니다.
- ***Technical Details***: Kiss3DGen의 기초 모델은 강력한 Flux 확산 변환 모델(Diffusion Transformer Model)을 활용하며, 여기에 LoRA(Low-Rank Adaptation)를 적용하여 특정 사용자의 캐주얼 설명에 따라 3D 번들 이미지를 생성합니다. 그런 다음, 생성된 이미지에서 ISOMER와 같은 메쉬 재구성 접근법을 사용해 완전한 3D 메시를 제작합니다. 또한, Kiss3DGen은 ControlNet과 통합되어 3D 편집 및 메시/텍스처 향상과 같은 다양한 기능을 지원합니다.
- ***Performance Highlights***: Kiss3DGen은 다양한 3D 생성 작업에서 최첨단 성능(State-of-the-Art Performance)을 보여주며, 텍스트-3D, 이미지-3D 생성 및 3D 편집 작업에서 높은 정렬성과 품질을 달성했습니다. 특히, 데이터셋 크기를 줄였음에도 성능이 뛰어나고, 매우 질 높고 사실적인 3D 모델 생성을 보여 줍니다.

### [From Hours to Minutes: Lossless Acceleration of Ultra Long Sequence Generation up to 100K Tokens](https://arxiv.org/abs/2502.18890)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.18890.png)

Vote: 7

Authors: Tong Wu, Zixia Jia, Zilong Zheng, Yuxuan Wang, Junzhe Shen

- ***What's New***: TOKENSWIFT는 초장형 시퀀스(ultra-long sequence) 생성의 속도를 획기적으로 향상시키는 새로운 프레임워크로, 기존의 대형 언어 모델(LLM)의 품질을 유지하면서도 최대 100K 토큰까지 손실 없이 초고속으로 생성할 수 있게 합니다.
- ***Technical Details***: TOKENSWIFT는 n-gram 검색과 동적 키-값(KV) 캐시 업데이트를 통해 초장형 시퀀스 생성의 속도를 높입니다. 다중 토큰 생성과 토큰 재활용(token reutilization)을 활용하여 LLM이 한번의 전방 전달 과정에서 여러 토큰을 생성하도록 합니다. 이를 통해 모델 재로드 문제를 완화하고, 동적 KV 캐시 업데이트로 캐시 로딩 시간을 줄이며, 맥락적 벌칙(contextual penalty)을 적용하여 출력물의 반복성을 줄입니다.
- ***Performance Highlights***: 다양한 규모의 모델(1.5B, 7B, 8B, 14B)과 아키텍처(MHA, GQA)에서 TOKENSWIFT는 AR(autoregessive 모델)에 비해 시퀀스 생성 시간 3배 이상의 속도 향상을 달성했습니다. 특히, LLaMA3.1-8b에서 100K 토큰 생성을 위한 시간이 약 5시간에서 90분으로 감소하였으며, 큰 모델일수록 더 큰 속도 향상을 보였습니다.

### [Large-Scale Data Selection for Instruction Tuning](https://arxiv.org/abs/2503.01807)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01807.png)

Vote: 5

Authors: Pang Wei Koh, Pradeep Dasigi, Hamish Ivison, Faeze Brahman, Muru Zhang

- ***What's New***: 이 논문은 지시 튜닝(Instruct Tuning) 언어 모델을 위한 대규모 데이터 선택의 확장 가능성을 체계적으로 연구한 것입니다. 580만 개의 샘플 중 최대 250만 개의 샘플을 선택하여 다양한 작업을 평가했으며, 표현 기반 데이터 선택(RDS+) 변형이 높은 성능을 보이면서도 컴퓨팅 효율적인 것을 발견했습니다.
- ***Technical Details***: 연구는 9가지 데이터 선택 방법을 테스트했습니다. 표현 기반 데이터 선택(RDS+)은 사전 학습된 LM의 숨은 상태를 가중 평균 풀링하여 지속적으로 다른 복잡한 방법을 뛰어넘는 성능을 보였습니다. 연구는 또한 RDS+가 여러 작업을 함께 선택하면서도 강력한 성능을 발휘하였으며, 큰 데이터 풀에 대한 확장성을 강조하고, 이를 통해 데이터 선택 기술의 잠재력을 평가했습니다.
- ***Performance Highlights***: RDS+는 여러 데이터 풀에서 경쟁 모델을 뛰어넘으며, 특히 범위가 큰 데이터 셋 선택에서 뛰어난 성능을 발휘했습니다. 이 연구는 무작위 선택보다 RDS+를 사용할 때 평균 2점 이상의 향상을 보고했으며, 이는 대규모 환경에서 데이터 선택 기술의 중요성을 시사합니다.

### [PodAgent: A Comprehensive Framework for Podcast Generation](https://arxiv.org/abs/2503.00455)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.00455.png)

Vote: 5

Authors: Fenglong Xie, Tan Lee, Haohan Guo, Lei He, Yujia Xiao

- ***What's New***: PodAgent는 팟캐스트와 유사한 오디오 프로그램 생성의 포괄적인 프레임워크로, 대화 스크립트 생성, 목소리와 역할의 일치, LLM에 의해 강화된 음성 합성을 통해 전체적인 자동화된 팟캐스트 생성이 가능하도록 합니다. 이는 기존의 오디오 생성 방법이 직면한 심층적인 콘텐츠 생성과 표현력 있는 음성 생성의 어려움을 해결하기 위한 혁신적인 접근 방식입니다.
- ***Technical Details***: PodAgent는 Host-Guest-Writer 멀티 에이전트 시스템을 설계하여 다양한 배경과 관점을 반영한 스크립트를 생성하며, 음성 특징 분석을 통해 구축된 음성 풀(voice pool)을 사용하여 적절한 역할-음성 연결을 수행합니다. LLM으로 예측된 스타일을 따르는 텍스트-음성 변환 모델을 도입하여 적절한 감정과 운율을 가진 고품질의 대화를 생성합니다. 또한, 포드캐스트 스타일 오디오 생성의 품질 평가를 위한 포괄적인 평가 지침을 마련했습니다.
- ***Performance Highlights***: PodAgent는 직접 GPT-4로 생성한 경우와 비교하여 주제-토론 대화에서 현저한 성과를 기록하였으며, 87.4%의 목소리 일치 정확도를 달성하고 LLM 가이드 합성을 통해 더욱 표현력 있는 대화를 생성합니다. 이는 Podcast와 같은 오디오 프로그램 생성에서 PodAgent의 높은 성능과 완성도를 보여주며, 구성하고 실험된 데이터는 영어를 기본으로 하지만, 중국어 기반의 팟캐스트 또한 데모 페이지에서 제공합니다.

### [CodeArena: A Collective Evaluation Platform for LLM Code Generation](https://arxiv.org/abs/2503.01295)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01295.png)

Vote: 5

Authors: Mingzhe Du, Dong Huang, Terry Yue Zhuo, See-Kiong Ng, Qian Liu, Anh Tuan Luu, Bin Ji, Xiaobao Wu

- ***What's New***: CodeArena는 LLM 코드 생성의 평가를 위한 온라인 플랫폼을 소개하고 있습니다. 이 플랫폼의 주요 혁신은 참여 모델들의 전체 성과를 기반으로 모델 점수를 동적으로 조정하는 집단 평가 메커니즘을 도입하여 벤치마크 누출에 의한 점수 편향을 완화한다는 점입니다. 또한 CodeArena는 모든 제출된 해결책과 테스트 케이스에 대한 공개 접근을 보장하며, 코드 평가 워크플로우를 간소화하기 위해 자동화 친화적인 API를 제공합니다.
- ***Technical Details***: CodeArena는 오픈 소스 OJ(SDL) 플랫폼을 기반으로 구축되어 네 가지 계층으로 구성됩니다: API Layer, Runtimes Layer, Dynamic Evaluation Layer, Data Layer. API Layer는 사용자 상호작용을 위한 REST API와 Python 라이브러리를 제공하며, Runtimes Layer는 다양한 프로그래밍 언어를 지원하는 표준화된 코드 실행 환경을 제공합니다. Dynamic Evaluation Layer는 모든 제출 후 점수를 동적으로 업데이트하며, Data Layer는 문제, 테스트 케이스 및 솔루션을 저장합니다.
- ***Performance Highlights***: CodeArena의 리더보드 분석 결과, 'DeepSeek-Coder-V2-Lite'와 같은 소규모 파라미터 스케일을 가진 모델이 높은 성능을 나타냈으며, 닫힌 소스 LLM은 열린 소스 LLM에 비해 더 높은 성능을 보이는 경향이 있었습니다. 특히, 닫힌 소스 LLM은 시간이 지나면서도 안정적인 Dynamic Point(DP) 점수를 유지했습니다.

### [Efficient Test-Time Scaling via Self-Calibration](https://arxiv.org/abs/2503.00031)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.00031.png)

Vote: 4

Authors: Chengsong Huang, Langlin Huang, Jiaxin Huang, Jiacheng Liu, Jixuan Leng

- **What's New**: 이 논문은 대형 언어 모델(LLMs)의 테스트 시간 계산 효율성을 개선하기 위해 새로운 자기 교정(Self-Calibration) 방법을 도입합니다. 이 접근법은 모델의 신뢰도 추정치를 향상시켜 다양한 난이도의 질의에 대해 샘플링 전략을 동적으로 조정합니다.
- **Technical Details**: 자기 교정 기법은 LLM이 테스트 시간에 단 한 번의 순방향 패스로 신뢰성 있는 신뢰도 추정치를 제공할 수 있도록 설계되었습니다. 이를 위해 다수의 응답 샘플에서 파생된 자기 일관성(Self-Consistency)의 신뢰도를 모델 내에 증류(distilling)합니다. 이후, 교정된 신뢰도를 기반으로 다양한 난이도의 문제에 적합한 샘플링 전략을 설계합니다.
- **Performance Highlights**: 실험 결과에 따르면, 신뢰도 기반 초기 정지(Early Stopping) 기법을 Best-of-N 샘플링에 적용하는 경우, MathQA 데이터셋에서 정확도가 81.0에서 83.6으로 향상되었습니다. 또한, 신뢰도 기반 자기 일관성 방법은 94.2%의 샘플을 절약하면서도 표준 자기 일관성과 동일한 정확도를 달성할 수 있었습니다.

### [Word Form Matters: LLMs' Semantic Reconstruction under Typoglycemia](https://arxiv.org/abs/2503.01714)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01714.png)

Vote: 4

Authors: Zirui Song, Tianle Gu, Chenxi Wang, Lang Gao, Xiuying Chen, Zhongyu Wei

- ***What's New***: 이 연구는 대형 언어 모델(LLMs)이 철자 오류(타이포글리세미아) 하에서도 어떻게 텍스트를 이해하고 재구성하는지를 조사합니다. 특히 단어 형식이 LLMs의 의미 재구성에서 핵심 역할을 하며, 사람들이 맥락 정보를 사용할 때와는 다르게 LLMs는 고정된 주의를 보이는 구조적 패턴을 보임을 발견했습니다.
- ***Technical Details***: 연구진은 새로운 측정 지표 SemRecScore를 제안하여, Scramble Ratio와 Context Integrity 두 변수에 따라 LLMs가 원문 단어의 의미를 얼마나 잘 재구성하는지를 정량화했습니다. SemRecScore는 원본 단어의 토큰 표현과 최종 서브워드 토큰의 표현 간 코사인 유사도로 정의됩니다. 또한 주의 메커니즘을 분석하여, 특정 주의 헤드가 단어 형식 정보를 처리하는 데 특화되어 있음을 확인했습니다.
- ***Performance Highlights***: 결과는 Scramble Ratio가 커질수록 SemRecScore가 떨어지며, 반면 다양한 Context Integrity 수준에서는 SemRecScore의 변화가 거의 없음을 보여줍니다. 즉, LLMs는 주로 단어 형식에 의존하게 되며, 맥락 정보는 예상보다 적은 영향을 미칩니다. 이는 LLMs가 고정된 주의 패턴을 보이며 사람과 달리 주된 재구성 자원으로 단어 형식을 활용함을 나타냅니다.

### [VideoUFO: A Million-Scale User-Focused Dataset for Text-to-Video Generation](https://arxiv.org/abs/2503.01739)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01739.png)

Vote: 3

Authors: Yi Yang, Wenhao Wang

- ***What's New***: VideoUFO는 텍스트-비디오 생성(Text-to-Video Generation)을 위한 사용자의 포커스에 맞춘 최초의 비디오 데이터셋입니다. 이 데이터셋은 1.09백만 개의 비디오 클립으로 구성되어 있으며, 주제별로 군집화된 실사용자의 텍스트-비디오 프롬프트 데이터셋 VidProM에서 1,291개의 사용자 집중 주제를 식별하고 이를 통해 유튜브 API를 통해 관련 비디오를 수집합니다.
- ***Technical Details***: VideoUFO 데이터셋은 다른 비디오 데이터셋과의 겹침을 최소화(0.29%)하며, 크리에이티브 커먼즈 라이선스하에 유튜브 API를 통해 검색된 비디오로만 구성되어 독창성과 법적 준수성을 담보합니다. 데이터셋은 사용자 제공 프롬프트로부터 추출한 주제를 기반으로 비디오를 검색, 짧은 클립으로 분할하고 각각의 클립에 대해 간단한 설명과 자세한 설명을 생성합니다. 1,091,712개의 클립은 주제 적합성을 위해 검증되었습니다.
- ***Performance Highlights***: 현재 텍스트-비디오 모델들은 모든 사용자 집중 주제에서 일관된 성능을 보이지 못하고 있으며, Worst-performing 주제에서 VideoUFO로 훈련된 모델이 다른 모델보다 뛰어난 성능을 보입니다. 실험 결과에 따르면 기존 16개의 텍스트-비디오 모델이 특정 주제에서 낮은 성능을, 새로 훈련한 모델이 Worst-performing 주제에서 가장 높은 유사성을 유지하며 최고의 성능을 보였습니다.

### [General Reasoning Requires Learning to Reason from the Get-go](https://arxiv.org/abs/2502.19402)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.19402.png)

Vote: 3

Authors: Seungwook Han, Samuel J. Gershman, Jyothish Pari, Pulkit Agrawal

- ***What's New***: 논문에서는 기존 대형 언어 모델(LLMs)이 다양한 상황에 적응하여 강력하게 추론하는 데 어려움을 겪고 있으며, 이를 극복하기 위해 지식과 추론을 분리하여 새로운 접근 방식을 제안합니다. 이 방법은 (1) 토큰 예측 대신 강화학습(RL)을 이용한 사전훈련, (2) 인공적인 작업을 활용한 효율적인 학습 방법, (3) 작은 컨텍스트 윈도우를 통해 더 일반화 가능한 추론 함수를 학습함으로써 기존의 한계를 극복하고자 합니다.
- ***Technical Details***: 연구자들은 LLMs의 알고리즘 이해력은 제한적이며, 새로운 프로그래밍 문법에 대한 전이 능력이 부족함을 발견했습니다. 이러한 문제를 해결하고자 LLMs의 대리 학습 과정에서 처리가 과도하게 얽혀있는 지식과 추론을 의도적으로 분리하여 시험하도록 하는 새로운 구조적 개념을 제안합니다. 이는 외부 메모리 은행을 사용하여 데이터를 저장하고, 짧은 컨텍스트 윈도우에서 작동하는 추론 모듈을 통해 이루어집니다.
- ***Performance Highlights***: 제안된 접근법을 통한 강화학습 기반 사전훈련은 기존 학습 방식인 감독 학습 기반 훈련보다 더 나은 성능을 보여주었습니다. 특히, 알고리즘적 과제를 다룰 때 o1이라는 모델이 다른 모델에 비해 월등히 뛰어난 성과를 보였고, 이는 강화학습을 위한 포스트 트레이닝을 통해 가능했습니다. 하지만, 이 모델조차도 향후 성능 개선의 여지가 있습니다.

### [Direct Discriminative Optimization: Your Likelihood-Based Visual Generative Model is Secretly a GAN Discriminator](https://arxiv.org/abs/2503.01103)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01103.png)

Vote: 2

Authors: Qinsheng Zhang, Jun Zhu, Yongxin Chen, Ming-Yu Liu, Kaiwen Zheng, Huayu Chen, Guande He

- ***What's New***: 본 논문에서는 DDO(직접 판별 최적화)를 제안하여 MLE(최대우도추정)의 한계를 넘어서도록 GAN의 객체 함수를 결합하는 새로운 방법을 소개합니다. 이는 MLE 기반 시각 생성 모델도 결국 GAN 판별자로 사용할 수 있음을 보여줍니다.
- ***Technical Details***: DDO는 학습 대상 모델과 기준 모델 간의 우도 비율(likelihood ratio)을 사용하여 암시적으로 판별자를 매개변수화합니다. 이 접근은 추가적인 네트워크 구조 변경이나 추론 프로토콜 변경 없이 사전 학습된 모델을 직접 조정할 수 있으며, 여러 라운드의 반복적 자기 플레이를 통해 성능을 점진적으로 향상시킵니다.
- ***Performance Highlights***: DDO는 CIFAR-10 및 ImageNet-64 데이터셋에서 획기적인 성능 향상을 달성했으며, 예를 들어 CIFAR-10 데이터셋에서 EDM 모델의 FID 점수를 1.79에서 1.30으로 낮추었습니다. 또한, VAR 모델에서는 CFG 없이도 향상된 성능을 보여주며, CFG와 함께 사용할 때 더 나은 FID-IS 거래를 제공합니다.

### [Unposed Sparse Views Room Layout Reconstruction in the Age of Pretrain Model](https://arxiv.org/abs/2502.16779)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.16779.png)

Vote: 2

Authors: Yixing Yuan, Jianan Wang, Xiangyu Yue, Xili Dai, Xianbiao Qi, Yaxuan Huang

- ***What's New***: 이 논문은 미세하게 조정된 3D 기초 모델 DUSt3R을 활용하여 멀티뷰 실내 레이아웃 복원 문제를 해결하는 'Plane-DUSt3R' 방법을 소개합니다. 기존의 멀티스텝 구조 설명에서 벗어나, 단일 스텝으로 실내 공간 구조를 재구성하는 혁신적인 방식을 제시합니다.
- ***Technical Details***: Plane-DUSt3R는 DUSt3R 프레임워크를 사용하여 건축 구조물을 예측하는 데 중점을 두고, Structure3D 데이터셋을 이용해 모델을 미세하게 조정함으로써 구조 평면을 추정합니다. 이 모델은 비디자인된 카메라 포즈(sparse view)로부터 공간 구조를 재구성하여 정규화된 예측값과 실제값 간의 유클리드 거리를 측정하는 회귀 손실을 사용하여 향상된 3D 정보를 제공합니다.
- ***Performance Highlights***: Plane-DUSt3R는 Structure3D 데이터베이스에서 첨단 기법보다 Relative Rotation Accuracy(RRA)에서 약 5.27% 향상된 성능을 보여줍니다. 이 접근 방법은 in-the-wild 데이터 세트나 다른 이미지 스타일에서도 강인하고 효과적인 성능을 입증했습니다.

### [CLEA: Closed-Loop Embodied Agent for Enhancing Task Execution in Dynamic Environments](https://arxiv.org/abs/2503.00729)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.00729.png)

Vote: 2

Authors: Ge Wang, Shuguang Cui, Yiming Zhao, Yao Guo, Yatong Han, Jinke Ren, Zhixin Mai, Qing Zhao, Mingcong Lei, Zhen Li

- ***What's New***: CLEA는 동적 환경에서의 과제 실행을 향상시키기 위해 설계된 새로운 아키텍처로, 폐쇄형 의사결정 루프(closed-loop decision-making)를 도입했습니다. CLEA는 4개의 독립적인 LLM(Large Language Models) 모듈을 통합하여 실시간 환경 피드백에 기반한 적응형 전략 수립을 가능하게 합니다. 특히, 환경 메모리에 기반한 동적 실행 가능한 하위 작업 생성을 위한 상호작용 작업 계획자와, 실행의 가능성을 확률적으로 평가하고 필요시 계층적 재계획 메커니즘을 촉발 시키는 멀티모달 실행 평가자(Multimodal Execution Critic)가 핵심 혁신점입니다.
- ***Technical Details***: CLEA는 관찰자(Observer), 메모리 모듈, 그리고 플래너-크리틱 에이전트(Planner-Critic Agent)로 구성되어 있습니다. 관찰자는 환경으로부터의 이미지 입력을 언어 기반 모듈로 변환하고, 메모리 모듈은 과거의 상호작용을 바탕으로 환경에 대한 현재의 신념 상태를 유지합니다. 플래너는 생성적 계획 추론을 통해 하위 목표와 행동 시퀀스를 생성하며, 크리틱은 실시간 환경 피드백을 통해 계획의 실행 가능성을 평가하고 필요시 수정 피드백을 제공합니다. CLEA는 두 로봇 간의 협업과 조정이 요구되는 복합적인 작업 수행을 통해 동적 환경에서의 잘못된 계획을 실시간으로 감시하고 수정할 수 있는 우수한 적응력을 보여줍니다.
- ***Performance Highlights***: CLEA는 12번의 실험에서 기준 모델 대비 성공률(Success Rate; SR)에서 평균 67.3% 향상, 작업 완료율(Task Completion Rate; AC)에서 52.8%의 개선을 보였습니다. 폐쇄 루프(closed-loop) 시스템은 과제 계획과 실행의 견고성을 크게 향상시킵니다. 특히, CLEA는 동적 환경에서 개방 루프(open-loop) 에이전트가 해결할 수 없는 문제들을 성공적으로 해결함을 보여주었습니다.

### [AI-Invented Tonal Languages: Preventing a Machine Lingua Franca Beyond Human Understanding](https://arxiv.org/abs/2503.01063)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01063.png)

Vote: 1

Authors: David Noever

- ***What's New***: 이 논문은 대규모 언어 모델(LLMs)이 기계 간 통신(M2M)을 위해 비공개 음조 언어(Tonal Language)를 개발할 수 있는 가능성을 탐구합니다. 이를 통해 기계 간의 효율적인 정보 전달을 가능하게 하고 인간의 청취 범위를 넘어 ultrasonic frequency를 활용하여 정보 전송 속도를 인간의 언어보다 더 높일 수 있습니다.
- ***Technical Details***: 이 연구에서는 ASCII 문자를 음악적 반음(Semitone)에 해당하는 주파수로 매핑하는 시스템을 구현하였습니다. 각 문자는 space(220 Hz)에서 tilde(50,175.42 Hz)까지의 unique한 주파수가 부여되며, 이는 약 7.8 옥타브에 걸쳐 있으며, 인간 청취 범위(20kHz 이하)를 초과하는 주파수도 포함됩니다. 이러한 주파수 매핑은 Western 음계의 동등한 온도 분포(Equal Temperament)에 기반하여 설계되었습니다.
- ***Performance Highlights***: 토널 인코딩을 통해 정보 전송률이 인간의 언어보다 초월할 수 있으며, 각각의 문자가 일관된 주파수로 전송되어 명확한 번역과 해독이 가능합니다. 또한 메시지 자체에 음악적 특성이 내재되어 있고, 이러한 특성은 기억력과 패턴 인식에 도움을 줄 수 있습니다.

### [Why Are Web AI Agents More Vulnerable Than Standalone LLMs? A Security Analysis](https://arxiv.org/abs/2502.20383)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.20383.png)

Vote: 1

Authors: Furong Huang, Yizheng Chen, Seungjae Lee, Jia-Bin Huang, Jeffrey Yang Fan Chiang

- ***What's New***: 웹 AI 에이전트(Web AI Agents)의 보안 취약점을 분석한 이 논문은, 독립 실행형 대형 언어 모델(Standalone LLMs)보다 웹 AI 에이전트가 더욱 취약하다는 사실을 밝혀냈습니다. 논문은 웹 AI 에이전트의 구조적 차이가 이러한 취약성에 기여하는 세 가지 주요 요인 - 사용자 목표의 시스템 프롬프트 내 포함, 다단계 작업 생성, 그리고 관찰 능력 - 을 제시하며, 보다 정교한 보안 평가 프레임워크를 제안합니다.
- ***Technical Details***: 이 연구는 웹 AI 에이전트의 취약성을 심층 분석하기 위해 세련된 평가 프레임워크를 도입하며, 사용자 목표를 시스템 프롬프트에 포함시키거나 다단계 작업 생성 방식을 채택하는 것이 취약성 증가에 기여함을 실험적으로 입증합니다. 또한 웹 페이지와의 상호작용 및 이벤트 스트림(Event Stream) 요소가 취약성 증가에 미치는 영향도 분석합니다.
- ***Performance Highlights***: 책임이 있는 웹 AI 에이전트는 46.6%의 높은 '제일브레이킹(Jailbreaking)' 성공률을 기록하는 반면, 독립 실행형 LLM은 이러한 악의적 요청을 따르는데에 있어 성공률 0%를 유지하여 상대적으로 높은 보안성을 보였습니다. 이는 같은 LLM을 기반으로 구축된 두 시스템 사이의 취약성 차이를 명확히 설명해줍니다.

