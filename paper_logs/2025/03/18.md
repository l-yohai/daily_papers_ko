## Daily Papers (2025-03-18)

### [DropletVideo: A Dataset and Approach to Explore Integral Spatio-Temporal Consistent Video Generation](https://arxiv.org/abs/2503.06053)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.06053.png)

Vote: 72

Authors: Xiaoli Gong, Baoyu Fan, Guoguang Du, Rengang Li, Jingjing Wang, Runze Zhang, Cong Xu, Lu Liu, Liang Jin, Qi Jia, Zhenhua Guo, Yaqian Zhao, Xiaochuan Li

- ***What's New***: DropletVideo는 비디오 생성에서 통합된 시공간 일관성(Integral Spatio-Temporal Consistency)을 탐색하기 위해 개발된 데이터셋과 모델을 소개합니다. 이 논문은 새로운 객체와 기존 객체 간 상호작용 연속성을 강조하며, 기존 연구들이 시간적 또는 공간적 일관성에 집중했던 것과 달리, 플롯 진행과 카메라 기술 간의 시너지를 연구합니다.
- ***Technical Details***: DropletVideo-10M 데이터셋은 1천만 개의 비디오 샘플로 구성되어 있으며, 각 비디오는 평균 206단어의 캡션으로 주석이 달려 있습니다. DropletVideo 모델은 라텐트 스페이스(Latent Space)를 활용하여 시공간 일관성을 유지하면서 비디오를 생성합니다. 이 모델은 확산 모델(Diffusion Model)과 3D Causal VAE를 통합하여 시각 정보와 텍스트 정보를 동시에 처리하는 변형기 구조를 사용합니다.
- ***Performance Highlights***: DropletVideo는 시간적 및 공간적 일관성에서 향상된 성능을 보였으며, 비교 대상인 I2VGen-XL, Animate-Anything, Nvidia-Cosmos보다 뛰어난 카메라 움직임 처리 능력을 보여줍니다. 특히, 드롭릿비디오는 비디오 생성 중 카메라 움직임을 정밀하게 제어해 스토리와 장면의 일관성을 높입니다.

### [Being-0: A Humanoid Robotic Agent with Vision-Language Models and Modular Skills](https://arxiv.org/abs/2503.12533)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12533.png)

Vote: 49

Authors: Yicheng Feng, Bohan Zhou, Börje F. Karlsson, Xinrun Xu, Yuhui Fu, Yi Zhan, Zongqing Lu, Haoqi Yuan, Yu Bai

- ***What's New***: Being-0은 인간 수준의 성능을 수행할 수 있는 자율형 로봇 에이전트를 구축하기 위해 개발된 새로운 계층적 프레임워크입니다. 이 프레임워크는 비전-언어 모델(Vision-Language Model; VLM) 기반의 Connector 모듈을 도입하여 고수준의 인공지능 모델과 저수준의 제어 기술을 효과적으로 연결합니다.
- ***Technical Details***: Being-0은 고수준의 계획 및 추론을 담당하는 Foundation Model(FM)과 강력한 이동 및 섬세한 조작 기술을 제공하는 모듈형 기술 라이브러리를 통합합니다. Connector 모듈은 VLM을 활용하여 언어 기반 계획을 실행 가능한 기술 명령으로 변환하고, 실시간으로 이동 및 조작 기술을 조정합니다. 이 모델은 대형 실내 환경에서 경량의 onboard 컴퓨팅 장치에서 실행될 수 있도록 설계되었습니다.
- ***Performance Highlights***: Being-0은 복잡한 환경에서의 장기 과제를 해결할 수 있는 능력을 보여주었으며, 전체적인 장기 과제 수행률에서 평균 84.4%라는 높은 완성률을 기록했습니다. 이 모델은 FM 기반 에이전트에 비해 네비게이션에서 4.2배의 효율성을 달성하며, 향후 인간형 로봇 에이전트 연구에 중요한 기여를 할 것으로 기대됩니다.

### [DreamRenderer: Taming Multi-Instance Attribute Control in Large-Scale Text-to-Image Models](https://arxiv.org/abs/2503.12885)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12885.png)

Vote: 35

Authors: Dewei Zhou, Mingwei Li, Yi Yang, Zongxin Yang

- ***What's New***: DreamRenderer는 대규모 텍스트-이미지(Text-to-Image) 모델의 플로그 앤 플레이(Plug-and-Play) 방식의 컨트롤러로서 사용자가 바운딩 박스(bounding boxes)나 마스크(masks)를 통해 각 인스턴스의 내용을 제어할 수 있게 합니다. 이는 전반적인 시각적 조화를 유지하면서 여러 인스턴스의 속성을 정확하게 제어할 수 있는 능력을 제공합니다.
- ***Technical Details***: DreamRenderer는 Hard Text Attribute Binding이라는 새로운 기술을 도입하여, 텍스트 임베딩이 각 인스턴스의 시각적 속성을 Joint Attention 과정 동안 정확하게 연결할 수 있게 합니다. 이 과정에서 브리지 이미지 토큰(Bridge Image Tokens)을 사용하여 단일 인스턴스 생성 프로세스를 시뮬레이션하고, FLUX 모델의 중요한 계층에 Hard Image Attribute Binding을 적용하여 시각적 속성을 정확하게 제어합니다.
- ***Performance Highlights***: COCO-POS와 COCO-MIG 벤치마크 실험에서 DreamRenderer는 FLUX 모델에 비해 이미지 성공 비율(Image Success Ratio)을 17.7% 향상시켰으며, GLIGEN 및 3DIS와 같은 레이아웃-이미지 모델에서도 최대 26.8%까지 성능을 개선하였습니다. 이러한 성능 향상은 특히 제어해야 할 인스턴스의 수가 많아질수록 더 두드러집니다.

### [Personalize Anything for Free with Diffusion Transformer](https://arxiv.org/abs/2503.12590)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12590.png)

Vote: 22

Authors: Zehuan Huang, Hairong Lv, Haoran Feng, Lu Sheng, Lin Li

- ***What's New***: Diffusion Transformer(DiT)를 기반으로 한 Personalize Anything이라는 훈련이 필요 없는 프레임워크를 제안하였습니다. 이는 단일 주제 개인화, 다중 주제 또는 주제-장면 조합, 인페인팅 및 아웃페인팅, 비주얼 스토리텔링 등 다양한 시나리오에서 강력한 적응력을 보여줍니다.
- ***Technical Details***: Personalize Anything은 DiT에서 주제 일관성을 유지할 수 있는 시간 단계 적응형 토큰 교체(timestep-adaptive token replacement)와 구조적 다양성을 높이는 패치 교란 전략(patch perturbation strategies)을 사용합니다. 이는 레이아웃 중심 생성(layout-guided generation), 다중 주제 개인화(multi-subject personalization), 마스크 제어 편집(mask-controlled editing)을 자연스럽게 지원합니다.
- ***Performance Highlights***: 테스트 결과, Personalize Anything은 높은 신원 보존 및 적응력을 입증했으며, DiT에서 정밀 튜닝된 모델보다 뛰어난 성능을 보였습니다. 이는 이미지-텍스트 정렬 및 다양한 개인화 작업에서 뛰어난 성능을 보이며, 신원 보존과 텍스트 제어 가능성도 효과적으로 유지합니다.

### [SPIN-Bench: How Well Do LLMs Plan Strategically and Reason Socially?](https://arxiv.org/abs/2503.12349)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12349.png)

Vote: 22

Authors: Tianqing Zou, Ryan Hsieh, Pramod Viswanath, Haisu Zhou, Zerui Cheng, Jianzhu Yao, Zhangyang Wang, Kevin Wang

- ***What's New***: SPIN-Bench는 대형 언어 모델(LLMs)의 전략적 계획 및 사회적 추론 능력을 측정하기 위한 새로운 평가 프레임워크를 소개합니다. SPIN-Bench는 기존 벤치마크가 다루지 못한 유연한 환경에서의 시뮬레이션 및 다양한 사회적 설정을 통해 AIl에이전트의 전략적 행동을 테스트합니다.
- ***Technical Details***: SPIN-Bench는 PDDL(Planning Domain Definition Language) 기반 단일 에이전트 공식 작업, 경쟁적인 보드 게임, 협력적인 카드 게임, 다중 에이전트 협상 시나리오를 포함하는 통합 프레임워크입니다. SPIN-Bench의 벤치마크는 행동 공간, 상태 복잡성 및 상호 작용하는 에이전트 수를 체계적으로 달리하여 다양한 사회적 설정을 시뮬레이션합니다. 이는 단순한 단계별 의사 결정뿐만 아니라 다른 참가자의 개념적 추론을 필요로 하며, AI 시스템의 계획 및 사회적 추론 한계를 드러냅니다.
- ***Performance Highlights***: 모델 o1 같은 일부는 장기 계획 시나리오에서 경쟁력을 보였지만, 행동 및 상태 공간이 크게 확장되면 다중 단계(hop) 추론에서 여전히 약점을 드러냈습니다. 대부분의 현재 LLM은 협력 및 협상 작업에서 저조한 성능을 나타내어 복잡한 전략 조건에서의 사회적 지능의 부족을 시사합니다. 특히, 대규모 사회적 상호 작용은 일반적으로 강력한 계획 모델인 o1의 사고 연결(chain-of-thought) 일관성에 부정적인 영향을 미치는 것으로 나타났습니다.

### [Edit Transfer: Learning Image Editing via Vision In-Context Relations](https://arxiv.org/abs/2503.13327)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.13327.png)

Vote: 21

Authors: Mike Zheng Shou, Qi Mao, Yuchao Gu, Lan Chen

- ***What's New***: Edit Transfer는 새로운 이미지 편집 설정을 제시하며, 단 하나의 소스–타겟 예제를 통해 변환을 학습한 뒤 새로운 쿼리 이미지에 이를 적용할 수 있는 모델을 도입했습니다. 기존의 텍스트 기반 편집 방법이 텍스트 프롬프트를 통해 의미론적 조작에 뛰어난 반면, 정확한 기하학적 세부 사항에서 제한적인 부분을 Edit Transfer는 확장합니다. 이를 통해 텍스트만으로는 어려운 비선형 변환을 가능하게 합니다.
- ***Technical Details***: 시각적 관계 인컨텍스트 학습을 위해, DiT 기반의 텍스트-이미지(T2I) 모델을 기반으로 하여, 예제 이미지와 쿼리 이미지를 하나의 네 패널 콤포지트로 배열하고 경량 LoRA(저순위 적응) 파인튜닝을 적용하여 복합 공간 변환을 포착합니다. 총 42개의 트레이닝 샘플만을 사용하여 다양한 비선형 시나리오에서의 편집 전달 효과를 대폭 향상시켰습니다.
- ***Performance Highlights***: Edit Transfer는 복잡한 공간 비선형 변환을 효과적으로 처리하고 구성 편집 작업에서 최첨단 TIE 및 RIE 방법보다 우수한 성능을 보이며, 이는 소수의 설계된 예제만으로도 성공적인 편집 행동을 생성할 수 있는 가능성을 입증합니다. 사용자 평가 및 비전-언어 모델(VLM) 평가에서도 최고 점수를 기록하며 타 모델들을 능가합니다.

### [BlobCtrl: A Unified and Flexible Framework for Element-level Image Generation and Editing](https://arxiv.org/abs/2503.13434)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.13434.png)

Vote: 19

Authors: Ying Shan, Yuexian Zou, Xiaoyu Li, Zhaoyang Zhang, Lingen Li, Guangzhi Wang, Hongxiang Li, Xiaodong Cun, Yaowei Li

- ***What's New***: BlobCtrl는 요소 수준 이미지 생성 및 편집을 위한 통합되고 유연한 프레임워크로, 요소의 공간적 위치, 의미 있는 내용, 아이덴티티 정보를 분리하여 정확한 조작을 가능하게 합니다. 에러 국면을 다루기 위해 확률적 blob 기반 표현을 사용하여 전경-배경 통합을 위한 이중-분기(diffusion architecture)를 사용합니다.
- ***Technical Details***: BlobCtrl은 계층적 특징 융합을 활용하는 이중-분기(diffusion architecture)와 자체 지도 학습(self-supervised training) 패러다임을 도입하여 요소 수준에서 시각 콘텐츠의 정확하고 세밀한 편집을 실현합니다. 시민 데이터 증가와 점수 기능, 그리고 제어 가능한 드롭아웃(dropout)을 이용해 정밀도와 다양성 간의 균형을 맞춥니다. BlobData라는 대규모 교육용 데이터셋 및 제어용 벤치마크 BlobBench를 소개합니다.
- ***Performance Highlights***: BlobCtrl는 기존의 방법들보다 요소 수준 생성 및 편집 작업에서 우수한 성능을 제공합니다. 정체성을 보존하면서 다양한 작업에서 더 나은 결과를 생성하며, 세부적이고 복잡한 작업에서도 유지되는 조화로운 통합을 보여줍니다. 이는 BlobBench 상에서 수치적으로도 좋은 평가를 받으며, 인간 평가에서도 강력한 선호도를 얻습니다.

### [WideRange4D: Enabling High-Quality 4D Reconstruction with Wide-Range Movements and Scenes](https://arxiv.org/abs/2503.13435)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.13435.png)

Vote: 16

Authors: Kaixin Zhu, Wentao Zhang, Ling Yang, Juanxi Tian, Hongjuan Pei, Bohan Zeng, Shuicheng Yan, Mingbao Lin

- ***What's New***: WideRange4D는 넓은 범위의 공간 움직임을 포함한 고품질 4D 재구성을 가능하게 하는 새로운 벤치마크로서, 4D 장면 생성을 위한 더욱 포괄적인 평가를 제공합니다. Progress4D라는 새로운 4D 재구성 방법 역시 소개하여, 다양한 복잡한 4D 장면 재구성 작업에서 안정적이고 고품질의 결과를 생성합니다.
- ***Technical Details***: WideRange4D는 넓은 공간적 변화를 포함한 4D 장면 데이터를 제공하며, 이는 기존의 4D 벤치마크에 비해 장면의 다양성과 재구성 난이도를 크게 향상시킵니다. Progress4D는 4D 생성을 두 단계로 나누는데, 먼저 고품질 3D 장면을 재구성하고, 그 다음 4D 동기화를 점진적으로 맞추어 넓은 공간 움직임이 포함된 복잡한 4D 장면에서도 안정성과 품질을 보장합니다.
- ***Performance Highlights***: Progress4D는 기존의 최신 4D 재구성 방법들에 비해 더 나은 성능을 발휘하였으며, 넓은 범위의 공간 운동을 포함한 4D 장면 재구성에서 최고 성능을 기록하였습니다. L1, PSNR, SSIM, LPIPS 등의 성능 평가 지표에서 뛰어난 결과를 보이며, WideRange4D의 테스트 데이터셋에서 확고한 성능을 입증하였습니다.

### [MicroVQA: A Multimodal Reasoning Benchmark for Microscopy-Based Scientific Research](https://arxiv.org/abs/2503.13399)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.13399.png)

Vote: 16

Authors: Jesus G. Galaz-Montoya, James Burgess, Jan N. Hansen, Yuhui Zhang, Ridhi Yarlagadda, Alejandro Lozano, Chad Liu, Disha Bhowmik, Laura Bravo-Sánchez, Wah Chiu, Zachary Coman, Sanket Rajan Gupte, Sarina M. Hasan, Manuel D Leonetti, Malvika G Nair, Emma Lundberg, Yuchang Su, Connor Zuraski, Jeffrey J Nirschl, Sarah Cohen, Alexandra Johannesson, Serena Yeung-Levy, William D. Leineweber

- ***What's New***: MicroVQA는 생물학적 현미경 기반 연구를 위한 최초의 멀티모달 추론 벤치마크(VQA Benchmark)로, 연구 워크플로우에 필요한 세 가지 주요 추론 능력: 전문가 이미지 이해, 가설 생성 및 실험 제안을 측정합니다. 이는 기존의 멀티모달 대화형 AI가 대학교 수준의 문제에만 초점을 맞춘 것과는 달리, 실제 과학적 발견에 필요한 복잡한 멀티모달 추론을 다룹니다.
- ***Technical Details***: MicroVQA는 생물학 연구 경험이 풍부한 전문가들이 다양한 현미경 기법을 사용해 생성한 1,042개의 다중 선택 질문(MCQs)으로 구성됩니다. 각 질문은 30분 이상 소요되며, 잘못된 언어적 단서를 제거하기 위해 새로운 두 단계 파이프라인을 사용하여 MCQ를 최적화하여 언어 지름길 유발을 방지합니다.
- ***Performance Highlights***: 최신 MLLM(Multimodal Large Language Model)에서는 최고 53%의 성능을 보였고, 작은 모델들도 전체적으로 약간의 성능 저하만 보였습니다. 이는 언어 기반의 추론보다 멀티모달 추론이 더욱 도전적이라는 것을 의미합니다. 또한, 과학 기사를 사용한 미세 조정이 성능을 향상시킨다는 점도 발견했습니다. 시각적 추론이 현 시점의 MLLM에게 큰 도전과제로 남아 있으며, 이는 향후 연구의 방향성을 제시합니다.

### [Multimodal Chain-of-Thought Reasoning: A Comprehensive Survey](https://arxiv.org/abs/2503.12605)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12605.png)

Vote: 14

Authors: Yuecheng Zhang, William Wang, Jiebo Luo, Shengqiong Wu, Yaoting Wang, Ziwei Liu, Hao Fei

- ***What's New***: Multimodal Chain-of-Thought Reasoning(MCoT)는 사람의 논리적 사고 체계를 여러 모달리티에서 확장하여 대형 멀티모달 모델(Multimodal Large Language Models; MLLMs)과의 통합을 통해 주요 연구 관심을 모으고 있습니다. 이 논문은 MCoT에 대한 최초의 체계적인 설문조사를 제공하며, MCoT가 로봇 공학, 의료, 자율 주행 및 멀티모달 생성과 같은 다양한 분야에서 어떻게 적용되고 있는지를 보여줍니다.
- ***Technical Details***: MCoT는 이미지, 비디오, 음성, 오디오, 3D 및 구조화된 데이터와 같은 여러 모달리티에서 발생하는 고유한 문제를 해결하기 위해 여러 가지 방법론과 혁신적인 추론 패러다임을 설계하며, 체인(Chain), 트리(Tree), 그래프(Graph)와 같은 다양한 사고 구조(Box of Thoughts)를 적용하여 인과 관계를 설명하고 높은 수준의 해석력을 제공합니다.
- ***Performance Highlights***: MCoT의 효과는 자율 주행, 로봇 공학, 헬스케어 등에서 입증되어 멀티모달 AGI를 향한 기반 기술로 자리 잡고 있습니다. 그러나 MCoT는 더 높은 효율성과 적응력을 위한 새로운 알고리즘적 혁신이 필요하며, 멀티모달 정보의 통합과 해석에서의 과제 및 불명확성을 해결하기 위한 연구가 지속되고 있습니다.

### [reWordBench: Benchmarking and Improving the Robustness of Reward Models with Transformed Inputs](https://arxiv.org/abs/2503.11751)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.11751.png)

Vote: 14

Authors: Zhaofeng Wu, Marjan Ghazvininejad, Andrew Cohen, Asli Celikyilmaz, Yoon Kim, Michihiro Yasunaga

- ***What's New***: reWordBench는 보상 모델(Reward Models; RMs)의 강건성(Robustness)을 평가하기 위해 입력 변환을 활용하는 새로운 벤치마크입니다. 기존 보상 모델들이 입력의 미세한 변환에도 성능 저하를 보이며 불안정함(Brittleness)을 드러낸다는 사실을 밝혀내어, 이를 개선하기 위한 새로운 훈련 기법을 제안합니다.
- ***Technical Details***: reWordBench는 RewardBench에서 다양한 의미나 순위 비교 변환을 통해 입력을 변형시켜 보상 모델의 성능을 평가합니다. 강건성을 향상시키기 위해, 원본 입력과 패러프레이즈(paraphrase)된 입력에 유사한 점수를 부여하도록 보상 모델을 정규화(Reguralization)합니다. 이는 다른 종류의 변환에 대해 일반화되어 강건성 향상을 가져옵니다.
- ***Performance Highlights***: 패러프레이즈 데이터로 정규화된 보상 모델은 기존 모델 대비 많은 경우에서 성능 저하를 절반으로 줄이며, 특히 RewardBench의 Chat Hard subset에서 약 16%의 성능 저하를 줄이는 데 성공했습니다. 이 결과는 정규화된 보상 모델이 하위 단계의 정렬에서 더 높은 품질의 출력을 생성함을 보여주며, 이를 통해 최대 59%의 사례에서 표준 보상 모델을 능가했습니다.

### [R1-VL: Learning to Reason with Multimodal Large Language Models via Step-wise Group Relative Policy Optimization](https://arxiv.org/abs/2503.12937)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12937.png)

Vote: 13

Authors: Jiaxing Huang, Shunyu Liu, Xikun Zhang, Dacheng Tao, Jingyi Zhang, Shijian Lu, Huanjin Yao

- ***What's New***: R1-VL는 StepGRPO(Step-wise Group Relative Policy Optimization)라는 새로운 온라인 강화 학습 프레임워크를 개발하여 멀티모달 대형 언어 모델(MLLMs)의 추론 능력을 개선합니다. 이는 MLLM이 긍정적인 추론 경로를 단순히 모방하는 것에 그치지 않고, 자체적인 강화를 통해 추론 능력을 향상시키도록 돕습니다.
- ***Technical Details***: StepGRPO는 단계별 추론 보상 메커니즘을 도입하여 MLLMs가 고유한 추론 능력을 발전시킬 수 있도록 설계되었습니다. 두 가지 주요 보상인 Step-wise Reasoning Accuracy Reward(StepRAR)와 Step-wise Reasoning Validity Reward(StepRVR)가 사용됩니다. StepRAR는 핵심 단계 매칭 기술을 활용하여 중요한 중간 추론 단계를 포함하는 경로에 보상을 주고, StepRVR은 논리적 완전성 및 구조화를 평가하여 일관성 있는 추론 경로를 장려합니다.
- ***Performance Highlights***: R1-VL 모델은 MathVista 및 다양한 벤치마크에서 기존 최첨단 MLLMs를 능가하며 특히 수학적 추론 작업에서 우수한 성능을 보였습니다. 예를 들어, R1-VL-7B는 MathVista에서 Mulberry-7B와 LlamaV-o1-11B보다 각각 0.6%, 9.3% 더 높은 성과를 기록하였습니다. 이는 StepGRPO가 제공하는 풍부한 단계별 보상 메커니즘 덕분에 가능한 결과입니다.

### [V-STaR: Benchmarking Video-LLMs on Video Spatio-Temporal Reasoning](https://arxiv.org/abs/2503.11495)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.11495.png)

Vote: 10

Authors: Chenyang Si, Wei Li, Zixu Cheng, Jian Hu, Ziquan Liu, Shaogang Gong

- ***What's New***: V-STaR은 비디오-대형 언어 모델(Video-LLMs)의 시공간적 추론 능력(spatio-temporal reasoning ability)을 평가하기 위한 새로운 벤치마크입니다. 이 벤치마크는 시공간적 관계를 명시적으로 평가하여 모델이 비디오 이해에서 객체 간의 상호작용을 얼마나 잘 이해하는지를 분석합니다.
- ***Technical Details***: V-STaR 벤치마크는 Reverse Spatio-Temporal Reasoning (RSTR)이라는 새로운 평가 과제를 도입하여 비디오-대형 언어 모델이 'what', 'when', 'where'의 세 가지 요소를 기반으로 논리를 적용하는 방식을 평가합니다. 이 과정은 사람이 문제를 해결하는 방식과 유사한 체인 방식의 논리(cognitive process)를 따르며, 세밀한 추론 과정을 평가하기 위해 GPT-4 기반의 반자동 파이프라인을 사용해 데이터셋을 구성합니다.
- ***Performance Highlights***: 14개의 Video-LLM 실험에서, GPT-4o와 Gemini-2-Flash가 시공간적 추론에서 높은 성능을 보였으나 여전히 전반적인 스코어는 낮았습니다. Qwen2.5-VL이 특히 균형 잡힌 성능을 보이며 상위 오픈 소스 모델로 나타났습니다. 그러나 대다수 모델은 spatio-temporal reasoning의 일관성을 유지하는 데 어려움을 겪고 있습니다.

### [Free-form language-based robotic reasoning and grasping](https://arxiv.org/abs/2503.13082)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.13082.png)

Vote: 9

Authors: Yiming Wang, Francesco Giuliari, Alice Fasoli, Sergio Povoli, Runyu Jiao, Matteo Bortolon, Fabio Poiesi, Guofeng Mei

- ***What's New***: FreeGrasp는 사전학습된 VLMs(Vision-Language Models) 지식을 활용하여 로봇의 잡기(Grasping) 문제를 해결하는 혁신적인 접근 방식으로, 자유형 언어 기반 명령을 해석하고 객체의 공간적 관계를 추론합니다. FreeGrasp는 MetaGraspNetV2 데이터셋을 확장하여 FreeGraspData라는 평가 데이터셋을 생성하였으며, 인간이 기술한 명령과 실제 잡기 순서를 포함합니다.
- ***Technical Details***: FreeGrasp 시스템은 주어진 RGB 이미지에서 모든 객체를 키포인트로 감지하고, 이를 마크 기반(Mark-based) 시각적 프롬프트로 주석하여 명확한 추론을 돕습니다. 인간의 명령에 의해 객체의 최적의 손잡이(Grasp)를 결정하며, 탐색된 객체의 ID와 클래스 이름을 기반으로 객체 분할(Object Segmentation)을 수행합니다. 실제 로봇 실험과 합성 데이터로 방법의 유효성을 검증하였습니다.
- ***Performance Highlights***: FreeGrasp는 ThinkGrasp와 비교하여 모든 난이도에서 우수한 성능을 보였습니다. 특히 중간 및 어려운 시나리오에서 FreeGrasp는 VLM 기반의 객체 검사 및 마크 기반 시각적 프롬프트 덕분에 객체 모호성 문제를 효과적으로 처리합니다. 연구 결과는 FreeGrasp의 효율성과 효과성을 최고 성능으로 입증하며, 실제 로봇 실험에서는 잡기 성공률과 경로 효율성 측면에서의 우위를 보였습니다.

### [VideoMind: A Chain-of-LoRA Agent for Long Video Reasoning](https://arxiv.org/abs/2503.13444)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.13444.png)

Vote: 8

Authors: Mike Zheng Shou, Ye Liu, Kevin Qinghong Lin, Chang Wen Chen

- ***What's New***: VideoMind는 비디오 언어 이해를 위한 혁신적인 에이전트로서, 비디오의 시간적 의미를 정확히 이해하는 역할 기반 워크플로우를 도입했습니다. 역할 기반의 워크플로우에는 플래너(Planner), 그라운더(Grounder), 검증자(Verifier), 응답자(Answerer) 등이 포함됩니다. 또한, 이 역할들을 효율적으로 결합하기 위해 체인-오브-LoRA(Chain-of-LoRA) 전략을 제안하여 여러 모델의 오버헤드를 피하면서 효율성과 유연성을 달성했습니다.
- ***Technical Details***: VideoMind의 설계에서는 비디오의 시간적 논리를 이해하기 위해 필수적인 여러 역량을 확인하고, 플래너, 그라운더, 검증자, 응답자로 구성된 역할 기반의 에이전트 워크플로우를 개발했습니다. 그라운더는 시간적으로 비디오 순간을 위치시키고, 검증자는 순간의 정확성을 평가하며, 응답자는 질문 답변을 담당합니다. 체인-오브-LoRA 전략은 기본적인 대형 멀티모달 모델(MLLM, Multi-modal Large Language Model)을 기반으로 구축되어, 가벼운 LoRA 어댑터를 통해 역할 전환을 용이하게 합니다.
- ***Performance Highlights***: VideoMind는 14개의 공개 벤치마크에서 최첨단 성능을 보여주며, 각각 3개의 근거된 비디오 QA, 6개의 비디오 시간적 근거, 5개의 일반적인 비디오 QA 작업에 대한 성능을 입증했습니다. 특히 긴 비디오(CGBench, 27분 기준)에서의 신뢰성 있는 성능을 입증하며 GPT-4o와 같은 뛰어난 모델을 능가했습니다. 더 작은 2B 모델도 상당한 성능 향상을 보였으며, 체인-오브-LoRA 전략이 전통적인 미세 조정 방법에 비해 두드러진 계산 효율성을 달성한 것으로 나타났습니다.

### [MTV-Inpaint: Multi-Task Long Video Inpainting](https://arxiv.org/abs/2503.11412)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.11412.png)

Vote: 7

Authors: Pengfei Wan, Zheng Gu, Liang Hou, Xin Tao, Xiaodong Chen, Jing Liao, Shiyuan Yang

- ***What's New***: MTV-Inpaint는 최신 멀티태스크 비디오 인페인팅(Multi-Task Video Inpainting) 프레임워크로, 객체 삽입 및 장면 완성 작업을 통합하여 긴 비디오를 처리할 수 있는 점이 특징입니다. 특히, 텍스트와 이미지 기반의 가이드를 통해 사용자 정의가 가능하며, 이 과정에서 사용자 입력의 컨트롤러블(Control) 및 유연성이 향상되었습니다.
- ***Technical Details***: MTV-Inpaint는 독특한 듀얼-브랜치(spatial attention)와 공유되는 시공간적 주의 메커니즘을 사용하여 비디오 인페인팅 작업을 해결합니다. T2V(Text-to-Video)와 I2V(Image-to-Video) 캡처링 모드를 통해 다양한 입력을 수용하며, 긴 비디오에 대해서는 키프레임을 이용한 두 단계의 인페인팅 파이프라인을 제안하여 시공간적 일관성을 유지합니다.
- ***Performance Highlights***: 실험 결과에 따르면 MTV-Inpaint는 객체 삽입 및 장면 완성 작업에서 기존의 베이스라인을 초과하는 최첨단 성능을 보여줍니다. 또한, 멀티모달 인페인팅 및 객체 편집과 제거와 같은 파생 응용을 지원하며, 전반적인 영상 품질(Visual Quality) 지표에서도 뛰어난 결과를 보였습니다.

### [Long-Video Audio Synthesis with Multi-Agent Collaboration](https://arxiv.org/abs/2503.10719)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10719.png)

Vote: 6

Authors: Yingcong Chen, Xiaojie Xu, Yehang Zhang, Xinli Xu, Li Liu

- ***What's New***: LVAS-Agent는 장편 비디오 오디오 합성(Long-Video Audio Synthesis)을 위한 멀티 에이전트 협업 프레임워크로, 전문 더빙 워크플로우를 모방하여 비디오 스크립트 생성, 오디오 설계, 고품질 오디오 합성을 자동화합니다. LVAS-Bench라는 첫 번째 장편 비디오 오디오 합성 벤치마크도 공개되어 장편 비디오 더빙의 표준화된 평가를 지원합니다.
- ***Technical Details***: LVAS-Agent는 장편 비디오를 의미적으로 분할하고, 시각적 요소를 CLIP 기반 기능으로 분석하여 스크립트를 생성한 후, 스펙트럴 살리언시 분석을 통해 전경 및 배경 소리를 분리합니다. 마지막으로 Text-to-Speech(TTS)와 분산 기반 환경 효과를 혼합하여 결합된 오디오를 합성합니다. 이 과정에서 두 가지 협업 전략인 Discussion-Correction과 Generation-Retrieval-Optimization을 사용하여 장면합성 및 스크립트 개선을 반복적으로 수행합니다.
- ***Performance Highlights***: 실험 결과, LVAS-Agent는 기존의 VTA(Video-to-Audio) 모델보다 더 높은 음향 품질, 의미적 일치, 및 시간 정렬을 통해 장편 비디오에 대한 더 나은 오디오 합성을 가능하게 했습니다. 이러한 성과는 추가적인 대규모 데이터 학습 없이도 이루어졌으며, 다양한 평가 지표에서 기존 베이스라인보다 우수한 성능을 보여주었습니다.

### [Rewards Are Enough for Fast Photo-Realistic Text-to-image Generation](https://arxiv.org/abs/2503.13070)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.13070.png)

Vote: 6

Authors: Weijian Luo, Jing Tang, Yihong Luo, Tianyang Hu, Kenji Kawaguchi

- ***What's New***: 이 논문은 R0라는 새로운 접근 방식을 소개하며, 보상 극대화를 통해 사진과 같은 품질의 빠른 텍스트-이미지 생성(Text-to-Image Generation)을 가능하게 합니다. 기존의 확산 유도 손실에 의존하는 대신, 조건이 복잡한 상황에서 보상이 주요한 역할을 감당함을 강조하며, 새로운 인간 중심 및 보상 중심 생성 패러다임을 제안합니다.
- ***Technical Details***: R0는 정규화된 보상 극대화(Regularized Reward Maximization)를 통한 조건부 생성 접근 방식입니다. 이미지 생성을 데이터 공간에서의 최적화 문제로 간주하며, 유효한 이미지를 탐색하는 것을 목표로 합니다. 생성기 파라미터화 및 적절한 정규화 기법을 통해 최첨단 몇 단계의 텍스트-이미지 생성 모델을 훈련할 수 있습니다. 특히, 다단계의 확산 생성 출력을 통해 보상 함수를 극대화하는 방법을 포함하고 있습니다.
- ***Performance Highlights***: 이 논문에서는 R0와 R0+가 이전의 확산 유도를 통한 보상 극대화 방법들과 비교했을 때, 더 나은 이미지 품질 및 텍스트-이미지 정합성을 보여준다는 것을 퀄리티와 정량적 결과를 통해 입증하였습니다. 특히, R0는 Human Preference Score (HPS), Aesthetic Score (AeS), 그리고 CLIP Score와 같은 다양한 평가 지표에서 SOTA 성능을 달성하였습니다.

### [Error Analyses of Auto-Regressive Video Diffusion Models: A Unified Framework](https://arxiv.org/abs/2503.10704)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10704.png)

Vote: 5

Authors: Xiaoli Li, Chao Du, Vincent Y. F. Tan, Fengzhuo Zhang, Zhuoran Yang, Jing Wang, Aixin Sun, Tianyu Pang

- ***What's New***: 이 논문은 Auto-Regressive Video Diffusion Models (ARVDM)의 이론적 분석을 제공하며, Meta-ARVDM이라는 통합 프레임워크를 개발하여 기존 ARVDM 방법들을 포함합니다. 이를 통해 ARVDM이 가지고 있는 두 가지 주요 문제인 오류 누적(Error Accumulation)과 메모리 병목(Memory Bottleneck)을 분석하고, 메모리 병목 문제를 완화하기 위한 새로운 네트워크 구조를 설계하였습니다.
- ***Technical Details***: Meta-ARVDM은 대부분의 기존 ARVDM 방법을 포괄하는 통합 프레임워크로, 오류 분석을 통해 비디오 생성 과정에서 발생하는 네 가지 오류 소스, 특히 ARVDM에 고유한 메모리 병목을 확인했습니다. 이를 해결하기 위해 과거 프레임 정보를 활용한 프레임 압축 및 네트워크 구조를 설계하였으며, 이를 통해 추론 효율성과 성능 간 개선된 절충을 달성하였습니다.
- ***Performance Highlights***: 실험 결과는 DMLab 및 Minecraft에서 우리의 방법론이 효과적임을 보여주며, 오류 누적과 메모리 병목 간의 상관 관계를 밝혔습니다. 메모리 병목을 효율적으로 완화함으로써 비디오 생성 품질이 개선되었으며, 프레임 압축을 통해 효율성을 높였습니다.

### [Sightation Counts: Leveraging Sighted User Feedback in Building a BLV-aligned Dataset of Diagram Descriptions](https://arxiv.org/abs/2503.13369)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.13369.png)

Vote: 3

Authors: Haemin Choi, Sangryul Kim, Wan Ju Kang, James Thorne, Ki Hoon Kwak, Eunki Kim, Na Min An

- **What's New**: Sightation은 시각 사용자 피드백을 활용하여 시각장애(BLV) 사용자가 이해하기 쉬운 다이어그램 설명 데이터셋을 최초로 구축했습니다. 이 연구는 시각적 설명 생성을 위한 기존의 인력 의존 방식을 대체하여 비용과 편향 문제를 극복하고, 대규모 BLV 맞춤형 데이터셋을 제시합니다.
- **Technical Details**: Sightation은 5천 개의 다이어그램과 13만 7천 개의 샘플로 구성된 데이터셋으로, 다중 통과 추론(multi-pass inference)을 통해 시각-언어 모델(Vision-Language Model; VLM)이 BLV 사용자를 대상으로 한 설명을 생성하도록 유도합니다. 시각 사용자가 VLM이 생성한 설명을 평가하여 BLV 교수진이 가르치는 학습자들에게 유용한 피드백을 제공합니다.
- **Performance Highlights**: BLV 교사들이 평가한 결과, 2B 모델은 설명의 다양성 면에서 0.9σ 증가, 요약 유용성에서 0.4σ 증가를 보였습니다. 이 모델은 ChartGemma(3B)의 8개 자동 메트릭 중 11개에서 더 나은 성능을 나타냈습니다. Retrieval task에서는 SIGHTATIONRETRIEVAL로 튜닝된 모델이 COCO test set에 비해 Precision@1에서 65%p 향상되었습니다.

### [CHOrD: Generation of Collision-Free, House-Scale, and Organized Digital Twins for 3D Indoor Scenes with Controllable Floor Plans and Optimal Layouts](https://arxiv.org/abs/2503.11958)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.11958.png)

Vote: 2

Authors: Chong Su, Shaojun Wang, Fangcheng Zhong, Yingbin Fu, Xuan Zhao, Zheyuan Hu, Cengiz Öztireli, Param Hanji, Jing Yang

- ***What's New***: CHOrD는 복잡한 평면 및 최적화된 레이아웃을 가진 3D 실내 씬의 디지털 트윈을 충돌 없이 생성할 수 있는 새로운 프레임워크입니다. 이는 기존 방법들과 달리 2D 이미지 기반의 중간 레이아웃 표현을 도입해 충돌 아티팩트를 생성 과정에서 방지하며, 다양한 형태의 입출력을 통해 제어 가능한 하우스 스케일 레이아웃을 생성 가능합니다.
- ***Technical Details***: CHOrD 프레임워크는 2D 이미지 기반 레이아웃 표현을 중간 단계로 사용하여 계층적으로 구성된 씬 그래프를 생성합니다. 이를 통해 각 물체의 충돌 아티팩트를 OOD(Out-Of-Distribution) 시나리오로 처리하여 효율적으로 제거할 수 있습니다. 또한, 다중 모달 입력을 활용해 복잡한 평면 구조를 적응적으로 처리하며, YOLOv8 모델을 활용해 물체 검출을 수행하여 공간적 관계를 정의합니다.
- ***Performance Highlights***: CHOrD는 3D-FRONT와 제안된 데이터셋에서 모두 최신 성능을 나타냅니다. 특히, 물체 충돌을 거의 제거하는데 있어 뛰어난 성능을 보이며, 기존 메서드와 비교해 전체적인 데이터 품질 및 커버리지를 크게 향상시켰습니다. 이를 통해 CHOrD는 실제 사용 사례에서 신뢰할 수 있는 정교한 레이아웃을 보장합니다.

### [Investigating Human-Aligned Large Language Model Uncertainty](https://arxiv.org/abs/2503.12528)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12528.png)

Vote: 2

Authors: Kyle Moore, Pamela Wisniewski, Jesse Roberts, Daryl Watson

- ***What's New***: 이 연구는 대형 언어 모델(Large Language Model; LLM) 불확실성을 인간의 불확실성과 비교하여 고찰한 최초의 연구입니다. 여러 가지 불확실성 측정 방법을 분석하였으며, 인간 집단의 불확실성과 가장 일치하는 것을 탐구했습니다. 특히, Top-k 엔트로피(Top-k Entropy)와 베이지안(Bayesian) 방법론이 모델의 크기에 따라 인간 행동과 일치하는 경향이 있음을 발견했습니다.
- ***Technical Details***: LLM에서의 불확실성 측정은 여러 방법으로 나눌 수 있으며, 대표적인 것으로는 자기 보고(Self-reported), 일관성 기반(Consistency), 로짓 기반(Logit-based), 엔트로피 기반(Entropy-based), 앙상블 기반(Ensemble-based) 방법이 있습니다. 본 연구에서는 Top-k 엔트로피, 선택 엔트로피(Choice Entropy), 누클리어스 사이즈(Nucleus Size), 인구 분산(Population Variance) 등의 다양한 언서턴티 측정을 사용하여 인간 불확실성과의 상관관계를 분석했습니다.
- ***Performance Highlights***: Top-k 엔트로피는 모델 크기가 커질수록 인간 불확실성과의 일치도가 감소하는 경향을 보였으며, 반면에 NS(누클리어스 사이즈)와 CE(선택 엔트로피)는 모델 크기에 큰 영향을 받지 않고 상당한 일치도를 나타냈습니다. 혼합된 불확실성 측정치를 사용하면 모델 크기에 대한 의존성을 줄일 수 있으며 인간 유사성을 비교할 때 고성능을 나타냈습니다.

### [Training Video Foundation Models with NVIDIA NeMo](https://arxiv.org/abs/2503.12964)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12964.png)

Vote: 2

Authors: Hao Wang, Nima Tajbakhsh, Oleg Sudakov, Xiaowei Ren, Ashwath Aithal, Linnan Wang, Yan Bai, Niket Agarwal, Sriharsha Niverty, Forrest Lin, Ekaterina Sirazitdinova, Ethan He, Shanmugam Ramasamy, Mingyuan Ma, Parth Mannan, Tommy Huang, Joseph Jennings, Vasanth Rao Naik Sabavat, Pallab Bhattacharya, Bobby Chen, Jacob Huffman, Ryan Wolf, Carl Wang, David Page, Sahil Jain, Rong Ou, Jack Chang, Zeeshan Patel, Zhuoyao Wang

- ***What's New***: NVIDIA NeMo를 사용한 대규모 Video Foundation Models (VFMs) 훈련에 대한 새로운 오픈 소스 프레임워크가 소개되었습니다. 이 프레임워크는 비디오 데이터 세트 큐레이션을 가속화하고, 멀티모달 데이터 로딩 및 비디오 디퓨전 모델의 병렬 훈련을 지원합니다.
- ***Technical Details***: NVIDIA NeMo는 NeMo Curator를 통한 비디오 큐레이션, Megatron Energon을 통한 멀티모달 데이터 로딩, Megatron Core를 통한 확장 가능한 비디오 디퓨전 모델 훈련 기능을 제공합니다. 클라우드 기반 WebDataset 포맷과 함께 대규모 데이터 처리 및 클립 생성 파이프라인을 지원하며, 이 과정에서 GPU 가속을 통해 큰 성능 향상을 도모합니다.
- ***Performance Highlights***: 비디오 디퓨전 모델 훈련에서 proposed 現던 방법들은 4D parallelization을 통해 기존보다 높은 MFU (Model FLOPs Utilization)를 달성합니다. 특정 구성에서 최대 1.85배의 성능을 보여주며, 모델 크기가 28B까지 스케일링 되는 동안에도 높은 성능을 유지합니다. 또한, inference 효율성을 높이기 위해 context parallelism을 통한 알고리즘을 제안하며, 다양한 설정에서 80-90%의 스케일링 효율성을 보고합니다.

### [Basic Category Usage in Vision Language Models](https://arxiv.org/abs/2503.12530)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12530.png)

Vote: 2

Authors: Kyle Moore, Jesse Roberts, Hunter Sawyer

- ***What's New***: 이 논문은 최근 공개된 두 가지 비전-언어 모델(Vision-Language Models; VLMs)인 Llama 3.2 Vision Instruct(11B)와 Molmo 7B-D에서 인간의 기본 수준 분류 행동이 일관된다는 것을 처음으로 연구했습니다. 또한 이 논문은 생물학적 vs 비생물학적 기본 수준 효과와 전문가 기본 수준 이동과 같은 인간의 복잡한 행동이 모델에 반영되어 있음을 보여줍니다.
- ***Technical Details***: 심리학적 기본 수준 분류 현상은 상향식(Bottom-up) 수준에서 주로 사용되며 정보 밀도가 높다는 특징을 가지고 있습니다. 이 논문에서는 KIetzmannLab의 Ecoset 데이터셋을 사용하여 총 565개의 기본 카테고리로 구성된 약 1.5백만 개의 이미지에서 VLM의 분류 행동을 평가했습니다. 모델들은 이미지에 최소 길이의 설명을 생성하도록 프롬프트를 받았으며, 기본 수준 설명은 최소 길이 제약 조건 아래에서 선정되었습니다.
- ***Performance Highlights***: Llama 3.2 모델은 기본 수준 카테고리화를 60%로 진행하였고, Molmo 7B-D 모델은 52%로 나타났습니다. 생물학적 vs 비생물학적 카테고리의 경우, 비생물학적 객체에 대해 두 모델 모두 기본 수준 카테고리 사용이 통계적으로 유의미하게 낮았습니다. 전문가 프롬프트 하에서는 Llama 3.2의 기본 수준 사용이 54%로 떨어졌고, Molmo 7B-D는 49%로 감소했습니다.

### [GenStereo: Towards Open-World Generation of Stereo Images and Unsupervised Matching](https://arxiv.org/abs/2503.12720)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12720.png)

Vote: 1

Authors: Feng Qiao, Nathan Jacobs, Eric Xing, Zhexiao Xiong

- ***What's New***: GenStereo는 시각적 품질과 기하학적 정확성을 동시에 해결하는 오픈 월드 스테레오 이미지 생성 및 비지도 학습 매칭을 위한 최초의 통합 프레임워크를 제안합니다. 이 방법은 시각적 일관성과 정확한 기하학적 제어를 통해 고품질 스테레오 이미지 생성과 비지도 스테레오 매칭에서 최첨단 성능을 달성합니다.
- ***Technical Details***: GenStereo는 두 가지 주요 혁신을 포함합니다: (1) 좌표 임베딩(Coordinate Embedding)과 모양 조정을 통한 분산을 기반으로 하는 편향 프로세스와 (2) 생성된 이미지와 조정된 이미지를 지능적으로 결합하는 적응적 융합 메커니즘. 다양한 장면과 기하학적 구성을 가진 11개의 스테레오 데이터세트에서 훈련하여 강력한 일반화 능력을 보입니다.
- ***Performance Highlights***: GenStereo는 Middlebury 2014와 KITTI 2015 테스트에서 최고 수준의 성능을 보여주었으며, 이는 전통적인 방법과 최신 기계 학습 방법을 압도합니다. 특별히, PSNR, SSIM, 그리고 LPIPS 측정 지표에서 우수한 결과를 기록하여 뛰어난 시각적 및 기하학적 일관성을 입증하였습니다.

### [WISA: World Simulator Assistant for Physics-Aware Text-to-Video Generation](https://arxiv.org/abs/2503.08153)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.08153.png)

Vote: 0

Authors: Zhanjie Zhang, Xiaodan Liang, Shanyuan Liu, Yuhui Yin, Jun Zheng, Jiasong Feng, Yuhang Ma, Ke Cao, Bo Cheng, Jing Wang, Ao Ma, Dawei Leng

- ***What's New***: WISA는 물리적 법칙을 고려한 텍스트-비디오 생성 모델(Text-to-Video Generation)을 위한 월드 시뮬레이터 어시스턴트(World Simulator Assistant)라는 새로운 프레임워크를 제안합니다. 기존 모델들이 추상적인 물리 원칙을 이해하는 데 어려움을 겪는 반면, WISA는 물리 원칙을 텍스트 설명, 질적 물리 범주, 그리고 양적 물리 속성으로 분해하여 모델에 적용함으로써 물리적 인식을 향상시킵니다.
- ***Technical Details***: WISA는 17개의 물리적 현상을 세 개의 물리 영역인 역학(Dynamics), 열역학(Thermodynamics), 광학(Optics)으로 나누어 다룹니다. 각 물리 범주는 Mixture-of-Physical-Experts Attention (MoPA)에 의해 전문가 헤드가 할당되어 물리 현상에 따라 활성화됩니다. 질적 및 양적 물리 속성은 AdaLN을 통해 모델에 주입되며, 물리적 분류기는 물리 범주 인식을 도와 물리 속성을 이해하도록 설계되었습니다.
- ***Performance Highlights***: WISA는 기존 텍스트-비디오 모델에서 물리적 법칙과의 일치성을 크게 향상시켰습니다. 이를 통해 비디오 생성의 현실감을 높이고, VideoPhy 및 PhyGenBench 벤치마크에서 최고 수준의 성능을 보여주었습니다. WISA는 비약적인 성능 개선을 이루며, 학습 및 추론 속도는 각각 3.5%와 5%만 증가시켜 효율성 또한 놓치지 않았습니다.

### [Using Mechanistic Interpretability to Craft Adversarial Attacks against Large Language Models](https://arxiv.org/abs/2503.06269)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.06269.png)

Vote: 0

Authors: Boussad Addad, Katarzyna Kapusta, Thomas Winninger

- ***What's New***: 이 논문은 메커니즘 해석(Mechanistic Interpretability)을 활용하여 LLMs에 대한 실제적이고 효율적인 역상 공격(adversarial attacks)을 설계하는 새로운 화이트박스 접근법을 제안합니다. 이 방법은 수용 서브스페이스(acceptance subspaces)와 거부 서브스페이스(refusal subspaces)를 식별하여 삽입된 악성 접미사로 임베딩을 재경로화하는(Subspace Rerouting) 개념을 통해 공격을 수행합니다.
- ***Technical Details***: 이 연구에서는 대형 언어 모델의 거부 서브스페이스에서 수용 서브스페이스로 임베딩을 재경로화하는 Subspace Rerouting(SSR) 기법을 개발했습니다. 이를 위해 선형 분류기(linear classifiers)를 사용하여 머신 러닝 모델의 거부 및 수용 서브스페이스를 구별하고, 사전 훈련된 시그모이드 출력 선형 프로브를 활용하여 공격 성공률을 최적화하는 방법을 소개합니다.
- ***Performance Highlights***: Subspace Rerouting 기법을 사용한 공격은 Gemma2, Llama3.2, Qwen2.5와 같은 최첨단 모델에서 80-95%의 높은 공격 성공률을 보였으며, 기존 기법에 비해 시간 소요를 크게 줄였습니다. 연구에 따르면 Llama3.2 1b 모델을 대상으로 한 사례에서 16초 만에 벤치마크를 기록했으며, 간단한 로지스틱 회귀 분류기를 활용하여 95% 이상의 정확도로 유해 콘텐츠를 식별할 수 있었습니다.

