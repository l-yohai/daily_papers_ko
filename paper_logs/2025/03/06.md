## Daily Papers (2025-03-06)

### [Babel: Open Multilingual Large Language Models Serving Over 90% of Global Speakers](https://arxiv.org/abs/2503.00865)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.00865.png)

Vote: 40

Authors: Chaoqun Liu, Mahani Aljunied, Yue Deng, Zhaodonghui Li, Lidong Bing, Yiran Zhao, Hou Pong Chan, Deli Zhao, Wenxuan Zhang, Yu Rong, Jiahao Ying

- ***What's New***: Babel은 전 세계 인구의 90% 이상을 포괄하도록 설계된 오픈 소스 다국어 대형 언어 모델(Open Multilingual Large Language Models)을 소개합니다. 이 모델은 25개의 상위 언어를 지원하며, 많은 오픈 소스 다국어 LLMs에서 간과된 언어들을 포함합니다. Babel은 전통적인 사전 훈련 접근방식과 달리 성능 상한을 높이기 위해 '레이어 확장(layer extension)' 기법을 사용합니다.
- ***Technical Details***: Babel 모델은 두 가지 변형을 제공합니다: Babel-9B와 Babel-83B. 이들은 각각 효율적인 추론과 미세 조정(fine-tuning)을 위한 Babel-9B, 그리고 새로운 다국어 LLM 표준을 설정하는 Babel-83B입니다. 레이어 확장은 모델 성능의 상한을 높이고 새 층을 기존 구조와 동일한 형식으로 추가하여 확장합니다. 초기 평가를 통해 가장 성능에 미치는 영향을 최소화하는 방법을 선택하였습니다.
- ***Performance Highlights***: Babel-9B-Base는 10B 크기의 모델들 사이에서 최고 점수 63.4를 기록하며 XCOPA, MGSM, XNLI, Flores-200 벤치마크에서 최고 성과를 보였습니다. Babel-83B-Base는 LLMs 간의 비교에서 평균 73.2 점으로 가장 높은 성능을 기록했습니다. 특히 상업용 모델 GPT-4o와의 경쟁에서도 뛰어난 성능을 발휘하여 매우 신뢰할 수 있는 다국어 학습 능력을 입증했습니다.

### [Process-based Self-Rewarding Language Models](https://arxiv.org/abs/2503.03746)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.03746.png)

Vote: 21

Authors: Xin Zhang, Junxiao Liu, Xiao Liu, Shujian Huang, Yeyun Gong, Shimao Zhang, Zheheng Luo

- ***What's New***: 이 논문은 수식 연산 시나리오에서 전통적인 자기보상(Self-Rewarding) 방법의 한계를 극복할 수 있는 처리 기반 자기보상 언어 모델(Process-based Self-Rewarding Language Models)을 제안합니다. 이 새로운 패러다임은 LLM(Large Language Model)들이 복잡한 연산 문제를 단계별로 사고하고 판단함으로써 인간의 능력을 능가할 가능성을 보여줍니다.
- ***Technical Details***: 처리 기반 자기보상 패러다임에서는 모델이 단계별 LLM-판사(Judge) 역할을 수행하며 각 중간 단계별 선호 최적화(Preference Optimization)를 실시합니다. 이를 통해 결국 답을 이끌어내는 것뿐만 아니라, 중간 단계를 올바르게 생성할 수 있도록 모델을 훈련합니다. 이 과정에서 모델은 최적화 과정에서 세부적으로 선호 데이터를 생성하여 단계별 판별을 위한 데이터를 제공합니다.
- ***Performance Highlights***: 수학적 연산 벤치마크를 통한 실험 결과, 이 프로세스 기반 자기보상 패러다임은 기존의 자기보상 알고리즘에 비해 LLM의 수학적 추론 능력을 효과적으로 향상시켰습니다. 이는 모델이 더 정교한 자기보상 학습을 통해 단계별로 스스로 보상 신호를 생성해 나가며, 인공지능이 인간의 성능을 뛰어넘을 가능성을 시사합니다.

### [Societal Alignment Frameworks Can Improve LLM Alignment](https://arxiv.org/abs/2503.00069)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.00069.png)

Vote: 15

Authors: Mehar Bhatia, Nicholas Meade, Timothy P. Lillicrap, Siva Reddy, Konstantin Böttinger, Ana Marasović, Nicolas Papernot, Jason Stanley, Sylvie Delacroix, Karolina Stańczak, Jessica Montgomery, Richard Zemel, Nicolas Chapados, Denis Therien, Gillian K. Hadfield, Jeremy Barnes, Hattie Zhou

- ***What's New***: 이 논문은 LLM(Large Language Models)의 정렬을 개선하기 위해 사회적 정렬 프레임워크(Societal Alignment Frameworks)를 도입하는 방안을 제공합니다. 정렬은 현대 AI의 중요한 과제로, 인간의 가치와 기대에 맞춰 모델을 조정하는 것을 말합니다. 이 프레임워크는 경제적, 계약적, 사회적 정렬 접근 방식을 통합하여 모델 정렬의 효과성을 높이고자 합니다.
- ***Technical Details***: 논문은 LLM 정렬을 '주인-대리인(Principal-Agent)' 개념으로 설명하며, 사용자(또는 개발자)가 주인(Principal) 역할을 하고, LLM이 대리인(Agent) 역할을 수행합니다. 이 관계는 주어진 행동(a)와 보상(r)이 포함된 계약으로 정의되며, 이를 통해 모델의 행동이 사용자의 목적에 맞도록 조정됩니다. 그러나 모든 상황을 사전에 예측하는 것은 불가능하므로, 불완전한 계약 문제가 존재하며 이는 정렬의 어려움의 근원으로 작용합니다.
- ***Performance Highlights***: 이 논문은 LLM 정렬을 불완전한 계약 문제로 프레임하여 접근하면서, 기존의 기술 중심적 해결책을 넘어서는 사회적 접근의 필요성을 강조합니다. 특히, 다양한 이해 관계자들이 참여하는 정렬 인터페이스 설계를 제안하여, LLM의 목적과 행동을 민주적으로 결정하는 기회를 제공합니다.

### [ABC: Achieving Better Control of Multimodal Embeddings using VLMs](https://arxiv.org/abs/2503.00329)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.00329.png)

Vote: 14

Authors: Wenhu Chen, Florian Kerschbaum, Benjamin Schneider

- ***What's New***: ABC는 새로운 멀티모달 임베딩 모델로서, 비전-언어 모델(Vision-Language Model; VLM)을 활용하여 이미지 특징과 자연어 지침을 깊이 통합합니다. 이는 MSCOCO 이미지-텍스트 검색에서 최상의 성능을 보여주며, 대규모 멀티모달 임베딩 벤치마크(MMEB)의 분류 및 VQA 작업에서도 최고의 성능을 나타냅니다.
- ***Technical Details***: ABC의 훈련은 두 단계로 이루어집니다. 첫 번째 단계에서는 대조적인 훈련을 통해 이미지-텍스트 쌍을 사용하여 고품질의 밀집 임베딩을 생성합니다. 두 번째 단계에서는 다양한 이미지 측면에 해당하는 자연어 지침을 사용하여 가벼운 어댑터를 훈련시킵니다. 이로 인해 유연한 자연어 제어가 가능한 강력한 멀티모달 임베딩 모델이 완성됩니다.
- ***Performance Highlights***: ABC 모델은 MSCOCO 데이터셋을 활용한 이미지-텍스트 검색에서 CLIP 기반 모델들을 능가하는 성과를 보였습니다. 또한, 19개 작업에 걸친 MMEB의 제로샷 분류 및 VQA 테스트에서 높은 성과를 보여주었습니다. CtrlBench 벤치마크를 통해 ABC가 자연어 지침을 활용해 본질적으로 모호한 시각적 검색 작업을 완수할 수 있음을 증명했습니다.

### [DoraCycle: Domain-Oriented Adaptation of Unified Generative Model in Multimodal Cycles](https://arxiv.org/abs/2503.03651)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.03651.png)

Vote: 13

Authors: Weijia Mao, Rui Zhao, Mike Zheng Shou

- ***What's New***: DoraCycle는 멀티모달 사이클(Multimodal Cycles)을 통합하여 특정 도메인에 적응 가능한 통합 생성 모델(Unified Generative Model)의 새로운 학습 프레임워크를 소개합니다. 새로운 도메인으로의 적응을 위해서는 대량의 쌍(pair)이 필요하지 않으며, 비페어드(Unpaired) 데이터만으로도 모델을 효과적으로 적응시킬 수 있도록 설계되었습니다.
- ***Technical Details***: DoraCycle는 텍스트-이미지-텍스트(T cycle)와 이미지-텍스트-이미지(I cycle)의 두 가지 멀티모달 사이클을 통합합니다. 사이클 반복의 끝에서 교차 엔트로피 손실(Cross-Entropy Loss)을 계산하여, 그 엔드포인트가 같은 모달리티를 공유하는 형태로 모델을 최적화합니다. 이러한 방식은 주석이 없는 텍스트-이미지 쌍이 없더라도 모델 자체적으로 발전할 수 있는 기회를 제공합니다. 실험적으로는 LoRA(Low-Rank Adaptation) 모듈을 사용하여 프리트레인(Pre-trained)된 생성 모델의 특정 층에 파라미터를 추가하여 도메인 적응을 돕습니다.
- ***Performance Highlights***: DoraCycle는 사이버펑크 스타일과 같은 비쌍 데이터를 사용하는 작업에서 높은 성능을 나타내며, 여러 도메인에서의 실험에서 최상급 또는 유사한 결과를 보여줍니다. 예를 들어, 스토리보드 데이터셋에서는 DreamBooth와 ITIT 같은 기존 방법들을 초월하는 성과를 나타내며, 적은 수의 쌍 데이터를 사용할 때에도 큰 성과를 보였습니다.

### [GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control](https://arxiv.org/abs/2503.03751)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.03751.png)

Vote: 13

Authors: Alexander Keller, Huan Ling, Thomas Müller, Jun Gao, Tianchang Shen, Merlin Nimier-David, Sanja Fidler, Yifan Lu, Xuanchi Ren, Jiahui Huang

- ***What's New***: 이번 연구에서는 GEN3C라는 비디오 생성 모델을 소개합니다. 이 모델은 카메라 컨트롤(Camera Control)과 3D 일관성(3D Consistency)을 통해 더욱 현실적이고 일관된 비디오를 생성할 수 있습니다. 이전 비디오 모델들과 달리 3D 정보의 활용이 적어 물체가 갑자기 사라지거나 나타나는 문제를 해결하기 어려웠던 것과 비교하여, GEN3C는 3D 캐시(cache)를 이용해 시드 이미지의 깊이를 예측하여 점구름(point clouds)을 생성하고 이를 기반으로 장면을 재구성합니다.
- ***Technical Details***: GEN3C는 사용자에 의해 제공된 카메라 트랙터리를 따라 3D 캐시를 2D로 렌더링하여 비디오 모델에 입력으로 사용합니다. 이러한 방법은 비디오 생성 시 이미지를 구조적으로 추론할 필요 없이, 보지 못한 지역에 대한 집중을 가능하게 합니다. 또한, 다중뷰의 경우 각각의 뷰에 대해 별도의 3D 캐시를 유지하여 잠재적 불일치와 정렬 문제를 해결합니다. 비디오 확산 모델(video diffusion model)을 사용하여 노이즈를 제거하고 고품질의 비디오로 변환합니다.
- ***Performance Highlights***: GEN3C는 드라이빙 시뮬레이션과 단일 이미지에서 다수의 카메라 뷰로 전환이 요구되는 상황에서도 소수의 뷰 정보만으로 가장 정밀한 카메라 컨트롤을 달성했습니다. 또한, 비디오 생성 실험에서는 기존의 재구성 방법보다 높은 품질의 결과를 보여주었습니다. 이는 현재의 sparse-view novel view synthesis의 한계를 뛰어넘고자 하는 연구에 중요한 기초 데이터를 제공합니다.

### [KodCode: A Diverse, Challenging, and Verifiable Synthetic Dataset for Coding](https://arxiv.org/abs/2503.02951)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.02951.png)

Vote: 12

Authors: Yang Liu, Zhangchen Xu, Radha Poovendran, Mingyuan Zhou, Yueqin Yin

- ***What's New***: KODCODE는 다양한 난이도와 도메인에 걸쳐 높은 품질의 검증 가능한 훈련 데이터를 제공하는 대규모 합성 데이터셋으로, 코딩 대형 언어 모델(Large Language Models; LLMs)의 훈련을 위한 새로운 접근법을 제시합니다. 질문, 솔루션 및 테스트 삼중 항목으로 구성되어 데이터의 정확성을 자체 검증 프로세스를 통해 보장합니다.
- ***Technical Details***: KODCODE는 12개의 다양한 소스에서 코딩 질문을 합성하고, 각 질문에 대해 솔루션 및 유닛 테스트를 생성하며, 자체 검증(Self-Verification)을 통해 솔루션의 정확성을 확인하는 세 단계의 파이프라인으로 구성됩니다. 이후 포스트 트레이닝 데이터 합성을 통해 질문 포맷을 다양화하고, DeepSeek R1을 사용해 새로운 응답을 생성합니다.
- ***Performance Highlights***: KODCODE로 파인튜닝된 모델은 HumanEval(+), MBPP(+), BigCodeBench, LiveCodeBench와 같은 코딩 벤치마크에서 최첨단 성능을 달성하였으며, Qwen2.5-Coder-32B-Instruct 및 DeepSeek-R1-Distill-Llama-70B와 같은 대형 모델들을 대부분의 실험에서 능가하였습니다.

### [Enhancing Abnormality Grounding for Vision Language Models with Knowledge Descriptions](https://arxiv.org/abs/2503.03278)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.03278.png)

Vote: 10

Authors: Julia A. Schnabel, Che Liu, Cosmin I. Bercea, Jun Li, Wenjia Bai, Rossella Arcucci

- ***What's New***: 이 논문에서는 비정상 감지를 위한 멀티모달 모델(Vision Language Models; VLMs)의 성능을 개선하기 위해 새롭게 개발된 지식 설명(Knowledge Descriptions) 기반 접근 방식을 제안합니다. 특히, 복잡한 의학 용어를 구체적인 시각적 속성으로 분해하여 텍스트 설명과 시각적 특징 간의 강한 정렬을 유도함으로써 의료 이미징에서의 비정상 감지 및 위치 지정이 개선되었습니다.
- ***Technical Details***: 이 접근 방식은 대형 VLM 대신 소규모 0.23B 파라미터의 모델을 활용하며, RadVLM 등과 같은 대형 모델에 비해 훨씬 적은 데이터로 유사한 성능을 달성합니다. 이를 위해 의료 이상을 형태(shape), 밀도(density), 위치(location) 같은 시각적 속성으로 표현하는 지식 설명을 사용하며, 이러한 속성을 모델 학습에 활용하여 광범위한 데이터셋 없이도 효과적인 비정상 감지가 가능합니다.
- ***Performance Highlights***: 본 연구에서는 제안된 모델이 VinDr-CXR 데이터셋에서 mAP50 측정 기준으로 대형 모델에 비해 더 높은 성능(25.5%)을 나타냈으며, zero-shot 설정에서도 기존의 다른 모델과 비교하여 경쟁력 있는 성과를 거두었습니다. 특히, PADChest-Known 데이터셋에서 로데오(RoDeO) 형태 매칭 및 분류 기준에서 가장 우수한 결과를 보였습니다. 이는 적은 파라미터로도 큰 데이터 필요 없이 효과적인 이상 감지 성능을 발휘할 수 있음을 시사합니다.

### [CrowdSelect: Synthetic Instruction Data Selection with Multi-LLM Wisdom](https://arxiv.org/abs/2503.01836)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01836.png)

Vote: 9

Authors: Weiwei Lin, Yao Wan, Lingfeng Yang, Yisen Li, Pan Zhou, Wenxuan Shen, Dongping Chen

- ***What's New***: CrowdSelect는 여러 대형 언어 모델들(Multi-LLM)의 지혜를 활용하여 합성 명령어 데이터의 선택을 혁신적으로 개선하였습니다. 이 방법은 다양한 언어 모델 응답(different LLM responses)과 보상 모델 평가(reward model assessment)를 기반으로 신규 탐험적 기반 메트릭을 제시하며, 클러스터링 기반의 접근법(clustering-based approach)을 결합하여 응답의 다양성을 유지합니다. 이러한 접근법은 데이터 선정의 정확성을 더욱 높이기 위해 다차원 신호(multi-dimensional signals)를 도입하였습니다.
- ***Technical Details***: CROWDSELECT 프레임워크는 다중 LLM의 사전 수집된 응답(responses)과 그에 대한 보상 점수를 서로 다른 신호로 취급하여 많은 LLM의 지혜를 활용합니다. 이는 각 명령어-응답 쌍을 고립된 채로 처리하는 대신 다양한 모델들로부터 다중 응답을 집계(aggregates)합니다. 그리고 각 응답의 보상 점수를 함께 고려하여 각 명령어에서 모델의 취급 방법에 대한 미묘한 차이를 밝힙니다. CROWDSELECT는 탐험적 메트릭 세 가지인 난이도(Difficulty), 분리도(Separability), 안정성(Stability)을 기반으로 하며, 다양성 보존 클러스터링(diversity-preserving clustering)과 멀티 메트릭 통합 전략(multi-metric integration)으로 고품질 명령어 데이터를 선정합니다.
- ***Performance Highlights***: CROWDSELECT는 다양한 모델들을 대상으로 실험한 결과 가장 강력한 성능 향상을 보여주었습니다. Llama-3.2-3b-instruct 모델의 경우 Arena-Hard 벤치마크에서 4.81% 개선을, MT-bench에서는 11.1% 개선을 통해 현존 최고 수준의 성과를 기록했습니다. 이러한 성과는 CROWDSELECT의 데이터 선정 방법이 효율적이며, 다양한 모델 및 벤치마크에 걸쳐 일반성을 보인다는 것을 입증합니다.

### [HoT: Highlighted Chain of Thought for Referencing Supporting Facts from Inputs](https://arxiv.org/abs/2503.02003)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.02003.png)

Vote: 9

Authors: Mohammad Reza Taesiri, Tin Nguyen, Logan Bolton, Anh Totti Nguyen

- ***What's New***: HoT(Highlighted Chain-of-Thought)은 대형 언어 모델(LLMs)이 쿼리에서 제공된 팩트에 XML 태그를 추가하여 팩트 기반 응답을 생성하도록 유도하는 기법입니다. 이 방법은 산술, 읽기 이해 및 논리적 추론을 포함한 다양한 17개 작업에서 몇 샷(얕은 학습) 환경에서 기존의 CoT(Chain of Thought) 방법보다 우수한 성능을 보였습니다.
- ***Technical Details***: HoT 기법은 주어진 입력 질문에 XML 태그를 붙여 쿼리의 주요 사실을 강조한 후, 입력에서 언급한 사실을 강조한 응답을 생성하도록 LLM에 요구합니다. 이 과정에서 사용자는 응답의 어떤 부분이 입력의 어떤 사실과 일치하는지 추적할 수 있습니다. 이 실험에서는 5개의 LLM(Gemini, Llama, GPT-4o 등)을 대상으로 17개의 작업을 수행하며, 방법 구성이 각 데이터셋에 적합한 강조 표시가 포함된 8개의 시범 예제를 사용하여 모델을 학습시켰습니다.
- ***Performance Highlights***: HoT 기법은 기존 CoT에 비해 산술, 질문 응답, 논리적 추론 작업에서 각각 평균 +1.60pp, +2.58pp, +2.53pp의 성능 향상을 보여주었습니다. 특히, 전략적 질문 응답과 같은 데이터셋에서 +4.91pp의 성능 향상을 보여주었으며, 학습된 LLM과 달리 사용자가 모델의 응답을 검토할 때 약 25% 빠르게 처리할 수 있음을 발견했습니다. 다만, LLM-verifier와 같은 추가적인 LLM이 평가할 때는 성능 향상의 편차가 있었습니다.

### [Fine-Tuning Small Language Models for Domain-Specific AI: An Edge AI Perspective](https://arxiv.org/abs/2503.01933)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01933.png)

Vote: 6

Authors: Kruthika KR, Rakshit Aralimatti, Syed Abdul Gaffar Shakhadri, Kartik Basavaraj Angadi

- ***What's New***: 본 논문은 에지 AI 환경에서 대규모 언어 모델의 한계를 극복하기 위해 개발된 Shakti Small Language Models(SLMs), 즉 Shakti-100M, Shakti-250M, Shakti-500M을 소개합니다. 이 모델들은 효율적인 아키텍처, 양자화(Quantization) 기술, 책임 있는 AI 원칙을 결합하여 스마트폰, 스마트 가전, IoT 시스템에 온디바이스 인텔리전스를 제공합니다.
- ***Technical Details***: Shakti 시리즈는 RoPE(Rotary Positional Embeddings)를 적용하여 공간 및 메모리 효율성을 극대화하고, GQA(Variable Grouped Query Attention)와 Block Sparse Attention 같은 혁신적인 주의 메커니즘을 사용합니다. 또한, SFT(Supervised Fine-Tuning)와 DPO(Direct Preference Optimization)를 통해 기계 학습 모델의 성능을 더욱 개선하고, 다양한 하드웨어 플랫폼에서 양자화 기술을 활용해 모델의 메모리 사용을 줄이고 처리 속도를 높였습니다.
- ***Performance Highlights***: Shakti 모델은 일반적인 NLP 작업에서 뛰어난 성능을 보였으며, 특히 도메인 특화된 작업(헬스케어, 금융, 법률)에서도 뛰어난 결과를 보였습니다. Shakti-250M은 헬스케어와 금융 도메인에서 매우 효율적이며, Shakti-500M은 다국어 처리와 복잡한 언어 이해 능력이 강화되어 대화형 AI와 가상 비서 응용 프로그램에 적합합니다. 양자화 기술을 통해 다양한 하드웨어 플랫폼에서 뛰어난 토큰 처리량을 보여줍니다.

### [Mixture of Structural-and-Textual Retrieval over Text-rich Graph Knowledge Bases](https://arxiv.org/abs/2502.20317)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.20317.png)

Vote: 6

Authors: Haoyu Han, Nedim Lipka, Franck Dernoncourt, Yu Wang, Mahantesh M Halappanavar, Yongjia Lei, Jiliang Tang, Ryan A. Rossi

- ***What's New***: MoR (Mixture of Structural-and-Textual Retrieval)은 텍스트가 풍부한 그래프 형식의 지식 기반(TG-KB)에서 구조적 및 텍스트적 지식을 결합하여 검색하는 새로운 접근법을 소개합니다. 이 방법은 'Planning-Reasoning-Organizing' 프레임워크를 사용하여 쿼리 로그리에 따른 계획적 그래프를 생성하고, 이를 바탕으로 구조적 탐색과 텍스트 매칭을 병행하여 문맥에 맞는 후보를 선택합니다.
- ***Technical Details***: MoR은 쿼리에 대한 텍스트 계획 그래프를 생성한 후, 논리 구조의 일관성을 갖춘 중간 후보를 얻기 위해 구조적 탐색과 텍스트 매칭을 혼합하는 방식으로 진행됩니다. 최종적으로는 구조적인 경로를 활용한 재정렬을 통해 상위 K개의 적합한 후보를 선별합니다. MoR의 프레임워크는 계획 단계에서 쿼리와 TG-KB에 따라서 텍스트 속성이 포함된 계획 그래프를 생성하고, 추론 단계에서는 혼합된 탐색을 수행하여 최종 후보를 얻는 체계로 구성됩니다.
- ***Performance Highlights***: MoR은 Amazon, MAG, Prime 데이터셋에서 구조적 및 텍스트적 검색 성능을 아우르며 뛰어난 성과를 보였습니다. 특히 MAG 데이터셋에서 우수한 성과를 기록하며 전체 평균 성능에서 가장 높은 점수를 얻었습니다. 하지만 생의학적 도메인에서는 여전히 도전적인 한계를 보이며, 이를 보완하기 위한 도메인 지식의 강화가 필요함을 보여줍니다.

### [QE4PE: Word-level Quality Estimation for Human Post-Editing](https://arxiv.org/abs/2503.03044)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.03044.png)

Vote: 5

Authors: Ana Guerberof-Arenas, Gabriele Sarti, Vilém Zouhar, Malvina Nissim, Arianna Bisazza, Grzegorz Chrupała

- ***What's New***: QE4PE는 단어 수준의 품질 예측(Quality Estimation; QE)이 기계 번역 후 편집에서 미치는 영향을 조사한 대규모 연구로, 42명의 전문 편집자가 참여했습니다. 이 연구는 인간이든 기계가 생성했든 오류 스팬 하이라이트가 편집 효율성과 질에 미치는 영향을 측정합니다.
- ***Technical Details***: 이 연구는 두 번역 방향(영어→이탈리아어, 영어→네덜란드어)에 걸쳐 4가지 하이라이트 모달리티를 비교했습니다. 데이터는 공개 소스 NLLB 3.3B MT 모델을 사용하여 번역된 생의학 및 소셜 미디어 도메인의 문서입니다. 하이라이트 방법에는 기계 학습 기반의 지도(supervised) 및 비지도(unsupervised) QE, 오라클 및 하이라이트 없음을 포함합니다. 체계적인 실험을 위해 하이라이트의 정확성을 평가하고, 편집자의 행동 로그를 통해 편집 효율성을 분석했습니다.
- ***Performance Highlights***: 오라클 방식의 하이라이트가 자동화된 QE에 비해 다소 높은 퍼포먼스를 보여주었지만, 하이라이트의 존재가 영어→이탈리아어 번역의 편집율을 두 배 이상 증가시켰습니다. 반대로 영어→네덜란드어에서는 편집 빈도가 크게 다르지 않았습니다. 결과적으로, 번역 도메인과 언어가 하이라이트의 유용성을 결정하는 중요한 요소임을 발견했습니다.

### [Exploring Rewriting Approaches for Different Conversational Tasks](https://arxiv.org/abs/2502.18860)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.18860.png)

Vote: 4

Authors: Vaishnavi Muppala, Iftikhar Ahamath Burhanuddin, Zhengmian Hu, Franck Dernoncourt, Tong Yu, Sungchul Kim, Mike Rimer, Xiang Chen, Ryan A. Rossi, Md Mehrab Tanjim, Ritwik Sinha, Wei Zhang

- ***What's New***: 이 논문은 다양한 대화형 작업을 위한 쿼리 재작성(query rewriting) 접근 방식을 체계적으로 조사합니다. 주로 질의 재작성(query rewrite)과 융합(fusion)이라는 두 가지 방법을 다양한 생성 작업에 적용하여 분석합니다. 이 연구는 특정 작업에 따라 가장 적절한 쿼리 접근 방식이 달라진다는 점을 강조합니다.
- ***Technical Details***: 이 논문은 두 가지 대화형 작업을 위해 두 가지 접근 방식을 제안합니다. 텍스트 기반 Q&A 작업에는 쿼리 재작성(query rewrite) 전략이, 텍스트-비주얼라이제이션(text-to-visualization) 작업에는 쿼리 융합(query fusion) 전략이 가장 효과적으로 작용합니다. Parameterized approach를 통해 다양한 쿼리 재작성 방법들을 구현할 수 있으며, 특히 쿼리 융합은 이전의 재작성된 질문을 활용하여 현재 질문과의 융합을 통해 대화의 간결함을 유지합니다.
- ***Performance Highlights***: 쿼리 재작성(query rewrite)은 텍스트 기반 Q&A 작업에서 쿼리 융합(query fusion)에 비해 코사인 유사도(Cosine Similarity)와 BERT F1 점수가 각각 3.9%, 9.8% 더 높았습니다. 반면, 텍스트-비주얼라이제이션 작업에서는 쿼리 융합이 쿼리 재작성에 비해 코사인 유사도에서 7.6%, BERT F1 점수에서 5.2% 더 높았습니다. 이는 각 작업의 특성에 맞춘 쿼리 접근 방식이 필요함을 보여줍니다.

### [Remasking Discrete Diffusion Models with Inference-Time Scaling](https://arxiv.org/abs/2503.00307)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.00307.png)

Vote: 4

Authors: Yair Schiff, Volodymyr Kuleshov, Guanghan Wang, Subham Sekhar Sahoo

- ***What's New***: Remasking Diffusion Models는 사전 학습된 마스크드 디퓨전 모델(Masked Diffusion Model; MDM)에도 적용 가능한 새 샘플러를 소개하며, 생성 과정 중 오류가 있는 토큰을 대체할 수 있는 도구를 제공합니다. 이 새로운 모델은 추론 시간 스케일링(Inference-Time Scaling)을 통해 고품질 자연어 출력물을 생성합니다.
- ***Technical Details***: 이 연구는 리마스킹(Remask) 디퓨전 모델을 제안하며, 핵 샘플링부터 다양한 리마스킹 일정까지 성능을 크게 향상시키는 여러 구성 요소로 샘플러를 보강합니다. 이는 확률적 모델에서 조상 샘플링(Ancestral Sampling)을 고안한 것으로, 사전 학습된 모델들 위에 이 샘플러를 사용할 것을 제시합니다. 특히, 리마스킹 디퓨전 모델은 정확도를 유지하면서 더 빠른 생성이 가능하게 합니다.
- ***Performance Highlights***: ReMDM은 특히 언어 모델링 작업에서 디노이징(Denoising) 단계를 늘리면서 이전의 디퓨전 모델과 비교해 더 높은 샘플 품질 메트릭을 달성합니다. 또한, 샘플링 단계를 줄여 속도를 증가시키더라도 이전 샘플러들보다 훨씬 적은 성능 저하를 겪습니다. 결과적으로, 다양한 응용 분야에서 강력한 개선 및 성능 확장을 보여줍니다.

### [Retrieval Models Aren't Tool-Savvy: Benchmarking Tool Retrieval for Large Language Models](https://arxiv.org/abs/2503.01763)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01763.png)

Vote: 4

Authors: Shuaiqiang Wang, Lingyong Yan, Dawei Yin, Pengjie Ren, Zhaochun Ren, Yuhan Wang, Zhengliang Shi

- ***What's New***: 이 연구는 TOOLRET이라는 새로운 벤치마크를 도입하여 대규모 언어 모델(Large Language Models; LLM)이 다양한 도구를 활용하는 능력을 평가하는 방법을 제안합니다. 7.6천개의 다양한 검색 작업과 4.3만 개의 도구로 이루어진 이 벤치마크는 정보 검색(IR) 모델의 도구 검색 능력을 철저히 테스트합니다.
- ***Technical Details***: TOOLRET는 기존 데이터셋에서 4.3만 개의 도구를 수집하고, 108개의 파이썬 코딩 작업과 유사한 형식으로 쿼리와 대상 도구(레이블)를 포함한 검색 작업을 표준화했습니다. IR 모델은 다양한 실험 설정 하에서 평가되었습니다. 각 작업은 강력한 LLM(e.g., GPT-4o)을 사용하여 생성된 지시문과 쌍을 이루고 있습니다.
- ***Performance Highlights***:  기존 IR 벤치마크에서 강력한 성능을 보이는 모델조차도 TOOLRET 벤치마크에서는 낮은 성능을 보여줍니다. NV-Embed-v1 모델은 nDCG@10 지표에서 33.83%의 성과를 기록했으며, 이는 도구 검색 작업의 높은 난이도를 강조합니다. LLM의 도구 사용 성능은 검색된 도구 사용으로 인해 상당히 낮아졌으며, 연구팀은 20만 개 이상의 사례가 포함된 대규모 훈련 데이터셋인 TOOLRET-train을 생성하여 IR 모델의 성능을 향상시키는 데 기여했습니다.

### [Benchmarking Large Language Models for Multi-Language Software Vulnerability Detection](https://arxiv.org/abs/2503.01449)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01449.png)

Vote: 3

Authors: Martin Weyssow, Yindu Su, Ting Zhang, Yikun Li, Eng Lieh Ouh, Hong Jin Kang, David Lo, Lwin Khin Shar, Hung Nguyen, Chengran Yang, Tan Bui

- ***What's New***: 이 논문에서는 다양한 프로그래밍 언어에서 소프트웨어 취약점 감지(Software Vulnerability Detection; SVD)의 효과성을 평가하기 위해 대형 언어 모델(LLM)의 성능을 종합적으로 분석합니다. Python, Java, JavaScript를 포함한 여러 언어의 취약점을 다루어 보다 포괄적인 연구를 제시합니다.
- ***Technical Details***: 연구진은 총 8,260개의 Python, 7,505개의 Java, 그리고 28,983개의 JavaScript 취약한 기능을 포함한 데이터셋을 구축했으며, 5개의 공개 모델(LLMs)을 다양한 방식, 즉 Prompt Engineering, Instruction Tuning, Sequence Classification Fine-Tuning을 통해 평가했습니다. 이들을 5개의 소형 언어 모델(SLM) 및 2개의 정적 분석 도구와 비교했습니다.
- ***Performance Highlights***: Python에서는 LLM이 SLM보다 우수한 성능을 보였으나, Java와 JavaScript에서는 SLM이 더 나은 결과를 보였습니다. 특히, JavaScript 데이터셋에서는 Sequence Classification Fine-Tuning이 가장 큰 효과를 발휘하여 최고 F1-score를 기록했습니다. 하지만 모든 언어에서 정적 애플리케이션 보안 테스트(SAST) 도구의 성능은 LMs보다 낮았습니다.

### [FLAME: A Federated Learning Benchmark for Robotic Manipulation](https://arxiv.org/abs/2503.01729)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01729.png)

Vote: 3

Authors: Santiago Bou Betran, Yuchong Zhang, Alberta Longhini, Miguel Vasco, Danica Kragic

- ***What's New***: FLAME은 로봇 조작 훈련을 위한 연합 학습(Federated Learning; FL) 벤치마크로, 다중 환경에서 분산되어 데이터 프라이버시를 보장하는 로봇 조작 학습의 새로운 패러다임을 소개합니다. 이는 다양한 시뮬레이션 환경에서 수집된 광범위한 조작 과제를 포함하여 총 160,000건 이상의 전문가 시연을 제공합니다.
- ***Technical Details***: FLAME 벤치마크는 RLBench를 기반으로 하며, 14가지의 다양한 변형 요인으로 구성된 20개의 로봇 조작 과제를 제공하여 다양한 환경적 조건에서 로봇 조작 과제를 학습하고 평가할 수 있습니다. FLOWER Python 라이브러리를 이용하여 연합 학습이 가능하도록 설계되었습니다. 각 로봇은 개인의 환경에서 모델을 학습하고 서버와 파라미터 업데이트를 주기적으로 공유합니다.
- ***Performance Highlights***: 실험 결과, 대부분의 연합 학습 방법이 국내 'Close Box' 과제에서는 효과적인 정책 학습이 가능하였으나, 'Peg in Square' 및 'Scoop'에서는 어려움을 겪었습니다. 이는 FLAME이 연합 학습 설정 내에서 현재 및 미래의 조작 방법의 일반화 및 강건성을 평가하는데 설계되었다는 점을 강조합니다. Krum과 같은 특정 방법은 개별 과제에서 좋은 성과를 보였으나 다른 것에서는 적은 성과를 보이며, 이는 다양한 조작 과제에서의 평가 필요성을 나타냅니다.

### [CognitiveDrone: A VLA Model and Evaluation Benchmark for Real-Time Cognitive Task Solving and Reasoning in UAVs](https://arxiv.org/abs/2503.01378)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01378.png)

Vote: 2

Authors: Artem Lykov, Dzmitry Tsetserukou, Muhammad Haris Khan, Grik Tadevosyan, Yasheerah Yaqoot, Valerii Serpiva, Artyom Myshlyaev, Oleg Sautenkov

- ***What's New***: CognitiveDrone은 무인 항공기(UAV)의 실시간 인지 작업 해결과 추론을 위한 새로운 비전-언어-액션(Vision-Language-Action; VLA) 모델이며, 이러한 인지 능력을 평가하는 첫 번째 전용 벤치마크인 CognitiveDroneBench를 도입하였습니다.
- ***Technical Details***: CognitiveDrone 시스템은 첫 번째 시점 시각 입력과 사용자 지시에 따라 4D 동작 명령을 생성하는 VLA 모델을 통합하고 있습니다. OpenVLA 모델을 기준으로 7억 개의 파라미터를 포함하고 있으며, 8,000개 이상의 시뮬레이션된 비행 에피소드를 통해 비행 물리와 효과적인 드론 제어를 학습하였습니다. 인지 능력을 강화하기 위해 VLM(Vision-Language Model) 기반의 부가적인 추론 모듈을 통합한 CognitiveDrone-R1 시스템을 제안하였습니다.
- ***Performance Highlights***: CognitiveDroneBench에서의 실험 결과, Racing 모델인 RaceVLA는 성공률 31.3%를 기록했던 반면, 기본 CognitiveDrone 모델은 59.6%를 달성했으며, CognitiveDrone-R1은 성공률 77.2%에 도달하였습니다. CognitiveDrone-R1은 특히 인간 인식에서 31%, 상징 이해에서 21% 성능을 향상시켰으며, 추론 능력을 약 6% 개선하였습니다.

### [SwiLTra-Bench: The Swiss Legal Translation Benchmark](https://arxiv.org/abs/2503.01372)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01372.png)

Vote: 2

Authors: Niko Grupen, Daniel Brunner, Sina Ahmadi, Boling Yang, Nan Wu, Yingqiang Gao, Robin Mamié, Julio Pereyra, Luka Nenadic, Stefan Palombo, Thomas Lüthi, Matthew Guillod, Christophe Gösken, Claude Humbel, Spencer Poff, Joel Niklaus, Lorenzo Tanzi, Jakob Merane, Cyrill A. H. Chevalley

- ***What's New***: SwiLTra-Bench는 스위스의 다국어 법률 문서 번역을 위한 포괄적인 벤치마크 데이터셋으로, 180,000개 이상의 스위스 법률 번역 쌍으로 구성되어 있습니다. 모든 스위스 언어와 영어로 구성된 이 데이터셋은 LLM(Large Language Models) 기반 번역 시스템의 성능을 평가하기 위해 설계되었습니다.
- ***Technical Details***: SwiLTra-Bench는 세 가지 하위 데이터셋으로 구성됩니다. 첫 번째는 CH-Law-Trans로 법률 전체, 개별 조항, 문단 수준의 번역을 포함합니다. 두 번째는 CH-Headnote-Trans로 스위스 연방 대법원의 중대한 판결에 대한 해설 번역으로 구성됩니다. 세 번째는 CH-Press-Trans로 스위스 대법원의 보도자료 번역을 포함합니다. 모든 데이터셋에는 독일어, 프랑스어, 이탈리아어 번역이 포함되어 있으며, 일부는 로만슈어와 영어 번역도 포함합니다.
- ***Performance Highlights***: 최첨단 모델들은 법률, 해설, 보도자료 모든 문서 유형에 걸쳐 우수한 번역 성능을 보였습니다. 반면, 특정 번역 시스템은 법률에서는 뛰어났으나 해설에서는 성능이 저조했습니다. 오픈 SLM(Small Language Models)의 미세 조정은 번역 품질을 크게 향상시켰지만, 최상의 제로샷 모델에 여전히 뒤처졌습니다. SwiLTra-Judge라는 평가 시스템은 인간 전문가 평가와 가장 잘 일치하는 자동화된 프레임워크를 제시합니다.

### [Interact, Instruct to Improve: A LLM-Driven Parallel Actor-Reasoner Framework for Enhancing Autonomous Vehicle Interactions](https://arxiv.org/abs/2503.00502)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.00502.png)

Vote: 2

Authors: Chen Lv, Peng Hang, Jian Sun, Jiaqi Liu, Shiyu Fang, Chengkai Xu

- ***What's New***: 이 논문에서는 큰 언어 모델(LLM; Large Language Models)을 활용한 병렬 Actor-Reasoner 프레임워크를 제안하여 자율주행 차량(AV; Autonomous Vehicles)과 사람 운전 차량(HV; Human-driven Vehicles) 간의 상호작용과 의도 표현을 실시간으로 향상시킵니다. 이 새로운 시스템은 LLM을 기반으로 HV와 AV 간의 양방향 명시적 상호작용을 가능하게 하며, 다양하고 복잡한 주행 시나리오에서 효과적인 의사결정을 지원합니다.
- ***Technical Details***: Actor-Reasoner 프레임워크는 행동 과학의 '직관적 빠른 반응'과 '신중한 느린 추론'이라는 이중 시스템 모델에 영감을 받아 설계되었습니다. Reasoner는 LLM을 기반으로 CoT(Chain-of-Thought) 추론을 통해 HV의 운전 스타일을 추정하고, eHMI(외부 인간-기계 인터페이스; external Human-Machine Interface) 정보를 생성합니다. Actor는 상호작용 중 환경에서 수집된 메모리를 활용하여 빠른 실시간 결정이 가능하도록 합니다. 또한, 메모리 파티션과 이중 레이어 메모리 검색을 통해 시스템 효율을 크게 향상시킵니다.
- ***Performance Highlights***: Ablation 연구와 여러 다른 의사 결정 방법과의 비교 실험에서 Actor-Reasoner 프레임워크는 교차로와 같은 복잡한 다차량 상호작용 환경에서도 1%의 위험 상호작용율로 가장 높은 안전성과 효율성을 보여주었습니다. 이 실험은 제안된 방법이 실제 도로 환경에서도 높은 일반성과 실용성을 가지고 있음을 입증합니다.

### [Diverse Controllable Diffusion Policy with Signal Temporal Logic](https://arxiv.org/abs/2503.02924)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.02924.png)

Vote: 0

Authors: Chuchu fan, Yue Meng

- ***What's New***: 이 논문은 Signal Temporal Logic (STL)와 Diffusion Models를 활용하여 통제 가능하고 다양한 규칙을 준수하는 정책을 학습하는 방법을 제안합니다. 이 기법은 NuScenes 데이터셋에서 다양한 규칙 준수 궤적을 생성하며, 기존 접근 방식에 비해 실행 시간 면에서도 뛰어난 성능을 보여줍니다. 사례 연구를 통해 인간-로봇 상호작용 시나리오에서도 다양한 궤적을 생성하는 데 효과적이라는 것을 증명합니다.
- ***Technical Details***: 이 접근 방식은 파라메트릭 STL을 사용하여 실질적인 교통 규칙을 유연하게 인코딩하고, DDPM(Denoising Diffusion Probabilistic Model) 및 RefineNet을 통해 다양한 궤적을 생성하는 과정으로 이루어집니다. 또한 기여한 Calibrate 시스템을 통해 현실 데이터셋에서 STL 파라미터를 추출하고, 트래젝토리 최적화를 통해 '다양한 결과(multiple-outcome)' 데이터를 생성합니다.
- ***Performance Highlights***: Open-loop 평가에서 본 연구의 접근 방법은 NuScenes 데이터셋에서 STL 준수율이 가장 높으며, 가장 다양한 궤적을 생성하는 것으로 나타났습니다. Closed-loop 테스트에서도 충돌률이 가장 낮고 차선 이탈률이 가장 적었습니다. 이러한 결과는 시뮬레이터에서 현실적인 에이전트의 다양성을 모델링하는 데 있어서 본 접근 방식의 잠재력을 보여줍니다.

### [Reliable and Efficient Multi-Agent Coordination via Graph Neural Network Variational Autoencoders](https://arxiv.org/abs/2503.02954)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.02954.png)

Vote: 0

Authors: Federico Pecora, Wenliang Liu, Yue Meng, Nathalie Majcherczyk, Scott Kiesel, Chuchu Fan

- ***What's New***: 이 연구는 그래프 신경망 변이 오토인코더(GNN-VAE)를 활용하여 대규모 다중 에이전트 조정 문제를 신속하게 해결하는 새로운 프레임워크를 제안합니다. 이는 최적화 문제로서의 그래프 문제로 조정 문제를 공식화하여, 단일 작업을 중앙에서 최적화하는 기존의 방법보다 훨씬 빠르게 솔루션을 생성할 수 있습니다.
- ***Technical Details***: GNN-VAE는 각 로봇 간에 발생할 수 있는 간섭 구간을 기반으로 최적의 조정 솔루션을 찾는 변이 오토인코더 기반의 학습 프레임워크입니다. 학습 과정에서는 혼합 정수 선형 프로그램(MILP) 솔버를 사용하여 얻은 솔루션을 잠재 공간에 인코딩하고, 추론 단계에서는 잠재 변수로부터 샘플링하여 최소 비용의 샘플을 조정 솔루션으로 선택합니다. 이 과정에서 GNN-VAE는 노드 랭크와 엣지 모드를 반감독식으로 학습하여 형식적 제한 조건을 항상 만족하는 솔루션을 생성합니다.
- ***Performance Highlights***: 매우 큰 규모의 문제에도 불구하고, 실험 결과는 우리의 GNN-VAE 프레임워크가 최대 250대의 로봇이 등장하는 문제에서도 높은 품질의 솔루션을 기존 방법보다 20배 빠르게 생성한다는 것을 보여줍니다. 이는 평균 완성 시간, 최대 완성 시간 등 다양한 목적 함수를 기준으로 했을 때도 우수한 성능을 나타냅니다.

