## Daily Papers (2025-03-26)

### [Long-Context Autoregressive Video Modeling with Next-Frame Prediction](https://arxiv.org/abs/2503.19325)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19325.png)

Vote: 59

Authors: Mike Zheng Shou, Yuchao Gu, Weijia Mao

- ***What's New***: 이 논문은 기계 번역을 위한 'Long-Context Autoregressive Video Modeling with Next-Frame Prediction'이라는 새로운 접근법을 제안합니다. 이는 언어 모델의 장기 컨텍스트 생성을 비디오 생성에 적용하여 프레임 간 인과 관계를 모델링합니다.
- ***Technical Details***: 제안된 프레임 오토회귀 모델인 FAR은 프레임별 인과 종속성을 학습하며, 기존의 비디오 디퓨전 트랜스포머보다 수렴 속도가 빠릅니다. FlexRoPE라는 새로운 시험 중 기법을 통해 16배 길어진 비전 컨텍스트를 추론할 수 있도록 하였으며, 긴 비디오에 효율적인 훈련을 가능하게 하는 'Long Short-Term Context Modeling'을 도입했습니다.
- ***Performance Highlights***: FAR는 UCF-101, BAIR, DMLab 및 Minecraft 데이터셋에서 최첨단 성능을 달성했으며, 특히 UCF-101의 조건부 비디오 생성에서 가장 낮은 FVD 점수를 기록했습니다. 또한, 긴 비디오 예측에서 기존의 다양한 방법들에 비해 뛰어난 예측 정확도를 보였습니다.

### [CoMP: Continual Multimodal Pre-training for Vision Foundation Models](https://arxiv.org/abs/2503.18931)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18931.png)

Vote: 26

Authors: Yitong Chen, Yu-Gang Jiang, Lingchen Meng, Wujian Peng, Zuxuan Wu

- ***What's New***: 이 논문은 CoMP(Continual Multimodal Pre-training)를 도입하여 비전 기반 모델(Vision Foundation Models; VFMs)을 멀티모달 방식으로 지속적으로 사전 학습할 수 있도록 제안합니다. 이를 통해 다양한 크기의 시각 입력을 처리하고 언어 표현과 더 잘 정렬된 시각적 표현을 생성할 수 있게 됩니다.
- ***Technical Details***: CoMP는 시각 모델의 C-ROPE(Continual Rotary Position Embedding)와 언어적 특징과 시각적 특징 사이의 Alignment Loss를 사용하여 멀티모달 표현을 정렬합니다. 이 과정에서 시각적 특징을 언어 모델의 텍스트 공간에 투영하여 시각-언어 간의 격차를 줄이고자 합니다.
- ***Performance Highlights***: CoMP를 사용한 DINOv2와 SigLIP 같은 기존 VFMs는 멀티모달 이해는 물론 일반적인 분류 및 세그먼트 작업에서도 뛰어난 성능 향상을 보였습니다. 특히, CoMP-SigLIP는 ChartQA에서 66.7점, DocVQA에서 75.9점을 기록했으며, ImageNet-1K에서 87.4%의 정확도, ADE20K에서 49.5 mIoU를 달성했습니다.

### [Exploring Hallucination of Large Multimodal Models in Video Understanding: Benchmark, Analysis and Mitigation](https://arxiv.org/abs/2503.19622)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19622.png)

Vote: 25

Authors: Li Su, Baolong Bi, Jiashu Qu, Jingyi Tang, Hongyu Chen, Li Liang, Hongcheng Gao, Qingming Huang, Yue Liu

- ***What's New***: 이 논문은 비디오 이해에서 대형 멀티모달 모델(Large Multimodal Models; LMMs)의 환각(Hallucination) 문제를 탐구하고 이를 평가하기 위한 포괄적인 벤치마크, HAVEN을 제안합니다. 특히, LMMs의 비디오 모달리티에서의 환각을 평가하기 위한 최초의 벤치마크로써, 비디오 이해의 복잡함을 다각도로 분석합니다.
- ***Technical Details***: HAVEN은 환각의 원인, 환각 측면, 질문 형식 등 세 가지 차원을 기반으로 제작된 6,497개의 질문을 포함하며, LMM 환각을 평가합니다. 또한, 감독된 추론 미세 조정(Supervised Reasoning Fine-Tuning; SRFT)과 직접 선호 최적화(Thinking-based Direct Preference Optimization; TDPO) 기법을 통해 모델의 사고 능력을 강화하고, 환각을 줄이는 새로운 비디오 사고 모델을 제안합니다.
- ***Performance Highlights***: 밸리-이글 7B와 GPT4o-mini가 가장 낮은 환각 비율을 기록하였으며, 실험 결과 모델의 성능은 비디오 길이, 질문 복잡성, 프레임 수 등에 따라 성능이 달라집니다. 'Valley-Eagle-7B'는 총 정확도 61.29%로 최고의 성능을 기록했으며, TDPO를 적용한 'LLaVA-NeXT-Video-7B-Thinking'은 환각 평가 정확도를 7.65% 향상시켰습니다.

### [Inference-Time Scaling for Flow Models via Stochastic Generation and Rollover Budget Forcing](https://arxiv.org/abs/2503.19385)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19385.png)

Vote: 25

Authors: Jaihoon Kim, Taehoon Yoon, Jisung Hwang, Minhyuk Sung

- ***What's New***: 이 논문에서는 플로우 모델(Flow Models)의 추론 시간 확장을 위한 방법론을 제안합니다. 주로 확률적 생성(Stochastic Generation)과 롤오버 예산 강제(Rollover Budget Forcing; RBF)를 도입하여 플로우 모델이 사용자의 선호에 더 잘 맞는 이미지를 생성하도록 합니다. 이를 통해 플로우 모델이 디퓨전 모델(Diffusion Models)의 한계를 넘어서도록 지원합니다.
- ***Technical Details***: 플로우 모델에서 추론 시간 확장을 쉽게 만들기 위해 세 가지 핵심 아이디어가 제안되었습니다. 첫째, SDE 기반 생성 방법을 사용하여 플로우 모델에서 입자 샘플링이 가능해졌습니다. 둘째, 인터폴란트를 변환하여 샘플 다양성을 증가시키고 탐색 공간을 확장합니다. 셋째, RBF를 도입하여 시간 단계별로 계산 리소스를 적절히 할당하여 예산의 효율적 사용을 극대화합니다. 실험을 통해 SDE 기반 생성, 특히 variance-preserving(VP) 인터폴란트 기반 생성이 플로우 모델의 입자 샘플링 효율을 개선하는 것을 보여줍니다.
- ***Performance Highlights***: 추론 시간 SDE 변환과 VP 인터폴란트 변환은 다양한 과제, 특히 컴포지션 텍스트-이미지 생성과 수량 인지 이미지 생성에서 일관된 보상 정렬 성능 개선을 실현했습니다. RBF를 사용했을 때 가장 높은 성능을 달성하였으며, 모든 기존의 입자 샘플링 접근 방식을 능가했습니다. 특히 VP-SDE를 이용한 플로우 모델은 5배 적은 요소 평가 횟수로도 디퓨전 모델을 성능 면에서 능가했습니다.

### [Scaling Vision Pre-Training to 4K Resolution](https://arxiv.org/abs/2503.19903)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19903.png)

Vote: 21

Authors: Han Cai, Yao Lu, Hongxu Yin, Boyi Li, Sifei Liu, Jan Kautz, Marco Pavone, Song Han, Baifeng Shi, Pavlo Molchanov, Trevor Darrell

- ***What's New***: PS3는 4K 해상도로의 비전 사전 학습(Pre-training)을 확장하여 기존 CLIP 스타일의 학습에 비해 거의 일정한 비용으로 고해상도의 시각적 지각을 구현했습니다. 높은 해상도의 이미지에서 로컬 영역을 선택적으로 처리하고 자세한 캡션과의 대조를 통해 고해상도 표현 학습을 가능하게 했습니다.
- ***Technical Details***: PS3는 전역 이미지 대조학습 대신, 로컬 영역과 그에 대한 세부 캡션 간의 대조를 통해 총 75M 개의 고해상도 이미지와 282M 쌍의 세부 캡션 및 주목할 만한 지역의 경계 상자를 비전 모델에 학습합니다. PS3를 사용하는 모델은 'top-down patch selection' 메커니즘을 통해, 사용자의 텍스트 프롬프트(prompt)에 따라 해상도가 높은 이미지 영역을 선택적으로 처리하고, 로컬 하이 레졸루션 패치를 상위 ViT의 키 및 값으로 처리하여 지역 맥락을 제공합니다.
- ***Performance Highlights***: PS3를 사용하는 VILA-HD는 다양한 벤치마크에서 최첨단의 MLLM을 능가하며, 최신 토큰 정리 접근법보다 우수한 효율성을 나타냅니다. 또, 4K 해상도에서의 시각적 지각을 평가하는 새로운 벤치마크인 4KPro에서 이전 모든 MLLM을 넘어서는 성능을 발휘했습니다. PS3는 4K 해상도에서 선택된 하이 레졸루션 패치만을 처리함으로써, 이전 모든 MLLM보다도 빠른 2.73배 속도를 기록했습니다.

### [Spot the Fake: Large Multimodal Model-Based Synthetic Image Detection with Artifact Explanation](https://arxiv.org/abs/2503.14905)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.14905.png)

Vote: 15

Authors: Peilin Feng, Hengrui Kang, Conghui He, Yize Chen, Jiang Wu, Siwei Wen, Wenjun Wu, Weijia Li, Junyan Ye, Zichen Wen

- ***What's New***: 이 연구는 대형 멀티모달 모델 기반의 합성 이미지 탐지와 아티팩트 설명을 위한 FakeVLM이라는 다기능 솔루션을 소개합니다. 이 모델은 합성 이미지와 DeepFake 탐지 작업에서 우수한 성능을 보여주며, 기존 전문가 모델과 비교할 수 있는 수준의 해석 가능성을 제공합니다. 또한, 10만 개 이상의 이미지를 포함한 FakeClue라는 데이터셋을 소개하며, 이는 다양한 카테고리와 자연어로 설명된 아티팩트 단서를 포함합니다.
- ***Technical Details***: FakeVLM은 LLaVA-v1.5 구조를 기반으로 하며, 클립-비전 트랜스포머(Charp-ViT)를 글로벌 이미지 인코더로 사용합니다. 모델은 이미지와 텍스트 모달리티를 연결하기 위한 2층 MLP 어댑터를 포함하며, 7B 파라미터를 가진 Vicuna-v1.5 모델을 대형 언어 모델(LLM)로 사용하여 합성 데이터에 대한 추론 능력을 강화합니다. FakeClue 데이터셋은 이미지와 대응하는 아티팩트 설명 문장을 자연어로 레이블 된 멀티-LMM 전략을 통해 생성합니다.
- ***Performance Highlights***: FakeVLM은 다양한 데이터셋에서 합성 탐지 및 아티팩트 설명 작업에서 뛰어난 성능을 입증하였으며, 특히 FakeClue와 LOKI 데이터셋에서는 가장 최신의 공개 모델(Qwen2-VL-72B)보다 평균적으로 36.1%의 Acc 및 41.3%의 F1 점수 향상을 기록했습니다. 또한, DD-VQA 데이터셋에서 Common-DF 모델보다 Acc에서 5.7%, F1에서 3%, ROUGE_L에서 9.5%의 향상을 보였으며, 다양한 실험 결과를 통해 인간을 뛰어넘는 성능을 입증했습니다.

### [MDocAgent: A Multi-Modal Multi-Agent Framework for Document Understanding](https://arxiv.org/abs/2503.13964)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.13964.png)

Vote: 12

Authors: Peng Xia, Yun Li, Siwei Han, Ruiyi Zhang, Hongtu Zhu, Huaxiu Yao, Tong Sun

- ***What's New***: MDocAgent는 문서 이해(Document Understanding)를 위한 혁신적인 멀티모달 멀티에이전트 프레임워크(Multi-Modal Multi-Agent Framework)입니다. 이 시스템은 다섯 개의 특수 에이전트를 통해 텍스트와 이미지의 통합된 분석을 수행하여 문서 내용에 대한 더 포괄적인 이해를 가능하게 합니다. 이 접근 방식은 텍스트 및 시각적 요소를 종합하여 문서 질문 응답(DocQA)의 정확성을 개선합니다.
- ***Technical Details***: MDocAgent는 RAG(Retrieval-Augmented Generation) 기반의 두 개의 병렬 파이프라인을 채택하며, 텍스트와 이미지 기반 RAG는 각각 다섯 개의 특수화된 에이전트를 지원합니다. 이 에이전트들은 일반 에이전트, 주요 에이전트, 텍스트 에이전트, 이미지 에이전트, 요약 에이전트로 구성되어 있으며, 각 에이전트는 텍스트 또는 이미지 모달리티 내에서 상세 분석을 수행합니다. 프레임워크는 다섯 단계로 작동하며, 문서 전처리부터 다중 모달 컨텍스트 검색, 초기 분석 및 주요 정보 추출, 특수화된 에이전트 처리, 최종 답변 합성으로 구성됩니다.
- ***Performance Highlights***: MDocAgent는 다섯 개의 벤치마크에서 평균 12.1%의 성능 향상을 보여주었습니다. 특히, MDocAgent는 기존의 RAG 기반 방법보다 높은 정확도를 기록했으며, 이는 복잡한 문서 QA에서 정보 과부하 및 크로스 모달리티 이해를 효과적으로 처리한 결과입니다. ColBERTv2 및 Qwen2-VL 같은 기존의 시스템 대비 각각 6.9%, 10.9% 향상을 보여주며, 다양한 정보원 및 모달리티를 잘 통합함으로써 정교하고 신뢰할 수 있는 응답을 생성했습니다.

### [Think Twice: Enhancing LLM Reasoning by Scaling Multi-round Test-time Thinking](https://arxiv.org/abs/2503.19855)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19855.png)

Vote: 11

Authors: Xiangang Li, Haotian Wang, Yiping Peng, Xiaoyu Tian, Shuaiting Chen, Han Zhao, Yunjie Ji, Sitong Zhao

- ***What's New***: 이 논문은 대형 언어 모델(LLMs)의 추론 능력을 강화하는 새로운 방법인 'Multi-round Thinking'을 소개합니다. 이전 라운드의 답변을 다음 라운드의 입력 프롬프트로 활용하여 모델 추론을 점진적으로 개선하는 기법입니다. 이는 모델이 이전의 답변을 독립적으로 재고려하여 추론의 질을 향상시키도록 설계되었습니다.
- ***Technical Details***: 'Multi-round Thinking' 접근법은 모델이 여러 추론 라운드를 통해 답변을 반복적으로 개선하는 방법입니다. 초기 사용자 프롬프트(Puser)에서 시작하여 첫 번째 라운드의 결과를 다음 라운드의 입력으로 사용하여 답변을 재평가합니다. 이러한 반복적 세정 과정을 통해 모델이 이전의 결론을 독립적으로 재검토하고, 인지적 관성을 최소화하며 추론 결과의 질을 체계적으로 향상시키는 것을 목표로 합니다.
- ***Performance Highlights***: 실험 결과, DeepSeek-R1 모델은 AIME 2024 벤치마크에서 정확도가 79.7%에서 82.0%로, GPQA-Diamond에서는 74.0%에서 74.8%로 개선되었습니다. QwQ-32B 모델은 AIME 2024에서 80.3%에서 82.1%로, GPQA-Diamond에서 63.0%에서 64.7%로 성능이 증가했습니다. 이러한 결과는 'Multi-round Thinking'이 다양한 벤치마크에서 모델의 성능을 안정적으로 향상시키는 데 효과적임을 보여줍니다.

### [CoLLM: A Large Language Model for Composed Image Retrieval](https://arxiv.org/abs/2503.19910)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19910.png)

Vote: 8

Authors: Son Tran, Ashish Tawari, Raffay Hamid, Trishul Chilimbi, Mubarak Shah, Jinyu Yang, Abhinav Shrivastava, Chuong Huynh

- ***What's New***: CoLLM은 합성 이미지 검색(Composed Image Retrieval; CIR)을 위해 고안된 새로운 방법론으로, 이미지-캡션 쌍에서 바로 세쌍둥이 자료(triplet)를 생성하여 주석이 달린 데이터셋의 필요성을 해소합니다.
- ***Technical Details***: CoLLM은 참조 이미지 임베딩(reference image embedding)과 수정 텍스트(modification text)를 이미지-캡션 쌍에서 동적으로 생성합니다. 이를 위해 Spherical Linear Interpolation(Slerp)과 사전 정의된 텍스트 템플릿을 사용합니다. 또한, MTCIR 데이터셋은 3.4백만 커플 이미지와 17.7백만 개의 수정 텍스트를 포함하여 데이터 다양성을 향상시킵니다.
- ***Performance Highlights***: CoLLM은 여러 CIR 벤치마크에서 최첨단 성능을 보이며, MTCIR을 통해 15%까지의 성능 향상을 달성하였습니다. 또한, CoLLM은 기존 벤치마크보다 더 신뢰할 수 있는 평가 기준을 제공하여 CIR 연구 발전에 기여합니다.

### [ReSearch: Learning to Reason with Search for LLMs via Reinforcement Learning](https://arxiv.org/abs/2503.19470)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19470.png)

Vote: 8

Authors: Haofen Wang, Tianpeng Li, Chenzheng Zhu, Fan Yang, Weipeng Chen, Huajun Chen, Mingyang Chen, Haoze Sun, Zenan Zhou, Yijie Zhou, Wen Zhang, Jeff Z. Pan

- ***What's New***: 이 연구에서는 대규모 언어 모델(LLMs)이 외부 검색(Search)을 이용한 추론 학습을 강화 학습(Reinforcement Learning)으로 수행하는 새로운 프레임워크인 ReSearch를 제안합니다. 이 방법은 추론 체인에서 검색 작업을 통합하여 언제 검색을 수행할지와 그 결과가 추후 추론에 어떻게 영향을 미칠지를 텍스트 기반 사고로 안내하는 혁신적인 접근법입니다.
- ***Technical Details***: ReSearch는 Qwen2.5-7B 및 Qwen2.5-32B 모델을 사용하여 처음부터 강화 학습으로 훈련되었습니다. 추론 체인은 텍스트 기반 사고와 검색 쿼리 및 검색 결과로 구성되며, 강화 학습 기법인 GRPO를 사용하여 설계되었습니다. 모델은 MuSiQue 데이터셋의 단일 훈련 세트로만 훈련되었으며, 다양한 벤치마크에서 평가되었습니다. 훈련 과정에서 규칙 기반의 보상 함수가 모델의 추론 능력을 자연적으로 끌어내며, 반영과 자기 교정과 같은 고급 추론 능력을 효과적으로 유도합니다.
- ***Performance Highlights***: ReSearch는 여러 다양한 벤치마크에서 실행된 결과, 베이스라인 대비 최대 22.4%의 성능 향상을 보여주었습니다. 특히 7B 파라미터 모델에서 Exact Match(EM) 기준 15.81%, LLM-as-a-judge(LJ) 기준 17.56%의 평균 향상을 기록하였고, 32B 파라미터 모델에서도 각각 14.82% 및 15.46%의 개선을 보여주었습니다. 이는 ReSearch의 견고한 일반화 능력을 보여줍니다.

### [Latent Space Super-Resolution for Higher-Resolution Image Generation with Diffusion Models](https://arxiv.org/abs/2503.18446)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18446.png)

Vote: 6

Authors: Seon Joo Kim, Jinho Jeong, Sangmin Han, Jinwoo Kim

- ***What's New***: 이 논문에서는 LSRNA라는 새로운 프레임워크를 제안하여, 디퓨전 모델(Diffusion Models)을 활용하여 더 높은 해상도의 이미지 생성을 가능하게 합니다. 이 방법은 잠재 공간에서의 초해상화(Latent Space Super-Resolution)를 통해 기존의 해상도 한계를 극복하는 혁신적인 접근법을 제공합니다.
- ***Technical Details***: LSRNA(framework)는 Latent space에서 초해상도(Super-Resolution; LSR)를 제공하며, Region-wise Noise Addition(RNA) 모듈을 결합하여 고주파 세부 사항을 강화합니다. 이 방법은 잔뜩 변형된 잠재 공간을 정렬하는 데 중점을 두고 있으며, 각 영역에 Gaussian noise를 적절히 주입하여 세부를 강조합니다.
- ***Performance Highlights***: LSRNA는 기존의 최첨단 참조 기반(reference-based) 방법들을 뛰어넘는 성능을 보입니다. 다양한 해상도와 메트릭에서 우수한 성능을 제공하며 빠르고 질적인 이미지 생성을 지원합니다. 특히, DemoFusion 및 Pixelsmith와 같은 여러 방법에서 성능 향상과 빠른 추론 시간을 보여주었습니다.

### [WikiAutoGen: Towards Multi-Modal Wikipedia-Style Article Generation](https://arxiv.org/abs/2503.19065)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19065.png)

Vote: 5

Authors: Jun Chen, Zhongyu Yang, Chun-Mei Feng, Mohamed Elhoseiny, Dannong Xu, Xiaoqian Shen, Junjie Fei, Liangbing Zhao

- ***What's New***: WikiAutoGen은 기존의 텍스트 전용 생성 방법론을 넘어 텍스트와 이미지를 동시에 통합하여 보다 풍부하고 시각적이며 정보적인 Wikipedia 스타일의 멀티모달 문서를 자동으로 생성하는 새로운 시스템입니다. 특히, WikiSeek이라는 새로운 벤치마크를 제시하여 더 도전적인 주제를 평가하려고 시도했습니다.
- ***Technical Details***: WikiAutoGen 프레임워크는 주제를 텍스트와 이미지로부터 해석하여 구조화된 아웃라인을 생성하는 것을 시작으로, 다양한 에이전트를 통한 지식 탐색, 그리고 다중 관점의 셀프 리플렉션 모듈을 통해 콘텐츠를 평가하고 개선하는 과정을 거쳐 최종 문서를 생성합니다. 특히, 새로운 멀티퍼스펙티브 셀프-리플렉션 모듈은 작성자, 독자, 편집자의 시각에서 콘텐츠를 개선합니다.
- ***Performance Highlights***: WikiAutoGen은 WikiSeek 벤치마크에서 기존 방법들에 비해 작성된 문서의 텍스트 품질과 이미지 품질에서 각각 8%-29%, 11%-14% 개선된 성능을 보였습니다. 텍스트와 이미지를 모두 사용하는 미디어 토픽에서 탁월한 성능을 발휘하여 종합적인 콘텐츠 품질, 정보성, 신뢰성 및 참여도에서 최고 점수를 기록했습니다.

### [LookAhead Tuning: Safer Language Models via Partial Answer Previews](https://arxiv.org/abs/2503.19041)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19041.png)

Vote: 4

Authors: Zhiqiang Zhang, Ningyu Zhang, Mengshu Sun, Mengru Wang, Huajun Chen, Kangwei Liu, Lin Yuan, Lei Liang, Jun Zhou, Yujie Luo

- ***What's New***: LookAhead Tuning은 대규모 언어 모델의 안전성을 유지하면서도 특정 도메인에 대한 적응성을 개선하기 위해 부분 답변 프리뷰를 사용하는 새로운 방법을 제안합니다. 이는 훈련 데이터에 소량의 수정을 가하여 모델의 안전 메커니즘을 강화하고, 퀄리티 저하 없이 작업 성능을 향상시키는 효율적인 솔루션으로 자리잡고 있습니다.
- ***Technical Details***: LookAhead Tuning은 두 가지 접근법을 제안합니다: 실질적 답변(Real Answer)과 가상 답변(Virtual Answer)입니다. 두 방법 모두 원본 모델 구조를 변경하지 않고, 훈련 데이터 자체를 수정하여 부분 답변 프리뷰를 제공함으로써 안전성을 보장합니다. Real Answer 방식은 답변의 초기 m개의 토큰을 훈련 데이터에 포함하고, Virtual Answer 방식은 명백한 답변 노출을 피하기 위해 일반적인 프리픽스(Prefix)를 사용하여 모델을 가이드합니다.
- ***Performance Highlights***: LookAhead Tuning 방법은 안전 평가와 다운스트림 태스크 모두에서 뛰어난 성능을 보였습니다. 특히, LookAhead Tuning(virtual)는 모든 평가 지표에서 최고 성능을 기록하였으며, Vanilla Fine-Tuning 대비 약간 더 많은 시간을 소모하지만 이는 매우 제한적이었습니다. 이 방법은 모델의 초기 토큰에 대한 작은 변화만 허용하여, 안전성을 보존하면서도 실제 답변의 프리픽스 노출을 방지합니다. 이는 또한 LLaMA2-7B-Chat 모델 위에서 실험되었고, 계산 시간을 크게 증가시키지 않고 결과적으로 효율적인 방법임을 입증했습니다.

### [xKV: Cross-Layer SVD for KV-Cache Compression](https://arxiv.org/abs/2503.18893)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18893.png)

Vote: 3

Authors: Kai-Chiang Wu, Luis Ceze, Wei-Cheng Lin, Chi-Chih Chang, Yash Akhauri, Chien-Yu Lin, Mohamed S. Abdelfattah

- ***What's New***: xKV는 클러스터 내 여러 레이어의 Key-Value 캐시(KV-Cache)를 공유된 저랭크 서브스페이스로 통합하는 간단한 사후 훈련 방법을 제안합니다. 이를 통해 기존 최첨단 레이어 간 압축 기술보다 최대 6.8배 높은 압축률을 달성하며, 정확도는 2.7% 향상되었습니다.
- ***Technical Details***: xKV는 Singular Value Decomposition(SVD)를 사용하여 여러 레이어의 KV-Cache를 공유된 저랭크 서브스페이스로 압축합니다. 서로 인접한 레이어 그룹의 지배적 특이 벡터가 잘 정렬되어 있음을 발견해 이를 활용하여 압축률과 정확도 간의 균형을 최적화하고 있습니다.
- ***Performance Highlights***: xKV는 RULER 벤치마크에서 Llama-3.1 및 Qwen2.5와 같은 LLM을 대상으로 테스트하여, 최대 6.8배의 압축률을 기록하며, 기존 방법 대비 2.7%의 정확도 향상을 보였습니다. 또한 Multi-Head Latent Attention(MLA)을 사용하는 모델에서도 정확도의 손실 없이 3배의 압축률을 달성했습니다.

### [When Words Outperform Vision: VLMs Can Self-Improve Via Text-Only Training For Human-Centered Decision Making](https://arxiv.org/abs/2503.16965)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16965.png)

Vote: 3

Authors: Zhe Hu, Yu Yin, Jing Li

- ***What's New***: 이 연구는 VLM(Visual Language Models)이 인간 중심의 의사 결정 과제를 다루는 데 있어 LLM(Language Models)에 비해 예상 외로 낮은 성능을 보인다는 것을 밝혀냈습니다. 이를 해결하기 위해, 이미지를 포함하지 않는 텍스트 기반 데이터로 VLM의 언어적 구성 요소를 강화하여, 대규모 이미지-텍스트 데이터 없이도 성능을 개선할 수 있는 새로운 텍스트-전용 훈련 방식을 제안합니다.
- ***Technical Details***: VLM의 성능을 향상시키기 위해 텍스트-전용 훈련 접근법을 도입하고, 이를 위해 GPT-4o를 사용하여 다양한 시나리오 기반의 훈련 데이터를 생성합니다. 이 접근법은 LLM에서 생성된 데이터로 VLM의 결정을 강화하며, 이미지-텍스트 데이터가 필요 없는 VLM 강화 방법론을 제시합니다.
- ***Performance Highlights***: 텍스트-전용 훈련을 통해 Mllama의 정확도는 75.65%에서 79.60%로, Qwen2-VL은 80.32%에서 83.15%로, LLaVA-OneVision은 78.31%에서 80.81%로 개선되었습니다. 이 결과는 텍스트-전용 훈련이 VLM의 의사 결정 능력을 효과적으로 향상시킨다는 것을 입증하며, 더 큰 교사 모델 없이도 LLM을 활용한 셀프-임프루브먼트(self-improvement)의 가능성을 보여줍니다.

### [FullDiT: Multi-Task Video Generative Foundation Model with Full Attention](https://arxiv.org/abs/2503.19907)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19907.png)

Vote: 3

Authors: Pengfei Wan, Kun Gai, Qiang Xu, Xuan Ju, Qiulin Wang, Quande Liu, Weicai Ye, Xintao Wang, Di Zhang

- ***What's New***: FullDiT는 멀티태스크 비디오 생성에 적합한 획기적인 기반 모델로, 통합 전면 주의 메커니즘을 통해 다양한 조건을 통합합니다. 이 모델은 개별 어댑터 설계로 인한 분기 충돌 및 파라미터 중복 문제를 해결하며, 확장 가능한 멀티태스크 및 멀티모달 제어를 가능하게 합니다.
- ***Technical Details***: FullDiT는 디퓨전 프로세스(Diffusion Process)의 가능성을 활용하며, 비디오 생성에서 조건의 통합을 위해 전면 주의(Self-Attention) 메커니즘을 사용합니다. 다양한 입력 조건을 통합된 시퀀스 표현으로 병합하여 긴 문맥 학습과 동적 조건 처리에 효과적입니다. 추가적으로 FullBench라 불리는 멀티태스크 비디오 생성 평가를 위한 새로운 벤치마크도 제시하였습니다.
- ***Performance Highlights***: FullDiT는 멀티태스크 비디오 생성 실험에서 기존의 어댑터 기반 방법을 능가하는 성과를 보여주며, Emergent Ability를 발휘해 다양한 조건을 효과적으로 결합할 수 있는 능력을 입증했습니다. 특히, 미세한 제어 가능성과 세분화된 영상 품질 향상에 기여하는 것으로 나타났습니다.

### [PhysTwin: Physics-Informed Reconstruction and Simulation of Deformable Objects from Videos](https://arxiv.org/abs/2503.17973)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.17973.png)

Vote: 3

Authors: Hao-Yu Hsu, Yunzhu Li, Kaifeng Zhang, Hsin-Ni Yu, Hanxiao Jiang, Shenlong Wang

- **What's New**: 이 논문에서는 간헐적 영상으로 변형 가능한 물체를 시뮬레이션 할 수 있는 물리적 디지털 트윈(PhysTwin)을 생성하는 새로운 프레임워크를 소개합니다. PhysTwin은 스프링-매스 모델(spring-mass model), 생성적 형태 모델(generative shape model), 가우시안 스플렛팅(Gaussian splatting)을 결합하여 현실적 외형과 정확한 물리적 매개변수를 가진 가상 복제본을 실시간 대화형으로 생성합니다.
- **Technical Details**: PhysTwin 프레임워크는 RGB-D 영상에서 협소한 시야로 재구성을 진행합니다. 스프링-매스 모델을 기반으로 움직임을 시뮬레이션하고 물리적 매개변수를 조정하고, 생성적 3D 모델을 사용하여 완전한 형태를 초기화합니다. 비차별 가능한 매개변수 최적화를 위해 0차 최적화(zero-order optimization)를 사용하고, 스프링 강성을 포함한 밀집 매개변수는 1차 최적화(first-order optimization)로 세밀히 조정합니다. 또한, 외형 모델링을 위해 초기화된 정적 가우시안을 시점 간의 변화를 바탕으로 동적으로 업데이트합니다.
- **Performance Highlights**: 실험 결과에 따르면, PhysTwin은 객체 복원, 렌더링, 미래 상태 예측 및 시뮬레이션 성능에서 경쟁 방법을 능가합니다. 특히 관측과의 일치성이 높으며, 예측된 3D 상태와 실제 관측과의 격차를 줄이는 데 있어 뛰어난 성능을 보입니다. 이러한 특성으로 인해 다양한 변형 가능한 물체의 정교한 상호작용이 가능하며, 로봇 모션 플래닝(model-based robotic motion planning)과 같은 응용에 유리합니다.

### [Gumbel-Softmax Flow Matching with Straight-Through Guidance for Controllable Biological Sequence Generation](https://arxiv.org/abs/2503.17361)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.17361.png)

Vote: 3

Authors: Sophia Tang, Alexander Tong, Yinuo Zhang, Pranam Chatterjee

- ***What's New***: 이 연구는 Gumbel-Softmax 플로우 매칭(Gumbel-Softmax Flow Matching)과 스트레이트-쓰루 가이던스(Straight-Through Guidance)를 사용하여 제어 가능한 생물학적 서열 생성(Controllable Biological Sequence Generation)을 위한 새로운 생성 프레임워크를 소개합니다. 이 접근법은 기존의 방법들이 높은 간섭성의 제약을 받는 문제를 해결하며, 훈련이 필요 없는 가이던스 전략(STGFlow)을 추가하여 강화됩니다.
- ***Technical Details***: 이 프레임워크는 시간이 의존적인 온도 매개변수를 갖는 새로운 Gumbel-Softmax 보간자(Gumbel-Softmax Interpolant)를 정의하고, 이를 통해 부드러운 카테고리 분포에서 단일 단 Simplex의 꼭대기로의 변환을 가능케 하는 매개변수화된 속도장을 생성합니다. 또한 학습된 텐선 흐름 간의 혼합을 따르는 새로운 속도장을 정의하여 고품질의 서열을 달성하게 합니다. STGFlow는 사전 훈련된 분류기(Classifier)를 활용하여, 최적의 꼭대기로의 유도(Guidance)를 효율적으로 수행합니다.
- ***Performance Highlights***: Gumbel-Softmax Flow Matching은 조건부 DNA 프로모터 설계 및 de novo 단백질 서열 생성에서 우수한 성능을 보입니다. 또한 희귀 질병 치료를 위한 타겟-결합 펩타이드(Target-Binding Peptide) 설계에 있어서도 최첨단 성능을 입증하였습니다. 이 방법은 전체적으로 단백질 시퀀스의 구조적 타당성을 유지하면서 서열의 다양성과 독창성을 극대화하는 데 효과적입니다.

### [Strong Baseline: Multi-UAV Tracking via YOLOv12 with BoT-SORT-ReID](https://arxiv.org/abs/2503.17237)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.17237.png)

Vote: 3

Authors: Yu-Hsi Chen

- ***What's New***: 이 논문은 최근 검출 및 추적 기술을 활용하여 열적 적외선 비디오에서 여러 무인 항공기(UAV)를 추적하기 위한 강력한 기반을 소개합니다. YOLOv12와 BoT-SORT에 기반한 새로운 트래킹 프레임워크를 제시하며, 이 방식은 기존의 YOLOv5 및 DeepSORT보다 성능이 우수하다는 것을 입증합니다.
- ***Technical Details***: YOLOv12는 R-ELAN(Residual Efficient Layer Aggregation Network)과 FlashAttention을 결합하여 높은 효율성과 정확성을 제공합니다. BoT-SORT는 Kalman Filter를 기반으로 카메라 움직임 보상 기법을 추가하여 동적인 환경에서도 안정적인 추적을 제공합니다. 추적 성능을 높이기 위해 다양한 실험과 전략이 추가되었습니다.
- ***Performance Highlights***: 이 벤치마크에서 제안된 방법은 4th Anti-UAV Challenge의 메트릭으로 평가되었으며, 기존 기준점 대비 약 2배 이상의 개선을 보여줍니다. 특히 Track 3에서 가장 경쟁력 있는 성과를 보이며, 이미지 개선 및 임시 정보 활용 없이도 높은 성능을 달성하였습니다.

### [Overcoming Vocabulary Mismatch: Vocabulary-agnostic Teacher Guided Language Modeling](https://arxiv.org/abs/2503.19123)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19123.png)

Vote: 2

Authors: Lei Ji, Haebin Shin, Yeyun Gong, Xiao Liu

- ***What's New***: 본 논문에서는 단어 사전 불일치 문제를 극복하기 위해 Vocabulary-agnostic Teacher Guided Language Modeling(VocAgnoLM)을 제안합니다. 이는 서로 다른 단어 사전을 사용하는 교사 모델과 학생 모델 간의 불일치를 다루며, Token-level Lexical Alignment과 Teacher Guided Loss라는 두 가지 핵심 기법을 통해 단어 사전 불일치의 간극을 메웁니다.
- ***Technical Details***: VocAgnoLM의 기술적 핵심은 Token-level Lexical Alignment로, 이는 학생과 교사 모델 간 토큰 시퀀스를 정렬합니다. Teacher Guided Loss는 교사 모델의 로짓 손실을 활용하여 학생 모델의 효과적인 훈련을 유도합니다. 주어진 예시에서 교사와 학생 모델은 각각 7B와 1B 모델로 훈련됩니다.
- ***Performance Highlights***: VocAgnoLM은 Qwen2.5-Math-Instruct를 통해 수행된 실험에서 단어 사전 중복이 6%에 불과함에도 불구하고, 단순한 지속적 사전훈련 방법에 비해 46%의 성능 향상을 달성하였습니다. 이는 현재의 확률 배포 기반 정렬과 비교하여 33% 높은 성능을 보였습니다. 다양한 교사 모델을 활용하여 일관된 성능 향상을 보여줌으로써, Vocabulary-agnostic 방식의 가능성을 실증하였습니다.

### [Frequency Dynamic Convolution for Dense Image Prediction](https://arxiv.org/abs/2503.18783)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18783.png)

Vote: 2

Authors: Lin Gu, Ying Fu, Chenggang Yan, Liang Li, Linwei Chen

- ***What's New***: 이 논문에서는 주파수 동적 컨볼루션(Frequency Dynamic Convolution; FDConv)을 소개하며, 이는 기존의 동적 컨볼루션이 가진 주파수 제한성과 높은 파라미터 비용 문제를 개선합니다. 새로운 방법은 푸리에 도메인에서 고정된 파라미터 예산을 학습하여 주파수-다양한 가중치를 생성하며, 커널 공간 변형(Kernel Spatial Modulation; KSM)과 주파수 대역 변형(Frequency Band Modulation; FBM)도 도입하여 적응성을 강화합니다.
- ***Technical Details***: FDConv는 푸리에 불연속 가중치(Fourier Disjoint Weight)라는 새로운 개념을 도입하며, 이는 푸리에 도메인에서 주파수 기반 그룹으로 파라미터를 분할하여 고유한 주파수 응답을 가진 가중치들을 구성합니다. KSM은 각 필터의 주파수 응답을 공간적 수준에서 동적으로 조정하고, FBM은 주파수 도메인에서 가중치를 서로 다른 주파수 대역으로 분리한 뒤, 지역적 콘텐츠에 따라 동적으로 조정합니다.
- ***Performance Highlights***: FDConv는 ResNet-50에 적용 시 +3.6M 파라미터의 적은 증가로 뛰어난 성능을 보여주며, CondConv, ODConv와 같은 기존 방법들보다 낮은 파라미터 비용으로 높은 성능을 달성합니다. 다양한 아키텍처, ConvNeXt, Swin Transformer 등에 통합이 가능하며, COCO, ADE20K, Cityscapes 같은 벤치마크에서 우수한 결과를 보여주었습니다.

### [FRESA:Feedforward Reconstruction of Personalized Skinned Avatars from Few Images](https://arxiv.org/abs/2503.19207)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19207.png)

Vote: 2

Authors: Junxuan Li, Rohan Joshi, Yaser Sheikh, Shunsuke Saito, Fabian Prada, Hongdong Li, Ziyan Wang, Jason Saragih, Rong Wang, Zhongshi Jiang, Igor Santesteban, Javier Romero, Chengxiang Yin

- ***What's New***: FRESA는 새로운 방식을 제안하여 단 몇 장의 이미지만으로 개인화된 피부가 움직이는 아바타(Skinned Avatars)를 신속하게 재구성하는 방법을 제안합니다. 개인화된 아바타의 형태, 스킨웨이트(Skinning Weights), 그리고 자세에 따른 변형(Pose-dependent Deformations)을 동시에 추정하여 기존 방식보다 더 정확한 기하학적 정확성을 제공합니다.
- ***Technical Details***: FRESA는 수천 명의 옷을 입은 인간 데이터를 학습하여 범용적인 사전 모델을 형성하였으며, 이를 통해 개인화된 기본 형태와 스킨웨이트를 추론하는 과정을 포함합니다. 자세 변화를 일반화하고 캐노니칼라이저(Canonicalization) 과정을 통해 픽셀 맞춤 초기 조건을 생산하여 자세 변화에 따른 기하학적 세부사항을 재구성할 수 있습니다.
- ***Performance Highlights***: FRESA는 다른 최첨단 방법들과 비교하여 더 높은 수준의 재구성 및 애니메이션 품질을 제공합니다. 테스트 결과, FRESA는 주어진 데이터에서 현재의 최고 방법들을 능가하는 성능을 보였으며, 캐주얼한 상황에서 촬영된 사진에서도 일반화가 가능합니다.

### [Towards a Unified Copernicus Foundation Model for Earth Vision](https://arxiv.org/abs/2503.11849)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.11849.png)

Vote: 2

Authors: Chenying Liu, Angelos Zavras, Zhitong Xiong, Yi Wang, Nikolaos Ioannis Bountos, Adam J. Stewart, Franziska Gerken, Laura Leal-Taixé, Thomas Dujardin, Xiao Xiang Zhu, Ioannis Papoutsis

- ***What's New***: 이번 연구에서는 Copernicus Sentinel 미션의 모든 주요 임무로부터 1,870만 장의 이미지를 통합하여 대규모 사전 학습 데이터셋 Copernicus-Pretrain을 제안합니다. 이어서 다양한 센서 모드를 처리할 수 있는 Copernicus-FM이라는 통합된 기초 모델을 제시하고, 마지막으로 15개의 단계적 다운스트림 작업으로 구성된 Copernicus-Bench라는 평가 벤치마크를 도입하여 기후 연구와의 연결 기회를 창출합니다.
- ***Technical Details***: Copernicus-FM은 동적 하이퍼네트워크(Dynamic Hypernetworks)를 사용하여 광학 및 비광학 입력을 처리하고 메타데이터 통합을 지원하는 유연한 인프라입니다. MIM(마스크 이미지 모델링)을 통한 훈련을 수행하며, 추가적인 계속적인 증류를 통해 다양한 응용 분야에서 활용될 수 있습니다.
- ***Performance Highlights***: Copernicus-FM은 기존의 EO(지구 관측) 모델보다 다양한 센서와 작업에 대해 확장성과 유연성을 크게 개선하였으며, 지표 및 대기 작업 모두에서 성능 향상을 증명하였습니다. 또한 기후 예측 시스템에 EO 임베딩을 결합하여 기후 예측의 정확도를 높이는 새로운 가능성을 열어주고 있음을 논의합니다.

### [ST-VLM: Kinematic Instruction Tuning for Spatio-Temporal Reasoning in Vision-Language Models](https://arxiv.org/abs/2503.19355)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19355.png)

Vote: 1

Authors: Vijay Kumar B. G, Manmohan Chandraker, Minseo Yoon, Dohwan Ko, Sihyeon Kim, Yumin Suh, Hyunwoo J. Kim

- ***What's New***: ST-VLM은 시각-언어 모델(Vision-Language Models; VLMs)에 시공간적 추론 능력을 강화하기 위한 운동학적 교육 데이터인 STKit과 STKit-Bench를 도입하여 역학적 이해를 중점적으로 다루고 있습니다. 이 모델은 다양한 동영상 기반 VLM 벤치마크에서 기존 모델을 능가하며 복잡한 다단계 추론을 가능하게 합니다.
- ***Technical Details***: STKit은 실제 동영상에 3D 주석을 포함하여 운동학적 요소를 추론하는 데이터셋으로, 이동 거리, 속도, 움직임 방향 등의 정보를 제공합니다. ST-VLM은 LLaVA-OneVision 모델을 개선하여 라벨링 된 데이터와 4D 재구성 기반의 의사라벨(unlabeled)의 데이터로 학습됨으로써 시공간적 추론 능력이 강화되었습니다.
- ***Performance Highlights***: ST-VLM은 STKit-Bench에서 31.3%의 향상된 성능을 보이며, 여러 가지 시공간적 벤치마크에서 기존의 최고 성능 모델들을 넘어섰습니다. 시각적, 시간적 이해 모두에 강점을 보이며, 다양한 도메인과 과제를 넘어 강력한 일반화 성능을 나타냅니다.

### [LPOSS: Label Propagation Over Patches and Pixels for Open-vocabulary Semantic Segmentation](https://arxiv.org/abs/2503.19777)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19777.png)

Vote: 1

Authors: Jiří Matas, Giorgos Tolias, Yannis Kalantidis, Vladan Stojnić

- ***What's New***: LPOSS는 비전과 언어 모델(Vision-and-Language Models; VLMs)을 활용하여 개방형 어휘 의미 분할을 위한 훈련이 필요 없는 새로운 방법을 제안했습니다. 이 방법은 패치 간 관계를 통합하여 레이블 전파(Label Propagation)를 통해 VLM의 초기 패치별 예측을 최적화합니다. 이를 통해 이미지 전체에 대한 맥락적 상호작용을 고려하여 패치 및 픽셀 수준에서 레이블을 예측하고 전파하며, 특히 클래스 경계 근처에서의 분할 정확도를 크게 향상시킵니다.
- ***Technical Details***: VLMs는 주로 교차 모달 정렬을 최적화하므로 내부 모달 유사성을 잘 포착하지 못합니다. 이를 개선하기 위해 LPOSS는 본래 패치별로 예측하는 VLM에 대해 별도의 비전 모델(Vision Model; VM)을 도입하여 초기 예측을 개선합니다. 특히, LPOSS+는 픽셀 수준 그래프에서 레이블 전파를 수행함으로써 패치 수준에서 픽셀 수준으로 예측 정확도를 더욱 정밀하게 조정합니다. 이는 초기 VLM 예측이 있는 상태에서 쌍별 용어를 포함한 이차 비용 함수 최적화를 통해 성능을 향상시킵니다.
- ***Performance Highlights***: 8개의 다양한 데이터셋에서 테스트한 결과, LPOSS+는 mIoU 및 Boundary IoU 측정항목에서 기존의 무훈련 방법을 능가하며 특히 클래스 경계 근처에서 뛰어난 성능을 보였습니다. CLIP-DINOiser나 ProxyCLIP 등과 비교했을 때 전체적으로 더 나은 성능을 발휘했으며, 특정 데이터셋에서는 여전히 향상 여지가 있습니다.

### [LLaVAction: evaluating and training multi-modal large language models for action recognition](https://arxiv.org/abs/2503.18712)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18712.png)

Vote: 1

Authors: Mackenzie W. Mathis, Alexander Mathis, Haozhe Qi, Shaokai Ye

- ***What's New***: LLaVAction은 멀티모달 대형 언어 모델(Multi-Modal Large Language Models; MLLMs)을 동작 인식(Action Recognition) 작업에서 평가하고 개선하기 위해 설계된 새로운 접근법입니다. 이 연구에서는 복잡한 행동 이해 작업을 위한 강력한 후보로 떠오른 MLLMs가 시각적으로나 의미적으로 유사한 행동을 인식하는데 어려움을 겪고 있다는 점을 발견했습니다. 이를 해결하기 위해 EPIC-KITCHENS-100 데이터셋을 비디오 다중 선택 질문 및 답변 형태(EPIC-KITCHENS-100-MQA)로 재구성하고, 여러 혁신적인 방법들을 제안하여 모델의 성능을 크게 향상시켰습니다.
- ***Technical Details***: EPIC-KITCHENS-100-MQA 벤치마크는 EPIC-KITCHENS-100 데이터셋을 기반으로 만들어졌으며, 비디오 다중 선택 질문 및 답변이라는 형태로 변환되었습니다. 어려운 '괴롭힘' 예제를 통해 MLLMs를 평가하였고, 모델의 동작 인식 능력을 향상시키기 위해 'Vision Token Supervision', 'Temporal Detection', 그리고 'Prior Action Memory'와 같은 여러 새로운 방법들을 도입하였습니다. 각 데이터는 최첨단(SOTA) 모델에 의해 생긴 도전 과제로 구성되었습니다.
- ***Performance Highlights***: LLaVAction은 EPIC-KITCHENS-100 및 EPIC-KITCHENS-100-MQA에서 최첨단 성능을 달성하였으며, GPT-4o를 21포인트 정확도 차이로 초과합니다. 또한 EgoSchema, PerceptionTest, LongVideoBench, VideoMME, MVBench와 같은 다양한 비디오 벤치마크에서도 개선된 성능을 보였습니다. 이러한 결과는 MLLMs가 복잡한 동작 인식 작업에 대한 유망한 경로임을 시사합니다.

### [Can Vision-Language Models Answer Face to Face Questions in the Real-World?](https://arxiv.org/abs/2503.19356)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19356.png)

Vote: 1

Authors: Apratim Bhattacharyya, Rishit Dagli, Guillaume Berger, Reza Pourreza, Sunny Panchal, Roland Memisevic

- ***What's New***: 이 논문은 Qualcomm Interactive Video Dataset(IVD)라는 새로운 데이터셋을 소개하며, 실제 세계에서 'face-to-face' 상호작용을 통해 대형 멀티모달 모델(Large Multimodal Models; LMMs)을 평가할 수 있는 환경을 제공합니다. 이를 통해 음성, 영상 입력을 실시간으로 처리하며 사용자의 질문에 답변할 수 있는 모델의 능력을 측정합니다.
- ***Technical Details***: Qualcomm IVD는 사용자가 카메라와 마이크를 통해 개방형 질문을 던지고, 시스템이 실시간으로 적절한 답변을 해야 하는 온라인 질문-응답 방식으로 구성됩니다. 데이터셋은 시각적 이해와 행동 이해, 오디오-비주얼 개념 통합 능력을 테스트하기 위해 다양한 상황에서 수집된 2900개의 동영상 클립과 질문-답변 쌍으로 구성됩니다. 각 동영상은 타임스탬프가 포함된 질문과 그에 대한 답변, 그리고 질문에 답하기 적절한 타이밍을 나타내는 레이블을 포함하고 있습니다.
- ***Performance Highlights***: 현재의 AI 모델들은 실시간 시나리오에서 인간의 성능에 비해 현저히 떨어지는 성능을 보이고 있으며, 특히 행동 횟수 세기, 시각-청각 정보의 통합, 적절한 답변 타이밍 판단 분야에서 약점을 드러냈습니다. 그러나 적절한 데이터로 모델을 미세 조정하는 경우, 이러한 성능 격차를 상당 부분 줄일 수 있음을 보여 주었습니다. 예컨대, 오디오 정보를 포함한 학습 데이터로의 미세 조정은 특정 작업 카테고리에서 상당한 성능 개선을 가져왔습니다.

### [DiffPortrait360: Consistent Portrait Diffusion for 360 View Synthesis](https://arxiv.org/abs/2503.15667)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.15667.png)

Vote: 0

Authors: Phong Tran, Yuming Gu, Adilbek Karmanov, Yujian Zheng, Hongyi Xu, Hao Li, Heyuan Li

- ***What's New***: DiffPortrait360은 단일 이미지에서 인간, 스타일화된 형태, 심지어 동물 형태의 360도 일관된 머리 이미지 생성을 가능하게 하는 새로운 접근법을 소개합니다. 이 기술은 컨트롤 가능한 3D 헤드 생성의 첫 번째 방법으로, 스타일과 세부 사항에 관계없이 전방위의 새로운 시점을 생성할 수 있습니다.
- ***Technical Details***: 이 방법은 기존의 DiffPortrait3D 프레임워크를 기반으로 개발되었으며, 특별히 뒷머리의 세부사항을 생성하기 위한 커스텀 ControlNet과 전방-후방 일관성을 보장하기 위한 이중 외형 모듈을 포함합니다. 연속적인 뷰 시퀀스를 사용하여 훈련하며, 추가적인 뒷참조 이미지를 통합하여 글로벌 외형 일관성을 개선합니다. 그리고 주요 이미지 생성기를 안정적인 Diffusion 디노이징 네트워크로 조건화합니다.
- ***Performance Highlights***: DiffPortrait360은 기존 방법들보다 뛰어난 디테일과 일관성을 가진 360도 머리 뷰를 생성할 수 있으며, 복잡한 조명 조건, 다양한 헤어스타일, 다양한 얼굴각도 및 다양한 스타일을 다룰 수 있습니다. 주요 성과 지표에 있어서, 본 방법은 인간 머리 생성 벤치마크에서 최신 생성 방법들보다 유의미한 향상을 보여줍니다.

### [OpenCity3D: What do Vision-Language Models know about Urban Environments?](https://arxiv.org/abs/2503.16776)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16776.png)

Vote: 0

Authors: Francis Engelmann, Valentin Bieri, Nicolas S. Blumer, Qingxuan Chen, Marco Zamboni

- ***What's New***: OpenCity3D는 도시 환경에 대한 높은 수준의 분석을 가능하게 하여 비전-언어 모델(Vision-Language Models; VLMs)의 새로운 활용을 제시합니다. 이 접근 방식은 대규모 도시 3D 장면 분석을 통해 범죄율, 인구 밀도, 부동산 가격 등 다양한 고급 속성을 예측합니다.
- ***Technical Details***: OpenCity3D는 다중 시점 공중 이미지에서 렌더링된 RGB-D 이미지로부터 VLM 기능이 통합된 언어 강화 점 구름(language-enriched point cloud)을 생성합니다. 이 점 구름은 건물 또는 인구 밀도와 같은 도시 객체와 속성의 분석을 가능하게 합니다. SigLIP와 같은 언어 인코더(language encoders)를 사용하여 자연어 쿼리 기반의 분석을 지원합니다.
- ***Performance Highlights***: 건물 연령 예측에서는 스피어만 상관계수가 네덜란드 7개 도시 중 4개 도시에서 50% 이상을 기록했습니다. 주택 가격 예측에서는 Zillow 데이터와 비교하여 0.25백만 달러의 평균 절대 오차(MAE)를 달성하였습니다. 이는 VLM이 도시 부동산 가치 이해에 일정한 영향을 미칠 수 있음을 시사합니다.

### [Co-SemDepth: Fast Joint Semantic Segmentation and Depth Estimation on Aerial Images](https://arxiv.org/abs/2503.17982)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.17982.png)

Vote: 0

Authors: Yara AlaaEldin, Francesca Odone

- ***What's New***: Co-SemDepth는 항공 이미지 내에서 단안 카메라를 활용하여 빠른 속도로 깊이 추정(Depth Estimation)과 의미론적 분할(Semantic Segmentation)을 동시에 수행하는 새로운 합동 심층 학습 아키텍처입니다. 이 아키텍처는 MidAir와 Aeroscapes와 같은 벤치마크 데이터셋에서 효과적으로 검증되었으며, 기존의 단일 및 합동 아키텍처 방법들과 경쟁하거나 우위에 있는 성능을 보입니다.
- ***Technical Details***: Co-SemDepth는 공유된 인코더와 깊이 및 의미론적 맵 예측을 위한 각각의 디코더로 구성된 다중 과제 처리(멀티태스킹) 공유 인코더 아키텍처를 채택하고 있습니다. 이 아키텍처는 M4Depth와 M4Semantic이라는 두 아키텍처를 통합하여 파라미터 수를 줄이고, 두 모듈 사이 학습된 특징을 공유함으로써 성능을 개선합니다. 중요한 손실 함수로는 깊이와 의미론적 손실을 결합하는 Ltotal이 사용됩니다.
- ***Performance Highlights***: Co-SemDepth는 NVIDIA Quadro P5000 GPU에서 초당 20.2 프레임(FPS)을 예측하며, 높은 효율성과 적은 메모리 소비를 자랑합니다. 또한, MidAir 데이터셋에서의 비교 테스트 결과, 다른 방법에 비해 깊이 및 의미론적 정확도 측면에서 더 나은 성능을 제공했습니다.

### [FirePlace: Geometric Refinements of LLM Common Sense Reasoning for 3D Object Placement](https://arxiv.org/abs/2503.04919)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.04919.png)

Vote: 0

Authors: Ian Huang, Yanan Bao, Alireza Fathi, Karen Truong, Leonidas Guibas, Cordelia Schmid, Howard Zhou

- ***What's New***: FirePlace는 오프 더 셀프 MLLM을 사용하여 자연어 지시와 함께 새로운 3D 객체를 기존 장면에 배치하는 혁신적인 프레임워크입니다. 이 프레임워크는 3D 장면에서 기하학적 세부사항을 추출하고, 공통 상식에 부합하는 최종 배치를 가지치기하는 방식을 통해 MLLM의 상식과 저수준의 기하학적 제약을 결합합니다.
- ***Technical Details***: FirePlace는 (1) 3D 장면의 Universal Scene Descriptor(USD), (2) 카메라 각도, (3) 배치될 새로운 객체 메쉬, (4) 객체 배치에 관한 언어적 설명을 입력으로 받아들입니다. 언어 설명은 '제약 스케치'로 번역되고, 이 스케치는 3D 제약을 정의하며, 각 제약에 대해 연관된 객체 표면을 명시합니다. 이어서 Batched Visual Selection 기법을 통해 수많은 후보군 중에서 시각적 선택을 실행하여 포괄적인 위치 후보군을 도출합니다.
- ***Performance Highlights***: FirePlace의 실험 결과, 복잡한 장면에서도 현실적이면서 공감되는 객체 배치를 더 효과적으로 생성하는 데 성공하였으며, 기존의 Holodeck과 LayoutGPT 기반 메서드를 각각 전반적인 평가 척도에서 초과했습니다. Human 평가에서는 물리적 타당성과 상식적인 배치 판단의 조화를 증명하며 사용자의 선호도에서 높은 점수를 기록했습니다.

### [Global-Local Tree Search for Language Guided 3D Scene Generation](https://arxiv.org/abs/2503.18476)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18476.png)

Vote: 0

Authors: Mengshi Qi, Wei Deng, Huadong Ma

- ***What's New***: 이 논문은 3D 실내 장면 생성을 위해 글로벌-로컬 트리 탐색 알고리즘(Global-Local Tree Search Algorithm)을 제안함으로써 대형 비전-언어 모델(Vision-Language Models; VLMs)의 추론 능력을 향상시킵니다. 이 알고리즘은 장면 구조를 계층적으로 표현하여 비전을 통해 공간적 추론을 수행합니다.
- ***Technical Details***: 논문은 계층적 장면 표현을 통해 주어진 입력에서 방(room) 수준, 영역(region) 수준, 바닥 객체(floor object) 수준, 지지 객체(supported object) 수준으로 장면을 분해합니다. 글로벌 트리 탐색(global tree search) 방법은 각 객체를 순차적으로 배치하며, 로컬 트리 탐색(local tree search)은 개별 객체의 위치를 결정하는 방식으로 작동합니다. VLM을 사용해 상위-하위 보기 공간을 조밀한 그리드로 불연속화하고 다양한 이모티콘을 사용하여 셀을 식별 가능한 상태로 만듭니다.
- ***Performance Highlights***: 양적 및 질적 실험 결과, 제안된 방법은 최첨단 접근 방식보다 더 설득력 있는 3D 장면을 생성하며, 사용자 연구 결과 제안된 방법이 세 가지 접근 방식 중 가장 우수한 것으로 평가됩니다.

