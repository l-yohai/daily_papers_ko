## Daily Papers (2025-03-10)

### [RuCCoD: Towards Automated ICD Coding in Russian](https://arxiv.org/abs/2502.21263)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.21263.png)

Vote: 110

Authors: Airat Valiev, Ivan Sviridov, Elena Tutubalina, Andrey Sakhovskiy, Aleksandr Nesterov, Vladimir Makharev, Petr Anokhin, Galina Zubkova

- ***What's New***: RuCCoD는 러시아어로 자동화된 ICD 코딩을 탐구하기 위한 새로운 데이터셋을 선보였습니다. 이 데이터셋은 10,000개 이상의 엔티티와 1,500개 이상의 고유한 ICD 코드를 포함하고 있으며, BERT, LLaMA with LoRA, RAG와 같은 최신 모델을 벤치마크로 사용하여 러시아어 임상 코딩의 잠재력을 평가합니다.
- ***Technical Details***: RuCCoD는 러시아어 건강기록(EHR)에서 치료 내역을 수집하고 ICD-10 CM(Clinical Modification) 시스템을 기반으로 라벨링된 데이터셋입니다. 입실된 의료 전문가들이 각 엔트리를 수동으로 검토하여 철저한 익명화를 보장하였습니다. 데이터셋은 RuCCoD에서 최적의 모델을 사용하여 2017~2021년 환자 기록을 레이블링하였으며, 대출 및 이식학습을 통해 각 도메인 간 성능 평가를 수행하였습니다.
- ***Performance Highlights***: 실험 결과, 자동으로 예측된 코드로 훈련하면, 의사에 의해 수동으로 주어진 코드에 비해 진단 예측 정확성이 28% 상승하는 Macro-averaged F1-score 증가를 보여주었습니다. 이 모델은 자동화된 레이블링 데이터로 훈련된 경우 원 데이터에 비해 빠른 학습 및 향상된 일반화를 통해 진단 성능을 크게 향상시킵니다. 이는 AI 기반 진단 예측의 가능성을 보여주는 결과입니다.

### [Unified Reward Model for Multimodal Understanding and Generation](https://arxiv.org/abs/2503.05236)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05236.png)

Vote: 86

Authors: Yibin Wang, Jiaqi Wang, Yuhang Zang, Cheng Jin, Hao Li

- **What's New**: UNIFIEDREWARD는 멀티모달 이해 및 생성 과제를 평가하기 위한 최초의 통합 보상 모델입니다. 이 모델은 다양한 시각적 작업에서 선호도 정렬을 가능하게 해주는 쌍별 순위(Pairwise Ranking)와 점별 점수(Pointwise Scoring)를 제공하여, 비전 모델(Vision Models)의 선호도 최적화에 활용할 수 있습니다.
- **Technical Details**: UNIFIEDREWARD는 이미지 및 비디오 생성/이해 작업을 포함한 대규모 사용자 선호 데이터셋을 기반으로 개발되었습니다. 이 보상 모델은 비전 모델의 출력을 쌍별 순위 및 점별 스코어링을 통해 단계적으로 필터링하여 고품질의 선호도 쌍 데이터를 자동으로 구축합니다. 이러한 데이터는 Direct Preference Optimization(DPO)을 통해 모델 선호도 정렬에 사용됩니다.
- **Performance Highlights**: 실험 결과, 다양한 시각적 작업을 공동 학습하는 것이 개별 분야의 성능을 크게 향상시킴을 보여줌으로써, 이미지 및 비디오 이해/생성 작업 모두에 걸쳐 품질 및 정렬이 크게 개선되었습니다. 이는 멀티모달 과제 전반에서 보상 모델의 범위를 확장하고 다양한 시각적 응용에서 더 적응 가능하고 일반화된 효과를 발휘하도록 만듭니다.

### [EuroBERT: Scaling Multilingual Encoders for European Languages](https://arxiv.org/abs/2503.05500)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05500.png)

Vote: 57

Authors: Emmanuel Malherbe, Nicolas Boizard, Nuno M. Guerreiro, Maxime Peyrard, Duarte M. Alves, Hippolyte Gisserot-Boukhlef, Ayoub Hammal, André Martins, João Alves, Gabriel Hautreux, Caio Corro, Pierre Colombo, Etienne Malaboeuf, Ricardo Rei, Patrick Fernandes, Céline Hudelot, Kevin El-Haddad, Fanny Jourdan, Manuel Faysse

- ***What's New***: EuroBERT는 유럽 및 세계 주요 언어를 아우르는 다중언어 인코더(multilingual encoders) 패밀리로, 최근 발전된 디코더(decoder) 모델의 아키텍처 향상을 통합하여 새롭게 개발된 모델입니다. 이 모델들은 수학과 코딩 관련 작업에서 뛰어난 성능을 보이며, 최대 8,192 토큰 시퀀스를 자연스럽게 지원합니다.
- ***Technical Details***: EuroBERT 모델은 표준 디스 트랜스포머(dense transformer) 아키텍처를 기반으로 설계되었으며, 그루핑 쿼리 어텐션(grouped query attention), 스위시 게이티드 선형 유닛(swish gated linear units), 루트 평균 제곱 레이어 정규화(root mean square layer normalization), 회전 위치 임베딩(rotary position embeddings)과 같은 여러 최신 아키텍처 변화를 통합했습니다. 5T토큰 다중언어 데이터셋을 사용하여 두 단계의 학습 파이프라인을 통해 마스크드 언어 모델링(masked language modeling) 목표로 학습되었습니다.
- ***Performance Highlights***: EuroBERT 모델은 여러 다중언어 작업과 수학 및 코딩 관련 작업에서 기존 모델보다 더 나은 성능을 보여줍니다. 가장 큰 EuroBERT-2.1B 모델은 12개의 벤치마크 중 7개에서 최고 성능을 기록했으며, XLM-RoBERTa-XL과 같은 모델보다 우수한 성능을 발휘했습니다. 특히, 문서 순위 및 코드, 수학 관련 작업에서 뛰어난 성능을 나타냅니다.

### [S2S-Arena, Evaluating Speech2Speech Protocols on Instruction Following with Paralinguistic Information](https://arxiv.org/abs/2503.05085)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05085.png)

Vote: 39

Authors: Zhiyu Lin, Feng Jiang, Fan Bu, Yuhao Du, Benyou Wang, Haizhou Li

- ***What's New***: S2S-Arena는 음성 모델(Speech Models)의 지시 수행 능력을 평가하는 새로운 벤치마크로, 음성 내외에서 마치 하나의 경기장처럼 구성해 실제 작업에서의 언어 및 패러링귀스틱(paralinguistic) 정보를 포함합니다.
- ***Technical Details***: S2S-Arena는 텍스트-음성 변환과 생방송 녹음을 결합한 154개의 샘플을 통해 네 가지 도메인에서 21개의 작업을 수행합니다. 이 벤치마크는 음성 이해와 생성 모두에서 패러링귀스틱 정보를 고려한 문제를 포함하며, 사람이 수작업으로 음성 모델을 경기장 방식으로 종합 평가합니다.
- ***Performance Highlights***: 실험 결과 GPT-4o 기반의 음성 모델이 지시 수행에서 우수한 성능을 보였지만, 여전히 패러링귀스틱 정보를 포함한 적절한 음성 생성에는 도전 과제가 남아 있는 것으로 나타났습니다. 카스케이드(Cascade) 방식 모델이 결합형 학습 모델보다 더 나은 성능을 보여줍니다. 반면, 비-멀티모달 및 다국어 지원의 부족함이 새로운 도전 과제로 지적되었습니다.

### [Sketch-of-Thought: Efficient LLM Reasoning with Adaptive Cognitive-Inspired Sketching](https://arxiv.org/abs/2503.05179)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05179.png)

Vote: 37

Authors: Jinheon Baek, Sung Ju Hwang, Simon A. Aytes

- ***What's New***: Sketch-of-Thought(SoT)는 LLMs에서의 중간 산출물의 장황함을 줄이기 위해 설계된 새로운 프롬프트 프레임워크입니다. 언어적 제약을 결합하여 토큰 사용을 최소화하면서 정확성을 유지하기 위한 SoT는 다양한 인지 과학 기반의 추론 패러다임을 동적으로 통합할 수 있습니다.
- ***Technical Details***: SoT는 대형 언어 모델(LLM)에 적용되는 새로운 추론 프레임워크로, 세 가지 인지 과학 기반 패러다임(Conceptual Chaining, Chunked Symbolism, Expert Lexicons)을 사용하여 자연어 프롬프트를 통해 효율적인 단계를 제공하며, 경량 라우터 모델을 통해 적절한 패러다임을 입체적으로 선택합니다. 이를 통해 중간 단계에서의 토큰 사용량을 76%까지 줄이면서 정확성을 크게 저해하지 않습니다.
- ***Performance Highlights***: 15개의 다양한 데이터셋에 대한 포괄적인 평가에서 SoT는 토큰 사용을 76.22%까지 줄이면서도 정확성 손실은 거의 없음을 보였습니다. 수학 및 멀티-홉 추론 영역에서는 오히려 정확성이 향상되는 모습을 보였으며, 이는 적은 토큰으로도 기존 체인-오브-폰트 프롬프트와 동등하거나 더 나은 성능을 발휘할 수 있음을 시사합니다.

### [R1-Zero's "Aha Moment" in Visual Reasoning on a 2B Non-SFT Model](https://arxiv.org/abs/2503.05132)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05132.png)

Vote: 21

Authors: Hengguang Zhou, Minhao Cheng, Ruochen Wang, Cho-Jui Hsieh, Xirui Li, Tianyi Zhou

- ***What's New***: 이 논문에서는 비지도 강화 학습(Non-Supervised Fine-Tuning; Non-SFT) 기반의 R1-Zero 모델이 멀티모달 추론 작업에서 'aha moment'와 같은 emergent 특성을 처음으로 성공적으로 복제한 사례를 제시합니다. 이 모델은 Qwen2-VL-2B 모델을 활용하여 SAT 데이터셋 상에 Reinforcement Learning을 적용함으로써 기존 모델 대비 약 30% 더 높은 성능을 기록하였습니다.
- ***Technical Details***: GRPO 알고리즘을 사용하여 Qwen-2-VL-2B(Non-SFT) 모델을 기반으로 멀티모달 추론 능력을 증진시켰습니다. 우리의 접근 방식은 SAT 데이터셋에서 각 질문을 해결하기 위한 Chat Template와 Prompting 전략을 활용하여 모델의 추론 능력을 강화합니다. GRPO는 기존의 PPO의 복잡한 요소들을 단순화하여 정책 모델의 응답을 평균 보상으로 삼는 방식으로 이점을 계산합니다. 보상 모델을 사용하지 않고, 응답 형식 및 정확도에 기반한 룰 기반 보상 함수를 활용했습니다.
- ***Performance Highlights***: VisualThinker-R1-Zero 모델은 CVBench에서 59.47%의 정확도를 기록하며, 그 성능은 전처리된 Non-SFT 모델 대비 약 30% 이상, 교육받은 SFT 모델보다 약 2% 높은 수준입니다. 또한 멀티모달 ‘aha moment’를 통해 모델이 피드백과 응답 길이 증가 중 자가 성찰을 하게 되는 과정을 관찰했습니다. 결과적으로, 비지도 하에 멀티모달 R1 논리 수준을 구현할 수 있음을 보여주었습니다.

### [Forgetting Transformer: Softmax Attention with a Forget Gate](https://arxiv.org/abs/2503.02130)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.02130.png)

Vote: 18

Authors: Evgenii Nikishin, Zhixuan Lin, Aaron Courville, Xu Owen He

- ***What's New***: 이 연구는 Transformer에 데이터에 따라 정보를 선택적으로 잊도록 설계된 'Forget Gate'를 통합한 새로운 모델인 Forgetting Transformer(FoX)를 소개합니다. FoX는 긴 문맥 언어 모델링 및 짧은 문맥 작업에서 기존 Transformer를 능가하며, FlashAttention 알고리즘과 호환됩니다.
- ***Technical Details***: FoX는 Forgetting Attention을 통해 기본 softmax attention을 수정하여 forget gate를 도입합니다. 이 메커니즘은 데이터에 따라 가중치를 조정하여 정보의 중요성을 다르게 평가합니다. Forgetting Attention은 수직적인 방식으로 구현되며 FlashAttention 알고리즘에 통합되어 하드웨어 효율성을 보장합니다.
- ***Performance Highlights***: 실험 결과, FoX는 긴 문맥 언어 모델링에서는 Transformer와 동일한 수준의 성능을 보여주며, 짧은 문맥 작업에서는 Transformer 및 여러 순환 신경망(recurrent sequence models)을 뛰어넘는 성능을 보입니다. 특히 'Pro' 블록 디자인을 도입하여 FoX와 Transformer 모두의 성능을 개선했습니다.

### [R1-Searcher: Incentivizing the Search Capability in LLMs via Reinforcement Learning](https://arxiv.org/abs/2503.05592)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05592.png)

Vote: 17

Authors: Jinhao Jiang, Huatong Song, Zhipeng Chen, Lei Fang, Wayne Xin Zhao, Ji-Rong Wen, Jie Chen, Yingqian Min

- ***What's New***: R1-Searcher는 대형 언어 모델(Large Language Models; LLMs)의 검색 기능을 강화하기 위해 보상학습(Reinforcement Learning; RL)을 활용한 새로운 두 단계의 결과 기반 RL 접근법을 제안했습니다. 이 방법은 LLMs가 외부 검색 시스템을 자동으로 호출하여 문제 해결 과정에서 추가적인 지식을 얻을 수 있도록 설계되었습니다.
- ***Technical Details***: R1-Searcher는 외부 검색 환경을 탐색할 수 있는 두 단계의 결과 기반 RL 방법을 설계했습니다. 첫 번째 단계에서는 검색 호출 형식을 학습하도록 하여 검색 작동을 유도하는 보상을 제공하며, 두 번째 단계에서는 외부 검색 시스템을 효과적으로 활용하여 문제를 해결하도록 장려합니다. 이 방법은 RL만을 사용하여 추론 과정에서 외부 정보를 탐색하고 학습할 수 있도록 하며, 롤아웃 및 검색 마스크 기반 손실 계산을 통한 최적화를 수행합니다.
- ***Performance Highlights***: R1-Searcher는 HotpotQA와 2WikiMultiHopQA의 네 가지 멀티홉 QA 벤치마크에서 기존 RAG 방법 및 GPT-4o-mini와 같은 폐쇄형 소스 모델에 비해 48.22% 및 21.72%까지 향상된 성능을 보여주었습니다. Bamboogle에서 32B 파라미터를 사용하는 Search-o1보다 11.4% 더 나은 성능을 나타내며, 온라인 검색 시나리오에서도 뛰어난 일반화를 보여주었습니다.

### [SafeArena: Evaluating the Safety of Autonomous Web Agents](https://arxiv.org/abs/2503.04957)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.04957.png)

Vote: 15

Authors: Xing Han Lù, Siva Reddy, Esin Durmus, Arkil Patel, Spandana Gella, Alejandra Zambrano, Nicholas Meade, Karolina Stańczak, Ada Defne Tur

- ***What's New***: SAFEARENA는 웹에서 자율 에이전트(Autonomous Web Agents)의 악의적 사용 가능성을 평가하는 최초의 벤치마크입니다. 이 벤치마크는 LLM(대형 언어 모델) 기반 웹 에이전트의 잘못된 사용을 평가하기 위해 개발되었습니다.
- ***Technical Details***: SAFEARENA는 네 개의 웹사이트에서 250개의 안전한 작업과 250개의 유해한 작업으로 구성되어 있으며, 유해한 작업은 허위 정보, 불법 활동, 괴롭힘, 사이버 범죄, 사회적 편견의 다섯 가지 유형으로 분류됩니다. 에이전트의 위험 수준을 네 가지 리스크 단계에 따라 평가하는 Agent Risk Assessment(ARIA) 프레임워크를 소개하여 LLM 기반 웹 에이전트의 악의적인 요청을 충족시키기 위한 규범을 제안합니다.
- ***Performance Highlights***: GPT-4o, Qwen-2와 같은 모델은 각각 유해 요청의 34.7%, 27.3%를 완료하여 의외로 악의적인 요청에 대해 높은 순응도를 보였습니다. 반면 Claude-3.5-Sonnet은 유해한 작업에 대해 비교적 높은 거부율을 보였지만, 모든 작업에서 비의도적 감옥회피(jailbreak)에 쉽게 공격 당할 수 있음을 확인했습니다. 이러한 결과는 웹 에이전트의 안전성을 높이기 위한 절차가 절실하다는 것을 강조합니다.

### [VideoPainter: Any-length Video Inpainting and Editing with Plug-and-Play Context Control](https://arxiv.org/abs/2503.05639)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05639.png)

Vote: 13

Authors: Mingdeng Cao, Qiang Xu, Xuan Ju, Yuxuan Bian, Ying Shan, Zhaoyang Zhang, Liangbin Xie

- ***What's New***: VideoPainter는 플러그 앤 플레이 방식으로 컨텍스트 제어가 가능한 어떠한 길이의 비디오에도 적용할 수 있는 최초의 이중 분기(dual-branch) 비디오 인페인팅(frame 복원) 및 편집(editing) 프레임워크입니다. 본 연구는 사용자 지정 제어 기능을 제공하며, 이는 어떠한 mask 타입에도 일반화가 가능하여 배경 통합과 전경 생성 능력을 향상시킵니다.
- ***Technical Details***: VideoPainter는 사전 학습된 Diffusion Transformer와 결합하여 손쉽게 통합(concatenation) 형태로 마스킹한 비디오의 특징을 추출하는 경량형 컨텍스트 인코더를 이용하는 이중 분기 아키텍처를 특징으로 합니다. 배경과 텍스처 힌트를 제공하는 것은 전경 생성을 용이하게 하며, 추가로 ID 일관성 유지를 위한 inpainting 영역 ID 재샘플링(resampling) 기법을 도입하여 어떠한 길이의 비디오 인페인팅에도 대응할 수 있도록 했습니다.
- ***Performance Highlights***: VideoPainter는 8개 핵심 지표 중 비디오 품질, 마스크 영역 보존 및 텍스트 일관성에서 최첨단 성능을 보여주었습니다. 다양한 유형의 비디오 및 마스크에 대한 실험에서는 고유 ID를 유지하면서도 긴 비디오에 대한 일관된 품질을 보여주었습니다.

### [R1-Omni: Explainable Omni-Multimodal Emotion Recognition with Reinforcing Learning](https://arxiv.org/abs/2503.05379)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05379.png)

Vote: 12

Authors: Xihan Wei, Jiaxing Zhao, Liefeng Bo

- ***What's New***: R1-Omni는 강화 학습(리인포스먼트 러닝; RL)과 검증 가능한 보상(Verifiable Reward; RLVR)을 사용하는 최초의 옴니 멀티모달 감정 인식 모델입니다. 기존의 텍스트와 이미지 조합을 넘어, 비디오 기반 모델에 오디오와 동적 시각 콘텐츠를 통합하여 감정 인식 정확성과 일반화 능력을 크게 향상시켰습니다.
- ***Technical Details***: RLVR는 전통적인 강화 학습 방법론에서 인간 피드백 없이, 자체적인 검증 기능을 통해 결과의 정확성을 평가합니다. 이 연구에서는 OMNI-비디오 모델에 RLVR을 적용하였으며, MAFW와 DFEW 데이터셋의 15,306개의 비디오 샘플을 사용해 훈련하였습니다. 또한 그룹 상대 정책 최적화(Group Relative Policy Optimization; GRPO)를 활용하여 여러 응답의 상대적인 품질을 평가함으로써 훈련 과정을 간소화하였습니다.
- ***Performance Highlights***: R1-Omni는 감정 인식 벤치마크인 MAFW와 DFEW에서 최상의 성능을 기록하였습니다. 특히, RAVDESS 데이터셋에서의 성능에서 이전의 SFT 모델을 뛰어넘었으며, UAR(비가중 평균 회상) 43.00%와 WAR(가중 평균 회상) 44.69%를 달성하였습니다. 이는 RLVR 기반 접근법이 자원을 효과적으로 활용하며 모델의 이해 및 추론 능력을 향상시킨다는 것을 보여줍니다.

### [Learning from Failures in Multi-Attempt Reinforcement Learning](https://arxiv.org/abs/2503.04808)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.04808.png)

Vote: 11

Authors: Wenyu Du, Stephen Chung, Jie Fu

- ***What's New***: 이 논문은 대형 언어 모델(Large Language Models; LLMs)의 강화 학습(Reinforcement Learning; RL) 능력을 향상시키기 위한 새로운 방법으로서 다중 시도(Multi-Attempt) 메커니즘을 도입했습니다. 이전의 단일 시도 기반 질문-대답 작업 형식에서 벗어나 사용자 피드백에 기반한 여러 번의 응답 기회를 통해 모델이 자체 수정을 학습할 수 있도록 했습니다.
- ***Technical Details***: 다중 시도 과제에서는 각 질문마다 여러 번의 시도를 허용하여, 처음 시도가 틀렸을 경우 피드백을 통해 모델이 다음 시도를 개선할 수 있도록 합니다. 이 방식은 기존의 단일 시도 과제에 비해 모델이 더 깊고 다양한 탐색을 가능하게 합니다. Reward Function은 정답일 경우 +1, 틀렸지만 형식이 맞다면 -0.5, 그 외의 경우 -1로 정의되어 있으며, 이 과정은 표준 Proximal Policy Optimization (PPO) 알고리즘을 통해 최적화됩니다.
- ***Performance Highlights***: 실험 결과, 다중 시도 과제에서 학습된 LLM은 수학 벤치마크 평가에서 시도가 1회에서 2회로 증가할 때 45.6%에서 52.5%로 정확도가 상승했습니다. 반면, 단일 시도 과제로 학습한 모델은 같은 상황에서 42.3%에서 43.2%로 미미한 향상을 보였습니다. 또한, 다중 시도 방식을 적용한 모델이 동일한 시도 횟수에서 단일 시도 방식을 적용한 모델보다 높은 정확도를 기록했습니다, 이는 모델이 사용자 피드백에 기반해 응답을 개선할 수 있는 강화 학습이 효과적임을 나타냅니다.

### [TrajectoryCrafter: Redirecting Camera Trajectory for Monocular Videos via Diffusion Models](https://arxiv.org/abs/2503.05638)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05638.png)

Vote: 9

Authors: Mark YU, Ying Shan, Wenbo Hu, Jinbo Xing

- ***What's New***: TrajectoryCrafter는 단안 비디오(Monocular Videos)의 카메라 궤적을 재설정하는 새로운 방법을 제안합니다. 이 방법은 사용자 정의의 카메라 궤적에 대한 정교한 제어를 실현하며, 통합된 포인트 클라우드 렌더링(Point Cloud Renders)과 소스 비디오를 조건으로 사용하는 듀얼 스트림 조건 비디오 확산 모델(Dual-Stream Conditional Video Diffusion Model)을 통해 정확한 궤적변환 및 일관된 4D 콘텐츠 생성을 보장합니다.
- ***Technical Details***: 이 방법은 결정론적 뷰 변환을 불확실한 콘텐츠 생성과 분리하여 사용자 정의 궤적을 정밀하게 조정할 수 있도록 합니다. 동적 포인트 클라우드를 생성하여 새로운 뷰를 렌더링한 후, 포인트 클라우드 렌더 및 소스 비디오를 조건으로 사용하여 고품질 비디오를 생성합니다. 다양한 웹 규모의 단안 비디오 및 정적인 멀티뷰 데이터 세트를 혼합하여 학습하여 모델의 일반화를 강화합니다.
- ***Performance Highlights***: TrajectoryCrafter는 멀티뷰와 대규모 단안 비디오에 대한 광범위한 평가에서 고충실도의 결과물을 생성하는 뛰어난 성능을 보여줍니다. 이는 4D 일관성 및 목표 궤적과의 정밀한 정렬을 통해 시청자의 몰입 경험을 한층 강화합니다.

### [TinyR1-32B-Preview: Boosting Accuracy with Branch-Merge Distillation](https://arxiv.org/abs/2503.04872)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.04872.png)

Vote: 9

Authors: Yongfu Zhu, Wenrui Liu, Weihong Lin, Xiaoqi Jian, Bin Cui, Junfeng Ran, Linglin Zhang, Junting Zhou, Xiangzheng Zhang, Zihan Jiang, Lin Sun, Tong Yang, Change Jia, Jinzhu Wu, Guangxiang Zhao, Sai-er Hu, Yuhan Wu

- ***What's New***: 이 연구는 Branch-Merge Distillation 접근법을 도입하여, 대형 언어 모델(Large Language Models; LLMs)에서 모델 크기를 줄이면서도 높은 성능을 유지하는 방법을 제안합니다. 이 접근법은 특정 도메인에서 전문적으로 훈련된 학생 모델을 서로 결합하여 상호 도메인 지식 전달을 가능하게 합니다.
- ***Technical Details***: Branch-Merge Distillation 접근법은 두 가지 단계로 이루어집니다: (1) Branch Phase에서는 대형 교사 모델로부터 선택적으로 지식을 전수받아, 수학, 코딩, 과학과 같은 분야에 특화된 학생 모델을 만듭니다. (2) Merge Phase에서는 이 학생 모델들을 Arcee Fusion 기법을 활용해 합침으로써, 도메인 간 지식 전이를 촉진하고 일반적인 성능을 향상시킵니다.
- ***Performance Highlights***: 최종 통합된 모델인 TinyR1-32B-Preview는 수학에서 +5.5, 코딩에서 +4.4, 과학에서 +2.9 포인트 향상된 성능을 보였으며, 이는 DeepSeek-R1 교사 모델과 유사한 성능을 발휘합니다. 또한 기존 방법 대비 시간이 90% 절감되었으며, 약 1,500 달러의 비용으로 결과를 재현할 수 있습니다.

### [ProReflow: Progressive Reflow with Decomposed Velocity](https://arxiv.org/abs/2503.04824)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.04824.png)

Vote: 8

Authors: Jiajun Li, Dongsheng Jiang, Lei Ke, Yu Li, Yuxuan Lin, Haoling Li, Yujiu Yang, Linfeng Zhang, Haohang Xu, Xuefei Ning

- ***What's New***: ProReflow는 디퓨전 모델(Diffusion Models)의 속도 분해 기술을 통해 진보된 리플로우 기법을 소개합니다. 이 모델은 기존의 플로우 매칭(flaw matching) 전략을 개선하여 속도 방향 매칭을 더 중시합니다. 이 방법은 이미지와 비디오 생성에서의 계산 효율성을 높이고자 합니다.
- ***Technical Details***: ProReflow는 기존의 리플로우(한 차례에 직선 궤적을 따르게 함)가 아닌 단계적 리플로우(Progressive ReFlow)를 통해 지역적 타임스텝을 점진적으로 조정합니다. 추가적으로 속도 예측에 있어 방향성을 중시한 'Aligned V-Prediction'을 도입하여 계산 과정에서의 정확성을 높였습니다. 이러한 방법은 SDv1.5 및 SDXL 모델에 적용되어, 빠르고 효율적인 디퓨전 프로세스를 실현했습니다.
- ***Performance Highlights***: ProReflow는 MSCOCO 2014 검증 세트에서 4 단계 샘플링(step sampling)만으로 FID 10.70을 달성하며, 이는 교사 모델(32 DDIM 단계, FID=10.05)과 근접한 성능입니다. 또한 COCO-2017 검증 세트에서 ProReflow-II는 FID 22.03 및 CLIP Score 29.95로 2-ReFlow 및 PeRFlow 등 기존 방법보다 우수한 성능을 보였습니다. 이러한 성과는 사전 학습된 프레임워크 대비 적은 단계에서의 뛰어난 샘플링 품질을 보여줍니다.

### [BEHAVIOR Robot Suite: Streamlining Real-World Whole-Body Manipulation for Everyday Household Activities](https://arxiv.org/abs/2503.05652)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05652.png)

Vote: 8

Authors: Jiajun Wu, Ruohan Zhang, Li Fei-Fei, Chen Wang, Shuran Song, Josiah Wong, Cem Gokmen, Hang Yin, Yanjie Ze, Yunfan Jiang

- ***What's New***: BEHAVIOR Robot Suite (BRS)는 다양한 일상 가정 활동을 위해 실제 환경에서의 전신 조작을 간소화하는 포괄적인 프레임워크로 소개되었습니다. 이 프레임워크는 조작 로봇의 하드웨어 설계와 저렴한 전신 텔레오퍼레이션 인터페이스인 JoyLo, 그리고 Whole-Body VisuoMotor Attention (WB-VIMA)이라는 새로운 학습 알고리즘을 통합하여 복잡한 실제 가정 작업을 수행할 수 있도록 합니다. BRS는 다방면의 조작 능력 향상을 목표로 하며, 누구나 접근 가능한 저비용 플랫폼으로 공개소스로 제공됩니다.
- ***Technical Details***: BRS는 두 개의 6-DoF (Degree of Freedom)의 팔과 4-DoF의 상체로 구성되며, Omni 방향의 이동 기지에 장착되어 안정적이고 정확한 내비게이션을 지원합니다. JoyLo는 저비용으로 제작 가능하며, 전체적인 로봇 조작을 가능하게 하는 인터페이스로, 사용자는 Nintendo Joy-Con을 통해 로봇을 직접 제어할 수 있습니다. WB-VIMA는 Transformer 기반의 모델로, 로봇의 동작을 시퀀셜로 예측하여 전체적인 동작의 조화를 이룹니다. 이 알고리즘은 다양한 모달리티 관찰을 자가 주의(causal self-attention) 메커니즘을 통해 통합하여 더 정교하고 혼란이 적은 정책 학습을 가능하게 합니다.
- ***Performance Highlights***: BRS는 다섯 가지의 대표적인 실제 가정 작업을 자동으로 완료하며, 평균 성공률 58%, 최고 성공률 93%를 기록했습니다. 이는 인간 원격 조작보다도 일부 서브작업에서는 더 나은 성능을 보였습니다. WB-VIMA는 복잡한 작업에서 높은 정확도와 협응 동작을 보이며, 특히 정교한 조작이 요구되는 작업에서 뛰어난 성과를 냈습니다. JoyLo를 통한 데이터 수집은 효율성이 높고, 정책 학습에 적합한 고품질 데이터를 제공하였습니다. BRS는 사전 훈련된 모델들과 비교해 향상된 성능을 나타내며, 로봇의 전신 조작을 보다 자연스럽고 안전하게 수행할 수 있도록 지원합니다.

### [Linear-MoE: Linear Sequence Modeling Meets Mixture-of-Experts](https://arxiv.org/abs/2503.05447)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05447.png)

Vote: 6

Authors: Disen Lan, Yu Cheng, Tong Zhu, Xiaoye Qu, Weigao Sun

- ***What's New***: Linear-MoE는 선형 순서 모델링(Linear Sequence Modeling; LSM)과 전문가 혼합(Mixture-of-Experts; MoE)을 결합한 대규모 모델을 위한 생산 수준의 시스템입니다. 이 시스템은 LSM 모듈을 사용하여 선형 복잡도의 순서 모델링을 수행하고, MoE 레이어를 통해 선택적으로 활성화하여 높은 성능을 유지하면서 효율적인 훈련을 목표로 합니다.
- ***Technical Details***: Linear-MoE 시스템은 두 가지 주요 하위 시스템으로 구성됩니다: 모델링(Modeling)과 훈련(Training)입니다. 모델링 하위 시스템은 선형 주의(Linear Attention), 상태 공간 모델(State Space Model; SSM), 선형 RNN과 같이 다양한 LSM 방법을 지원하는 단일화된 순서 모델링 프레임워크를 제공합니다. 훈련 하위 시스템은 Sequence Parallelism과 같은 다양한 병렬 기술을 통합하여 효율적인 훈련을 가능하게 합니다. 이 시스템은 고급 모델링 기법과 훈련 기술을 미래에 통합할 수 있도록 확장 가능성을 염두에 두고 설계되었습니다.
- ***Performance Highlights***: Linear-MoE는 A0.3B-2B 및 A1B-7B와 같은 모델 시리즈에서 다양한 벤치마크에서 경쟁력을 유지하면서 효율성을 보여주었습니다. 모델은 SlimPajama 데이터셋에서 사전 훈련되었으며, 이를 통해 Linear-MoE 아키텍처의 훈련 및 추론 효율성이 검증되었습니다. 이러한 결과는 Linear-MoE가 차세대 기초 모델 아키텍처로써의 잠재력을 가지고 있음을 시사합니다.

### [An Empirical Study on Eliciting and Improving R1-like Reasoning Models](https://arxiv.org/abs/2503.04548)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.04548.png)

Vote: 6

Authors: Zheng Liu, Jinhao Jiang, Yang Lu, Zhongyuan Wang, Zhipeng Chen, Beichen Zhang, Lei Fang, Daixuan Cheng, Wayne Xin Zhao, Ji-Rong Wen, Jie Chen, Xu Miao, Yingqian Min

- ***What's New***: 이 논문은 느린 사고 모델(Slow-thinking Models)의 발전을 위한 STILL 프로젝트의 세 번째 기술 보고서를 제시합니다. 강화 학습(RL) 훈련이 중심 기술로 자리잡으면서, 대규모 추론 모델(Large Reasoning Models)의 성능을 향상시키는 기술적 경로를 체계적으로 실험하고 문서화했습니다.
- ***Technical Details***: 기본 모델(QWEN2.5-32B)에 RL 훈련을 적용하여 응답 길이와 테스트 정확도를 향상시켰으며, DEEPSEEK-R1-DISTILL-QWEN-1.5B와 같은 고성능 모델이 더 높은 정확도를 달성할 수 있음을 보여주었습니다. 또한 도구 조작을 통한 모델의 추론 성능 강화 가능성을 탐구하여, 탐욕적 탐색으로 86.67%의 정확도를 달성하는 등 모델 능력을 효과적으로 개선했습니다.
- ***Performance Highlights***: STILL-3-1.5B 모델은 AIME 2024에서 39.33%의 정확도를 기록하며 성능이 크게 개선되었습니다. 도구 조작을 통해 STILL-3-TOOL-32B 모델은 AIME 2024에서 탐욕적 탐색을 통해 86.67%의 정확도를 달성했습니다. 이러한 결과는 도구 조작 및 RL 훈련이 모델의 추론 성능을 강화할 수 있음을 보여줍니다.

### [LONGCODEU: Benchmarking Long-Context Language Models on Long Code Understanding](https://arxiv.org/abs/2503.04359)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.04359.png)

Vote: 5

Authors: Jia Li, Zhi Jin, Ge Li, Fang Liu, Lei Li, Chongyang Tao, Kechi Zhang, Zhengwei Tao, Xuyuan Guo, Yuqi Zhu

- ***What's New***: LONGCODEU는 대형 문맥 언어 모델(Long-context Language Models; LCLMs)의 긴 코드 이해 능력을 평가하기 위한 새로운 벤치마크로, 실질적인 소프트웨어 엔지니어링 애플리케이션을 위한 네 가지 측면에서 평가합니다: 코드 유닛 인식, 코드 내부 유닛 이해, 코드 유닛 간 관계 이해, 그리고 긴 코드 문서화 이해입니다.
- ***Technical Details***: LONGCODEU 벤치마크는 실세계 코드 저장소에서 수집된 500개의 긴 코드 예제를 포함합니다. 각 작업은 0~128K의 토큰을 포함하는 다양한 길이의 예제를 따르며, 기존 벤치마크가 지원하는 36.5K보다 훨씬 깁니다. 평가 대상인 9개의 인기 있는 LCLMs가 있으며, 여기에는 6개의 일반 모델과 3개의 코드 모델이 포함됩니다.
- ***Performance Highlights***: 실험 결과, 현존하는 LCLMs는 긴 코드 이해에 있어 주요한 한계를 가지고 있음을 보여줍니다. 특히, LCLMs의 성능은 코드 길이가 32K 이상일 때 급격히 떨어지며, 주어진 128K~1M의 문맥 창(window)보다 훨씬 부족합니다. 네 가지 측면 중에서 코드 유닛 간 관계 이해는 LCLMs에게 가장 도전적인 과제입니다.

### [SAGE: A Framework of Precise Retrieval for RAG](https://arxiv.org/abs/2503.01713)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01713.png)

Vote: 4

Authors: Guoliang Li, Jintao Zhang, Jinyang Su

- ***What's New***: 이 연구에서는 RAG(검색 보강 생성) 시스템의 정확한 검색을 위한 새로운 프레임워크인 SAGE를 소개합니다. SAGE는 의미 기반 세분화(semantic segmentation), 구문 선택 알고리즘(chunk selection algorithm), LLM 자기 피드백(self-feedback) 메커니즘을 통합하여 RAG 시스템의 검색 정확도를 향상시킵니다. 이는 기존 시스템의 한계를 극복하기 위한 시도로, 검색된 정보가 의미적으로 완전하고 관련성이 있도록 설계되었습니다.
- ***Technical Details***: SAGE는 가벼운 모델을 훈련하여 코퍼스를 의미적으로 완전한 구문으로 빠르고 정확하게 세분화합니다. 이 접근 방식은 의미적으로 일관된 세그먼트를 보장하면서 검색된 정보를 LLM의 추론(inference) 비용을 줄이는 것을 목표로 합니다. 구문 선택은 변화율에 따라 점수가 크게 떨어지기까지 가장 관련성이 높은 세그먼트를 동적으로 선택하는 방식으로 수행됩니다. 자기 피드백 메커니즘은 수집된 구문이 과도하거나 부족한지를 평가하여 필요에 따라 컨텍스트의 양을 조정합니다.
- ***Performance Highlights***: 실험 결과, SAGE는 기존의 베이스라인과 비교하여 QA 품질에서 평균 61.25% 향상을 달성했으며, LLM 추론 시 불필요한 데이터를 피하여 평균 49.41%의 비용 효율성을 증대시켰습니다. 이를 통해 SAGE는 더 효과적인 RAG 시스템의 개발에 중요한 통찰력을 제공합니다.

### [Know You First and Be You Better: Modeling Human-Like User Simulators via Implicit Profiles](https://arxiv.org/abs/2502.18968)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.18968.png)

Vote: 3

Authors: Xianfei Li, Kuang Wang, Feng Jiang, Li Zhou, Haizhou Li, Shenghao Yang

- ***What's New***: 이 연구에서는 사용자가 암묵적으로 드러내는 특성들을 프로파일링하여, 개인화되고 현실감 있는 대화를 생성할 수 있는 사용자 시뮬레이터 프레임워크인 USP(User Simulator with Implicit Profiles)를 제안합니다. 이는 기존 텍스트만을 기반으로 한 시뮬레이터에서 한 걸음 더 나아가, 대화 속에서 토론자의 성격, 말투, 목표 등을 암묵적 프로파일링을 통해 추론합니다.
- ***Technical Details***: USP 프레임워크는 대화 내에서 사용자 성향을 추출하는 LLM(대형 언어 모델, Large Language Model) 기반 추출기를 사용하여 포괄적인 프로파일 스키마를 통해 사용자 프로파일을 구성합니다. 이후 사용자의 발화를 프로파일 기반 조건부 지도 학습 및 주어진 프로파일과의 일치성을 유지하도록 주기하 강화 학습(Reinforcement Learning with Cycle Consistency)을 사용해 시뮬레이션을 정교화합니다. 마지막으로, 실제 사용자 프로파일 분포를 포착하기 위해 다양한 프로파일 샘플러를 채택하였습니다.
- ***Performance Highlights***: 실험 결과, USP는 기존 강력한 기준 모델들보다 진정성과 다양성 면에서 우수한 성과를 발휘하며, 일관성 측면에서도 유사한 성능을 보여줍니다. 특히 USP 기반 다중 턴 대화 평가에서의 결과는 주요 벤치마크와 강력한 상관성을 나타내며, 실제 응용 분야에서의 효과성을 입증합니다.

### [EAGLE-3: Scaling up Inference Acceleration of Large Language Models via Training-Time Test](https://arxiv.org/abs/2503.01840)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01840.png)

Vote: 3

Authors: Fangyun Wei, Chao Zhang, Yuhui Li, Hongyang Zhang

- ***What's New***: EAGLE-3는 기존의 EAGLE 모델의 특징 예측 제약을 제거하고, 다층 피처 융합(multi-layer feature fusion)을 통해 직접 토큰을 예측하는 새로운 방법을 제안합니다. 이를 통해 훈련 데이터의 확장 덕분에 속도와 성능이 크게 향상되었습니다.
- ***Technical Details***: EAGLE-3의 주요 혁신은 training-time test 기술을 통해 다층 피처를 융합하여 피처 예측 제약을 제거하고, 초층(top-layer) 피처 대신 낮은, 중간, 그리고 높은 레벨의 피처를 통합하여 풍부한 의미 정보를 캡처하는 것입니다. 이로써 EAGLE-2의 드래프팅 트리(drafting tree) 기술과도 완벽하게 호환됩니다.
- ***Performance Highlights***: EAGLE-3는 EAGLE-2 보다 약 1.4배의 지연 시간 개선을 이루며, 최대 6.5배의 속도 향상을 달성했습니다. 이는 EAGLE-2 대비 20-40%의 성능 향상이며, 여러 작업과 모델에서 가장 높은 수준의 수락률을 보여주었습니다.

### [LoRACode: LoRA Adapters for Code Embeddings](https://arxiv.org/abs/2503.05315)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05315.png)

Vote: 2

Authors: Laurent Bindschaedler, Saumya Chaturvedi, Aman Chadha

- ***What's New***: LoRACode는 코드 검색을 위한 Parameter-Efficient Fine-Tuning (PEFT) 방법을 제안하며 Low-Rank Adaptation (LoRA)을 활용하여 코드 임베딩을 향상시킵니다. 이 방법은 코드 검색 작업을 위한 작업별 및 언어별 어댑터를 생성하여 기존 모델을 기반으로 하여 전산 자원을 최소화하면서 상태 최첨단(SOTA) 성능을 달성합니다.
- ***Technical Details***: LoRACode는 CodeBERT, GraphCodeBERT, UniXcoder와 같은 기초 모델에 LoRA 어댑터를 추가하여 각 Base Model의 Attention Layer에 저랭크(decomposition matrices) 행렬을 도입합니다. ContrastiveTrainer를 사용하여 유사도 점수를 최적화하고 결과를 Mean Reciprocal Rank (MRR)로 평가합니다. 언어별 어댑터를 통해 6개 프로그래밍 언어에서 Text2Code 검색을 향상시킵니다.
- ***Performance Highlights***: 코드 검색 작업에서 Mean Reciprocal Rank(MRR)가 Code2Code 검색에서 9.1%까지, Text2Code 검색 작업에서는 86.69%까지 증가했습니다. 2개의 H100 GPU에서 2백만 개의 코드 샘플을 처리하고, LoRACode는 전통적인 방식보다 훨씬 적은 1.83%-1.85%의 매개변수를 사용하여 고효율의 미세 조정을 지원합니다.

### [AnyAnomaly: Zero-Shot Customizable Video Anomaly Detection with LVLM](https://arxiv.org/abs/2503.04504)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.04504.png)

Vote: 1

Authors: Sanghyun Park, Kijung Lee, Sein Kwon, Inpyo Hong, Youngwan Jo, Sunghyun Ahn

- ***What's New***: 본 연구는 다양한 환경에 적응 가능한 커스터머블 비디오 이상 감지(Customizable Video Anomaly Detection; C-VAD) 기술과 AnyAnomaly 모델을 제안합니다. 이 모델은 사용자 정의 텍스트를 이상 이벤트로 간주하며, 이를 통해 영상에서 지정된 이벤트가 포함된 프레임을 감지할 수 있습니다. 특히 LVLM을 활용하여 대규모 비전-언어 모델을 이용해 비디오 이상 감지를 수행하며, 추가적인 학습 없이도 우수한 성능을 발휘합니다.
- ***Technical Details***: AnyAnomaly는 세그먼트 레벨로 프레임을 그룹화하여 처리할 수 있도록 설계되어 대기 시간을 줄입니다. 키 프레임 선택 모듈(Key Frames Selection Module; KSM)을 도입하여 세그먼트를 대표하는 프레임을 선택하고, 각 세그먼트마다 비주얼 질문 답변(Visual Question Answering; VQA)을 수행합니다. 또한, 위치 컨텍스트(Position Context; PC)와 시간 컨텍스트(Temporal Context; TC)를 추가 활용하여 장면에 대한 심도 있는 이해를 가능하게 합니다. 이 두 모듈은 추가적인 훈련 없이 작동하여, C-VAD의 쉬운 적용을 지원합니다.
- ***Performance Highlights***: AnyAnomaly는 UBnormal 데이터셋에서 최첨단(State-of-the-Art; SOTA) 성능을 달성하였으며, 모든 데이터셋에서의 일반화에서도 다른 방법들을 능가했습니다. 제안된 접근법은 VAD 기술을 실제 응용 프로그램에 배포하는 데 효과적인 솔루션이 될 것입니다.

