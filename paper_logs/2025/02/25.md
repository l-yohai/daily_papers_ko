## Daily Papers (2025-02-25)

### [Thus Spake Long-Context Large Language Model](https://arxiv.org/abs/2502.17129)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.17129.png)

Vote: 51

Authors: Siyang He, Xuanjing Huang, Zhigeng Liu, Mianqiu Huang, Xiaoran Liu, Qipeng Guo, Qun Liu, Xipeng Qiu, Ruixiao Li, Yaqian Zhou, Qiqi Wang, Linlin Li, Yuerong Song

- ***What's New***: 이 논문은 자연어 처리(NLP)에서 장문맥(long context) 문제를 다루며, 최근 대형 언어 모델(LLM)의 문맥 길이가 수백만 토큰까지 확장되었음을 보고합니다. 또한, 장문맥의 생애주기를 네 가지 관점에서 조사하여 현재의 연구 방향을 제시합니다.
- ***Technical Details***: 장문맥을 다루기 위해 LLM 아키텍처, 인프라, 교육, 평가의 네 가지 측면을 중점적으로 분석합니다. 특히 Transformer 아키텍처의 문맥 추론 및 KV 캐시 최적화, 메모리 관리에 대한 심도 있는 논의를 포함하고, RoPE와 같은 위치 임베딩(Position Embedding) 기술을 조명합니다. 또한, 버틸 학습 및 유추 능력 향상을 위한 다양한 기술을 활용합니다.
- ***Performance Highlights***: 최근 장문맥 LLM은 2K에서 2M 토큰의 문맥 길이를 효과적으로 처리할 수 있으며, 이는 여러 NIAH 및 RULER 벤치마크의 테스트에서 성능 향상을 나타냅니다. 그러나 장문맥의 확장에는 여전히 주목할 만한 도전이 있으며, LLM의 문맥 확장이 과거의 업적에 비추어 혁신 요소로 자리 잡고 있습니다.

### [VideoGrain: Modulating Space-Time Attention for Multi-grained Video Editing](https://arxiv.org/abs/2502.17258)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.17258.png)

Vote: 45

Authors: Xiangpeng Yang, Hehe Fan, Linchao Zhu, Yi Yang

- ***What's New***: VideoGrain은 클래스, 인스턴스, 파트 레벨의 수정을 포함한 멀티 그레인 비디오 편집(Multi-grained Video Editing)을 위한 최초의 제로샷 방법론입니다. 이번 연구는 텍스트-영역 컨트롤의 강화와 피처 분리를 통해 이러한 어려움을 해결하고, 실세계의 시나리오에서 첨단 성능을 입증했습니다.
- ***Technical Details***: VideoGrain은 공간-시간 크로스와 셀프 어텐션(Cross and Self Attention)을 변조하여 텍스트-영역 컨트롤 및 영역 간 피처 분리를 제공합니다. 각 로컬 프롬프트의 주의를 해당하는 공간 분리 영역에 확대하고 교차 어텐션에서 관련 없는 영역과의 상호작용을 최소화합니다. 또한 셀프 어텐션에서는 영역 내 인식을 강화하고 영역 간 상호작용을 줄이며 피처의 결합을 방지합니다.
- ***Performance Highlights***: VideoGrain은 CLIP-T와 같은 자동화된 메트릭에서 36.56, Q-edit에서 25.75의 점수를 기록하며, 기존 방법들에 비해 특히 높은 성능을 보였습니다. 인간 평가에서도 VideoGrain은 편집 정확도 및 시간적인 일관성 면에서 우수한 평가를 받았습니다.

### [DICEPTION: A Generalist Diffusion Model for Visual Perceptual Tasks](https://arxiv.org/abs/2502.17157)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.17157.png)

Vote: 40

Authors: Tong He, Huanyi Zheng, Muzhi Zhu, Hao Chen, Mingyu Liu, Zhiyue Zhao, Chunhua Shen, Canyu Zhao

- ***What's New***: DICEPTION은 제한된 컴퓨팅 자원 및 훈련 데이터 내에서 여러 시각적 인식 작업을 처리할 수 있는 범용적 퍼셉션 모델을 제안합니다. 이 모델은 대규모 이미지 데이터를 사전 학습한 텍스트-이미지 디퓨전 모델을 활용하여 최소한의 데이터로도 최첨단 모델과 동등한 성능을 달성합니다.
- ***Technical Details***: DICEPTION은 다수의 인식 작업을 RGB 공간으로 통합하여 처리합니다. 다양한 시각적 인식 작업(모노큘러 깊이 추정, 노멀 예측, 세그먼테이션 등)을 디퓨전 모델의 사전 학습된 텍스트-이미지 모델의 프리어를 활용해 학습합니다. 이를 통해 복잡한 네트워크 설계나 대규모 데이터 없이도 높은 성능을 유지하면서 새로운 작업에 신속히 적응할 수 있습니다. 예를 들어, 새로운 작업에 적응할 때 50장의 이미지와 모델 파라미터의 약 1%만 재학습하여도 충분한 성능을 발휘합니다.
- ***Performance Highlights***: DICEPTION은 특정 세그멘테이션 작업에서 SAM-vit-h와 동등한 성능을 단 0.06%의 데이터(예: 600K 대 1B)로 달성하였습니다. 다수의 테스트 데이터셋에서 다양한 작업 간의 성능 편차가 거의 없이 안정적으로 높은 결과를 나타냈습니다. 이는 데이터 효율성과 견고성을 보여주는 사례로, 스페셜 모델에 비해 적은 데이터만으로도 경쟁력 있는 결과를 생성할 수 있음을 입증합니다.

### [Slamming: Training a Speech Language Model on One GPU in a Day](https://arxiv.org/abs/2502.15814)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.15814.png)

Vote: 37

Authors: Avishai Elmakies, Gallil Maimon, Yossi Adi

- ***What's New***: 이 연구는 'Slam'이라는 새로운 트레이닝 레시피를 소개하여, 단일 A5000 GPU와 24시간만으로 고품질의 음성 언어 모델(Speech Language Models; SLMs)을 훈련할 수 있음을 보여줍니다. 이 방법은 기존의 대규모 연구소 환경에 비해 매우 제한된 자원을 사용하고도 최신 모델에 필적할 만한 성능을 달성할 수 있도록 해줍니다. SLM의 스케일링 법칙과 관련된 맥락에서, 이 연구 결과는 예측된 컴퓨팅 최적 성능을 훨씬 능가하여 SLM의 실현 가능성에 대한 낙관적인 관점을 제공합니다.
- ***Technical Details***: Slam 레시피는 모델 초기화와 아키텍처, 다양한 옵티마이저 및 학습 속도 스케줄러, 데이터 선택 전략에 대한 광범위한 실험을 포함합니다. 이 방법은 합성 훈련 데이터와 텍스트 삽입을 포함한 선호도 최적화(preference optimisation)를 사용하여 24시간 내의 제한된 컴퓨팅 환경에서 모델 성능을 극대화합니다. 이 연구는 이러한 전략의 개발 및 타당성을 입증하여 음성 및 오디오 연구 커뮤니티가 SLM 연구를 추진할 수 있도록 돕습니다.
- ***Performance Highlights***: Slam를 사용하여 Qwen2.5-0.5B 모델을 훈련한 결과, 단일 A5000 GPU에서 24시간의 훈련으로도 최신 모델과 동등하거나 더 나은 성능을 발휘하는 것을 보여주었습니다. 결과적으로, Slam은 160*V100 일의 컴퓨팅과 비교할 때 훨씬 적은 자원을 사용하며 탁월한 성능을 발휘합니다. 이는 Slam이 예측된 컴퓨팅 최적 성능을 크게 초과하여 제한된 자원 내에서의 SLM 훈련 가능성을 입증한 것입니다.

### [Audio-FLAN: A Preliminary Release](https://arxiv.org/abs/2502.16584)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.16584.png)

Vote: 25

Authors: Xinsheng Wang, Haohe Liu, Wei Xue, Sitong Cheng, Ziya Zhou, Xinshen Zhang, Shuai Fan, Haohan Guo, Yujia Xiao, Chuanbo Zhu, Zixuan Shen, Ge Zhang, Jiahao Pan, Yike Guo, Emmanouil Benetos, Zixuan Li, Yinghao Ma, Liumeng Xue, Dongchao Yang, Zeyue Tian, Ruibin Yuan, Tianchi Liu

- **What's New**: Audio-FLAN은 대규모의 지시 조정 데이터셋으로, 음성, 음악, 소리 도메인의 80개 다양한 작업을 다루며, 1억 개 이상의 인스턴스를 포함하고 있습니다. Audio-FLAN은 이해(Task Understanding)와 생성(Task Generation) 모두를 아우르는 통합 오디오-언어 모델의 기반을 마련합니다.
- **Technical Details**: Audio-FLAN 데이터셋은 52개의 공용 데이터셋에서 수집된 데이터를 지시-기반 형식으로 표준화하여 구성됩니다. 각 오디오 샘플은 하나 이상의 지시문과 기대되는 출력과 함께 제공되며, 지시문은 작업 템플릿을 통해 생성됩니다. 지시문의 다양성을 증가시키기 위해 LLaMA와 같은 모델을 사용하여 여러 변형을 생성했습니다.
- **Performance Highlights**: Audio-FLAN은 통합 모델이 다양한 오디오 작업에 대해 제로샷(Zero-shot) 일반화를 달성할 수 있도록 설계되었습니다. 음성 도메인에는 8개의 주요 작업, 34개의 소작업, 1억 개 이상의 인스턴스가 포함되어 있고, 음악 도메인에는 7개의 주요 작업, 28개의 소작업이 포함되어 다양한 멀티모달 작업을 지원합니다.

### [GCC: Generative Color Constancy via Diffusing a Color Checker](https://arxiv.org/abs/2502.17435)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.17435.png)

Vote: 19

Authors: Cheng-De Fan, Chen-Wei Chang, Yi-Chen Lo, Jiun-Long Huang, Yu-Chee Tseng, Chia-Che Chang, Yu-Lun Liu

- ***What's New***: GCC는 강력한 프리트레인 된 확산 모델(Diffusion Models)을 이용하여 색상 체크를 인페인팅(Inpainting)함으로써 조명 추정을 수행하는 새로운 방법론을 제시합니다. 이는 카메라별로 훈련 데이터를 필요로 하지 않는 새로운 조명 추정 경로를 제공하여, 기존의 색 균일성(Color Constancy) 기법에서 벗어난 접근을 가능하게 합니다.
- ***Technical Details***: GCC는 확산 모델을 통한 색상 체크 인페인팅을 이용하여 조명 추정을 수행합니다. 이는 (1) 단일 단계 결정론 추론을 통해 장면 조명을 반영하는 색상 체크 생성, (2) 라플라시안(Laplacian) 컴포지션 기술을 이용하여 색상 체크 구조를 보존하면서 조명 기반 색 적응을 가능하게 함, (3) 부정확한 색상 체크 주석을 처리하는 마스크 기반 데이터 증강 전략을 포함합니다.
- ***Performance Highlights***: GCC는 다양한 카메라 환경에서 뛰어난 견고성을 발휘하며, 양방향 평가에서 최악의 25% 오차율이 5.15°와 4.32°로 최첨단 성능을 기록했습니다. 이는 센서별 훈련 없이 다양한 카메라 특성에 대해 뛰어난 안정성과 일반화 능력을 보여주며, 실생활 응용에 대한 범용 솔루션을 제공합니다.

### [CodeCriticBench: A Holistic Code Critique Benchmark for Large Language Models](https://arxiv.org/abs/2502.16614)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.16614.png)

Vote: 19

Authors: Yejie Wang, Yancheng He, Yuanxing Zhang, Zhaoxiang Zhang, Yingshui Tan, Alexander Zhang, Marcus Dong, Tianyu Liu, Wenhao Huang, Wei Zhang, Jian Yang, Jiaheng Liu, Wangchunshu Zhou, Zhexu Wang, Ken Deng, Ge Zhang, Zhongyuan Peng, Weixun Wang

- ***What's New***: CodeCriticBench는 대형 언어 모델(LLMs)의 코드 비판 능력을 평가하는 종합적인 기준점으로 소개되었습니다. 코트리틱벤치(CodeCriticBench)는 코드 생성과 코드 QA라는 두 가지 주요 코드 작업을 포함하고 있으며, 각 작업은 다양한 난이도로 제공됩니다. 이를 통해 LLMs의 코드 비판 능력을 다각도로 평가할 수 있습니다.
- ***Technical Details***: CodeCriticBench는 코드 생성과 코드 QA 작업을 위한 기본 비판 평가(Basic Critique Evaluation)와 고급 비판 평가(Advanced Critique Evaluation)를 포함합니다. 기본 평가는 '정확/오류' 응답과 관련된 논리 과정을 제공하고, 고급 평가는 세밀하게 설계된 평가 체크리스트를 통해 모델의 비판 능력을 더욱 정교하게 평가합니다. 코드 생성 작업은 코드 생성 및 코드 QA에 대한 정밀한 세부 평가 기준이 잘 설계되어 있습니다.
- ***Performance Highlights***: 38개의 LLMs에 대한 포괄적인 실험 결과가 나왔으며, 코드 생성 관련 작업에서 DeepSeek-R1이 MSE 3.92로, '코드 QA' 작업에서 Claude3.5-Sonnet이 MSE 1.02로 가장 높은 성능을 보였습니다. 이러한 결과는 '코드 QA'가 '코드 생성'보다 대화 최적화로 인해 고품질 응답을 우선시하는 경향이 있음을 반영합니다.

### [Make LoRA Great Again: Boosting LoRA with Adaptive Singular Values and Mixture-of-Experts Optimization Alignment](https://arxiv.org/abs/2502.16894)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.16894.png)

Vote: 16

Authors: Xiaoye Qu, Wei Wei, Chenghao Fan, Chengfeng Gu, Sichen Liu, Zhenyi Lu, Yu Cheng

- ***What's New***: 이 논문에서는 Low-Rank Adaptation(LoRA)을 Adaptive Singular Values와 Mixture-of-Experts(MoE) 최적화 정렬을 통합하여 성능을 향상시키는 GOAT(Great LoRA Mixture-of-Expert)를 제안합니다. GOAT는 입력 기반으로 적응적으로 사전 학습된 지식을 통합하고, MoE 구조와 일치시키는 이론적 스케일링을 도입하여 최적화를 정렬합니다.
- ***Technical Details***: GOAT는 SVD로 구조화된 MoE 프레임워크를 활용하여 LoRA MoE 전문가를 다양한 단일값 세그먼트로 초기화하며, 이론적인 스케일링 요인을 도출합니다. 이를 통해 각 전문가가 사전 학습된 무게와 가까워지는 효과적인 최적화를 지원하며, 아키텍처나 훈련 알고리즘을 변경하지 않고도 LoRA MoE의 효율성과 성능을 향상시킵니다.
- ***Performance Highlights***: GOAT는 자연어 이해, 상식 추론, 이미지 분류, 자연어 생성 등 25개의 데이터셋을 통해 실험한 결과, 최첨단 성능을 달성하며 Full Fine-Tuning과의 성능 격차를 줄였습니다. 이는 사전 학습된 모델의 지식을 더 효율적으로 활용함으로써 기존 방법론보다 뛰어난 성능을 입증하였습니다.

### [Linguistic Generalizability of Test-Time Scaling in Mathematical Reasoning](https://arxiv.org/abs/2502.17407)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.17407.png)

Vote: 14

Authors: Guijin Son, James Thorne, Jiwoo Hong, Hyunwoo Ko

- ***What's New***: MCLM은 55개 언어로 구성된 경쟁 수준의 문제를 특징으로 하는 다국어 수학 벤치마크로 소개되었습니다. 이를 통해 테스트 시점 스케일링(Test-Time Scaling)이 다국어 작업에서도 효과를 발휘할 수 있는지를 탐구합니다.
- ***Technical Details***: 연구에서는 세 가지 테스트 시점 스케일링 방법(Outcome Reward Modeling, Process Reward Modeling, Budget Forcing)을 다국어 LLM(MR1-1.5B)과 Qwen2.5-1.5B Math 모델에서 분석합니다. 각 방법론은 모델의 성능과 언어 일관성을 평가하기 위해 정확도와 일관성 메트릭을 사용합니다.
- ***Performance Highlights***: 최고의 성능은 Qwen2.5-1.5B Math와 ORM을 사용하여 MCLM에서 35.8점을 기록했으며, Budget Forcing을 사용한 MR1-1.5B는 35.2점을 달성했습니다. English AIME 벤치마크에서 Budget Forcing은 20점의 향상을 보였으나, 다른 언어에서는 평균 1.94점의 향상에 그쳤습니다.

### [Multimodal Inconsistency Reasoning (MMIR): A New Benchmark for Multimodal Reasoning Models](https://arxiv.org/abs/2502.16033)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.16033.png)

Vote: 14

Authors: Shan Jiang, Yang Zhao, Xin Eric Wang, Qianqi Yan, Ching-Chen Kuo, Yue Fan, Xinze Guan, Hongquan Li

- ***What's New***: Multimodal Inconsistency Reasoning (MMIR)는 멀티모달 추론 모델의 능력을 평가하기 위한 새로운 벤치마크로, 실세계에서 일어날 수 있는 불일치를 다루는 데 중점을 두고 있습니다. 이 벤치마크는 모델들이 웹페이지, 프리젠테이션 슬라이드, 포스터 등에서 발생하는 사실의 모순, 정체성 오인, 문맥 불일치, 수량 불일치, 시간/공간 불일치를 식별하고 추론할 수 있는지를 평가합니다.
- ***Technical Details***: MMIR 벤치마크는 534개의 도전적인 샘플을 포함하고 있으며, 각각 다섯 가지 풍부한 추론 카테고리에 따라 인위적으로 주입된 불일치 오류를 가지고 있습니다. 여섯 가지 최신 MLLM을 평가한 결과, 전용 멀티모달 추론 능력을 가진 모델이 더 우수하게 성능을 발휘한 반면, 오픈 소스 모델은 특히 불일치 오류에 취약한 것으로 나타났습니다. MMIR의 데이터는 서로 다른 도메인에서 수집한 다양한 실제 사례들로 구성되어 있습니다.
- ***Performance Highlights***: MMIR 벤치마크에서, 최신 모델들은 멀티모달 불일치를 다루는 데 여전히 많은 어려움을 겪고 있으며, 특히 오픈 소스 모델은 25% 이하의 정확도를 기록했습니다. 반면, 멀티모달 추론 능력이 강력한 'o1' 모델은 50% 이상의 정확도를 기록하며 전체적으로 가장 성능이 뛰어났습니다. 이 실험은 현재 MLLMs의 한계를 드러내며 향후 연구의 필요성을 강조합니다.

### [Stable-SPAM: How to Train in 4-Bit More Stably than 16-Bit Adam](https://arxiv.org/abs/2502.17055)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.17055.png)

Vote: 11

Authors: Tianjin Huang, Shiwei Liu, Tianlong Chen, Zhangyang Wang, Zhenyu Zhang, Qingsong Wen, Lu Liu, Xiang Li, Gaojie Jin, Haotian Hu, Li Shen

- ***What's New***: Stable-SPAM은 4비트 훈련에서 안정성을 개선하기 위해 개발된 새로운 옵티마이저로, 기존의 SPAM optimizer에서 한 단계 발전한 것입니다. 이를 통해 16비트 Adam optimizer보다 더 안정적으로 훈련할 수 있습니다.
- ***Technical Details***: Stable-SPAM은 4비트 훈련에서 발생하는 그래디언트 노름의 불안정성을 완화하기 위해 여러 기법을 도입했습니다. Adaptive Spike-Aware Clipping (AdaClip)과 Adaptive Gradient Norm (AdaGN)을 통해 그래디언트 스파이크를 감지하고 적응적으로 클리핑하며, 모멘텀 리셋 (Momentum Reset)을 SPAM에서 가져와 Adam의 첫 번째와 두 번째 모멘트를 주기적으로 초기화하여 스파이크 그래디언트의 누적 효과를 줄입니다.
- ***Performance Highlights***: Stable-SPAM을 사용한 4비트 LLaMA-1B 모델은 BF16에서 Adam으로 훈련된 모델보다 최대 2 perplexity 더 나은 성능을 보였으며, 4비트 환경에서 Adam과 동일한 손실을 달성하는데 필요한 훈련 스텝 수를 절반으로 줄였습니다. 또한, 4비트 환경에서 Adafactor와 SPAM보다 우수한 성능을 제공합니다.

### [RIFLEx: A Free Lunch for Length Extrapolation in Video Diffusion Transformers](https://arxiv.org/abs/2502.15894)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.15894.png)

Vote: 11

Authors: Hongzhou Zhu, Min Zhao, Yixiao Chen, Guande He, Jun Zhu, Chongxuan Li

- ***What's New***: RIFLEx는 비디오 확산 변환기(Video Diffusion Transformers)에서 길이 외삽 길이(Length Extrapolation)를 위한 혁신적인 방법으로, 반복을 억제하고 운동 일관성을 유지하면서도 추가적인 수정 없이 효과적인 결과를 제공합니다. 새로운 모델 훈련 없이도 고품질의 2배 외삽을 가능하게 하며, 3배 외삽은 최소한의 미세 조정으로 품질을 향상시킵니다.
- ***Technical Details***: RIFLEx는 위치 인코딩의 주파수 성분을 체계적으로 분석하여 본질적인 주파수를 줄임으로써 반복 패턴을 최소화합니다. 이를 통해 시간적 반복을 억제하면서 운동의 일관성을 유지합니다. 실험적으로 CogVideoX-5B 및 HunyuanVideo와 같은 최신 비디오 확산 변환기에서 2배의 시간 외삽을 완전히 훈련이 필요 없는 방식으로 수행하였고, 미세 조정된 20,000개의 영상만으로 3배 외삽을 성공적으로 구현했습니다.
- ***Performance Highlights***: RIFLEx는 최근 비디오 확산 변환기에서 높은 품질의 외삽 성능을 달성했습니다. CogVideoX-5B 모델에 대해 2배 외삽에서 훈련 없이도 높은 품질의 영상을 생성할 수 있었고, HunyuanVideo 모델의 경우에는 사용자의 61.6%가 기존 길이 대비 RIFLEx 외삽 결과를 우선시하는 것으로 나타났습니다. 각각의 외삽에서 반복 발생을 줄이고, 영상의 질을 개선하는 데 성공했습니다.

### [Mobile-Agent-V: Learning Mobile Device Operation Through Video-Guided Multi-Agent Collaboration](https://arxiv.org/abs/2502.17110)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.17110.png)

Vote: 10

Authors: Junyang Wang, Fei Huang, Jitao Sang, Haiyang Xu, Ji Zhang, Ming Yan, Xi Zhang

- ***What's New***: 이 논문은 Mobile-Agent-V라는 새로운 프레임워크를 소개합니다. 이 프레임워크는 비디오 가이던스를 활용하여 모바일 장치 자동화를 향상시킵니다. Mobile-Agent-V는 비디오 입력을 통해 사용자의 작업 프로세스를 녹화하고 이를 참조하며, 에이전트가 자체적으로 과제를 효율적으로 학습하고 수행할 수 있도록 설계되었습니다. 기존 방식에 비해 30%의 성능 향상을 달성했습니다.
- ***Technical Details***: Mobile-Agent-V 프레임워크는 슬라이딩 윈도우 전략을 사용하여 비디오 입력을 처리합니다. 비디오 에이전트는 비디오의 현재 상태를 분석하고 윈도우를 적응적으로 앞으로 이동시켜, 선택된 프레임이 의사 결정에 관련성이 있도록 보장합니다. Deep-Reflection 에이전트는 비디오와 의사결정 에이전트의 출력을 분석하여 필요시 오류를 수정할 수 있도록 돕습니다.
- ***Performance Highlights***: 실험 결과, Mobile-Agent-V는 기존의 프레임워크보다 작업 수행 시 30%의 성능 향상을 보여주었습니다. 비디오 기반 지식 주입을 통해 Mobile-Agent-V는 기존의 내장 운영 규칙 의존의 한계를 극복하며, 수작업 지식 주입에 견줄만한 효과를 보였습니다.

### [Forecasting Open-Weight AI Model Growth on Hugging Face](https://arxiv.org/abs/2502.15987)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.15987.png)

Vote: 9

Authors: Kushal Raj Bhandari, Jianxi Gao, Pin-Yu Chen

- ***What's New***: 이 논문은 Hugging Face에서 오픈 웨이트 AI 모델 성장 예측을 위한 새로운 프레임워크를 제안합니다. 과학 연구의 인용 다이내믹스(citation dynamics)와 유사하게, 이 프레임워크는 오픈 웨이트 모델의 영향력을 걸쳐 예측하기 위해 세 가지 주요 매개변수, 즉, 즉시성(immediacy), 지속성(longevity), 상대적 적합성(relative fitness)을 활용합니다. 이 접근은 각 모델의 미세 조정된 모델 수의 누적 변화를 모니터링함으로써 예측력을 확보합니다.
- ***Technical Details***: 제안된 분석 프레임워크는 Wang et al.이 제안한 인용 모델을 변형한 것으로, 오픈 웨이트 모델의 채택 동향을 설명합니다. 이 모델은 즉시성(μi), 지속성(σi), 상대적 적합성(λi)을 포함하는 매개변수를 통해 각 모델 발표 후의 t 기간 동안에 누적된 미세 조정 모델 수(ct)를 측정합니다. 또한, 통계적 방법을 통해 Hugging Face 생태계에 적합한 데이터를 설정하고, 논문에서는 이러한 매개변수 간의 관계와 그 상호작용을 분석합니다.
- ***Performance Highlights***: 대부분의 모델에서 인용 스타일의 채택 곡선(adoption curve)은 매끄럽게 잘 들어맞았으며, λ, μ, σ 값은 상대적으로 적절한 범위 내에 있었습니다. 하지만 일부 모델에서는 급격한 사용량 증가 혹은 이례적 경향 때문인지 극단적으로 큰 혹은 작은 매개변수 값을 보였습니다. 이러한 결과는 오픈 웨이트 모델의 다양한 사용 패턴을 드러내며, 특정 모델에 대한 채택곡선의 과소평가 또는 예외적인 분석이 필요함을 시사합니다.

### [Towards Fully-Automated Materials Discovery via Large-Scale Synthesis Dataset and Expert-Level LLM-as-a-Judge](https://arxiv.org/abs/2502.16457)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.16457.png)

Vote: 8

Authors: Sun-Jin Choi, Kyung-Won Kwak, Jisu Bae, Taeyang Jeon, Heegyu Kim, Ga-Yeon Baek, Chihoon Lee, Dong-Hee Lee, Jihoon Hong, Yunseo Kim, Sungbum Cho, Hyunsouk Cho, Seungtaek Choi, Jinsung Park, Dongwon Jeon

- ***What's New***: 이 연구는 대규모 합성 데이터세트(Large-Scale Synthesis Dataset)을 기반으로 한자재 검색을 완전 자동화하는 새로운 접근 방식을 제안합니다. 또한, 전문가 수준의 대규모 언어 모델(LLM-as-a-Judge)을 활용하여 데이터 기반 자재 합성 예측을 수행합니다.
- ***Technical Details***: 연구에서 개발된 AlchemyBench는 새로운 벤치마크로, 재료 과학 분야의 대규모 언어 모델에 적용할 수 있는 끝에서 끝까지의 프레임워크를 제공합니다. AlchemyBench는 원자재 및 장비 예측, 합성 절차 생성, 특성화 결과 예측 등의 주요 작업을 포괄하며, 평가 과정을 자동화하는 'LLM-as-a-Judge' 프레임워크를 제안하여 전문가 평가와 높은 통계적 일치를 시연합니다.
- ***Performance Highlights***: Retrieval-Augmented Generation (RAG) 방법을 사용하여 AlchemyBench에서 성능 향상을 통해 제안된 데이터의 적용 가능성을 입증했으며, 이러한 방법이 자재 합성 예측 모델을 자동화된 벤치마킹으로 평가하여 비용과 시간 소모적인 전문가 평가 의존성을 크게 줄이면서 높은 평가 신뢰성을 유지할 수 있음을 보여주고 있습니다.

### [Beyond Release: Access Considerations for Generative AI Systems](https://arxiv.org/abs/2502.16701)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.16701.png)

Vote: 8

Authors: Andrew Trask, Dan Hendrycks, Rishi Bommasani, Ariel Herbert-Voss, Yacine Jernite, Aviya Skowron, Irene Solaiman

- ***What's New***: 이 논문은 생성 AI 시스템의 제공에 대한 결정이 시스템 구성 요소의 이용 가능 여부를 결정하지만, 제공 자체는 사용자가 시스템과 상호작용하는 방식에 영향을 미치는 여러 요소를 다루지 않는다는 점을 지적합니다. 제공을 넘어서, 시스템 구성 요소의 접근성은 잠재적인 위험과 이점을 파악하는 데 중요하다는 것이 강조되었습니다.
- ***Technical Details***: 접근성을 세 가지 축으로 나누어 분석했습니다: 자원 할당(resourcing), 기술적 사용성(technical usability), 그리고 유용성(utility). 각 부문에서 시스템 구성 요소에 대한 변수 집합이 거래 차이를 명확히 합니다. 예를 들어 자원 할당에서 모델 가중치를 서비스하려면 컴퓨팅 인프라에 대한 접근이 요구됩니다. 또한, 네 가지 고성능 언어 모델의 접근성을 비교하며, 각 모델의 접근 변수에 기초하여 접근성을 평가합니다.
- ***Performance Highlights***: 이 논문은 접근 변수가 사용자에게 접근성을 확장하거나 확장할 수 있는 기반을 마련한다고 주장하며, 접근성의 규모와 그 규모가 위험 관리와 개입 능력에 미치는 영향을 조사합니다. 결과적으로, 이 프레임워크는 시스템 제공 및 연구, 정책 결정을 알리기 위한 시스템 릴리스의 풍경과 위험-이익 거래를 보다 포괄적으로 설명합니다.

### [Reflective Planning: Vision-Language Models for Multi-Stage Long-Horizon Robotic Manipulation](https://arxiv.org/abs/2502.16707)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.16707.png)

Vote: 8

Authors: Jianlan Luo, Jiaming Han, Zhuoran Yang, Yunhai Feng, Xiangyu Yue, Sergey Levine

- ***What's New***: 본 논문은 VLM(Vision-Language Models)을 활용하여 다단계의 장기 로봇 조작(Manipulation) 작업에서 물리적 추론 능력을 향상시키기 위한 새로운 테스트-타임(Test-Time) 계산 프레임워크를 소개합니다. 핵심은 '반영' 메커니즘을 기반으로 하여 사전훈련된 VLM을 반복적으로 개선하여 미래의 세계 상태를 상상하게 하고, 이러한 예측을 통해 행동 선택을 가이드하며, 잠재적인 비최적성을 비판적으로 반성하여 추론을 개선합니다.
- ***Technical Details***: ReflectVLM 방법은 확산 기반의 동적 모델(Diffusion-based Dynamics Model)을 사용하여 계획된 작업으로 인한 미래 상태에 대한 시각적 예측을 생성하고, 이러한 예측을 분석하여 VLM이 계획된 행동을 비판하고 개선할 수 있도록 하는 반영 과정을 포함합니다. 해당 기법은 물리적 제약 조건과 그에 대한 계획 행동의 함의를 더 잘 이해할 수 있게 합니다.
- ***Performance Highlights***: 제시된 프레임워크는 최신 상용 VLM 모델 및 Monte Carlo Tree Search(MCTS)와 같은 기존의 계획 접근 방식을 능가하는 성능을 보여주었습니다. 특히, 동일한 양의 레이블이 붙은 데이터를 사용하면서도 SFT(Supervised Fine-Tuning)와 같은 사후 훈련 기법보다 우수한 성능을 나타냈습니다. 이로 인해 물리적으로 기반하는 작업에서 VLM 성능을 향상시키는 강력한 전략이 될 가능성을 보였습니다.

### [Benchmarking Temporal Reasoning and Alignment Across Chinese Dynasties](https://arxiv.org/abs/2502.16922)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.16922.png)

Vote: 7

Authors: Jialong Wu, Pengfei LI, Deyu Zhou, Yong Jiang, Zhenglin Wang

- ***What's New***: 이 논문에서는 중국 왕조의 연대기를 이용하여 대형 언어 모델(LLMs)의 시간적 추론 능력을 평가하기 위한 새로운 벤치마크인 CTM(Chinese Time Reasoning)을 소개합니다. CTM은 기존의 규칙 기반 벤치마크가 가진 한계를 극복하고, 맥락화된 문화적 이해와 상호 엔터티 관계 및 시간 정렬 능력을 강조하여 LLMs의 시간적 추론을 종합적으로 평가합니다.
- ***Technical Details***: CTM 벤치마크는 10개의 주요 중국 왕조 시대(-2100에서 1912년 사이)에 걸쳐 있는 4,700개의 중국 문화적 엔터티로 구성되어 있으며, 8,750개의 질의응답(QA) 쌍과 60개의 Timeline Ito Game 인스턴스를 포함하고 있습니다. 엔티티의 시간 및 맥락 정보를 바탕으로 다양한 QA 작업과 Timeline Ito Game을 수행하며, 이 작업들은 주어진 시간 좌표 시스템 내에서 엔티티 간의 시간적 관계 이해를 평가합니다. 다양한 규칙과 Chain-of-Thought(CoT) 설정을 사용하여 LLM의 성능을 평가하고 있습니다.
- ***Performance Highlights***: CTM 벤치마크에서 LLM들은 다양한 질의응답 작업 및 Timeline Ito Game에서 과제를 수행했으나, Pass@8 성능 측정에서 강력한 모델 GPT-4o조차도 40점을 넘지 못했습니다. 특히 엔티티 간의 짧은 시간 간격일수록 추론이 어려웠으며, 작은 파라미터 모델들은 여러 차원에서 엔티티를 정렬하는 데 실패했습니다. 연구를 통해 현존 LLM들이 더욱 정교한 시간 이해를 필요로 함을 드러내고, 향후 프리트레이닝, 지식 통합 및 추론 메커니즘 개선의 필요성을 제시합니다.

### [InductionBench: LLMs Fail in the Simplest Complexity Class](https://arxiv.org/abs/2502.15823)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.15823.png)

Vote: 6

Authors: Tyler Wong, Adam Jardine, Liangming Pan, Wenyue Hua, Sun Fei, William Yang Wang

- ***What's New***: InductionBench는 대형 언어 모델(LLMs)의 귀납적 추론 능력을 평가하기 위해 설계된 새로운 벤치마크입니다. 이 벤치마크는 LLMs가 유한한 입력-출력 쌍을 기반으로 문자열 전환(string-to-string transformation)을 추론할 수 있는지를 테스트하도록 설계되었습니다.
- ***Technical Details***: InductionBench는 순정규 계층(subregular hierarchy)에 속하는 함수들 중 가장 단순한 복잡성 클래스를 활용해 LLMs의 귀납적 추론 능력을 평가합니다. 이 함수들은 입출력 기반의 마르코프 과정(Markovian process)으로 작동하며, 다항 시간(polynomial time) 내에 학습 가능하다고 증명된 클래스로 구성되어 있습니다. 실험에서는 최첨단 LLMs를 평가하였으며, 주어진 데이터에서 함수의 가장 간결한 표현을 유도할 수 있는지를 분석합니다.
- ***Performance Highlights***: 실험 결과 o3-mini 모델만이 일부 호환성을 보였으며, 다른 모델은 모든 경우에서 호환성 점수가 0으로 나타났습니다. 대형 언어 모델들이 이런 기본적인 귀납적 추론 작업에서도 분명한 한계를 드러내고 있음을 보여주었습니다.

### [TAG: A Decentralized Framework for Multi-Agent Hierarchical Reinforcement Learning](https://arxiv.org/abs/2502.15425)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.15425.png)

Vote: 6

Authors: Giuseppe Paolo, Balázs Kégl, Hamza Cherkaoui, Albert Thomas, Abdelhakim Benechehab

- ***What's New***: 이 논문에서는 다중 에이전트 계층적 강화 학습(Multi-Agent Hierarchical Reinforcement Learning)을 위한 완전히 분산된 프레임워크인 TAG(TAME Agent Framework)를 소개합니다. 이는 기존의 두 레벨로 제한되거나 중앙 집중식 학습이 필요한 HRL 방식의 한계를 극복하고, 다양한 에이전트 유형의 통합을 가능하게 하는 새로운 LevelEnv 개념을 도입하여 임의 깊이의 계층 구조를 지원합니다.
- ***Technical Details***: TAG 프레임워크는 각 계층 레벨을 바로 윗 레벨 에이전트의 환경으로 추상화하는 LevelEnv 개념을 통해 다중 레벨 다중 에이전트 시스템을 구성합니다. 이 구조는 상호 수평, 수직적 조정을 가능하게 하고, 에이전트가 목표 복잡성에 맞는 이질적 학습 알고리즘을 사용할 수 있도록 지원합니다. 학습 과정에서는 각 에이전트가 개별 보상을 기반으로 저장된 경험을 통해 자신의 정책을 독립적으로 업데이트합니다.
- ***Performance Highlights***: TAG 기반 시스템은 MPE-Spread와 VMAS의 Balance 환경에서 기존의 평면적 및 얕은 다중 에이전트 기준선을 능가하는 성능을 보였습니다. 특히, 깊이가 증가할수록 샘플 효율성과 최종 성능 모두 개선되었으며, 이는 고도의 협력이 필요한 태스크에서 TAG의 효율성을 입증합니다. 학습된 통신 기능이 특정 환경에서의 성능 향상에 중요함을 보여주었으며, 이는 향후 연구에서 에이전트 간의 효율적인 통신 학습의 필요성을 시사합니다.

### [X-Dancer: Expressive Music to Human Dance Video Generation](https://arxiv.org/abs/2502.17414)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.17414.png)

Vote: 5

Authors: Di Chang, Zeyuan Chen, Chenxu Zhang, Chao Wang, Hongyi Xu, Guoxian Song, Xin Chen, Linjie Luo, You Xie

- ***What's New***: X-Dancer는 새로운 제로샷(Zero-shot) 음악 기반 이미지 애니메이션 파이프라인으로, 단일 정적 이미지에서 다채롭고 길이감 있는 인간 춤 비디오를 생성합니다. 이 연구는 기존의 3D 인간 움직임 생성 대신 2D 춤 움직임을 모델링하여 음악 비트와의 미세한 정렬을 캡처하는 방법을 제안합니다.
- ***Technical Details***: X-Dancer는 자동회귀 변환기(autoregressive transformer) 모델을 통해 2D 신체, 머리 및 손의 자세를 위한 확장된 음악 동기화 토큰 시퀀스를 생성하고, 이를 확산 모델(diffusion model)을 사용하여 일관되고 현실적인 춤 비디오 프레임으로 변환합니다. [AdaIN(AdaIN)]을 통해 이러한 포즈를 자연스럽게 적용하여 단일 참조 이미지에서 고화질 비디오를 생성하는 전단적(end-to-end) 프레임워크를 형성합니다.
- ***Performance Highlights***: X-Dancer는 다양성, 표현력, 현실성 측면에서 최신 방법을 상당히 능가하는 춤 비디오를 생성할 수 있습니다. 이 모델은 FVD, DIV, BAS와 같은 척도에서 우수한 성능을 보여주며, 다양한 음악 장르 및 인간 형태에 적응할 수 있는 확장 가능성과 사용자 정의 기능을 강조합니다.

### [Can Community Notes Replace Professional Fact-Checkers?](https://arxiv.org/abs/2502.14132)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14132.png)

Vote: 5

Authors: Greta Warren, Nadav Borenstein, Desmond Elliott, Isabelle Augenstein

- ***What's New***: 이 논문은 커뮤니티 노트(Community Notes)가 전문적인 팩트체커들을 대체할 수 있는지에 대해 조사한 연구입니다. 트위터/X와 메타와 같은 소셜 미디어 플랫폼이 팩트체크 조직과의 파트너십을 줄이고 사용자 기반의 커뮤니티 노트를 더욱 신뢰하려는 정책 변화를 보이고 있는 점을 지적합니다.
- ***Technical Details***: 이 연구에서는 트위터/X의 커뮤니티 노트를 대규모로 분석하여 주제, 인용된 출처, 잘못된 정보 내러티브를 반박하는지 여부 등의 속성을 언급합니다. 언어 모델을 활용하여 이러한 속성들을 자동으로 주석하였으며 커뮤니티 노트가 팩트체크 출처를 얼마나 인용하는지를 분석했습니다.
- ***Performance Highlights***: 연구 결과 커뮤니티 노트의 약 5%가 전문 팩트체커의 자료를 인용하고 있으며, 이는 이전 보고보다 최대 5배 높은 수치입니다. 특히 보건이나 정치 등의 중요한 주제에서는 팩트체크의 의존도가 더욱 높았습니다. 이로 인해 고품질의 커뮤니티 노트를 생산하기 위해서는 여전히 전문적 팩트체킹이 필요함을 시사하고 있습니다.

### [Grounded Persuasive Language Generation for Automated Marketing](https://arxiv.org/abs/2502.16810)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.16810.png)

Vote: 5

Authors: Fei Fang, Jibang Wu, Haifeng Xu, Chaoqi Wang, Hao Zhu, Chenghao Yang, Simon Mahns

- ***What's New***: Grounded Persuasive Language Generation은 대형 언어 모델(LLMs; Large Language Models)을 활용하여 부동산 마케팅 이상의 다양한 분야에서 설득적이고 근거 있는 마케팅 콘텐츠를 자동 생성하는 새로운 프레임워크입니다. 인간 전문가가 작성한 설명보다 선호도가 높은 결과물을 생성함으로써 설득력 있는 콘텐츠 생성의 새로운 가능성을 제시합니다.
- ***Technical Details***: 이 새로운 방법은 대형 언어 모델(LLM) 기반 에이전트를 통해 세 가지 주요 모듈로 구성됩니다: (1) 전문가의 행동을 모방하여 시장성 있는 기능을 예측하는 'Grounding Module', (2) 사용자 선호에 맞춘 콘텐츠를 생성하는 'Personalization Module', (3) 사실 관계를 정확히 하고 지역 특성을 통합하는 'Marketing Module'. 각 모듈은 부동산 마케팅 실험을 통해 효과가 입증되었습니다.
- ***Performance Highlights***: 실험 결과, 이 프레임워크가 생성한 마케팅 설명은 전문가가 작성한 내용을 70%의 명확한 차이로 선호받으며, 인간 전문가와 동일한 수준의 엘로(Elo) 평가에서 더 높은 점수를 획득했습니다. 이를 통해 학습된 마케터의 효과적이고 확장 가능한 솔루션이 됨을 검증했습니다.

### [Investigating the Impact of Quantization Methods on the Safety and Reliability of Large Language Models](https://arxiv.org/abs/2502.15799)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.15799.png)

Vote: 3

Authors: Viktor Moskvoretskii, Egor Shvetsov, Kseniia Studenikina, Bykov Mikhail, Evgeny Burnaev, Artyom Kharinaev

- ***What's New***: 이번 연구에서는 대형 언어 모델(Large Language Models; LLMs)의 안전성과 신뢰성을 평가하는데 있어, 양자화 방법(Quantization Methods)의 영향을 조사했습니다. 이를 위해 OpenSafetyMini라는 새로운 데이터셋을 개발하였으며, 이는 모델 간의 안전성을 구별하는 데 더욱 효과적으로 설계되었습니다.
- ***Technical Details***: 본 연구는 국가 재원과 모델 최적화의 관점에서 LLaMA와 Mistral 모델을 대상으로 4가지 최첨단 양자화 기술을 통해 4가지 벤치마크를 평가합니다. 특히 4비트와 2비트 정밀도에서 서로 다른 양자화 방법의 최적 동작을 분석하여 안전성과 신뢰성을 평가하였습니다. 양자화 후 모델의 오픈 엔디드 생성 성능을 평가하기 위해 인공지능 판단기법인 LLM-as-a-Judge 접근법을 사용하였으며, 이 평가가 실제 인간의 평가와 높은 일치를 보였습니다.
- ***Performance Highlights***: 연구 결과, 4비트 정밀도의 경우 최적의 양자화 방법이 모델에 따라 다르며, 2비트 정밀도에서는 벡터 양자화(Vector Quantization) 기법이 가장 높은 안전성과 신뢰성 성능을 제공합니다. 이러한 결과는 양자화된 모델이 엄격한 테스트에서 안전하지 않은 행동을 나타냄을 보여주었으며, 향후 연구를 위한 기초적인 방향성을 제공합니다.

### [MONSTER: Monash Scalable Time Series Evaluation Repository](https://arxiv.org/abs/2502.15122)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.15122.png)

Vote: 2

Authors: Lynn Miller, Angus Dempster, Geoffrey I. Webb, Chang Wei Tan, Mahsa Salehi, Navid Mohammadi Foumani, Charlotte Pelletier, Amish Mishra, Daniel F. Schmidt

- ***What's New***: MONSTER(MONash Scalable Time Series Evaluation Repository)는 시계열 분류(Time Series Classification) 분야의 조밀한 벤치마크 데이터셋을 대체할 수 있는 대규모 데이터셋 모음을 소개합니다. 이 저장소는 기존의 소규모 데이터셋 편중을 극복하고, 대규모 데이터셋을 활용하여 이론적이고 실질적인 학습 도전을 장려합니다.
- ***Technical Details***: MONSTER는 29개의 대규모 단변량 및 다변량 데이터셋을 제공합니다. 데이터셋은 Audio, Satellite Image, EEG, Human Activity Recognition(HAR), Count 등의 분야에 걸쳐 다양하게 구성되어 있습니다. 각 데이터셋은 5배 교차 검증을 사용할 수 있는 인덱스와 함께 제공되며, Python용 .npy와 레거시용 .csv 형식으로 제공됩니다. 또한, 데이터셋의 별도 처리 과정을 통해 접근성을 최대한 용이하게 했습니다.
- ***Performance Highlights***: 기준 성능 결과에 따르면, 대규모 데이터셋에서 퀀트(Quant)가 평균 0–1 손실이 가장 낮았으며, 콘브트란(ConvTran)과 H인셉션타임(HInceptionTime)이 그 뒤를 이었습니다. 일반적인 '오프 더 셸프(off-the-shelf)' 분류기인 극단적으로 무작위화된 트리(Extremely Randomised Trees)는 상대적으로 손실이 높았으며, 오디오 데이터셋에서는 특히 저성능을 보였습니다.

### [Early-Exit and Instant Confidence Translation Quality Estimation](https://arxiv.org/abs/2502.14429)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14429.png)

Vote: 2

Authors: Maike Züfle, Jan Niehues, Vilém Zouhar, Beni Egressy, Julius Cheng

- ***What's New***: 이 논문에서는 머신 번역(Machine Translation; MT)의 품질 평가(Quality Estimation; QE)를 더 효율적으로 구현하기 위해 두 가지 주요 과제를 해결합니다. 첫째, 대규모 시스템에서 비용을 줄이고, 둘째, 불확실성 추정(Uncertainty Estimation)을 비용 증가 없이 수행하는 방법을 제안합니다. 새로운 불확실성 인식 품질 추정 모델인 Instant Confidence COMET과, 소비를 줄이기 위해 이른 단계에서 품질 점수와 자신감을 계산하는 Early-Exit COMET을 도입합니다.
- ***Technical Details***: Instant Confidence COMET은 품질 추정 성능에 영향을 주지 않으면서 불확실성을 효율적으로 추정하는 모델입니다. 이 모델은 각 층에서 품질 점수와 에러를 추정합니다. Early-Exit COMET은 상위 자신감 범위 밴디트(Upper Confidence Bound Bandit) 알고리즘과 결합되어 평가 과정에서 계산 비용을 약 50% 절감하면서 번역 후보자 재랭킹에 활용됩니다. 머신 번역 모델의 대규모 후보군에서 최선의 후보자를 찾기 위해 사용됩니다.
- ***Performance Highlights***: 제안된 방법은 전체 계산의 50% 미만을 사용하면서도 품질 추정 성능의 저하를 거의 일으키지 않았습니다. 또한, 번역 후보자 재랭킹 작업에서는 기존의 무작위 및 로그확률 기반 기반 통신선보다 작용하여, 효율적으로 대규모 후보군을 재랭킹했습니다.

### [Pandora3D: A Comprehensive Framework for High-Quality 3D Shape and Texture Generation](https://arxiv.org/abs/2502.14247)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14247.png)

Vote: 1

Authors: Weixuan Sun, Weizhe Liu, Taizhang Shang, Pan Ji, Senbo Wang, Shenzhou Chen, Jiayu Yang, Hongdong Li, Ziang Cheng, Xibin Song

- ***What's New***: 이 논문은 다양한 입력 프롬프트를 통해 고품질 3D 모양과 텍스처를 생성할 수 있는 포괄적인 프레임워크 Pandora3D를 소개합니다. 이 프레임워크는 3D 모양 생성과 텍스처 생성을 포함하며, 특히 Variational Autoencoder(VAE)와 Diffusion Network를 활용하여 입력 프롬프트를 기반으로 잠재 공간에서 3D 모양을 생성합니다.
- ***Technical Details***: Pandora3D는 VAE를 사용하여 3D 기하를 잠재 공간에 인코딩하고, Diffusion Network를 통해 입력 프롬프트에 조건화된 잠재 표현을 생성합니다. 텍스처 생성은 프론트 이미지 생성, 다중 뷰 이미지 생성, RGB-대-PBR 텍스처 변환, 고해상도 텍스처 개선의 여러 단계로 진행됩니다. 각 단계에서는 픽셀 단위의 일관성을 보장하기 위해 Consistency Scheduler가 통합됩니다. 다중 해상도 훈련 전략을 채택하여 모델의 성능을 점진적으로 향상시킵니다.
- ***Performance Highlights***: Pandora3D 시스템은 다양한 입력 형식을 효과적으로 처리하며, 고품질의 3D 콘텐츠를 생성하는 데 성공적임을 실험 결과로 보여줍니다. 특히 복잡한 3D 모양과 정교한 텍스처를 정밀하게 복구하고 재현할 수 있는 능력을 입증합니다.

### [PLDR-LLMs Learn A Generalizable Tensor Operator That Can Replace Its Own Deep Neural Net At Inference](https://arxiv.org/abs/2502.13502)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.13502.png)

Vote: 1

Authors: Burc Gokden

- ***What's New***: 이 논문에서는 PLDR-LLM(Large Language Model from Power Law Decoder Representations)이 추론 시점에서 자체 신경망을 대체할 수 있는 일반화 가능한 텐서 연산자(Tensor Operator)를 학습한다는 내용을 다룹니다. 이러한 접근 방식은 모델의 추론 특성을 향상시키기 위한 새로운 기제를 제시합니다.
- ***Technical Details***: PLDR-LLM은 Power Law Graph Attention(PLGA) 네트워크를 활용하여 학습되고 추론 단계에서 GLM(Energy-Curvature Tensor)를 사용하여 모델의 일부를 대체할 수 있습니다. 특히 PLDR-LLM은 GLM의 캐싱(G-cache) 및 KV-cache를 통해 추론 속도를 향상시킵니다. 이 모델은 iSwiGLU 활성화 기능과 밀집 행렬 연산을 통해 추론 시 비선형 변환을 단순화하고 효율적으로 대체합니다.
- ***Performance Highlights***: PLDR-LLM은 기존의 Scaled Dot-Product Attention(SDPA) 기반 모델보다 미세하게 더 우수한 성능을 보여주며, 특히 특정 데이터 세트와의 전이 학습에서도 뛰어난 성능을 확보합니다. GLM을 사용하여 유도된 출력의 RMSE는 15열 자리까지 동일한 값을 유지하며, 다양한 벤치마크에서도 강력한 성능을 발휘합니다.

### [M3-AGIQA: Multimodal, Multi-Round, Multi-Aspect AI-Generated Image Quality Assessment](https://arxiv.org/abs/2502.15167)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.15167.png)

Vote: 1

Authors: Chuan Cui, Kejiang Chen, Weiming Zhang, Zhihua Wei, Wen Shen, Nenghai Yu

- ***What's New***: M3-AGIQA는 AI 생성 이미지의 품질 평가를 위한 새로운 프레임워크로, 다중 모달, 다중 라운드, 다중 측면 접근 방식을 활용하여 연관성, 독창성, 그리고 퍼셉션 품질을 종합적으로 평가합니다. 이 방법은 Multimodal Large Language Models(MLLMs)을 이미지 및 텍스트 인코더로 활용하며, 오프라인 MLLM에 학습된 이미지 캡션 기능을 추출하여 사용합니다.
- ***Technical Details***: M3-AGIQA의 핵심은 MLLM의 시퀀스 로짓(sequence logits) 기능을 활용하고, xLSTM과 회귀 헤드(regression head)를 결합하여 예상 모듈 점수(Mean Opinion Scores; MOS)를 예측합니다. 이 방법은 MLLM의 이미지와 텍스트 간의 높은 연관성을 디코딩하여 사람의 지각 판단과 일치시키려는 목표를 가지고 있습니다. 특히, 로라(Low-Rank Adaptation) 기법을 사용하여 온라인 MLLM의 캡션 기능을 오프라인 모델로 전수받는 과정이 포함됩니다.
- ***Performance Highlights***: M3-AGIQA는 여러 벤치마크 데이터셋에서 최첨단 성능을 발휘했으며, 특히 품질, 독창성, 연관성 측면에서 타 기법보다 탁월한 성능을 입증했습니다. 데이터셋 간 검증에서도 다른 데이터셋으로 일반화 가능한 높은 강건성을 보여주었으며, 이는 이 프레임워크의 뛰어난 일반화 성능을 입증합니다.

### [Self-Taught Agentic Long Context Understanding](https://arxiv.org/abs/2502.15920)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.15920.png)

Vote: 1

Authors: Jiang Liu, Jialian Wu, Ze Wang, Zicheng Liu, Yufan Zhuang, Yusheng Su, Xiaodong Yu, Emad Barsoum, Jingbo Shang, Ximeng Sun

- ***What's New***: 이번 연구에서는 Agentic Long-Context Understanding (AgenticLU)라는 새로운 프레임워크를 제안하여 대형 언어 모델(LLMs)의 장문 맥락 이해 능력을 강화했습니다. 이 프레임워크는 스스로 생성한 명확화 질문과 맥락적 근거를 통해 모델의 추론 과정을 동적으로 개선하는 'Chain-of-Clarifications (CoC)'를 통합합니다.
- ***Technical Details***: AgenticLU의 핵심은 CoC입니다. 여기서 모델은 명확화 질문 생성과 긴 맥락으로부터의 관련 정보 검색, 그리고 수집된 증거를 바탕으로 명확화 질문에 대한 답변을 통해 이해를 개선합니다. 이런 과정은 대규모 트리 탐색을 통해 수행되며, CoC 경로는 직접적인 명확화와 증거 기반의 질문-답변을 포함합니다. 데이터 수집 동안에는 매질의 관련성과 관계를 확인하기 위해 반복적인 LLM 질의를 활용하여 적절한 맥락을 찾아내며, 모델 학습 단계에서는 한 번의 추론 통과만으로 유관 문단 인덱스를 생성합니다.
- ***Performance Highlights***: AgenticLU 모델은 장문 맥락 입력에서 추론을 확실하게 수행하며, 다양한 장문 맥락 태스크에 걸쳐 기존 최첨단 특화된 LLM들과 프롬프트 방법들을 크게 능가하는 성과를 보였습니다. 특히, HotpotQA 등과 같은 멀티 홉 추론이 필요한 데이터셋에서 뛰어난 성능을 기록했습니다.

### [Diagnosing COVID-19 Severity from Chest X-Ray Images Using ViT and CNN Architectures](https://arxiv.org/abs/2502.16622)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.16622.png)

Vote: 1

Authors: Lucia Eve Berger, Shawn Whitfield, Luis Lara, Rajesh Raju

- ***What's New***: 이 연구에서는 COVID-19 환자의 흉부 X-선 이미지(Chest X-ray; CXR)를 기반으로 질병의 중증도를 예측하는 데 기여할 수 있는 대규모 COVID 중증도 데이터셋을 구성하고, ImageNet 및 CXR 모델로 전이 학습(Transfer Learning)을 적용하여 중증도 회귀 및 분류 과제를 해결했습니다. DenseNet161 모델이 3개의 중증도 예측에서 최고 성능을 보여주었으며, ViT(Vision Transformer)는 회귀 과제에서 가장 낮은 평균 절대 오차(Mean Absolute Error; MAE)를 기록했습니다.
- ***Technical Details***: COVID 중증도 데이터셋은 여러 출처의 데이터세트를 병합해 만들어졌으며, 데이터는 3개의 등급으로 분류되어 있습니다. DenseNet-121, ResNet50, InceptionNet, VGG-16 및 SqueezeNet 같은 다양한 사전훈련된 모델들을 적용하여 성능을 테스트했습니다. 특히 ViT는 사전학습된 14백만 이미지 기반 모델을 사용하여 X-ray 이미지 내의 픽셀 간 관계를 이해하고 중증도를 예측하는데 주력했습니다. 데이터 증강 및 전처리 과정으로 이미지 크기 조정, 회전, 플립 등을 수행했습니다.
- ***Performance Highlights***: DenseNet161 모델은 3가지 중증도 분류 과제에서 80% 정확도를 달성하여 최고 성능을 기록했습니다. ViT 모델은 회귀 과제에서 MAE 0.5676을 기록하여 Radiologist 예측 점수와의 절대 편차를 최소화했습니다. 이는 ViT가 이미지 간 주목 메커니즘을 사용함으로써 더욱 높은 성능을 나타낼 수 있음을 보여주며, 세부적인 분석이 필요합니다.

### [MegaLoc: One Retrieval to Place Them All](https://arxiv.org/abs/2502.17237)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.17237.png)

Vote: 0

Authors: Carlo Masone, Gabriele Berton

- ***What's New***: MegaLoc은 다양한 컴퓨터 비전 작업에서 성능을 발휘할 수 있도록 설계된 새로운 이미지 검색 모델입니다. Visual Place Recognition(VPR), Landmark Retrieval(LR), Visual Localization(VL) 등 여러 작업의 다양한 요구에 대응하도록 구성되어 있습니다.
- ***Technical Details***: MegaLoc은 다양한 데이터셋을 기반으로 학습되었으며, DINO-v2-base 백본과 SALAD 집계 레이어가 사용되었습니다. 5개의 데이터셋(GSV-Cities, MSLS, MegaScenes, ScanNet, SF-XL)에서 추출된 서브 배치를 사용해 multi-similarity loss를 적용합니다. 이 모델은 PyTorch를 사용하여 메모리 효율적으로 학습되었으며, 64개의 클러스터와 256개의 채널을 가진 SALAD 레이어를 포함합니다.
- ***Performance Highlights***: MegaLoc은 Baidu 같은 실내 데이터셋에서 강력한 성능을 보여주며, LaMAR 데이터셋에서도 높은 성과를 기록했습니다. Landmark Retrieval 수행 시 기존의 VPR 모델보다 더 우수한 결과를 보였으며, 여러 VRP 데이터셋에서 최고 성능을 나타냈습니다. 그러나 MSLS 데이터셋에서는 CliqueMining에 의해 약간 뒤처졌습니다.

### [Mind the Gap! Static and Interactive Evaluations of Large Audio Models](https://arxiv.org/abs/2502.15919)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.15919.png)

Vote: 0

Authors: Michael J Ryan, Kunat Pipatanakul, William Barr Held, Potsawee Manakul, Minzhi Li, Hao Zhu, Diyi Yang

- ***What's New***: 지속적인 음성 기반 AI 상호작용을 평가하기 위해 기존의 검증되지 않은 대형 오디오 모델(Large Audio Models; LAMs)에 대한 상호작용적 평가 방식을 도입했습니다. 사용자의 요구와 선호도에 기반한 평가를 통해 LAM의 개발 방향을 새롭게 제시하는 연구입니다.
- ***Technical Details***: 본 연구는 7500개의 LAM 상호작용 데이터를 484명의 참가자로부터 수집하였으며, 사용자 쿼리에 대한 주제 모델링을 통해 오디오 인터페이스의 주요 사용 사례를 식별했습니다. 실험에서 상호작용 성능과 정적 벤치마크(prediction of interactive performance)를 비교해 개별 벤치마크는 상호작용 결과와 강하게 상관관계가 없음을 밝혔습니다(τ ≤ 0.33). 다양한 조합의 주요 특징들을 통해 중간 수준의 예측성을 보여주었습니다(R2=0.30).
- ***Performance Highlights***: 상호작용 평가에서 Whisper와 Llama3-8B-Instruct를 결합한 ASR 파이프라인이 가장 선호되었으며, 이는 텍스트 의미에 주로 의존하기 때문입니다. 정적 벤치마크와 사용자 선호도 간에는 제한된 예측력이 존재하며, Public-SG-Speech와 CommonVoice 나이 예측 같은 특수한 데이터셋만이 사용자 의견과 의미 있는 상관관계를 나타냈습니다.

### [The snake in the Brownian sphere](https://arxiv.org/abs/2502.13074)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.13074.png)

Vote: 0

Authors: Emmanuel Jacob, Omer Angel, Brett Kolesnik, Grégory Miermont

- ***What's New***: 이 논문은 'Brownian Sphere'로부터 'Brownian Snake'를 구성하는 과정을 설명합니다. 이는 연속적인 감정表示중에 'Cori–Vauquelin–Schaeffer (CVS) Bijection'의 역을 기술함으로써 이루어집니다. 특히, 'Brownian Sphere'를 통해 측정 가능한 함수로 'Brownian Snake'를 구성하는 방법을 나타내어 이 두 사이의 관계를 밝히고 있습니다.
- ***Technical Details***: 'Brownian Sphere'는 무작위 측정 공간(Random Metric Space)으로, 2차원 구와 위상적으로 동형입니다. 이 논문에서는 'Brownian Sphere'로부터 'Brownian Snake'를 측정 가능한 함수로 구성하기 위해 방향성을 다루는 특별한 기술을 강조합니다. 'Cori–Vauquelin–Schaeffer Bijection'의 연속적 버전은 Aldous의 'Continuum Random Tree (CRT)'와 Brownian 라벨을 가진 'Brownian Snake'를 통해 'Brownian Sphere'로의 매핑을 설명합니다.
- ***Performance Highlights***: 이 논문에서는 제시된 방법이 여러 가지 무작위 평면 지도(Planar Maps) 및 Liouville 양자 중력(Liouville Quantum Gravity)과의 연결에 대해 성공적인 해석을 제공함을 보여줍니다. 또한, 이 연속적 CVS 매핑의 역을 성공적으로 구현함으로써 근대 확률이론에서 중요한 객체로서 'Brownian Tree' 및 'Brownian Snake'가 가지는 역할을 효과적으로 제안합니다.

