## Daily Papers (2025-02-13)

### [Fino1: On the Transferability of Reasoning Enhanced LLMs to Finance](https://arxiv.org/abs/2502.08127)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.08127.png)

Vote: 38

Authors: Qianqian Xie, Jimin Huang, Lingfei Qian, Weipeng Zhou, Xueqing Peng, Yan Wang

- ***What's New***: Fino1은 금융 분야의 복잡한 금융 과제를 해결하기 위해 고안된 최초의 금융 추론 강화 모델입니다. LLama-3.1-8B-Instruct를 기반으로 CoT (Chain-of-Thought) 미세 조정과 금융 전용 추론 경로를 사용하여 개발되었습니다.
- ***Technical Details***: Fino1은 Llama3.1-8B-Instruct 모델을 백본으로 선택하여, 금융 데이터셋 FinQA를 사용하여 추출한 추론 경로로 미세 조정되었습니다. 모델의 추론 능력 강화를 위해 강화학습(RL)과 도메인 특화 추론 경로를 적용하였습니다. 실험에서는 GPT-4o를 사용하여 여러 데이터셋에서 강력한 성능을 보였습니다.
- ***Performance Highlights***: 간단한 금융 데이터셋으로 미세 조정하였음에도 불구하고, Fino1은 모든 8B 모델을 능가하고, 평균적으로 Llama3-70B-Instruct 및 Llama3.1-70B-Instruct를 뛰어넘는 일관된 10%의 성능 향상을 달성했습니다. 실험 결과는 금융 과제에서 도메인 특화 적응이 필요함을 강조합니다.

### [TextAtlas5M: A Large-scale Dataset for Dense Text Image Generation](https://arxiv.org/abs/2502.07870)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07870.png)

Vote: 33

Authors: Weiming Han, Libo Qin, Lijuan Wang, Linjie Li, Dongxing Mao, Jiawei Zhang, Fuwei Zhang, Min Li, Zhuobai Dong, Alex Jinpeng Wang, Zhengyuan Yang, Yiqi Lin

- ***What's New***: TextAtlas5M은 대량의 장문 텍스트를 포함한 이미지 생성을 위한 대규모 데이터셋을 제공하여, 기존에 주로 간단한 텍스트를 다루는 데이터셋의 한계를 극복하고자 합니다. 이 데이터셋은 장문 텍스트와 시각적 데이터를 다양하게 생성 및 수집하여, 장문 이미지 생성 능력 평가를 지원합니다.
- ***Technical Details***: TextAtlas5M은 5백만 개의 장문 텍스트 기반 이미지로 구성되어 있으며, 다양한 데이터 유형을 포함합니다. TextAtlasEval 테스트 세트는 3000개의 인간 개선 테스트 세트로 구성되어 있습니다. TextVisionBlend와 같은 인터리브된 데이터에서 이미지와 텍스트를 섞어 저장합니다. 이 데이터셋은 세 가지 레벨의 합성 데이터와 실제 이미지를 통합하여, 모델 평가를 위한 여러 데이터 유형과 시나리오를 제시합니다.
- ***Performance Highlights***: 텍스트 조건 이미지 생성에서 TextAtlas5M은 선진적인 모델들(GPT4o와 DALL-E 3 등)을 포함한 여러 모델 평가에서 주요한 도전과제를 제시합니다. FID 점수와 CLIP 점수를 통해 모델의 이미지와 텍스트 일치 정도를 평가했으며, SD-3.5 Large 모델이 특히 OCR 관련 성능이 우수하였지만, 이미지의 레이아웃 유지에는 어려움을 겪었습니다.

### [Light-A-Video: Training-free Video Relighting via Progressive Light Fusion](https://arxiv.org/abs/2502.08590)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.08590.png)

Vote: 32

Authors: Pan Zhang, Yujie Zhou, Xiaoyi Dong, Yuhang Cao, Pengyang Ling, Qidong Huang, Jiazi Bu, Yuhang Zang, Anyi Rao, Tong Wu, Jiaqi Wang, Jinsong Li, Li Niu

- ***What's New***: Light-A-Video는 훈련이 필요 없는 비디오 조명 변경 기법으로, Progressive Light Fusion(점진적 빛 융합) 전략을 통해 비디오 재조명을 실현합니다. 이 기술은 일관된 조명 주의를 통해 프레임 간 상호작용을 강화하며, 기존의 이미지 재조명 모델(IC-Light)과 비디오 확산 모델(CogVideoX, AnimateDiff)을 기반으로 조명을 제어합니다.
- ***Technical Details***: Light-A-Video는 Consistent Light Attention(CLA; 일관된 빛 주의) 모듈을 도입하여 셀프-어텐션 레이어에서 프레임 간 상호작용을 강화하고 조명 소스의 생성을 안정화합니다. 그리고 빛 전달의 독립성 현상을 활용하여 원래 비디오의 외관과 빛 적용된 외관을 선형적으로 혼합하는 Progressive Light Fusion(PLF) 전략을 사용하여 조명 전이를 부드럽게 만듭니다. 이 방법은 높은 프레임 품질을 유지하면서 재조명 비디오의 시간적 일관성을 향상시킵니다.
- ***Performance Highlights***: Light-A-Video는 재조명 비디오의 시간적 일관성을 개선하면서 이미지 품질을 유지합니다. 실험 결과는 제안된 기법이 다수의 설정에서 효과적임을 보여주며, 전체 입력 비디오 뿐만 아니라 입력 전경 시퀀스의 배경 생성과 함께 재조명도 지원합니다.

### [BenchMAX: A Comprehensive Multilingual Evaluation Suite for Large Language Models](https://arxiv.org/abs/2502.07346)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07346.png)

Vote: 31

Authors: Xu Huang, Conghui He, Shujian Huang, Wenhao Zhu, Lei Li, Fei Yuan, Hanxu Hu

- ***What's New***: BenchMAX는 대형 언어 모델(Large Language Models; LLMs)의 언어-비종속적 능력을 평가할 수 있는 종합적인 다국어 평가 벤치마크로 소개되었습니다. 이전의 다국어 벤치마크가 주로 이해 과제에 집중한 반면, BenchMAX는 지시 따르기, 추론, 긴 문맥 이해, 코드 생성 등 고급 능력을 다양한 언어에서 공정하게 비교할 수 있도록 설계되었습니다.
- ***Technical Details***: BenchMAX는 17개 언어로 구성된 다중 경로 평가 벤치마크입니다. 데이터는 영어에서 16개의 다른 언어로 기계 번역된 후, 각 샘플은 세 명의 모국어 주석자가 독립적으로 주석을 달아 언어적 다양성과 문장 체계의 다양성을 강조합니다. 고품질 주석을 위해, 인간의 노력과 LLM 피드백을 통합하는 주석 프레임워크를 설계하였습니다.
- ***Performance Highlights***: BenchMAX의 실험 결과, 언어별로 핵심 능력의 효과가 다양하게 나타났으며, 모델 크기를 단순히 키우는 것으로는 성능 격차를 해소할 수 없음을 확인했습니다. 예컨대, 현저히 낮은 자원 언어에 대한 성능이 고자원 언어보다 지속적으로 떨어진다거나, 특정 모델이 특정 비주류 언어에서 더 우수한 성과를 보이는 경우도 있었습니다.

### [CineMaster: A 3D-Aware and Controllable Framework for Cinematic Text-to-Video Generation](https://arxiv.org/abs/2502.08639)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.08639.png)

Vote: 29

Authors: Pengfei Wan, Tianfan Xue, Xiaoyu Shi, Kun Gai, Xu Jia, Di Zhang, Xintao Wang, Yawen Luo, Qinghe Wang, Huchuan Lu

- ***What's New***: CineMaster는 텍스트-영상 생성(Text-to-Video Generation)을 위한 3D 인식 및 조절 가능한 새로운 프레임워크로, 사용자가 영화 감독처럼 3D 공간에서 객체와 카메라의 위치를 정밀하게 조정할 수 있는 기능을 제공합니다.
- ***Technical Details***: CineMaster는 두 가지 단계로 작동합니다. 첫 번째 단계는 사용자가 3D 공간에서 객체의 위치와 카메라 움직임을 설정할 수 있도록 돕는 인터랙티브 워크플로우를 설계합니다. 이 신호는 3D 엔진을 활용하여 생성되고, 텍스트-영상 확산 모델(Text-to-Video Diffusion Model)로 전달되어 사용자가 원하는 비디오 콘텐츠 생성 지침을 제공합니다. 또한, 자동 데이터 어노테이션 파이프라인을 구축하여 대규모 비디오 데이터에서 3D 객체 바운딩 박스와 카메라 경로를 추출합니다.
- ***Performance Highlights***: CineMaster는 기존의 방법들에 비해 높은 수준의 제어력을 보여주며, 실험 결과 모든 성능 지표에서 우수한 성능을 기록했습니다. 특히 객체의 공간적 배치를 더 잘 따르는 비디오를 생성하여, 사용자 의도의 공간적 설계를 충실히 구현했습니다.

### [TransMLA: Multi-head Latent Attention Is All You Need](https://arxiv.org/abs/2502.07864)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07864.png)

Vote: 22

Authors: Fanxu Meng, Zengwei Yao, Muhan Zhang

- ***What's New***: TransMLA는 기존 Group Query Attention(GQA) 기반의 사전 훈련된 모델들을 Multi-head Latent Attention(MLA) 기반 모델로 전환하는 후처리 방법론을 소개합니다. 이 전환은 KV cache 크기를 유지하면서도 모델의 표현력을 향상시키며, 향후 MLA-specific inference acceleration strategies를 개발하여 변환된 모델의 낮은 대기 시간을 유지하려는 계획도 포함하고 있습니다.
- ***Technical Details***: TransMLA는 GQA 구조를 MLA로 변환하기 위해 low-rank matrices를 사용하는데, 이는 key-value(KV) 계층에서 낮은 랭크 행렬을 활용하여 KV cache를 압축 저장할 수 있게 합니다. 이를 통해 동일한 KV cache 오버헤드 아래에서 GQA보다 높은 표현력을 제공할 수 있으며, 실험에서는 여러 대형 GQA 모델들을 MLA로 변환하여 fine-tuning을 통해 향상된 성능을 확인하였습니다.
- ***Performance Highlights***: TransMLA로 변환된 모델은 특히 수학 및 코드 작업에서 높은 정확도를 보여주며, KV 캐시 크기를 증가시키지 않고도 모델의 표현성을 높이는데 성공했습니다. 실험 결과, MLA의 구조적 이점이 주로 orthogonal decomposition 방법 덕분이라는 것을 입증하면서, 단순히 학습 가능한 매개변수의 증가만으로는 설명할 수 없는 큰 성능 향상을 이뤘습니다.

### [WorldGUI: Dynamic Testing for Comprehensive Desktop GUI Automation](https://arxiv.org/abs/2502.08047)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.08047.png)

Vote: 20

Authors: Henry Hengyuan Zhao, Mike Zheng Shou, Difei Gao

- ***What's New***: WorldGUI는 다양한 초기 상태에서 GUI 작업을 설계하여 실제 컴퓨터 사용자 상호작용을 시뮬레이션하는 새로운 GUI 벤치마크를 제공합니다. 10개의 인기 소프트웨어 응용 프로그램에 걸쳐 다양한 작업을 포함하며, 동적인 GUI 자동화 작업의 도전 과제를 해결하기 위해 GUI-Thinker라는 새로운 GUI 프레임워크를 제안합니다. GUI-Thinker는 비판적 사고(Critique Mechanism)를 활용하여 GUI 상호작용의 예측 불가능성과 복잡성을 효과적으로 관리합니다.
- ***Technical Details***: WorldGUI 벤치마크는 Microsoft PowerPoint, VSCode, Adobe Acrobat 등 10개의 데스크톱 소프트웨어 응용 프로그램에서 315개의 작업을 특징으로 합니다. 각 작업에는 사용자 쿼리, 설명 영상, 관련 프로젝트 파일이 제공되며, 4명의 훈련된 주석자가 데이터를 검증합니다. 또한, GUI-Thinker라는 새로운 GUI 에이전트 프레임워크를 소개하여, 계획 비판(Post-Planning Critique), 실행 전 확인(Pre-Execution Validation), 및 작업 후 평가(Post-Action Evaluation)의 세 가지 주요 설계를 포함합니다.
- ***Performance Highlights***: 실험 결과, GUI-Thinker는 WorldGUI 작업에서의 성공률에서 Claude-3.5(Computer Use) 모델을 14.9% 초과하여 성능을 획기적으로 향상시켰습니다. 이 개선은 GUI 자동화 향상에 있어 비판적 사고 중심의 프레임워크가 얼마나 효과적인지를 강조합니다.

### [LASP-2: Rethinking Sequence Parallelism for Linear Attention and Its Hybrid](https://arxiv.org/abs/2502.07563)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07563.png)

Vote: 19

Authors: Yu Cheng, Disen Lan, Yiran Zhong, Weigao Sun, Xiaoye Qu

- ***What's New***: 이번 논문에서는 LASP-2라는 새로운 시퀀스 패러럴리즘 방법을 제안하여 Linear Attention에서 커뮤니케이션과 컴퓨테이션 파라렐리즘을 개선하였습니다. 지난 연구 LASP에서 사용했던 대화형 커뮤니케이션을 재고하여 단일 AllGather 집합 커뮤니케이션으로 개선하였으며, 이를 통해 시퀀스 길이에 무관하게 중간 메모리 상태의 크기를 유지하여 효율성을 극대화했습니다.
- ***Technical Details***: LASP-2는 Linear Attention 층에서의 시퀀스 패러럴리즘(SP)을 위한 최소한의 커뮤니케이션 요구 사항을 재고하여 전체 커뮤니케이션 및 컴퓨테이션 워크플로우를 재구성했습니다. 이를 통해 각 반복의 전방 또는 후방 단계에서 단일 AllGather 집합 커뮤니케이션만 필요하게 설계되었습니다. 또한 LASP-2H를 통해 선형과 표준 어텐션 층을 혼합한 하이브리드 모델에 대해서도 효율적인 SP 솔루션을 제공합니다.
- ***Performance Highlights***: Linear-Llama3 모델을 통해 LASP-2의 효과를 검증한 결과, LASP-2는 LASP 대비 15.2%, Ring Attention 대비 36.6%의 학습 속도 향상을 달성하였습니다. 이는 64개의 GPU를 이용해 시퀀스 길이 2048K에서 실시한 결과입니다. 이 실험을 통해 LASP-2가 대규모 분산 시스템에서의 긴 시퀀스 처리에 있어 뛰어난 성능을 보여주며, 기존 방법에 비해 효율성을 크게 향상시킴을 알 수 있었습니다.

### [Distillation Scaling Laws](https://arxiv.org/abs/2502.08606)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.08606.png)

Vote: 17

Authors: Russ Webb, Floris Weers, Dan Busbridge, Jason Ramapuram, Etai Littwin, Amitis Shidani

- ***What's New***: 이 연구는 distillation scaling law를 소개하여 학생 모델의 성능을 추정하는 방법을 제공합니다. 컴퓨팅 자원에 따라 교사와 학생 모델 간의 자원 분배를 최적화함으로써 distillation의 효과를 극대화할 수 있습니다.
- ***Technical Details***: distillation scaling law는 학생 모델의 cross-entropy를 예측하기 위한 식으로, 교사 모델의 성능, 학생 모델의 크기, 데이터 사용량 등의 변수에 기반하여 성능을 추정합니다. 실험은 다양한 크기의 교사와 학생 모델을 사용하여 수십억 개의 토큰에 대해 실행되었습니다.
- ***Performance Highlights***: 실험 결과, distillation은 충분한 데이터나 컴퓨팅 자원을 사용할 경우 supervised learning을 능가할 수 없지만, 자원이 제한적일 때 더 효과적인 것으로 나타났습니다. 컴퓨팅 자원 할당에 따라 distillation은 특정 상황에서 더 효율적일 수 있습니다. 예를 들어, 교사 모델이 이미 존재하거나 다수의 학생 모델이 필요한 경우 distillation이 효용이 있습니다.

### [Ignore the KL Penalty! Boosting Exploration on Critical Tokens to Enhance RL Fine-Tuning](https://arxiv.org/abs/2502.06533)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06533.png)

Vote: 10

Authors: Nathanaël Beau, Jean Vassoyan, Roman Plaud

- ***What's New***: 이 논문은 큰 언어 모델(LLM)의 장기 목표 달성을 위한 새로운 접근법을 제안합니다. 기존의 KL 패널티를 무시하고 '중요한 토큰(critical tokens)'에 대한 탐색을 강화하여 강화 학습(RL) 미세 조정의 효율성을 높이는 기법을 소개합니다.
- ***Technical Details***: 기존의 KL-divergence 패널티를 수정하여 사전 훈련된 모델의 신뢰도에 따라 토큰 별로 가중치를 부여하는 방법을 사용합니다. 이 방법은 RL 미세 조정 단계 동안 '중요한 토큰'의 탐색을 촉진하여 모델이 새로 배우는 정책과 오래된 정책 간의 균형을 더 잘 잡도록 합니다.
- ***Performance Highlights***: 변형된 KL 패널티를 적용한 모델은 표준 KL 패널티를 사용한 모델보다 더 효율적으로 탐색하여 높은 정확도를 유지하면서 새 과제(특히 새로운 숫자 길이)에서 더 나은 성능을 보였습니다.

### [DPO-Shift: Shifting the Distribution of Direct Preference Optimization](https://arxiv.org/abs/2502.07599)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07599.png)

Vote: 10

Authors: Xiao Li, Xiliang Yang, Feng Jiang, Qianen Zhang, Lei Zhao

- ***What's New***: DPO-Shift는 Direct Preference Optimization(DPO)의 '확률 치환' 문제를 해결하기 위해 새롭게 제안된 방법입니다. 이 방법은 Bradley–Terry (BT) 모델에 새로운 매개변수 함수 f(λ)를 도입하여, 선택된 응답의 확률 분포를 제어 가능한 방식으로 이동시킵니다. 이렇게 함으로써, 선택된 응답 확률을 높이면서도 보상 마진을 적게 희생하는 '기본적인 트레이드오프'를 제공합니다.
- ***Technical Details***: DPO-Shift는 DPO와 유사하게 데이터셋 {(x, yw, yl)}를 기반으로 학습되며, yw와 yl은 선택된 응답과 거부된 응답을 나타냅니다. DPO-Shift에서는 거부된 응답의 보상에 f(λ)를 추가하여 두 유사한 응답 사이의 대립을 줄이는 방향으로 모델을 최적화합니다. 중요한 매개변수인 f(λ)는 0와 1 사이에서 선택되며, 이를 통해 DPO와 비교하여 선택된 응답 확률(log πθ(yw|x))을 개선하고 보상 마진을 적게 감소시킵니다.
- ***Performance Highlights***: 실험 결과, DPO-Shift는 DPO와 비교하여 선택된 응답 확률을 꾸준히 향상시키며, 특히 f(λ)를 1에 가깝게 선택함으로써 보상 마진의 손실 없이 선택된 확률을 확대할 수 있음을 보여줍니다. 또한, MT-Bench를 통한 다운스트림 작업에서 DPO-Shift가 여러 세팅에서 DPO보다 우수한 성능을 보였으며, 모형의 일반화 능력을 개선시키고 있음을 확인할 수 있었습니다.

### [SARChat-Bench-2M: A Multi-Task Vision-Language Benchmark for SAR Image Interpretation](https://arxiv.org/abs/2502.08168)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.08168.png)

Vote: 10

Authors: HaiPeng Wang, Zhiming Ma, Xiayang Xiao, Qingyun Pan, Peidong Wang, Sihao Dong

- ***What's New***: SARChat-Bench-2M은 SAR 이미지 해석을 위한 최초의 대규모 멀티모달 대화 데이터셋으로, 약 200만 개의 고품질 이미지-텍스트 쌍을 포함하고 있다. 이 데이터셋은 다양한 시나리오와 세부적인 목표 주석을 포함하며, SAR 도메인 내에서 Vision-Language Models(VLMs)의 기능을 평가할 수 있는 멀티모달 벤치마크를 통합한다.
- ***Technical Details***: SARChat-2M 데이터셋은 해양, 지상, 도시 시나리오에 걸친 약 200만 개의 고품질 SAR 이미지-텍스트 쌍으로 구성되며, 크로스 모달 표현 학습을 통해 이미지 캡션 생성, VQA(비주얼 질문 응답), 시각적 지역화 및 객체 탐지와 같은 다중 작업 학습을 지원한다. 이 연구는 SARDet-100K 등의 기존 SAR 검출 벤치마크를 활용하여 다중 모달 적응과 향상된 언어 주석을 추가하였다.
- ***Performance Highlights***: 16개의 주요 VLMs를 대상으로 SARChat-Bench에서 검증한 결과, 모델 크기가 세밀한 설명 작업 높은 성능에 영향을 미치지만, 분류 작업에는 적은 영향을 미쳤다. 큰 모델은 크로스 모달 식별과 기본적인 공간 위치 확인 작업에서 탁월한 성능을 보였으며, 참조, 계수, 세부 설명, 다중 대상 공간 관계에서는 상대적으로 도전 과제를 지닌 것으로 나타났다.

### [LLM Pretraining with Continuous Concepts](https://arxiv.org/abs/2502.08524)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.08524.png)

Vote: 7

Authors: Jack Lanchantin, Ilia Kulikov, Janice Lan, Shibo Hao, Xian Li, Jason Weston, Jane Yu, Yuandong Tian, Jihoon Tack, Andrew Cohen

- ***What's New***: 이번 연구에서는 Continuous Concept Mixing (CoCoMix)라는 새로운 사전 학습 프레임워크를 소개합니다. 이는 기존의 다음 토큰 예측에 연속적인 개념(Continuous Concepts)을 결합하여 표준 훈련 방식보다 더 효과적으로 표본을 사용할 수 있게 합니다.
- ***Technical Details***: CoCoMix는 미리 학습된 희소 오토인코더(Sparse Autoencoder; SAE)를 활용하여 개념을 추출합니다. 이 개념들은 중요한 영향력에 따라 선택되고, 예측된 개념은 연속적인 개념 벡터로 압축되어 모델의 숨겨진 상태에 삽입됩니다. 이러한 과정은 SAE의 선택적 활성화 함수와 교차 엔트로피 손실에 기반하여 이루어집니다.
- ***Performance Highlights***: CoCoMix는 21.5% 더 적은 훈련 토큰을 사용하면서도 1.38B 파라미터 모델의 경우 NTP와 유사한 성능을 제공합니다. 또한, 개념 추출로 약한 모델이 강한 모델의 학습을 돕는 약한-강한 감독 시나리오에서 강력한 성능을 보여주었습니다. CoCoMix는 다양한 벤치마크에서 일관된 성능 향상을 나타냈습니다.

### [Next Block Prediction: Video Generation via Semi-Autoregressive Modeling](https://arxiv.org/abs/2502.07737)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07737.png)

Vote: 6

Authors: Shuhuai Ren, Furu Wei, Xu Sun, Shuming Ma

- ***What's New***: 이 연구에서는 전통적인 자회귀(Autoregressive; AR) 비디오 생성 방식의 한계를 극복하기 위해 반자회귀(Semi-Autoregressive; Semi-AR) 프레임워크인 'Next Block Prediction'(NBP)을 제안했습니다. 기존 방식의 단점을 보완하여 각 블록 내의 모든 토큰이 동시에 다음 블록의 대응되는 토큰을 예측하도록 설계되어, 공간적 의존성을 강화하고 더 빠르고 효율적인 추론을 가능케 했습니다.
- ***Technical Details***: NBP 프레임워크는 비디오 콘텐츠를 크기 동일한 블록(예: 행 또는 프레임)으로 균일하게 분해하여 개별 토큰이 아닌 블록을 생성 단위로 삼습니다. 블록 내에서는 양방향 주의를 사용하여 공간적 의존성을 캡처하며, 이를 통해 추론 스텝 수가 크게 줄어드는 반자회귀(Semi-AR) 모델을 구현합니다. 실험에서는 700M부터 3B 파라미터까지 다양한 모델 스케일을 탐색했으며, 비디오 생성 품질이 큰 폭으로 향상되었습니다.
- ***Performance Highlights***: NBP 모델은 UCF101 데이터셋에서 FVD 점수 103.3을 기록해, 전통적인 NTP 모델보다 평균 4.4 낮은 점수를 보였고, K600에서는 FVD 점수가 25.5에서 19.5로 개선되었습니다. 또한 NBP 모델은 128x128 해상도에서 초당 8.89 프레임을 생성하여 11배의 속도 향상을 달성했습니다. 이는 비디오 생성의 품질과 추론 효율성을 모두 강화함을 보여줍니다.

### [NoLiMa: Long-Context Evaluation Beyond Literal Matching](https://arxiv.org/abs/2502.05167)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.05167.png)

Vote: 5

Authors: Trung Bui, Franck Dernoncourt, Hanieh Deilamsalehy, Hinrich Schütze, Seunghyun Yoon, Ali Modarressi, Ryan A. Rossi

- ***What's New***: NOLIMA는 Long-Context Language Models의 성능을 평가하기 위한 새로운 벤치마크로, 기존의 Needle-in-a-Haystack(NIAH) 테스트의 한계를 보완하고자 설계되었습니다. NOLIMA는 질문과 답변 사이의 문자적 일치(literal matching)를 최소화하여, 모델이 숨겨진 연관성을 추론하는 능력을 요구합니다.
- ***Technical Details***: NOLIMA는 12개의 최신 대형 언어 모델을 평가하며, 128K 이상의 토큰을 처리할 수 있도록 claim하는 모델들을 대상으로 합니다. 각 테스트는 32K 컨텍스트 길이에서 모델의 성능이 현저히 감소함을 보여주었습니다. NOLIMA를 위한 'needle' 세트는 매우 적은 문맥적 일치를 가지며, 모델이 지식과 상식적인 사실을 기반으로 추론하도록 강조합니다.
- ***Performance Highlights***: 대부분의 모델은 짧은 컨텍스트(<1K)에서는 높은 성능을 보이지만, 긴 컨텍스트 길이로 갈수록 성능이 급감합니다. GPT-4o조차도 기준 99.3%에서 69.7%로 성능이 감소했습니다. 이는 긴 컨텍스트에서 주의 메커니즘의 한계로 인해 모델이 관련 정보를 찾기 어려워지기 때문임을 나타냅니다.

### [Animate Anyone 2: High-Fidelity Character Image Animation with Environment Affordance](https://arxiv.org/abs/2502.06145)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06145.png)

Vote: 4

Authors: Lian Zhuo, Peng Zhang, Xin Gao, Zhen Shen, Liefeng Bo, Dechao Meng, Bang Zhang, Li Hu, Guangyuan Wang

- ***What's New***: 이 논문에서는 Animate Anyone 2라는 새로운 프레임워크를 제안하며, 캐릭터 애니메이션을 환경 제공 대상(Environment Affordance)으로 구성하여 높은 일관성을 제공합니다. 기존의 캐릭터 애니메이션 기법과 달리 환경 표현을 추가로 활용하여 주변 환경과의 일체화를 달성합니다.
- ***Technical Details***: Animate Anyone 2는 소스 비디오(Source Video)에서 움직임 신호 뿐만 아니라 환경의 표현을 조건부 입력으로 사용합니다. 환경은 캐릭터를 제외한 영역으로 정의되며, 모델은 이러한 환경 정보와 캐릭터의 관계를 학습합니다. Shape-agnostic Mask 전략을 통해 캐릭터와 환경 경계의 관계를 강조하며 객체 상호작용의 사실성을 향상시키기 위해 객체 특징을 추출하여 공간 혼합(Spatial Blending) 과정에 주입합니다.
- ***Performance Highlights***: 애니메이션의 성능은 TikTok 및 커스텀 데이터셋에서 기존 방법들보다 우수한 결과를 나타냅니다. PSNR, SSIM, LPIPS와 같은 지표에서 높은 점수를 기록하며, 캐릭터와 환경의 통합에서 더 나은 성능을 보여줍니다. 특히, 복잡한 동작 패턴에 대응하는 모델의 강력한 일반화를 입증합니다.

### [Towards Trustworthy Retrieval Augmented Generation for Large Language Models: A Survey](https://arxiv.org/abs/2502.06872)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06872.png)

Vote: 4

Authors: Md Mehrab Tanjim, Yongjia Lei, Erik Blasch, Tyler Derr, Ryan Rossi, Luna Dong, Leyao Wang, Yu Wang, Krishnaram Kenthapadi, Qingkai Zeng, Bo Ni, Nesreen Ahmed, Yinglong Xia, Zheyuan Liu, Yuying Zhao, Franck Dernoncourt, Meng Jiang, Xiaorui Liu, Wenqi Fan, Xueqi Cheng

- ***What's New***: 이 논문은 대형 언어 모델(Large Language Models; LLMs)의 신뢰성 있는 정보 생성(Retrieval-Augmented Generation; RAG)을 위한 포괄적인 로드맵을 제안하며, RAG 시스템의 신뢰성을 증대시키기 위한 통합적인 관점 및 프레임워크를 제공합니다.
- ***Technical Details***: RAG는 문맥 검색을 콘텐츠 생성과 통합하여 외부 지식을 첨가하고 환각을 줄이며, 다양한 작업에서 관련 문맥을 보장하는 기술입니다. 그러나 RAG는 강건성, 개인정보 보호, 적대적 공격 및 책임 문제와 같은 새로운 위험도 동반합니다. 본 논문은 신뢰성, 개인정보 보호, 안전성, 공정성, 설명 가능성 및 책임을 중심으로 한 RAG 시스템의 신뢰성을 구축하기 위한 프레임워크와 분류체계를 제안합니다.
- ***Performance Highlights***: RAG 시스템의 성능은 주어진 문맥과 관련된 신뢰성을 보장하기 위해 검색 과정과 검색된 콘텐츠에 기반한 생성 과정을 평가해야 합니다. 이에 따라, 신뢰성 측면에서는 불확실성 및 강건한 일반화를 고려하여 시스템의 성능을 평가합니다. 또한, 개인정보 보호를 위한 다양한 방어 방법을 도입해 데이터를 안전하게 관리하며, 공정성과 설명 가능성을 강화하여 결과의 편향성과 해석 가능성을 증가시킵니다.

### [Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing](https://arxiv.org/abs/2502.04411)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04411.png)

Vote: 3

Authors: Xinglin Pan, Xiaowen Chu, Xiang Liu, Peijie Dong, Kunfeng Lai, Li Shen, Bo Li, Haolan Chen, Zhenheng Tang

- ***What's New***: 이 논문에서는 Mediator라는 프레임워크를 제안하여 대형 언어 모델(LLMs)을 병합하는 새로운 방법을 소개합니다. 이 프레임워크는 적응형 계층 구조의 병합 및 라우팅 전략을 통해 매개 변수 충돌을 완화하며, 태스크 기반 전문가 라우팅을 사용하여 특정 작업에 대한 지식의 손실을 최소화합니다.
- ***Technical Details***: Mediator는 계층 별로 매개 변수 충돌 수치를 산출하고, 충돌이 적은 계층은 평균을 내고 충돌이 큰 계층은 라우팅 합니다. 이러한 프로세스에서 LLM은 베이스 모델과 여러 개의 작업 전문가로 분해됩니다. 이렇게 하면 압축을 통해 메모리 비용을 줄이고 작업 불확실성을 기반으로 전문가를 선택할 수 있습니다. Algorithm 1과 2는 이를 위한 자세한 프로세스를 설명합니다.
- ***Performance Highlights***: Mediator를 실험한 결과, 기존 방법보다 시스템 비용이 적으면서도 성능이 크게 향상됨을 보였습니다. Qwen-2.5 7B x 4 모델의 집합을 단일 RTX 4090 GPU에서 실행할 수 있으며, Mediator는 다양한 입력에 대해 동적인 전문가 선택을 통해 적응성을 높입니다.

### [MetaSC: Test-Time Safety Specification Optimization for Language Models](https://arxiv.org/abs/2502.07985)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07985.png)

Vote: 1

Authors: Víctor Gallego

- ***What's New***: MetaSC라는 새로운 프레임워크는 언어 모델(LM) 가중치를 변경하지 않고 추론 시간에 안전성 추론을 최적화하여 안전성을 향상시키는 새로운 동적 안전 프레임워크입니다. 이는 자기 비판 기법을 기반으로 하며, 메타 비판(meta-critique) 메커니즘을 사용하여 안전 프롬프트(specifications)를 반복적으로 업데이트함으로써 적응적으로 비판 및 수정 과정을 진행합니다.
- ***Technical Details***: MetaSC는 테스트 시간에 안전성 추론 프롬프트를 최적화하여 다양한 안전 인접 작업에 적응할 수 있도록 하는 메타 비판 프레임워크입니다. 프롬프트나 명령어 시퀀스를 제공받은 후, 언어 모델에서 초기 응답을 샘플하고, 비판을 생성한 후 비판에 따라 원래 응답을 수정하여 일반 원칙 또는 헌법과 더 잘 일치하도록 조정합니다. 이 과정에서 테스트 시간 계산을 통해 사양(spec)을 온라인으로 최적화하고, LLM이 메타 비판자로서 작용하여 새로운 안전 사양(spect+1)을 제안합니다.
- ***Performance Highlights***: MetaSC 프레임워크는 다양한 안전 관련 작업에서 기존의 고정 시스템 프롬프트 및 정적 자기 비판 방어보다 상당히 높은 안전 점수를 기록했습니다. 특히, 메타SC는 여러 대형 언어 모델에 대한 테스트 공격 방어에서 거의 완벽에 가까운 안전성을 달성했으며, 최소한의 계산 오버헤드로 다양한 안전 작업에서 강력한 성능을 발휘했습니다.

### [LLM Modules: Knowledge Transfer from a Large to a Small Model using Enhanced Cross-Attention](https://arxiv.org/abs/2502.08213)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.08213.png)

Vote: 1

Authors: Konstantin Kolomeitsev

- ***What's New***: 이 논문에서는 대형 사전 학습 모델에서 작은 모델로 지식을 전이하는 'LLM 모듈' 구조를 소개합니다. 이 구조는 개선된 크로스-어텐션(Enhanced Cross-Attention) 메커니즘을 사용하여 지식 전이를 실현합니다. Qwen2-1.5B 모델의 표현을 고정한 상태로 특별히 설계된 어텐션 레이어를 통해 GPT-Neo-125M 모델에 전달하여 제한된 컴퓨팅 자원으로 훈련할 수 있는 새로운 아키텍처를 제시합니다.
- ***Technical Details***: 기술적으로, 대형 사전 학습 모델인 Qwen2-1.5B는 고정된 상태로 외부 표현을 생성하며, 이러한 표현은 개선된 크로스-어텐션 레이어를 통해 GPT-Neo-125M 모델로 전달됩니다. 이 레이어는 선형 투영, 어댑터 블록, 게이팅 메커니즘으로 구성되어 있어 원본 모델의 정보를 보다 많이 유지함으로써 전체 모델을 재훈련할 필요 없이 지식을 흡수할 수 있습니다.
- ***Performance Highlights***: 제안된 결합 모델은 Bespoke-Stratos-17k 데이터셋에서 15에포크 훈련 후 기존 소형 모델보다 질적으로 우수한 결과를 생성했습니다. 특히 수학 연산 작업에서 단계별 설명을 제공하여 추론 구성 요소의 존재를 강조했습니다. 실험 결과, 제한된 데이터를 사용하더라도 향상된 크로스-어텐션을 통한 지식 전이가 논리적으로 일관된 출력을 생성하는 데 유리하다는 것이 입증되었습니다.

### [PDE-Controller: LLMs for Autoformalization and Reasoning of PDEs](https://arxiv.org/abs/2502.00963)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.00963.png)

Vote: 1

Authors: Kye Emond, Jialin Song, Mauricio Soroco, Wuyang Chen, Mengzhou Xia, Weiran Sun

- ***What's New***: PDE-Controller는 부분 미분 방정식(Partial Differential Equations; PDEs)을 자동으로 형식화하고 제어할 수 있는 대형 언어 모델(Large Language Models; LLMs)을 통합한 혁신적인 프레임워크입니다. 이 시스템은 자연어로 주어진 비형식적인 명령을 형식 사양으로 변환하고, PDE 제어의 효율성을 개선하기 위한 추론과 계획 단계를 실행합니다.
- ***Technical Details***: PDE-Controller는 200만 개 이상의 합성 샘플과 수작업으로 작성된 사례로 구성된 데이터 세트를 구축하여 LLMs가 PDE 제어 문제를 자동화할 수 있도록 지원합니다. 이 모델은 신호 시간 논리(Signal Temporal Logic; STL)를 사용해 제약을 형식화하며, 사람의 피드백을 활용한 강화 학습(RLHF)과 감독된 미세 조정을 통해 최적화됩니다.
- ***Performance Highlights***: PDE-Controller는 최신 LLMs보다 최대 62%의 효율성 향상을 달성하며, PDE 제어 응용 프로그램에서의 유틸리티를 대폭 개선합니다. 실험 결과, 다양한 테스트 수준에서 전반적으로 높은 성공률과 유틸리티 증가를 보여주어 현재 방법보다 우수한 성과를 입증했습니다.

### [Homeomorphism Prior for False Positive and Negative Problem in Medical Image Dense Contrastive Representation Learning](https://arxiv.org/abs/2502.05282)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.05282.png)

Vote: 0

Authors: Rongjun Ge, Yang Chen, Boyu Wang, Yuting He, Guanyu Yang, Shuo Li

- ***What's New***: 이번 연구는 의료 이미지 밀집 대조 표현 학습(Dense Contrastive Representation Learning; DCRL)에서 발생할 수 있는 대규모의 거짓 양성(False Positive) 및 거짓 음성(False Negative) 문제를 해결하기 위해 Homeomorphism Prior을 활용하는 새롭고 혁신적인 방법인 GEMINI 학습을 제안합니다.
- ***Technical Details***: GEMINI 학습은 의료 이미지의 위상적 일관성을 배경으로 하여 변형 가능 호모모르피즘 학습(Deformable Homeomorphism Learning; DHL)과 지오메트릭 의미 유사성(Geometric Semantic Similarity; GSS) 방식을 사용하여 신뢰할 수 있는 대응성을 발견합니다. 이를 통해 신뢰성 있는 양성 쌍의 학습이 진행되며, 소프트 페어링 학습로의 진행을 돕습니다.
- ***Performance Highlights***: GEMINI는 여러 데이터셋에서 DCRL 방법보다 뛰어난 성능을 보여주며, 특히 융통성 있는 학습 방법 덕분에 양성 쌍에서는 보다 신뢰성 있는 학습을, 음성 쌍에서는 소프트 학습을 실현합니다. 그 결과, 다양한 종류의 의료 이미지에서 더욱 강력한 표현력을 제공합니다.

### [Learning Conformal Abstention Policies for Adaptive Risk Management in Large Language and Vision-Language Models](https://arxiv.org/abs/2502.06884)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06884.png)

Vote: 0

Authors: Sina Tayebati, Ranganath Krishnan, Amit Ranjan Trivedi, Dinithi Jayasuriya, Nastaran Darabi, Divake Kumar

- ***What's New***: 이 논문에서는 대규모 언어 모델(LLMs) 및 비전-언어 모델(VLMs)에서 안전성 평가와 신뢰성을 향상시키기 위해 적응형 리스크 관리 메커니즘으로서 학습 가능한 컨포멀 포기 정책(Conformal Abstention Policies)을 제안합니다. 이를 통해 모델은 작업의 복잡성과 데이터 분포의 변화를 고려하여 예측 포기 임계값을 동적으로 조정합니다.
- ***Technical Details***: 제안한 프레임워크는 강화 학습(reinforcement learning; RL)을 컨포멀 예측(conformal prediction; CP)과 결합하여 예측 포기 임계값을 동적으로 최적화합니다. 정책 네트워크(policy network)는 선택적 포기로 인한 정보성(informativeness)과 리스크를 균형 있게 조절하도록 하는 CP의 하이퍼파라미터(α와 β)를 조정합니다. 이러한 학습된 정책은 캘리브레이션(calibration) 문제를 개선하여 70%-85% 감소시키고, 여러 LLM/VLM 벤치마크에서 90%의 커버리지 목표를 안전하게 유지합니다.
- ***Performance Highlights***: 제안한 기법은 Least Ambiguous Classifiers(LAC)와 Adaptive Prediction Sets(APS) 방법을 능가하는 것으로 나타났습니다. 예측 정확도는 최대 3.2% 향상되었으며, 헛소리 탐지를 위한 AUROC는 22.19% 증가하였고, 불확실성에 따라 선택적으로 생성하는 AUARC는 21.17% 증가했습니다. 베이스라인 대비 평균적인 캘리브레이션 오류는 70%-85% 감소함으로써 더욱 효과적이고 유연한 솔루션으로 부상하고 있음을 보여줍니다.

