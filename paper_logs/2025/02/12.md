## Daily Papers (2025-02-12)

### [Expect the Unexpected: FailSafe Long Context QA for Finance](https://arxiv.org/abs/2502.06329)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06329.png)

Vote: 102

Authors: Muayad Ali, Kiran Kamble, Melisa Russak, Waseem AlShikh, Dmytro Mozolevskyi, Mateusz Russak

- ***What's New***: FailSafeQA라는 새로운 금융 벤치마크가 제안되어, LLMs가 금융 기반의 질의응답 시스템에서 발생할 수 있는 인간-인터페이스 상호작용의 6가지 변형에 대한 강건성과 컨텍스트 인식을 테스트합니다. 이 연구는 쿼리 실패와 컨텍스트 실패라는 두 가지 시나리오에 집중합니다.
- ***Technical Details***: FailSafeQA 벤치마크는 LLM-as-a-Judge 방법론을 활용하여 Qwen2.5-72B-Instruct로 24개의 기존 모델을 평가하며, 쿼리 및 컨텍스트 실패 시나리오를 설정하여 강건성(Robustness)과 컨텍스트 기반(Context Grounding), 준수(Compliance)를 평가합니다. 공개적으로 доступ 가능한 연차 보고서를 사용하며, 25k 토큰 이하로 컨텍스트 윈도우를 유지하기 위해 10-K 신고서를 자릅니다.
- ***Performance Highlights***: 연구 결과, Pallyra-Fin-128k-Instruct는 가장 준수한 모델로 인식되지만, 17%의 테스트 케이스에서 강력한 예측을 유지하는 데 도전을 받았습니다. 가장 강력한 모델로 평가된 OpenAI o3-mini는 41%의 테스트 케이스에서 정보를 조작하는 경향이 보였습니다. 이러한 결과는 고성능 모델들조차 상당한 개선의 여지를 가지고 있으며, FailSafeQA가 금융 응용에서의 신뢰성을 최적화한 LLMs 개발에 기여할 수 있음을 강조합니다.

### [Competitive Programming with Large Reasoning Models](https://arxiv.org/abs/2502.06807)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06807.png)

Vote: 38

Authors: Alexander Wei, Francis Song, Mark Chen, Oleg Mürk, Borys Minaev, Max Schwarzer, OpenAI, Vineet Kosaraju, Jerry Tworek, Ahmed El-Kishky, Lorenz Kuhn, Szymon Sidor, o3 contributors, Rhythm Garg, Ignasi Clavera, Rui Shu, Mostafa Rohaninejad, Daniel Selsam, David Dohan, Nat McAleese, Wenda Zhou, Andre Saraiva, Lukasz Kaiser, Hunter Lightman, Jakub Pachocki

- ***What's New***: 이 연구에서는 대형 언어 모델(LLM)에 강화 학습을 적용해 복잡한 코딩과 추론 작업의 성능을 크게 향상시켰음을 보여줍니다. OpenAI o1과 o3 모델이 도메인 특화 시스템인 o1-ioi와 해석 전략 없이도 높은 성과를 내는 것을 비교하였습니다. o3는 특정 전략에 의존하지 않고도 IOI 2024에서 금메달을 달성했고, CodeForces 평점에서도 엘리트 인간 경쟁자들과 견줄만한 성과를 냈습니다.
- ***Technical Details***: 강화 학습을 통해 대형 문장 모델이 '사고의 연쇄(chain-of-thought reasoning)'를 사용해 복잡한 수학 및 코딩 작업을 처리할 수 있도록 훈련되었습니다. OpenAI o1-ioi는 2024 국제 정보 올림피아드(IOI)에서 경쟁하기 위해 코딩 작업에 특화된 테스트 전략과 추가 학습을 통해 성능을 극대화했습니다.
- ***Performance Highlights***: o3 모델은 OpenAI의 o1 및 o1-ioi를 능가하며 CodeForces 평점 2724 (99.8% 백분위수)를 달성했으며, IOI 2024에서는 50 서브미션 내에서 금메달 기준을 초과하는 395.64점을 획득했습니다. 이는 사람에 의해 설계된 서브태스크별 휴리스틱 없이도 강력한 성능을 보여주며, o1-ioi 대비 우수한 성과를 보였습니다.

### [Retrieval-augmented Large Language Models for Financial Time Series Forecasting](https://arxiv.org/abs/2502.05878)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.05878.png)

Vote: 25

Authors: Zihao Jiang, Yueru He, Dong Li, Yuecheng Jiang, Min Peng, Zhengyu Chen, Sophia Ananiadou, Lingfei Qian, Qianqian Xie, Mengxi Xiao, Jimin Huang, Ruey-Ling Weng, Yijing Xu

- ***What's New***: 이 논문에서는 금융 시계열 예측을 위한 최초의 검색 증강 생성(RAG; Retrieval-Augmented Generation) 프레임워크를 제안합니다. 이 프레임워크는 금융 데이터의 복잡함을 처리하기 위해 1B 파라미터의 대형 언어 모델(LLM; Large Language Model)을 미세 조정한 StockLLM을 백본으로 사용하며, LLM의 피드백을 활용한 후보 선택 방법과 질의와 과거의 중요 시퀀스 간의 유사성을 최대화하는 학습 목표를 통합함으로써 더욱 정확한 금융 예측을 가능하게 합니다.
- ***Technical Details***: 재정 시계열 예측에 특화된 FinSeer라는 새로운 검색기를 개발하였습니다. FinSeer는 금융 지표와 역사적 주식 가격을 포함하여 새롭게 구축된 데이터셋으로 훈련되며, LLM 피드백을 활용해 의미 있는 패턴을 발견하고 금융 데이터의 복잡성과 잡음을 효과적으로 줄이는 역할을 합니다. 또한, query에 대한 긍정 및 부정 후보군을 선별하여 검색된 데이터를 LLM의 예측 우선순위와 조화시키는 새로운 훈련 메커니즘을 적용했습니다.
- ***Performance Highlights***: FinSeer를 통한 RAG 프레임워크는 BIGDATA22 벤치마크에서 기존 StockLLM 및 무작위 검색 방법에 비해 8% 높은 정확도를 달성하며, 다른 검색 방법들 대비 뛰어난 효과를 보였습니다. 이는 FinSeer가 LLM의 시간 시계열 데이터 예측 능력을 크게 향상시킬 수 있음을 보여줍니다.

### [CodeI/O: Condensing Reasoning Patterns via Code Input-Output Prediction](https://arxiv.org/abs/2502.07316)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07316.png)

Vote: 21

Authors: Junxian He, Junlong Li, Daya Guo, Yu Wu, Dejian Yang, Runxin Xu

- ***What's New***: CODEI/O는 코드 입력-출력 예측(Code Input-Output Prediction)을 통한 다양한 추론 패턴의 체계적 압축을 제안하여 대규모 언어 모델(Large Language Models; LLMs)에서의 일반적인 추론 능력을 향상시킵니다. 코드를 자연어의 Chain-of-Thought(CoT) 이론로 변환하여 함축적이고 실행 가능한 추론 패턴을 학습시키는 새로운 접근 방식입니다.
- ***Technical Details***: CODEI/O 접근법은 다양한 소스에서 수집한 코드 파일을 실행 가능한 함수로 변환하고, 주어진 함수와 대응하는 텍스트 쿼리에 대해 입력값을 예측하거나 출력값을 예측하는 방식으로 훈련합니다. 450K 이상의 함수로부터 3.5M개의 학습 샘플을 생성하여 다양한 추론 능력을 강화하며, CODEI/O++는 코드 실행 기반 피드백을 활용한 다중 턴 수정 과정(Multi-turn Revision)을 포함하여 성능을 추가로 향상시킵니다.
- ***Performance Highlights***: CODEI/O는 여러 벤치마크에서 일관된 성능 향상을 보여줍니다. 다양한 벤치마크 평가에서 CODEI/O 및 CODEI/O++는 다른 대규모 데이터 기반과 비교하여 더 높은 평균 점수를 기록하며, 거의 모든 경우에서 균형 잡힌 성능 향상을 시현하였습니다.

### [Magic 1-For-1: Generating One Minute Video Clips within One Minute](https://arxiv.org/abs/2502.07701)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07701.png)

Vote: 18

Authors: Shitong Shao, Li Yuan, Enze Xie, Tian Ye, Jiantong Zhao, Michael Lingelbach, Hongwei Yi, Qingyu Yin, Yonghong Tian, Daquan Zhou

- ***What's New***: Magic 1-For-1은 텍스트에서 이미지, 그리고 이미지에서 비디오로의 두 가지 하위 작업으로 비디오 생성 작업을 분리하여 더 효율적인 비디오 생성 모델을 제안했습니다. 이를 통해 텍스트에서 비디오로의 변환보다 훨씬 빠르게 수렴할 수 있음을 보였습니다.
- ***Technical Details***: Magic 1-For-1은 이미지-텍스트 비디오(I2V) 생성 모델을 개선하기 위해 양자화 기술을 도입하여 메모리 사용량을 최적화하였습니다. 또한 Multi-modal Guidance를 사용하여 모델 수렴 속도를 가속화했으며, Adversarial Step Distillation을 적용하여 추론 지연을 줄였습니다. 이 모델은 3초 안에 5초짜리 비디오 클립을 생성할 수 있으며, Test Time Sliding Window 방식을 통해 1분 길이의 비디오를 1분 이내에 생성 가능하게 했습니다.
- ***Performance Highlights***: Magic 1-For-1 모델은 여러 개의 오픈 소스 이미지-텍스트 비디오 모델들, 예를 들어 SEINE-512x320, VideoCrafter-I2V 등을 성능 면에서 능가했으며, 비디오 생성 품질과 효율성에서 뛰어난 트레이드오프를 보였습니다. 특히 4-스텝 DMD2로 교육된 모델은 매우 신속하게 수렴했고, 8-스텝 생성기보다 빠른 성능을 보였습니다.

### [LLMs Can Easily Learn to Reason from Demonstrations Structure, not content, is what matters!](https://arxiv.org/abs/2502.07374)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07374.png)

Vote: 17

Authors: Dacheng Li, Ion Stoica, Xiangxi Mo, Shishir G. Patil, Tyler Griggs, Shiyi Cao, Shu Liu, Matei Zaharia, Joseph E. Gonzalez

- **What's New**: 본 논문에서는 LLMs가 적은 데이터와 파라미터로도 뛰어난 추론 능력을 학습할 수 있음을 보여줍니다. Long Chain-of-Thoughts(Long CoT)을 통해 복잡한 문제를 해결하는 방법을 고안하여, 콘텐츠보다 구조가 학습에 더욱 중요하다는 사실을 발견했습니다.
- **Technical Details**: Qwen2.5-32B-Instruct 모델은 데이터 효율적인 완전 지도 학습(supervised fine-tuning, SFT)와 파라미터 효율적인 LoRA(low-rank adaptation)를 사용하여 뛰어난 Long CoT 추론을 학습했습니다. 실험에서는 잘못된 예제나 추론 키워드를 제거하는 등의 콘텐츠 변화에 민감하지 않았으나, 논리적 일관성을 해치는 구조적 변화에는 성능이 크게 저하되었습니다.
- **Performance Highlights**: Qwen2.5-32B-Instruct 모델은 17,000개의 Long CoT 훈련 샘플을 기반으로 AIME 2024에서 56.7%의 정확도를 기록했으며, 이는 경쟁사 모델인 o1-preview의 44.6%를 초과했습니다. 더 나아가, 데이터가 불완전하거나 잘못된 경우에도, 구조적 일관성만 유지된다면 돌이킬 수 있는 범위 내의 성능 저하만 발생했습니다.

### [Gemstones: A Model Suite for Multi-Faceted Scaling Laws](https://arxiv.org/abs/2502.06857)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06857.png)

Vote: 16

Authors: Abhinav Bhatele, Ashwinee Panda, John Kirchenbauer, Micah Goldblum, Tom Goldstein, Siddharth Singh, David Yu Miller, Sean McLeish

- ***What's New***: 이번 연구는 Gemstones라는 종합적인 개방형 소스 스케일링 법칙 데이터셋을 공개했습니다. 이는 최대 20억 개의 파라미터를 가진 트랜스포머(Transformers) 모델의 4000개 이상의 체크포인트를 포함하고 있으며, 다양한 학습률(Learning Rates), 쿨다운(Cooldown) 일정, 아키텍처 형태로 훈련된 모델을 제공합니다.
- ***Technical Details***: Gemstones는 Gemma 아키텍처의 축소 버전을 기반으로 한 모델들로, 파라미터 수, 너비/깊이 비율(Width/Depth Ratio), 학습 토큰, 학습률, 및 쿨다운 일정에서 다양성을 가집니다. 이 모델들은 폭넓은 아키텍처와 하이퍼파라미터 선택을 사용하여 스케일링 법칙을 연구합니다. 체크포인트에 스케일링 법칙을 적용하여, 스케일링 법칙의 파라미터와 해석이 모델과 피팅 절차에 따라 어떻게 달라지는지 확인했습니다.
- ***Performance Highlights***: 스케일링 법칙은 기존의 모델들이 계산하는 FLOPs를 추측할 수 있는 예측을 예시합니다. 최근 스케일링 법칙과 레거시 모델의 벤치마크 성능과 비교하여 새로운 법칙은 더 높은 토큰당 파라미터 비율을 유지해야 한다는 결과를 보입니다. 벤치마크 성능과 모델 예산 사이의 최적 비율을 제시하여 산업 모델에서도 일관된 설계를 도출할 수 있습니다.

### [Teaching Language Models to Critique via Reinforcement Learning](https://arxiv.org/abs/2502.03492)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.03492.png)

Vote: 15

Authors: Jie chen, Weichao Mao, Liyu Chen, Lingpeng Kong, Jingjing Xu, Zhihui Xie

- ***What's New***: 저자들은 비판과 수정을 통해 대형 언어 모델(LLMs)을 향상시키는 새로운 강화 학습 프레임워크인 CTRL(Critic Training via Reinforcement Learning)을 제안했습니다. 이 프레임워크는 사람이 아닌 상태에서 비판 모델을 훈련시켜 고정된 생성자 모델의 수정 성능을 극대화하는 피드백을 생성하도록 합니다.
- ***Technical Details***: CTRL은 강화 학습을 통해 비판 모델을 훈련시킵니다. 이는 실행 피드백을 사용하여 높은 품질의 비판을 합성하고, 그런 다음 Group Relative Policy Optimization (GRPO)를 통해 비판을 최적화하는 두 단계 파이프라인을 사용합니다. 또한 각 문제에 대한 텍스트 기반 피드백을 제공하여 테스트 시간 동안 성능을 확장할 수 있습니다. 훈련은 다양한 프로그래밍 벤치마크에서 평가되어 실행되며, 해답을 일관성 있게 개선하도록 설계되었습니다.
- ***Performance Highlights***: 실험 결과, CTRL은 CodeContests 벤치마크에서 단일 회차 비판으로 11.76%의 pass@1 향상을 기록했으며, 최대 106.1%의 상대적 향상을 보여주었습니다. CTRL을 기반으로 한 비판 모델은 여러 차례 반복을 할 때도 성능을 크게 개선하며, 심지어 더 강력한 생성자 모델(GPT-4o)와 쌍을 이룰 때도 효과적인 지침을 제공할 수 있음을 입증했습니다.

### [Scaling Pre-training to One Hundred Billion Data for Vision Language Models](https://arxiv.org/abs/2502.07617)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07617.png)

Vote: 13

Authors: Keran Rong, Xiaohua Zhai, Xiao Wang, Daniel Salz, Zhe Li, Ibrahim Alabdulmohsin

- ***What's New***: 이 논문은 비전 언어 모델(Vision-Language Models; VLMs)의 사전 훈련을 1,000억 개 예제로 확장하는 최초의 연구를 제시합니다. 기존의 웨스턴 중심 벤치마크에서는 성능 향상이 주로 포화 상태였지만, 문화적 다양성이나 저자원 언어에서는 상당한 성능 개선을 보였습니다. 이는 포괄적인 멀티모달 시스템 구축에 있어 큰 데이터를 사용하는 것이 중요함을 시사합니다.
- ***Technical Details***: 1,000억 개의 이미지-텍스트 쌍으로 구성된 WebLI-100B라는 새로운 데이터셋을 도입하여 VLM을 훈련했습니다. 이 데이터는 크로스모달 벤치마크에서 다양한 언어를 지원하며, 원본 데이터셋에 특정 문화적 내용을 포함시켜 데이터의 다양성을 유지했습니다. 또한, CLIP와 같은 품질 필터를 사용하여 데이터 다양성을 유지하면서도 특정 성능 지표에서는 필터링된 데이터셋의 한계를 확인했습니다.
- ***Performance Highlights***: 기존의 웨스턴 중심 벤치마크에서는 100억에서 1,000억 예제까지 데이터 규모를 확장해도 큰 성능 향상이 나타나지 않았지만, 문화적 다양성 테스트에서는 유의미한 성능 향상이 확인되었습니다. 예를 들어, Dollar Street의 10샷 지오로컬리제이션 작업에서, 데이터 규모 확장을 통해 5% 이상의 절대 성능 향상이 나타났습니다. 이러한 결과는 대규모 데이터가 VLMs의 문화적 다양성과 포용성을 크게 향상시킬 수 있음을 시사합니다.

### [Enhance-A-Video: Better Generated Video for Free](https://arxiv.org/abs/2502.07508)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07508.png)

Vote: 12

Authors: Kaipeng Zhang, Zhangyang Wang, Xuanlei Zhao, Wenqi Shao, Kai Wang, Yang You, Yang Luo, Mengzhao Chen

- ***What's New***: Enhance-A-Video는 기존의 Diffusion Transformer(DiT) 기반 비디오 생성 모델에 대한 무훈련(Training-free) 방식의 향상 방법을 소개합니다. 이 방법은 비디오 생성 시 프레임 간의 일관성과 시각 품질을 향상시키기 위한 것입니다. 이 연구는 프레임 간 상관성을 강화하여, 추가적인 학습이나 매개변수 조정 없이 여러 DiT 기반 비디오 생성 프레임워크에 쉽게 적용할 수 있는 간단한 디자인을 채택했습니다.
- ***Technical Details***: Enhance-A-Video는 비대각선 시간적 주의 분포 내에서 프레임 간 정보(Cross-Frame Information)를 포착하기 위해 크로스-프레임 강도(Cross-Frame Intensity) 및 이를 조정하는 강화 온도 파라미터(Enhance Temperature Parameter)를 도입한 혁신적인 방법입니다. 이 접근법은 시간적 일관성을 유지하면서 미세한 시각적 세부사항을 효과적으로 보존합니다. 그리고 DiT 기반의 비디오 생성 모델의 느낌한 개선을 이끌어냅니다. 또한 프레임 간 주의도를 조정하여 생성된 비디오의 품질을 더욱 향상시킵니다.
- ***Performance Highlights***: Enhance-A-Video는 HunyuanVideo, CogVideoX, LTX-Video 및 Open-Sora와 같은 여러 벤치마크 DiT 기반 비디오 생성 모델에 통합하여 실험한 결과, 비디오 품질이 현저히 개선되었습니다. 특히 시간적 일관성 문제를 해결하고 비주얼 품질을 최적화하는 데 성공했습니다. 실험 결과, Enhance-A-Video 사용 시 사용자 선호도 조사에서 참가자들이 생성된 비디오에서 더 많은 일관성과 품질을 느꼈다고 보고했습니다. 이는 기존 모델 대비 Enhance-A-Video의 사용이 성능을 크게 향상시킴을 보여줍니다.

### [Hephaestus: Improving Fundamental Agent Capabilities of Large Language Models through Continual Pre-Training](https://arxiv.org/abs/2502.06589)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06589.png)

Vote: 10

Authors: Yifan Gao, Binxuan Huang, Bing Yin, Jingfeng Yang, Zheng Li, Xin Liu, Priyanka Nigam, Chao Zhang, Kewei Cheng, Pei Chen, Zhengyang Wang, Yuchen Zhuang, Ruijie Wang, Nasser Zalmout, Tianyi Liu, Haoming Jiang, Rongzhi Zhang, Sanket Lokegaonkar, Qing Ping

- ***What's New***: Hephaestus는 대형 언어 모델(LLM) 기반 에이전트의 근본적 능력을 강화하기 위해 설계된 첫 대규모 사전 교육 데이터 세트인 Hephaestus-Forge를 도입합니다. 이 데이터 세트는 API 함수 호출, 내재적 추론 및 계획, 환경적 피드백 적응 능력을 개선하기 위한 103B 에이전트-특화 데이터를 포함하고 있습니다.
- ***Technical Details***: Hephaestus-Forge에는 76,537개의 API와 다양한 도구 문서화가 포함되어 있으며, API 함수 호출 경로를 강화하여 내재적 추론을 키웁니다. 또한 데이터 혼합 비율을 최적화하기 위한 스케일링 법칙(scaling laws)을 조사하여 지속적인 사전 훈련(Continual Pre-Training)을 통해 Hephaestus 모델 성능을 개선했습니다.
- ***Performance Highlights***: Hephaestus-8B 모델은 소규모에서 중규모 오픈소스 LLM을 능가하며, 상업용 LLM과 비교해도 세 가지 에이전트 벤치마크에서 유사한 성능을 보였습니다. 특히, 방법론적 실험을 통해 API 함수 호출 및 문제 해결 능력에서 우수성을 입증했으며, 일반화를 통해 새로운 작업이나 환경에 효과적으로 적응하는 능력을 보여주었습니다.

### [Éclair -- Extracting Content and Layout with Integrated Reading Order for Documents](https://arxiv.org/abs/2502.04223)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04223.png)

Vote: 9

Authors: Timo Roman, Jupinder Parmar, Andrew Tao, Amala Sanjay Deshmukh, Ilia Karmanov, Jarno Seppänen, Karan Sapra, Lukas Voegtle, Joseph Jennings, Philipp Fischer, Kateryna Chumachenko

- ***What's New***: Éclair는 문서 이미지로부터 구조적 텍스트를 효율적으로 추출하기 위한 종합적인 OCR 도구입니다. 텍스트의 읽기 순서와 바운딩 박스, 그리고 그에 맞는 의미 클래스를 제공하며, 이는 다양하고 복잡한 문서 레이아웃을 처리할 수 있는 최초의 전방위적인 텍스트 추출 모델입니다.
- ***Technical Details***: Éclair는 ViT(Visual Transformer) 스타일의 인코더와 자동 회귀 디코더로 구성된 다중 모달 모델(MLLM)로, arXiv-5M과 같은 대규모 데이터셋에서 사전 학습되었습니다. 이 모델은 마크다운/LaTeX 형식으로 텍스트를 출력하며 바운딩 박스의 좌표와 의미적 클래스 정보를 동시 예측할 수 있습니다. DROBS라는 새로운 벤치마크가 Éclair의 성능을 평가하는 데 사용됩니다.
- ***Performance Highlights***: Éclair는 DROBS 벤치마크에서 최신 정확도를 기록하며, 기존의 모델들보다 월등한 성능을 보여주었습니다. 특히, 'GOT' 벤치마크에서도 우수한 성능을 나타내었으며, 더 큰 모델들보다 우수한 결과를 보였습니다. 또한, 다중 토큰 생성 방식을 통해 기존 모델보다 2배 이상 빠른 추론 속도를 자랑하여 높은 효율성을 입증했습니다.

### [NatureLM: Deciphering the Language of Nature for Scientific Discovery](https://arxiv.org/abs/2502.07527)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07527.png)

Vote: 9

Authors: Qizhi Pei, Yingce Xia, Tian Xie, Hongxia Hao, Houtian Zhu, Shufang Xie, Yeqing Lu, Qian Wang, Zun Wang, Peggy Dai, Han Yang, Kaiyuan Gao, Yue Wang, Jia Zhang, Haiguang Liu, Yeqi Bai, Zhao Yang, Zequn Liu, Chang Liu, Yuan-Jyue Chen, Lijun Wu, Yaosen Min, Xixian Liu, Chuan Cao, Liang He, Tao Qin, Ziheng Lu, Peiran Jin, Jielan Li, Kehan Wu, Chen Hu, Pan Deng, Zekun Guo, Renqian Luo, Guoqing Liu, Wei Zhang, Zilong Chen, Shuxin Zheng, Marwin Segler, Tie-Yan Liu, Yanting Li, Mingqian Ma, Krzysztof Maziarz, Yu Shi, Jianwei Zhu

- ***What's New***: Nature Language Model(NatureLM)은 다양한 과학 도메인에서의 학문적 발견을 위한 시퀀스 기반의 과학적 기초 모델입니다. NatureLM은 작은 분자, 단백질, DNA, RNA, 재료 등 다양한 과학 도메인의 데이터를 통합하여 통합된 모델을 제공하며, 과학적 발견의 여러 응용 프로그램(예: 소형 분자, 단백질, RNA 및 재료 생성 및 최적화)에서 최고 성능을 달성합니다.
- ***Technical Details***: NatureLM은 여러 과학 도메인에서 수집된 143억 개 토큰의 말뭉치로 훈련되었고, Transformer 디코더 아키텍처를 따릅니다. 모델은 1 billion, 8 billion, 46.7 billion 개의 매개변수로 구성된 버전으로 개발되었고, 모델 크기가 증가함에 따라 성능이 향상되는 것을 확인했습니다. 트레이닝 데이터는 텍스트, 작은 분자, 단백질, DNA, RNA, 재료 등 순서 기반으로 구성되었습니다.
- ***Performance Highlights***: NatureLM은 다양한 과학적 과제에서 뛰어난 성능을 보이며, 특히 소형 분자의 생성/디자인 및 최적화, 도메인 간 생성 작업에서 높은 성과를 나타냅니다. 모델 크기가 클수록 22개 평가된 과제의 18개 영역에 걸쳐 명확한 성능 향상을 보여주며, 이는 과학적 발견을 위한 큰 기초 모델의 잠재력을 강조합니다. Retrosynthesis와 같은 특정 작업에서, NatureLM은 기존의 도메인 특화 모델보다 높은 정확도를 기록했으며 Cross-domain task에서도 우수한 성과를 보였습니다.

### [CAD-Editor: A Locate-then-Infill Framework with Automated Training Data Synthesis for Text-Based CAD Editing](https://arxiv.org/abs/2502.03997)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.03997.png)

Vote: 8

Authors: Qi Liu, Shizhao Sun, Yu Yuan, Jiang Bian

- ***What's New***: CAD-Editor는 텍스트 기반 CAD 편집(Text-Based CAD Editing)을 위한 최초의 프레임워크입니다. CAD-Editor는 원본 모델과 편집된 CAD 모델 간의 정확한 대응이 요구되는 삼중 데이터(triplet data) 생성 문제를 해결하기 위해 자동화된 데이터 합성 파이프라인을 제안합니다.
- ***Technical Details***: CAD-Editor는 Locate-Then-Infill 프레임워크를 사용하여, 작업을 두 가지 하위 작업으로 나누었습니다. 첫 단계에서는 편집이 필요한 영역을 'locating'하고, 두 번째 단계에서는 이러한 영역을 적절하게 'infilling'합니다. 데이터 합성 파이프라인은 디자인 변이 모델(design variation models)과 대형 비전-언어 모델(LVLMs)을 활용하여 CAD 모델의 차이점을 편집 지시문으로 요약합니다.
- ***Performance Highlights***: CAD-Editor는 텍스트와 CAD 모델 간의 정렬(text-CAD alignment), 생성의 합법성(validity), 그리고 전반적인 생성 품질에서 기존의 방법들을 능가하는 성능을 보입니다. 또한, 사용자 조정 데이터(selective dataset)를 이용하여 모델의 편집 지시문과 편집된 CAD 모델 간의 일치를 향상시켰습니다.

### [Mask-Enhanced Autoregressive Prediction: Pay Less Attention to Learn More](https://arxiv.org/abs/2502.07490)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07490.png)

Vote: 6

Authors: Zhenyu Zhang, Shiwei Liu, Zheng Cao, Jianjin Li, Li Shen, Xialie Zhuang, Zhikai Jia

- ***What's New***: 마스크-강화 오토레그레시브 예측 (MEAP)은 대형 언어 모델(LLM)의 상황 내 정보 검색 능력을 향상시키기 위해 Masked Language Modeling (MLM)을 Next-Token Prediction (NTP)에 통합한 새로운 훈련 패러다임입니다. MEAP은 양방향 주의(attention)나 인코더-디코더 아키텍처가 필요 없으며, 추가적인 계산 오버헤드 없이 기존 NTP와 유사한 성능을 제공하도록 설계되었습니다.
- ***Technical Details***: MEAP은 디코더 전용 Transformer 구조 내에서 소수의 입력 토큰을 무작위로 마스킹한 후, 표준 다음 토큰 예측을 수행하는 간단한 방법입니다. 예비 훈련(pre-training)에서는 입력 시퀀스의 15%를, 미세 조정(fine-tuning)에서는 10%를 마스킹하여, 훈련의 난이도를 조절하고 학습 신호를 최적화합니다. 이는 모델이 비마스킹된 토큰에 더 집중하도록 하여 주의 집중성을 높이는 방식입니다.
- ***Performance Highlights***: MEAP은 전통적인 NTP에 비해 정보 검색 작업에서 최대 33% 및 다중 문서 질문 응답(MDQA)에서 27.2%의 성능 향상을 보여주며, 특히 Needle in a Haystack 작업에서 60B 훈련 토큰으로 85.8%의 정확도를 달성했습니다. 또한 MEAP는 'Lost in the middle' 시나리오에서 NTP 대비 11.77% 포인트의 성능 개선을 보였습니다. 이로써 MEAP은 긴 문맥의 정보 검색과 논리적 추론에서 우수한 성능을 입증했습니다.

### [VidCRAFT3: Camera, Object, and Lighting Control for Image-to-Video Generation](https://arxiv.org/abs/2502.07531)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07531.png)

Vote: 6

Authors: Yanwei Fu, Sixiao Zheng, Hang Xu, Yi Zhu, Xiangru Huang, Zimian Peng, Yanpeng Zhou

- ***What's New***: VidCRAFT3는 이미지-비디오 생성(Image-to-Video Generation) 분야에서 카메라, 객체, 조명 방향을 제어할 수 있는 최초의 프레임워크입니다. 이 시스템은 말그대로 3D 포인트 클라우드 렌더링, 궤적 기반 객체 모션 인코딩, 조명 인식 주의 메커니즘을 결합하여 높은 수준의 분리된 제어를 가능하게 합니다.
- ***Technical Details***: VidCRAFT3는 세 가지 주요 기술 발전을 통합합니다. 첫째, Image2Cloud 모듈은 DUSt3R를 활용해 단일 참조 이미지에서 3D 포인트 클라우드를 재구성하여 정확한 카메라 모션 제어를 구현합니다. 둘째, ObjMotionNet은 가우시안 스무딩된 옵티컬 플로우 맵에서 다중 스케일 모션 특징을 추출하여 현실적인 객체 모션을 유도합니다. 셋째, 조명 방향 통제를 위한 Spatial Triple-Attention Transformer는 조명 임베딩을 이미지-텍스트 임베딩과 통합하여 일관된 광원 효과를 보장합니다. 이러한 모듈들은 함께 사용자 정의된 제어 신호를 처리하여 정교한 비디오 생성 결과를 제공합니다.
- ***Performance Highlights***: VidCRAFT3는 실험 결과, 기존 최첨단 방법을 뛰어넘는 결과를 보였습니다. RealEstate10K 데이터셋에서 FVD 49.77, CLIPSIM 32.32, CamMC 4.07을 기록하였으며, WebVid-10M에서는 FVD 120.65, CLIPSIM 32.99, ObjMC 3.51을 기록했습니다. 이로 인해 다양한 시나리오에서 제어의 정밀도와 시각적 일관성을 크게 개선하며, 최첨단 성능을 보였습니다.

### [CoS: Chain-of-Shot Prompting for Long Video Understanding](https://arxiv.org/abs/2502.06428)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06428.png)

Vote: 6

Authors: Zixu Cheng, Shaogang Gong, Jian Hu, Chenyang Si, Wei Li

- ***What's New***: CoS(Chain-of-Shot Prompting)은 긴 비디오 이해를 위한 테스트-타임 최적화 전략으로, 비디오 내 중요한 장면(shot)을 선택하고, 이들 사이의 시각적 맥락을 최적화함으로써 Multi-modal Large Language Models(MLLMs)가 비디오 이해력을 높일 수 있도록 합니다.
- ***Technical Details***: CoS는 두 가지 주요 부분으로 구성됩니다. 첫째, 바이너리 비디오 요약(Binary Video Summary) 단계에서는 모사이싱 기법을 활용하여 장면의 관련성을 이진 코딩하여 임시적 기준을 제공합니다. 둘째, 비디오 공동 추론(Co-Reasoning) 모듈은 이를 기반으로 긍정적인 작업 관련 장면과 부정적인 작업 무관한 장면을 구성하여 모델이 관련있는 정보에 집중하도록 합니다. 이러한 선택 최적화 과정을 통해 긴 비디오에 내포된 중요한 정보를 효율적으로 추출합니다.
- ***Performance Highlights***: 다섯 개의 데이터셋과 17개 모델의 비교 실험에서 CoS는 강력한 성능을 보였습니다. CoS는 긴 비디오 처리 능력을 극대화하며, 기존 방법론에 비해 정확성이 향상되었고, 특히 LLaVA-Video 등과 결합 시 성능이 두드러졌습니다.

### [Hypencoder: Hypernetworks for Information Retrieval](https://arxiv.org/abs/2502.05364)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.05364.png)

Vote: 6

Authors: Hansi Zeng, Hamed Zamani, Julian Killingback

- ***What's New***: 이 논문은 정보 검색(Information Retrieval; IR)에 대해 새로운 패러다임을 제안하며, 쿼리에 대한 벡터 대신 작은 신경망을 생성하여 관련성 점수(Relevance Score)를 산출하는 방법을 소개합니다. 이러한 신경망은 하이퍼네트워크(Hypernetwork)를 이용해 생성되며, 이를 Hypencoder라고 명명하고 있습니다.
- ***Technical Details***: Hypencoder는 쿼리에 종속된 다층 신경망을 학습하여 문서 표현에 적용되는 유사함수를 구현합니다. 쿼리 인코더는 하이퍼헤드 레이어(Hyperhead Layer)를 통해 주어진 쿼리의 텍스트 임베딩을 변환하여 q-net을 위해 가중치와 편향을 생성합니다. 또한 그래프 기반의 탐욕적 탐색 알고리즘을 통해 방대한 코퍼스에서의 효율적인 검색을 가능하게 합니다.
- ***Performance Highlights***: Hypencoder는 MS MARCO 및 TREC의 여러 데이터셋에서 뛰어난 성능을 보였습니다. 단일 벡터 문서 표현에서도 효과적이며, 복잡한 검색 과제에서도 높은 성능을 발휘합니다. 특히, 대규모 언어 모델로 인해 증가하는 복잡한 쿼리에 대응하여 중요한 발전을 이끌어낸다는 점에서 Hypencoder의 중요성을 강조합니다.

### [Pippo: High-Resolution Multi-View Humans from a Single Image](https://arxiv.org/abs/2502.07785)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07785.png)

Vote: 4

Authors: Su Zhaoen, Julieta Martinez, Ethan Weber, Jin Kyu Kim, Rawal Khirodkar, Igor Gilitschenski, Shunsuke Saito, Timur Bagautdinov, Yash Kant

- ***What's New***: Pippo는 단일 이미지로부터 1K 해상도의 고밀도 다각도 사람 회전 비디오를 생성할 수 있는 생성 모델입니다. 추가적인 입력 없이도 (예: 맞춤형 파라메트릭 모델 또는 카메라 매개변수) 작동하며, 3B 인류 이미지 데이터셋을 기반으로 사전 훈련된 후 스튜디오에서 촬영된 사람들로 다각도 중간 및 사후 훈련을 수행합니다.
- ***Technical Details***: Pippo는 다각도 확산 변환기(Multi-view Diffusion Transformer)로, 중간 훈련 단계에서 48개의 저해상도 뷰를 소거하고 MLP를 사용하여 카메라를 조정합니다. 사후 훈련 단계에서는 고해상도 뷰를 덜 소거하고 픽셀 정렬 제어(Spatial Anchor 및 플루커 레이)를 사용하여 3D 일관성을 높였습니다. 추론 시에는 주의 편향 기술(Attention Biasing)을 도입하여 훈련 시보다 5배 이상의 뷰를 동시에 생성할 수 있습니다.
- ***Performance Highlights***: Pippo는 3D 일관성을 평가하는 새로운 지표를 제안하며, 기존 연구를 능가하는 단일 이미지로부터 다각도의 인간 생성을 달성합니다. 이러한 성능은 기존의 다각도 인간 생성 모델보다 정확하고 일관성 있게 다양한 관점을 생성할 수 있음을 입증합니다.

### [Forget What You Know about LLMs Evaluations - LLMs are Like a Chameleon](https://arxiv.org/abs/2502.07445)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07445.png)

Vote: 4

Authors: Seffi Cohen, Bracha Shapira, Nurit Cohen-Inger, Yehonatan Elisha, Lior Rokach

- ***What's New***: 이 논문은 대형 언어 모델(LLMs)의 벤치마크별 대당 착용성을 감지할 수 있는 새로운 방법론인 카멜레온 벤치마크 과적합 탐지기(Chameleon Benchmark Overfit Detector; C-BOD)를 소개합니다. C-BOD는 벤치마크의 프롬프트에 통제된 텍스트 왜곡을 가하여 LLMs가 표면 패턴 또는 특정 데이터셋에 과도하게 의존하는지를 평가하여 모델의 진정한 언어 이해 능력을 테스트합니다.
- ***Technical Details***: C-BOD는 N개의 샘플로 구성된 벤치마크 데이터셋 D에 대해 왜곡 매개변수 µ를 사용하여 텍스트 변형을 가하고, 모델 E의 성능 차이의 통계적 유의성을 McNemar's test를 통해 평가합니다. 이 방법론은 변형된 데이터셋의 원본 텍스트와 재구성된 텍스트 사이의 차이를 측정하여 모델이 표면 패턴에 과도하게 의존하는지 여부를 파악합니다.
- ***Performance Highlights***: MMLU 벤치마크에서 26개의 주요 LLMs를 평가한 결과, 대부분의 모델은 원본 테스트 세트에 비해 재구성된 테스트에서 평균 2.15%의 성능 저하를 보였으며, 특히 규모가 큰 모델일수록 프롬프트 변형에 민감한 경향을 보였습니다. 그러나, Llama 모델 가족은 변화가 미미하여 보다 지속적인 성능을 나타냈습니다.

### [Jakiro: Boosting Speculative Decoding with Decoupled Multi-Head via MoE](https://arxiv.org/abs/2502.06282)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06282.png)

Vote: 3

Authors: Fuwei Yang, Yang Liu, Dong Li, Haiduo Huang, Jinze Li, Zhenhua Liu, Emad Barsoum, Pengju Ren, Xuanwu Yin, Yixing Xu

- ***What's New***: 이 논문에서는 Jakiro라는 새로운 디코딩 기술을 소개하며, Mixture of Experts (MoE) 메커니즘을 통해 독립적인 전문가들이 다양한 예측을 생성해 후보 간의 상호 연관성을 효과적으로 분리합니다. 이는 기존의 제한적 예측 다양성과 전반적인 예측 정확도를 개선하며, 새로운 SOTA를 달성합니다.
- ***Technical Details***: Jakiro는 초기에 자동회귀 디코딩(autoregressive decoding)을 사용하여 초기 토큰을 생성하며, 그 후 단계에서 병렬 디코딩(parallel decoding)과 contrastive 메커니즘을 결합하여 정확성을 높입니다. 독립적인 MoE 전문가들이 후보 토큰을 생성하며, 이 과정에서 계층 구조 내에서 독립성을 유지하여 예측의 다양성을 높입니다.
- ***Performance Highlights***: Jakiro는 Vicuna 모델에서는 평균적으로 3.38배의 속도 향상을 보여 다른 모델보다 우수한 성능 개선을 입증하였습니다. 이는 높은 수용 길이와 결합하여 모델 출력 품질에서 손실 없이 추론 속도를 증가시킵니다.

### [Sparse Autoencoders for Scientifically Rigorous Interpretation of Vision Models](https://arxiv.org/abs/2502.06755)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06755.png)

Vote: 3

Authors: Samuel Stevens, Tanya Berger-Wolf, Yu Su, Wei-Lun Chao

- ***What's New***: 이 논문에서는 희소 오토인코더(Sparse Autoencoders; SAEs)를 사용하여 비전 모델의 해석 가능성과 조작 가능성을 실험적으로 결합하는 새로운 프레임워크를 제시합니다. 기존 방법론과 달리 SAEs는 인간이 해석할 수 있는 시각적 특징을 발견하고 이를 정밀하게 조작하여 모델의 행동에 대한 가설을 검증할 수 있습니다.
- ***Technical Details***: 이번 연구에서 제안된 SAEs는 고차원의 희소 표현을 통해 복잡한 모델 활성화 벡터를 해석 가능한 시멘틱 개념으로 분해합니다. 이는 Visual Transformer (ViT)에서 중간 레이어 활성화를 기반으로 훈련되었습니다. SAEs는 원본 입력을 재구성하면서 재구성 오류를 최소화하고 희소성을 독려하는 메트릭을 사용하여 트레이닝됩니다.
- ***Performance Highlights***: 제안된 SAEs는 CLIP과 DINOv2 같은 최신 비전 모델에서 문화적 이해 및 추상적 개념 형성 능력을 탐구합니다. CLIP은 언어감독(Language Supervision)을 통해 다양한 시각 스타일에 걸쳐 추상적 개념을 형성할 수 있는 반면, 시각적 훈련만을 받은 DINOv2는 스타일별 특성을 보다 세분화된 형태로 학습합니다.

### [Auditing Prompt Caching in Language Model APIs](https://arxiv.org/abs/2502.07776)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07776.png)

Vote: 3

Authors: Xiang Lisa Li, Chenchen Gu, Rohith Kuditipudi, Percy Liang, Tatsunori Hashimoto

- ***What's New***: 이 연구는 대형 언어 모델 API의 프롬프트 캐싱(LLM API Prompt Caching)이 사용자 프롬프트에 대한 타이밍 변화를 초래하고, 이로 인해 사이드-채널 타이밍 공격의 위험성을 낳는다는 점을 밝혔습니다. 특히, 캐시가 사용자 간 공유될 때, 공격자는 빠른 API 응답 시간을 통해 다른 사용자의 프롬프트 정보를 알아낼 수 있는 잠재적 가능성이 있습니다.
- ***Technical Details***: 논문에서는 API 제공자의 캐싱 정책을 검증하기 위한 통계적 감사를 개발했습니다. 이 감사는 프롬프트 캐싱 여부와 캐시 공유 수준을 결정하기 위한 통계적 가설 검정을 사용합니다. 감사에서는 두 가지 절차(캐시 히트와 캐시 미스)로부터 응답 시간을 샘플링하여, 이를 통해 캐싱 여부를 감지합니다. 2024년 9월과 10월, 17개의 실제 언어 모델 API 제공자를 감시하여, 그 중 8개의 제공자에서 프롬프트 캐싱을 감지했습니다.
- ***Performance Highlights***: 조사된 API 중 7개 제공자에서 글로벌 캐시 공유가 발견되어, 공격자가 타이밍 데이터를 통해 다른 사용자의 프롬프트 정보를 학습할 수 있는 잠재적 위험이 존재함을 보여주었습니다. 특히 OpenAI의 임베딩 모델이 디코더 전용 구조임을 처음으로 입증했습니다.

### [Skill Expansion and Composition in Parameter Space](https://arxiv.org/abs/2502.05932)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.05932.png)

Vote: 2

Authors: Jianxiong Li, Xianyuan Zhan, Tenglong Liu, Yixing Lan, Haoyi Niu, Xin Xu, Yinan Zheng

- ***What's New***: 본 논문은 파라미터 공간에서의 스킬 확장 및 구성(Parametric Skill Expansion and Composition; PSEC)이라는 새로운 프레임워크를 제안합니다. 이는 저차원 어댑터(LoRA; Low-Rank Adaptation) 모듈을 활용하여 자율 에이전트가 효율적으로 새로운 도전을 해결할 수 있도록 합니다.
- ***Technical Details***: PSEC 프레임워크는 에이전트의 스킬 라이브러리를 유지하면서 새로운 과제에 맞춰 점진적으로 새로운 스킬을 통합하는 구조입니다. 각 스킬은 LoRA 모듈로 인코딩되어 있으며, 공유되는 정보를 기반으로 다양한 스킬을 조합하여 새로운 스킬을 프로그래밍하는 메커니즘을 활용합니다. 또한, 상황에 따라 다른 스킬을 동적으로 활성화할 수 있도록 설계된 상황 인식 모듈이 포함되어 있습니다.
- ***Performance Highlights***: 제안된 PSEC 방법은 D4RL, DSRL 벤치마크 및 DeepMind Control Suite에서 상당한 성능 향상을 보여주었으며, 이는 기존 방식보다 우수한 훈련 효율성과 샘플 효율성을 제공합니다. 이는 인간 수준의 자율 에이전트 개발을 위한 유망한 경로를 제시합니다.

### [FocalCodec: Low-Bitrate Speech Coding via Focal Modulation Networks](https://arxiv.org/abs/2502.04465)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04465.png)

Vote: 2

Authors: Mirco Ravanelli, Francesco Paissan, Luca Della Libera, Cem Subakan

- ***What's New***: FocalCodec는 음성 데이터를 매우 낮은 비트레이트로 압축하는 효율적인 코덱입니다. 이 코덱은 'focal modulation'을 기반으로 하여 단일 바이너리 코드북(single binary codebook)을 사용해 0.16에서 0.65 kbps 사이의 비트레이트로 음성을 압축하며, 다국어 음성 및 노이즈 환경에서도 우수한 성능을 발휘합니다.
- ***Technical Details***: FocalCodec의 아키텍처는 새로운 압축기-양자화기-압축 해제기(compressor-quantizer-decompressor) 모듈을 통합하여 VQ-VAE 프레임워크에 기반하고 있습니다. 초점 변조(focal modulation) 기반의 혁신적 모듈을 사용하여 음향 및 의미 정보 모두를 보존하면서 효율적으로 설계되었습니다. 이 음성 코덱은 다국어 환경과 노이즈가 있는 조건에서도 우수한 성능을 발휘하며, 생성적 모델링에도 적합합니다.
- ***Performance Highlights***: FocalCodec는 최신 기술과 비교하여 낮은 비트레이트에서 음성 재구성 및 음성 변환에서 경쟁력 있는 성능을 발휘합니다. 평가 결과는 이 코덱이 다운스트림 작업을 위한 충분한 의미 및 음향 정보를 성공적으로 보존함을 보여주며, 생성적 모델링에 잘 적합하다는 것을 입증했습니다.

### [Goedel-Prover: A Frontier Model for Open-Source Automated Theorem Proving](https://arxiv.org/abs/2502.07640)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07640.png)

Vote: 2

Authors: Danqi Chen, Jia Li, Mengzhou Xia, Hongzhou Lin, Chi Jin, Bohan Lyu, Yong Lin, Shange Tang, Sanjeev Arora, Kaiyu Yang, Jiayun Wu

- ***What's New***: Goedel-Prover는 수학 문제에 대한 자동화된 공식 증명 생성 분야에서 최첨단(SOTA) 성능을 이루어낸 오픈 소스 대형 언어 모델(LLM)입니다. 수학 문제를 자연어에서 포멀 언어(Lean 4)로 변환하는 'statement formalizer'를 훈련하여 164만 개의 포멀 문항 집합을 생성하였고, 이를 바탕으로 증명을 생성하는 provers를 반복적으로 훈련하여 성능을 향상시켰습니다.
- ***Technical Details***: 우리는 공식 언어로 작성된 수학 문항이 부족하다는 문제를 해결하기 위해, Numina에서 추출한 자연어 수학 문제를 Lean 4로 변환하는 훈련된 포멀라이저를 사용하여 포멀 문항을 생성했습니다. 그 후, DeepSeek-Prover-V1.5-RL과 같은 기존 모델을 능가하는 새로운 증명을 추가적으로 훈련 세트에 포함시키며 여러 번의 반복 학습을 통해 최적화된 증명을 수집했습니다.
- ***Performance Highlights***: Goedel-Prover는 miniF2F 벤치마크에서 Pass@32 기준으로 57.6%의 성공률을 기록하며, 이전 오픈 소스 최상 모델인 DeepSeek-Prover-V1.5-RL을 7.6% 초과했습니다. 또한, Lean Workbook 문제에서 29,700개의 증명을 생성하여 이전 연구보다 두 배 가까이 늘렸고, PutnamBench에서 7개의 문제를 성공적으로 풀어서 순위표에서 1위를 차지했습니다.

### [Forbidden Science: Dual-Use AI Challenge Benchmark and Scientific Refusal Tests](https://arxiv.org/abs/2502.06867)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06867.png)

Vote: 1

Authors: David Noever, Forrest McKee

- ***What's New***: 이 논문은 민감한 과학 영역에서 LLM(Large Language Models)의 이중용도(Dual-Use) 어플리케이션을 평가하기 위한 새로운 벤치마크와 과학 거절 테스트를 제안합니다. 데이터셋과 평가 프레임워크를 통해 주로 통제 물질 쿼리를 분석하고, 주요 모델들의 응답을 체계적으로 평가합니다.
- ***Technical Details***: 이 벤치마크는 생물학, 화학, 컴퓨터 보안에 관한 주제들을 포함하며, 각 모델이 특정 과학적 질문에 대해 잠재적 거부 반응을 어떻게 나타내는지를 조사합니다. 실험 설계는 단일 및 다중 프롬프트 변형을 통해 모델의 응답 일관성을 조사하며, 코사인 유사성을 통해 구조화된 질문 문구 변형이 모델 성능에 미치는 영향을 분석합니다.
- ***Performance Highlights***: 다양한 모델별 과학적 안전 프로파일을 요약하면, 오픈 소스 모델인 Mistral는 모든 질문에 응답하려고 시도하는 한편, Claude-3.5-sonnet은 가장 보수적이어서 73%의 거부율을 보였습니다. 프롬프트 변형에 대한 테스트는 1개의 프롬프트 사용 시 85%의 응답 일관성을 보였으나 5개로 늘리면 65%로 감소한다는 것을 나타냈습니다.

