## Daily Papers (2025-02-24)

### [LLM-Microscope: Uncovering the Hidden Role of Punctuation in Context Memory of Transformers](https://arxiv.org/abs/2502.15007)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.15007.png)

Vote: 95

Authors: Ivan Oseledets, Elizaveta Goncharova, Matvey Mikhalchuk, Temurbek Rahmatullaev, Polina Druzhinina, Anton Razzhigaev, Andrey Kuznetsov

- ***What's New***: 이 논문에서는 LLM-Microscope라는 도구를 소개하여 대형 언어 모델(Large Language Models; LLMs)에서 구두점과 같은 사소한 토큰이 문맥 이해에 중요한 역할을 한다는 것을 밝혀냈습니다. 이를 통해 LLM들이 내부적으로 문맥 정보를 어떻게 저장하고 사용하는지를 파악할 수 있게 되었습니다.
- ***Technical Details***: LLM-Microscope는 특정 토큰의 비선형성(nonlinearity)과 문맥 메모리(context memory)를 측정하여, 각 층에서 토큰 다음 예측이 어떻게 변하는지를 시각화합니다. 이를 위해 Logit Lens를 사용하여 중간 층의 기여도를 분석하고, 내재적 차원(intrinsic dimensionality)을 측정합니다.
- ***Performance Highlights***: MMLU 및 BABILong-4k와 같은 벤치마크에서 단순해 보이는 'filler' 토큰을 제거했을 때 성능이 일관되게 저하되는 현상을 발견했습니다. 이는 이들 토큰이 문맥 연결을 유지하는 데 중요한 역할을 한다는 것을 시사하며, 심지어 GPT-4o로 이러한 토큰을 선택적으로 제거하더라도 성능 저하가 지속됩니다.

### [SurveyX: Academic Survey Automation via Large Language Models](https://arxiv.org/abs/2502.14776)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14776.png)

Vote: 73

Authors: Zhiyu li, Jiawei Yang, Hanyu Wang, Chen Tang, Simin Niu, Bo Tang, Xun Liang, Zifan Zheng, Shichao Song, Feiyu Xiong, Yezhaohui Wang, Keming Mao

- ***What's New***: SurveyX는 대형 언어 모델(Large Language Models; LLMs)을 활용하여 학문적 설문 조사를 자동으로 생성하는 혁신적인 시스템입니다. 이 시스템은 설문 조사 작성을 준비 단계와 생성 단계로 분할하여 수행되며, 온라인 참조 조회, AttributeTree라는 사전 처리 방법, 재광택 프로세스를 도입하여 설문 작성의 효율성을 크게 향상시킵니다.
- ***Technical Details***: SurveyX는 준비 단계에서 주제에 따른 참조 자료를 인터넷에서 검색하고 필터링하여 효율적인 참조 데이터베이스를 구축하고, 이를 기반으로 한 검색 강화 생성(Retrieval Augmented Generation; RAG) 기술을 사용합니다. 생성 단계에서는 얻은 정보로 설문의 윤곽을 잡고 본문을 작성하며, 표와 그림을 사용해 표현을 풍부하게 합니다. 또한, AttributeTree라는 참조 전처리 방법을 통해 문서에서 핵심 정보를 효율적으로 추출하여, LLM의 이해력 및 컨텍스트 창 활용을 최적화합니다.
- ***Performance Highlights***: SurveyX는 기존 자동 설문 조사 생성 시스템보다 콘텐츠 품질과 인용 품질에서 뛰어난 성과를 보이며, 사람 전문가의 성능에 근접합니다. 설문 콘텐츠의 평균 톰슨은 4.590으로 높은 평가를 받았으며, 인용 정확도에서도 F1 점수 81.52를 기록하였습니다. 이는 SurveyX가 생성한 설문이 높은 신뢰성과 타당성을 갖추고 있음을 보여줍니다.

### [MaskGWM: A Generalizable Driving World Model with Video Mask Reconstruction](https://arxiv.org/abs/2502.11663)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.11663.png)

Vote: 36

Authors: Lewei Lu, Jingcheng Ni, Zehuan Wu, Yuxin Guo, Yichen Liu, Rui Chen

- ***What's New***: MaskGWM은 자율주행을 위한 일반화 가능한 주행 세계 모델을 제안하며, 비디오 마스크 재구성(Video Mask Reconstruction)을 통합하여 생성 손실(Generation Loss)과 MAE 스타일의 특징 수준 맥락 학습을 결합했습니다. 이로써 예측 충실도와 일반화 능력을 향상시키고, 장기 예측(Long-horizon Prediction)과 다중 관점 생성(Multi-view Generation)을 가능하게 했습니다.
- ***Technical Details***: MaskGWM은 더 확장 가능한 디퓨전 트랜스포머(Diffusion Transformer; DiT) 구조를 기반으로 하며, 특별한 마스크 토큰과 행별 마스크(aligned row-wise masks)를 활용하여 공간-시간적 맥락을 학습합니다. 생성 과정과 마스크 재구성 간의 상호 작용을 강화하기 위해 디퓨전 관련 마스크 토큰을 도입했으며, 두 개의 변형: MaskGWM-long과 MaskGWM-mview를 설계했습니다.
- ***Performance Highlights***: nuScene, OpenDV-2K, Waymo 데이터셋의 실험 결과, MaskGWM은 기존 주행 세계 모델보다 비디오 품질과 일반화 능력이 크게 향상되었습니다. nuScene 데이터셋 기준에서 FID 5.6 및 FVD 92.5를 달성하며, 기존의 VISTA 모델보다 성능이 우수함을 입증했습니다. Waymo 데이터셋에서의 제로샷 평가에서도 탁월한 FVD 성능을 보였습니다.

### [Mol-LLaMA: Towards General Understanding of Molecules in Large Molecular Language Model](https://arxiv.org/abs/2502.13449)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.13449.png)

Vote: 33

Authors: Wonbin Lee, Sung Ju Hwang, Dongki Kim

- ***What's New***: Mol-LLaMA는 다중모달 지시 튜닝(multi-modal instruction tuning)을 통해 분자에 대한 일반적인 지식을 습득하는 대형 분자 언어 모델입니다. 이 새로운 접근 방식은 분자 구조와 관련된 핵심 데이터를 포함하며, 다양한 분자 인코더를 통합하는 모듈을 도입하여 더 나은 분자 분석 지원을 목표로 합니다.
- ***Technical Details***: Mol-LLaMA는 2D 및 3D 인코더에서 보완적인 정보를 결합하기 위해 교차 주의 메커니즘을 사용하는 블렌딩 모듈을 통합합니다. 데이터셋은 세 가지 주요 데이터 유형으로 구성되며, 이는 분자의 구조적 설명, 구조-특성 관계 설명, 종합적 대화를 포함하여 분자 특징의 본질을 포괄합니다.
- ***Performance Highlights***: Mol-LLaMA는 구조적, 화학적, 생물학적 수준에서 분자의 일반적인 특징을 이해함을 실험적으로 검증하여, 기존의 LLM 및 분자 LLM보다 뛰어난 성능을 보입니다. 특히, 분자 속성 예측 작업에서 분자의 특성을 정확하게 예측하고 관련 설명을 생성함으로써 분자 분석을 위한 범용 보조 도구로서의 잠재력을 입증합니다.

### [PhotoDoodle: Learning Artistic Image Editing from Few-Shot Pairwise Data](https://arxiv.org/abs/2502.14397)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14397.png)

Vote: 30

Authors: Mike Zheng Shou, Xueyin Wang, Yiren Song, Shijie Huang, Yuxuan Zhang, Jiaming Liu, Hailong Guo

- ***What's New***: PhotoDoodle는 아티스트가 장식 요소를 사진에 올려놓을 수 있도록 돕는 새로운 이미지 편집 프레임워크입니다. 이전 방법들이 글로벌 스타일 이전이나 지역적 인페인팅에 주로 초점을 맞췄던 반면, PhotoDoodle은 두 단계로 나눠진 학습 전략을 채택하여 아티스트의 고유한 스타일을 효과적으로 학습할 수 있습니다.
- ***Technical Details***: PhotoDoodle은 먼저 대규모 데이터셋을 사용하여 OmniEditor라는 일반적인 이미지 편집 모델을 학습하고, 이후에 EditLoRA를 사용하여 아티스트가 큐레이션한 '전 후' 이미지 쌍으로 모델을 미세 조정합니다. 이어서 위치 인코딩 재사용(Position Encoding Reuse) 메커니즘을 도입하여 생성된 결과의 일관성을 강화합니다. PhotoDoodle 데이터셋은 여섯 가지 고품질 스타일을 특징으로 하며, 포지셔널 인코딩 복제를 통해 원본 이미지의 공간적 일관성을 유지합니다.
- ***Performance Highlights***: PhotoDoodle은 일반 및 맞춤형 이미지 편집 시 기존 방법들보다 높은 CLIP Score와 GPT Score를 기록하며 우수한 성능을 보여주었습니다. 오미니에디터(OmniEditor)를 사용한 실험에서 PhotoDoodle은 스타일 복제와 배경 일관성 면에서 현존하는 방법들보다 탁월한 성능을 입증했으며, 사용자 연구 결과에서도 높은 선호도를 보였습니다.

### [SIFT: Grounding LLM Reasoning in Contexts via Stickers](https://arxiv.org/abs/2502.14922)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14922.png)

Vote: 19

Authors: Boxiu Li, Zihao Zeng, Xuyao Huang, Zhijie Deng

- ***What's New***: SIFT는 대규모 언어 모델(LLM)에서 발생할 수 있는 문맥 오해의 문제를 해결하기 위해 개발된 새로운 사후 훈련 기법입니다. 이 기법은 문맥의 핵심 정보를 명확히 강조하는 'Sticker'를 생성하여 LLM의 추론을 문맥에 맞춰 서술하는 방식을 취합니다.
- ***Technical Details***: SIFT의 핵심은 모델 자체가 생성한 Sticker입니다. Sticker를 바탕으로 두 가지 예측을 생성하는데, 하나는 원래의 쿼리로부터, 다른 하나는 Sticker가 추가된 쿼리로부터 입니다. 두개의 예측이 다를 경우, Sticker를 순차적으로 개선합니다. 직접 최적화(forward optimization)를 통해 쿼리와 더 잘 맞추고, 역방향 생성(inverse generation)을 통해 모델의 성향에 맞추어 보다 신뢰도 높은 추론 결과를 냅니다.
- ***Performance Highlights***: SIFT는 다양한 모델(3B부터 100B+까지)과 벤치마크(GSM8K, MATH-500)에서 일관된 성능 향상을 가져왔습니다. 특히, DeepSeek-R1 모델의 AIME2024에서의 pass@1 정확도를 78.33%에서 85.67%로 향상시키며 오픈 소스 커뮤니티의 새로운 최고치를 기록했습니다.

### [VLM^2-Bench: A Closer Look at How Well VLMs Implicitly Link Explicit Matching Visual Cues](https://arxiv.org/abs/2502.12084)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.12084.png)

Vote: 18

Authors: Yi R., Renjie Pi, Jianshu Zhang, Fung, Paul Pu Liang, Dongyu Yao

- ***What's New***: VLM2-Bench는 VLMs(Vision-Language Models)의 시각적 단서를 연결하는 능력을 평가하기 위해 새롭게 설계된 벤치마크입니다. VLM2-Bench는 9가지 하위 과제와 3,000개 이상의 테스트 케이스를 통해 VLMs가 시각적으로 일치하는 단서들을 얼마나 잘 연결할 수 있는지를 평가하는데 초점을 맞추고 있습니다.
- ***Technical Details***: VLM2-Bench는 세 가지 시각적 단서 유형인 일반 단서(General Cue), 객체 중심 단서(Object-centric Cue), 인물 중심 단서(Person-centric Cue)로 구성되며, 각각 여러 하위 과제를 포함하고 있습니다. 각 하위 과제는 T/F, 다중 선택, 수치 및 주관식 질문 형태의 다양한 평가 방식을 통해 모델의 성능을 체계적으로 평가합니다.
- ***Performance Highlights***: 현재의 VLMs는 시각적 단서를 연결하는 데 상당한 도전 과제를 안고 있습니다. 특히, GPT-4o는 인간 수준의 정확도에 비해 34.80% 낮은 성능을 보였으며, 오픈 소스 모델들은 대부분 확률 수준의 성능을 보여주었습니다. 이는 현재 VLMs가 성능을 대폭 개선할 여지가 있음을 시사합니다.

### [LightThinker: Thinking Step-by-Step Compression](https://arxiv.org/abs/2502.15589)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.15589.png)

Vote: 17

Authors: Huajun Chen, Mengshu Sun, Jintian Zhang, Ningyu Zhang, Da Zheng, Lun Du, Yujie Luo, Shuofei Qiao, Yuqi Zhu

- ***What's New***: LightThinker는 대형 언어 모델(LLMs)이 복잡한 추론 작업을 수행할 때 중간 생각 과정을 동적으로 압축할 수 있도록 만들어진 새로운 방법입니다. 인간의 인지 과정을 본떠 만든 이 기술은, 길고 복잡한 생각 과정을 간결한 표현으로 압축하여 컨텍스트 윈도에 저장되는 token의 수를 크게 줄입니다.
- ***Technical Details***: LightThinker는 언제 그리고 어떻게 압축할지를 학습시키기 위해 모델을 훈련하는 방식으로 이루어지며, 숨겨진 상태를 압축된 요약 token으로 매핑하고, 전문화된 주의 마스크를 생성합니다. 또한, 생성 중 역사적 token에 대한 의존성을 측정하여 압축 정도를 정량화하는 종속성(Dependency; Dep) 메트릭을 도입하였습니다.
- ***Performance Highlights***: LightThinker는 네 가지 데이터셋과 두 가지 모델에 대한 광범위한 실험에서 피크 메모리 사용량을 70%까지 줄이고 추론 시간을 26% 감소시키면서도 경쟁력 있는 정확도를 유지합니다. 이를 통해 LLM 추론의 효율성을 개선할 수 있는 새로운 방향성을 제시합니다.

### [StructFlowBench: A Structured Flow Benchmark for Multi-turn Instruction Following](https://arxiv.org/abs/2502.14494)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14494.png)

Vote: 12

Authors: Jinnan Li, Yue Wang, Jinzhe Li, Yuan Wu, Yi Chang

- ***What's New***: StructFlowBench는 대화 시스템의 다중 턴 인스트럭션 팔로잉 기능을 평가하기 위한 신규 벤치마크입니다. 기존의 평가 벤치마크는 단일 턴 간의 상호작용을 단순히 연속된 대화로 보는 경향이 있는데 반해, StructFlowBench는 대화 턴 사이의 구조적 연결성을 강조하여 더욱 현실적인 평가를 가능하게 합니다.
- ***Technical Details***: StructFlowBench는 독창적으로 여섯 가지 기본적인 턴 간 관계(Follow-up, Refinement, Recall, Summary, Expansion, Unrelatedness)를 정의하여 평가하기 위한 구조적 흐름(Structural Flow) 프레임워크를 제공합니다. 이 프레임워크는 모델 평가를 위한 새로운 구조적 제약을 도입하며, 시나리오에 맞춤화된 대화 흐름 생성의 매개변수로도 활용될 수 있습니다.
- ***Performance Highlights***: 13개의 주요 오픈소스 및 클로즈드소스 LLM을 체계적으로 평가한 결과, 현 모델들이 다중 턴 대화 구조 이해에 상당한 결핍이 있음을 확인했습니다. 특히, 최고의 모델인 DeepSeek-v3는 높은 평가 점수를 기록하며, 폐쇄형 모델들을 능가하여 향후 연구 발전을 위한 통찰을 제공했습니다.

### [Is Safety Standard Same for Everyone? User-Specific Safety Evaluation of Large Language Models](https://arxiv.org/abs/2502.15086)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.15086.png)

Vote: 11

Authors: Chanyoung Park, Yeonjun In, Wonjoong Kim, Kibum Kim, Sungchul Kim, Mehrab Tanjim, Kanghoon Yoon

- ***What's New***: 이 논문에서는 대형 언어 모델(Large Language Models; LLMs)의 사용자 맞춤형 안전성을 평가할 수 있는 최초의 벤치마크를 소개합니다. 기존의 안전 평가가 일반적인 기준에 집중했던 것과 달리, U-SAFEBENCH는 사용자 특정 프레임워크를 정의하여 개별 사용자의 안전성을 평가합니다.
- ***Technical Details***: U-SAFEBENCH는 157개의 사용자 프로파일과 1,900개 이상의 실세계 사용자 지시로 구성된 사용자 프로파일 기반의 LLM 안전성을 평가하는 벤치마크입니다. 각 사용자 지시에 대해 모델의 응답이 사용자 특정한 안전성을 갖추었는지를 평가하며, 이는 각 사용자 프로파일의 잠재적 위험 시나리오를 반영하여 설계되었습니다.
- ***Performance Highlights***: 18개의 LLMs에 대한 U-SAFEBENCH 평가 결과, 사용자 특정 안전성 점수에서 평균 18.6%라는 낮은 성과를 보이며, 이는 일반적 안전성 기준에서의 성과보다 상당히 낮습니다. 이에 대응하기 위해, 체인 오브 생각(Chain-of-Thought) 접근 방식을 통한 간단한 대책이 제안되었고, 이는 U-SAFEBENCH에서 사용자 특정 안전성을 향상시키는 효과를 나타냈습니다.

### [MoBA: Mixture of Block Attention for Long-Context LLMs](https://arxiv.org/abs/2502.13189)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.13189.png)

Vote: 8

Authors: Jiezhong Qiu, Weiran He, Neo Y. Zhang, Yuzhi Wang, Jianlin Su, Zhiqi Huang, Enzhe Lu, Xinyu Zhou, Guokun Lai, Yanru Chen, Mingxing Zhang, Huabin Zheng, Huan Yuan, Chao Hong, Xinran Xu, Zhilin Yang, Yulun Du, Enming Yuan, Junjie Yan, Shaowei Liu, Tao Jiang, Jingyuan Liu, Yuxin Wu, Suting Xu, Zhejun Jiang

- ***What's New***: MoBA(Mixture of Block Attention)는 긴 문맥을 가진 대형 언어 모델(Long-Context LLMs)의 주의(attention) 메커니즘을 개선하기 위해 Mixture of Experts(MoE) 원리를 도입한 새로운 구조입니다. MoBA는 전체 문맥을 블록으로 나누고, 동적 게이팅(gating) 메커니즘을 사용하여 쿼리 토큰이 가장 관련 있는 키-값(KV) 블록으로 라우팅되도록 함으로써 전통적인 주의 메커니즘의 계산 복잡성을 줄입니다.
- ***Technical Details***: MoBA는 전체 문맥을 여러 블록으로 분할하고, 각 쿼리 토큰이 선택적으로 몇몇 블록에만 주의를 기울이도록 합니다. 이를 위해 MoE에서 사용하는 탑-k 게이팅 메커니즘을 사용합니다. MoBA는 Transeformer 모델의 주의 메커니즘에 MoE 원리를 적용하여 보다 효율적이고 효과적인 긴 시퀀스 처리가 가능하도록 설계되었습니다. 구현 측면에서는 FlashAttention과 MoE의 최적화 기법을 결합하여 MoBA의 고성능을 구현했습니다.
- ***Performance Highlights***: MoBA는 전통적인 전체 주의 메커니즘과 비교하여 최소한의 학습 비용으로 기존 모델과 원활하게 통합될 수 있으며, 높은 성능을 유지하면서도 주의 계산의 효율성을 크게 향상시킵니다. 실험 결과, MoBA는 여러 벤치마크에서 높은 성능을 유지하면서도 계산 복잡성을 경제적인 부분-사각형(sub-quadratic) 수준으로 줄였습니다. 특히, 1M 토큰 프리필(prefill) 단계에서 최대 6.5배의 속도 향상을 달성했습니다.

### [KITAB-Bench: A Comprehensive Multi-Domain Benchmark for Arabic OCR and Document Understanding](https://arxiv.org/abs/2502.14949)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14949.png)

Vote: 5

Authors: Ahmed Heakl, Salman Khan, Omar Maher, Zhiqiang Shen, Ghazi Ahmed, Mukul Ranjan, Fahad Khan, Abdullah Sohail, Rania Hossam, Mohamed El-Geish

- ***What's New***: KITAB-Bench는 아랍어 OCR(Optical Character Recognition)과 문서 이해를 위한 포괄적인 벤치마크로, 현재 평가 시스템의 격차를 메우기 위해 설계되었습니다. 이 벤치마크는 9개의 주요 도메인과 36개의 하위 도메인에 걸쳐 8,809개의 샘플을 포함하며, 다양한 문서 유형(예: 손글씨, 구조화된 테이블, 21개의 차트 유형 등)을 대상으로 합니다.
- ***Technical Details***: KITAB-Bench는 현대 시각-언어 모델(Vision-Language Models; VLMs)과 전통적인 OCR 접근법들을 평가할 수 있도록 설계되었습니다. 벤치마크는 PDF-to-Markdown 변환, 레이아웃 탐지, 차트 및 다이어그램 인식 등의 과제를 포함하며, 각 작업에는 CharTeX 및 CODM과 같은 세부 평가 메트릭이 적용됩니다. 데이터 생성에는 LLM 보조의 인간 참여 파이프라인이 활용되었습니다.
- ***Performance Highlights***: 현대 시각-언어 모델은 전통적인 OCR 방법을 문자 오류율(Character Error Rate; CER)에서 평균 60% 정도 뛰어 넘었으며, PDF-to-Markdown 변환에서는 가장 성능이 좋은 모델인 Gemini-2.0-Flash가 65%의 정확성을 기록했습니다. 이는 아랍어 텍스트의 정확한 인식에서의 어려움, 복잡한 폰트, 숫자 인식 오류, 어휘 연장 문제 등을 강조합니다.

### [ReQFlow: Rectified Quaternion Flow for Efficient and High-Quality Protein Backbone Generation](https://arxiv.org/abs/2502.14637)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14637.png)

Vote: 5

Authors: Angxiao Yue, Zichong Wang, Hongteng Xu

- ***What's New***: ReQFlow는 빠르고 고품질의 단백질 백본 생성(Protein Backbone Generation)을 위해 고안된 새로운 정류 쿼터니언 흐름(Rectified Quaternion Flow) 매칭 방법입니다. 이 연구는 기존의 확산 및 흐름 기반 생성 모델이 디자인 가능성과 계산 효율성에서 한계를 보이는 문제를 해결하고자 합니다. ReQFlow는 랜덤 노이즈로부터 각 잔기에 대해 로컬 변환과 3D 회전을 생성하며, 이를 단위 쿼터니언으로 표현하여 지수를 사용하는 구면 선형 보간(SLERP)을 통해 흐름을 형성합니다.
- ***Technical Details***: ReQFlow 모델은 쿼터니언 흐름 매칭 전략인 QFlow를 통해 학습되며, 수치적 안정성을 보장하고, 모델의 추론을 가속화하고 생성된 단백질 백본의 설계 가능성을 개선합니다. 이러한 과정을 통해 제안된 ReQFlow 모델이 도출되며, 비교 실험을 통해 기존의 확산 및 흐름 기반 방법론에 비해 우수한 성능을 입증하였습니다.
- ***Performance Highlights***: ReQFlow는 각 샘플링 단계에서 최적의 성능을 유지하며, 단백질 백본 생성에서 획기적인 속도와 효과를 입증했습니다. 예를 들어, 단백질 백본 길이 300을 생성할 때 RFDiffusion 보다 37배, Genie2보다 62배 빠른 속도로 생성하였으며, 디자인 가능성 부분에서 Fraction 점수가 0.972를 기록하여 현존하는 최첨단 성능을 달성했습니다.

### [Think Inside the JSON: Reinforcement Strategy for Strict LLM Schema Adherence](https://arxiv.org/abs/2502.14905)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14905.png)

Vote: 5

Authors: Ishan Joshi, Bhavik Agarwal, Viktoria Rojkova

- ***What's New***: 이 논문에서는 대형 언어 모델(LLM)이 정확한 스키마(Schema)에 맞춰 출력을 생성하도록 엄격하게 제어하는 새로운 방법인 'Think Inside the JSON'을 제안합니다. 새로운 강화를 통해 LLM의 추론 능력을 강화하여, 구조적 데이터 세트 생성 및 커스텀 보상 함수와 함께 스키마 일관성을 유지한 출력을 제공합니다.
- ***Technical Details***: 본 연구는 DeepSeek R1 강화 학습 체계를 기반으로 하며, Group Relative Policy Optimization (GRPO)과 지도학습(Supervised Fine-Tuning; SFT)을 결합하여 1.5B 파라미터 모델의 구조적 추론 능력을 훈련시킵니다. 20K 샘플의 비구조적-구조적 데이터를 사용한 R1 강화학습과 별도의 10K 샘플 데이터로 스키마 준수성을 위한 지도학습을 수행합니다. 이 과정은 약 20시간의 GRPO 훈련과 3시간의 SFT를 필요로 하며, 비교적 적은 자원 소요로도 인증된 스키마 일관성을 보여줍니다.
- ***Performance Highlights***: 실험 결과, ThinkJSON은 6.5K 행의 구조적 데이터 추출 벤치마크에서 62.41%의 평균 일치율을 기록하며, 가장 낮은 0.27%의 평균 잡음을 보여주었습니다. 기존 DeepSeek R1 (671B) 및 Gemini 2.0 Flash (70B) 등과 비교하여 더 나은 성능을 보여주며, 작은 모델에서 리소스 효율적 스키마 준수를 달성할 수 있음을 입증했습니다.

### [InterFeedback: Unveiling Interactive Intelligence of Large Multimodal Models via Human Feedback](https://arxiv.org/abs/2502.15027)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.15027.png)

Vote: 5

Authors: Mike Zheng Shou, Haiyang Mei, Yifei Tao, Henry Hengyuan Zhao, Wenqi Pei

- ***What's New***: InterFeedback는 대형 멀티모달 모델들(Large Multimodal Models; LMMs)의 인터랙티브 지능을 평가하기 위한 새로운 프레임워크입니다. 이 프레임워크는 인간 사용자와의 상호작용을 통해 LMMs의 피드백 해석 능력을 테스트하기 위한 벤치마크인 InterFeedback-Bench를 도입하여 기존 벤치마크와 차별화됩니다.
- ***Technical Details***: InterFeedback는 LMM의 성능을 개선하기 위해 피드백을 활용하는 상호작용 문제 해결 과정을 부분 관측 마코프 결정 과정(POMDP; Partially Observable Markov Decision Process)의 형태로 공식화합니다. 또한, InterFeedback-Bench는 MMMU-Pro와 MathVerse의 두 가지 데이터셋을 사용하여 상호작용적 문제 해결 능력과 피드백 해석 능력을 평가합니다. InterFeedback-Human 데이터셋도 새롭게 수집되어, OpenAI-o1이나 Claude-3.5-Sonnet 같은 주요 모델들에 대한 인간 평가를 지원합니다.
- ***Performance Highlights***: 실험 결과, 최첨단 LMM (예: OpenAI-o1)은 인간 피드백을 통해 결과를 수정할 수 있는 비율이 50% 이하로 나타났습니다. 또한, 피드백이 제공되었을 때 대부분의 모델이 오류를 부분적으로 수정했지만 피드백을 통한 성능 향상에는 여전히 한계가 있었습니다. 고품질 피드백은 중요하며, 낮은 품질의 피드백은 단순한 이진적(correct/incorrect) 신호보다 성능을 더욱 악화시킬 수 있었습니다.

### [Tree-of-Debate: Multi-Persona Debate Trees Elicit Critical Thinking for Scientific Comparative Analysis](https://arxiv.org/abs/2502.14767)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14767.png)

Vote: 4

Authors: Tal August, Ishika Agarwal, Priyanka Kargupta, Jiawei Han

- ***What's New***: Tree-of-Debate(ToD)는 다중 퍼소나 디베이트 트리(Multi-Persona Debate Tree)를 통해 과학 연구 논문의 중요성, 독창성, 점진적 발견 등을 평가하는 새로운 프레임워크입니다. 각 논문을 인격체로 변환하여 디베이트를 통해 각각의 기여도를 평가합니다.
- ***Technical Details***: ToD는 과학 논문을 인격화하여 각각의 차별성과 기여도를 논해주세요. 디베이트 트리는 각 주제별로 세부 문제를 독립적으로 평가할 수 있도록 설계되었습니다. 또한, ToD는 디베이트 트리의 각 노드에서 주제를 세분화하여, 모더레이터가 디베이트를 진행하고 세부 주제를 설정합니다.
- ***Performance Highlights***: 다양한 학문적 분야의 논문 분석 실험 결과, ToD는 정보 제공과 논문의 대조를 통해 연구자들에게 효과적인 문헌 검토를 지원하는 것으로 평가되었습니다. ToD의 핵심적 강점은 다층적인 디베이트 구조를 통해 보다 심층적이고 맥락화된 대조 요약을 생성하는 것입니다.

### [MedHallu: A Comprehensive Benchmark for Detecting Medical Hallucinations in Large Language Models](https://arxiv.org/abs/2502.14302)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14302.png)

Vote: 4

Authors: Jiawei Xu, Zhangyang Wang, Kaidi Xu, Shrey Pandit, Junyuan Hong, Tianlong Chen, Ying Ding

- ***What's New***: MedHallu는 대형 언어 모델(Large Language Models; LLMs)의 의료 분야에서 헛소리(Medical Hallucinations)를 탐지하기 위한 최초의 포괄적인 벤치마크입니다. PubMedQA로부터 유래된 10,000개의 고품질 질문-답변 쌍으로 구성되며, 체계적으로 생성된 허구의 답변을 통해 헛소리 탐지 능력을 평가합니다.
- ***Technical Details***: MedHallu 벤치마크는 질문-답변 쌍을 세 가지 난이도—쉽게, 중간, 어려운—로 구분하여, LLM이 허구의 내용을 탐지하는 능력에 대해 세밀하게 평가합니다. 이 데이터셋은 이중 방향(entailment) 클러스터링 및 도메인 지식의 도입을 통해 정확도와 F1 점수를 개선하는 방법론을 포함하여 중요한 허구를 탐지하는 고도화된 기술을 제공합니다.
- ***Performance Highlights***: 최신 LLM들, 예를 들어 GPT-4o, Llama-3.1, UltraMedical 등은 '어려운' 범주의 허구를 탐지하는데 있어서 모든 모델 중 최고 F1 점수인 0.625조차 얻지 못하여, 이 영역에서의 능력 부족을 드러냈습니다. '모르겠다(not sure)' 카테고리를 도입함으로써 정밀도와 F1 점수는 최대 38% 향상되었습니다.

### [FantasyID: Face Knowledge Enhanced ID-Preserving Video Generation](https://arxiv.org/abs/2502.13995)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.13995.png)

Vote: 4

Authors: Yonggang Qi, Mu Xu, Qiang Wang, Yunpeng Zhang, Yaqi Fan, Fan Jiang

- ***What's New***: FantasyID는 대규모 사전 학습된 비디오 확산 모델에서 얼굴 구조를 안정적으로 유지하며 더 다양한 얼굴 표현과 머리 자세를 포착할 수 있도록 다각도 얼굴 증강(multi-view face augmentation) 전략을 활용하여 기존 ID 보존 텍스트-비디오 생성(identity-preserving text-to-video generation; IPT2V)의 한계를 극복합니다. 또한, 3D 얼굴 지오메트리 프라이어(facial geometry prior)를 도입하여 비디오 생성 시 신뢰성을 높입니다.
- ***Technical Details***: FantasyID는 참고 얼굴 이미지로부터 2D 및 3D 얼굴 특징을 추출하여 이를 결합한 후, 레이어 인식 주입 메커니즘(layer-aware injection mechanism)을 사용해 DiT(Diffusion Transformer) 층에 선택적으로 삽입합니다. 이를 통해 각 DiT 계층의 역할에 맞는 최적의 신호를 학습하고, 안정성과 여러 계층에 걸친 시간적 일관성을 확보합니다.
- ***Performance Highlights***: FantasyID는 Ref. Similarity(RS)에서 0.57로 가장 높은 점수를 기록하며, 이는 신원 보존의 뛰어남을 보여줍니다. 얼굴 움직임 점수(Face Motion)는 0.61을 기록하며, 이는 다양한 얼굴 표현을 제작함에 있어 뛰어난 다이나믹스를 나타냅니다. ConsisID와 ID-Animator와의 비교에서, FantasyID는 시각적 품질, 정체성 보존, 모션 일관성 측면에서 우수한 결과를 보였습니다.

### [Evaluating Multimodal Generative AI with Korean Educational Standards](https://arxiv.org/abs/2502.15422)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.15422.png)

Vote: 4

Authors: Sanghee Park, Geewook Kim

- **What's New**: 이 논문에서는 다중 모드 생성 AI 시스템을 평가하기 위해 한국의 국가 교육 시험을 활용한 새로운 벤치마크인 KoNET을 소개합니다. KoNET은 초등학교부터 대학교까지의 교육 수준을 포괄하며, 한국어에 중점을 둔 모델 성능을 평가할 수 있는 인사이트를 제공합니다.
- **Technical Details**: KoNET은 한국초등학교졸업학력인정고시(KoEGED), 중학교(KoMGED), 고등학교(KoHGED), 대학수학능력시험(KoCSAT)으로 구성된 4개의 시험으로 이루어져 있으며, 각 시험은 문제의 난이도와 오류율 데이터 (Human Error Rate)를 제공합니다. 이 벤치마크는 모델의 다양한 분석 프레임워크를 통해 인간과 모델의 오류율 관계를 분석하는 데 도움을 줍니다.
- **Performance Highlights**: 주요 모델들 중에서는 한국어 기반의 주권 AI 모델인 EXAONE-3.0-7.8B-Instruct 모델이 K-NET 점수에서 45.5점을 기록하며 두드러진 성과를 보였습니다. KoCSAT의 경우 특히 엄격한 난이도와 현실 세계 표준을 반영하여 모델의 성능 격차가 명확하게 나타났습니다. 또한, 많은 오픈 소스 모델들이 한국어 관련 도메인에서 비효율적인 점을 드러냈습니다.

### [The Relationship Between Reasoning and Performance in Large Language Models -- o3 (mini) Thinks Harder, Not Longer](https://arxiv.org/abs/2502.15631)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.15631.png)

Vote: 4

Authors: Marthe Ballon, Vincent Ginis, Andres Algaba

- ***What's New***: 이번 연구에서는 다양한 OpenAI 모델(o1-mini, o3-mini (m), o3-mini (h))의 추론 능력과 성능 간의 관계를 체계적으로 분석하여, 더 능숙한 모델이 더 길어진 추론 체인을 요구하지 않는다는 것을 발견했습니다. 또한 총 추론 토큰 수가 길어질수록 정확도가 전반적으로 감소하지만, 보다 능숙한 모델일수록 이러한 감소 효과가 작다는 점을 강조합니다. 이는 더 강력한 모델이 '더 깊게' 생각할 수 있음을 시사합니다.
- ***Technical Details***: Omni-MATH 벤치마크를 사용하여 o1-mini와 o3-mini 모델 간의 추론 토큰 사용량과 성능을 비교하였습니다. 분석 결과, 더 능숙한 o3-mini (m) 모델은 o1-mini보다 추론 토큰 수를 늘리지 않고도 높은 정확도를 달성했습니다. 반면, o3-mini (h) 모델은 동일 문제에 대해 더 많은 추론 토큰을 사용하면서도 약 4% 정도의 정확도 향상을 보여주었으나, 그 정확도 향상은 상당한 추가 계산 비용과 동반되었습니다.
- ***Performance Highlights***: o1-mini는 모든 도메인에서 40-60%의 정확도를 달성하며 성능이 향상되었고, o3-mini (m) 모델은 이보다 더 높은 정확도를 기록했습니다. 특히, o3-mini (h)는 Algebra와 Calculus 분야에서 80% 이상의 정확도를 달성하는 등 전반적으로 성능이 향상되었습니다. 그러나 Discrete Mathematics에서는 모든 모델이 전반적인 성능 경향과 일치하지 않는 결과를 보여 이 분야에서의 몇 가지 도전 과제를 나타냈습니다.

### [mStyleDistance: Multilingual Style Embeddings and their Evaluation](https://arxiv.org/abs/2502.15168)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.15168.png)

Vote: 3

Authors: Marianna Apidianaki, Justin Qiu, Ajay Patel, Chris Callison-Burch, Jiacheng Zhu

- ***What's New***: MSTYLEDISTANCE는 다양한 언어에서 스타일 임베딩을 가능하게 하는 새로운 모델로, 스타일 분석과 스타일 변환에 유용한 다국어 스타일 임베딩(Multilingual style embeddings)을 제공하는 최초의 접근 방식입니다. 자연어 스타일 보존뿐만 아니라 번역에서의 스타일 보존 평가에도 활용될 수 있습니다.
- ***Technical Details***: MSTYLEDISTANCE는 대조학습(contrastive learning)과 합성 데이터(synthetic data)를 활용하여 9개의 언어에서 스타일 임베딩을 학습합니다. 40개 스타일 특징을 포함하며, 각각의 특징에 대해 긍정적인 예시와 부정적인 예시 쌍을 생성하여 다중언어 훈련을 통해 학습합니다. 이러한 쌍들은 GPT-4 모델을 통해 생성되었으며, 데이터 검증은 인체 주석 및 자동 검증 방법을 활용했습니다.
- ***Performance Highlights***: MSTYLEDISTANCE는 기존의 영어 기반 스타일 임베딩 모델보다 다국어 SoC(STEL-or-Content) 벤치마크에서 뛰어난 성능을 보였으며, 교차 언어 평가에서도 높은 보존 점수를 기록하여 다양한 언어와 보지 못한 스타일 특징에 잘 일반화되는 것으로 나타났습니다.

### [One-step Diffusion Models with f-Divergence Distribution Matching](https://arxiv.org/abs/2502.15681)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.15681.png)

Vote: 3

Authors: Arash Vahdat, Weili Nie, Yilun Xu

- ***What's New***: 이 논문은 f-발산(f-Divergence) 최소화를 통해 학생 생성 모델을 1단계로 축약하는 새로운 일반화된 방법론을 제안합니다. 기존의 역-KL 발산(reverse-KL Divergence) 방식과 달리, 다양한 발산 선택을 통한 모드 커버리지와 훈련 분산 간의 절충을 허용합니다.
- ***Technical Details***: f-distill이라 불리는 이 프레임워크는 다양한 f-발산을 최소화하여 학생과 교사의 분포를 맞추는 방식입니다. f-발산의 그래디언트 계산은 교사와 학생 분포의 점수 차이와 밀도 비율에 의존하는 가중치 함수의 곱으로 표현됩니다. 이는 GAN 목표에서 사용하는 판별자를 통해 밀도 비율을 쉽게 얻을 수 있습니다. 실증적으로는 JS 발산이나 forward-KL 발산 등이 기존 역-KL 보다 이미지 생성 작업에서 우수한 성능을 보입니다.
- ***Performance Highlights***: ImageNet64와 MS-COCO의 생성 작업에서 JS 발산을 사용한 f-distill은 최신 1단계 생성 성능을 달성했습니다. FID 점수가 ImageNet-64 데이터셋에서 1.16, MS-COCO에서 7.42를 기록하며, 이는 이전 모든 베이스라인 메서드를 뛰어넘는 결과입니다. 또한, 다양한 데이터셋에서 모드 탐색 성향이 약한 발산들이 일반적으로 더 나은 성능을 보여주었습니다.

### [Superintelligent Agents Pose Catastrophic Risks: Can Scientist AI Offer a Safer Path?](https://arxiv.org/abs/2502.15657)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.15657.png)

Vote: 3

Authors: Pietro Greiner, Adam Oberman, Matt MacDermott, Damiano Fornasiere, Joumana Ghosn, Yoshua Bengio, Sören Mindermann, Oliver Richardson, Pierre-Luc St-Charles, Jesse Richardson, Michael Cohen, Marc-Antoine Rondeau, David Williams-King

- ***What's New***: 이 논문에서는 과학자 AI(Scientist AI)라는 비에이전트적인(non-agentic) AI 시스템을 도입하여 현재의 에이전트 기반 AI 시스템이 가지고 있는 잠재적 위험성을 줄이고자 합니다. 과학자 AI는 세계를 이해하고 설명하는 이론을 생성하며, 이 이론을 바탕으로 질문에 대한 확률적 추론을 제공합니다.
- ***Technical Details***: 과학자 AI는 두 가지 주요 구성 요소로 이루어져 있습니다: 세계 모델(world model)과 추론 기계(inference machine). 세계 모델은 데이터를 관찰하여 인과 관계를 설명하는 가설을 생성하며, 추론 기계는 이러한 가설을 바탕으로 주어진 질문에 대한 확률적 응답을 제공합니다. 이 시스템은 베이지안 방식을 사용하여 불확실성을 처리함으로써 과도한 확신으로 인한 위험을 줄입니다. 특히, 우리는 질의에 대한 단일한 확률 값을 제공하여 예상치 못한 에이전트 행동이 발생하지 않도록 설계했습니다.
- ***Performance Highlights***: 과학자 AI는 점점 증가하는 데이터와 컴퓨팅 파워에 따라 점진적으로 개선되며, 이는 일반적인 AI 훈련 방법과는 대조적으로 성능과 안전성을 증대시킵니다. 이 시스템은 과학적 연구를 가속하고, 다른 AI 시스템을 감시하며, 더 안전한 초지능 AI 발전을 지원할 수 있도록 설계되었습니다.

### [Learning to Discover Regulatory Elements for Gene Expression Prediction](https://arxiv.org/abs/2502.13991)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.13991.png)

Vote: 1

Authors: Degui Zhi, Shuiwang Ji, Xingyu Su, Haiyang Yu

- ***What's New***: 이 논문에서는 DNA 서열로부터 유전자 발현을 예측하는 데 필요한 조절 요소(Regulatory Elements)를 발견하고 추출하는 새로운 네트워크인 Seq2Exp를 소개했습니다. 이 네트워크는 유전자 발현을 조장하는 조절 요소를 발견하여 유전자 발현 예측 정확성을 향상시킵니다.
- ***Technical Details***: Seq2Exp는 DNA 서열과 후성유전학 신호(Epigenomic Signals) 사이의 인과적 관계를 분석하여 핵심 조절 요소를 찾아냅니다. 이를 위해 인과적 활성 조절 요소를 조건으로 DNA 서열과 후성유전학 신호를 분해하고, 정보 병목(Information Bottleneck)과 Beta 분포를 활용하여 비인과적 요소를 제거합니다. Seq2Exp는 특히, 기존의 봉우리 탐지 방법(peak detection)인 MACS3 보다 효과적으로 영향을 미치는 영역을 발견합니다.
- ***Performance Highlights***: Seq2Exp는 유전자 발현 예측 작업에서 기존의 여러 기준선 모델을 능가하는 성능을 보였습니다. K562와 GM12878 두 가지 세포 유형을 대상으로 한 실험 결과, Seq2Exp는 MSE, MAE 및 Pearson 상관 계수에서 최고의 성능을 기록했으며, 이는 후성유전학 신호와 DNA 서열을 통합하여 예측 성능을 높인 결과입니다.

### [Benchmarking LLMs for Political Science: A United Nations Perspective](https://arxiv.org/abs/2502.14122)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14122.png)

Vote: 1

Authors: Congying Xia, Haoran Wang, Yueqing Liang, Kai Shu, Xiongxiao Xu, Chen Wang, Rui Meng, Liangwei Yang, Ali Payani

- ***What's New***: 이 논문은 UN(United Nations) 의사결정 과정에 LLMs(Large Language Models)를 적용하기 위한 최초의 종합적인 벤치마크인 UNBench를 소개합니다. 이 벤치마크는 LLMs의 정치적 역학을 이해하고 시뮬레이션하는 능력을 평가하기 위해 고안된 네 가지 상호 연결된 정치 과학 작업을 다룹니다.
- ***Technical Details***: UNBench는 1994년부터 2024년까지의 UN 안전보장이사회 기록을 종합하여 구축된 데이터셋으로, 초안 결의안, 투표 기록, 외교 연설을 포함합니다. 이 데이터셋을 통해 LLMs의 성능을 평가하며, 코-팬홀더 판단(Co-Penholder Judgment), 대표 투표 시뮬레이션(Representatives Voting Simulation), 초안 채택 예측(Draft Adoption Prediction), 대표 성명 생성(Representative Statement Generation)과 같은 다단계 정치 과학 작업을 수행합니다.
- ***Performance Highlights***: 실험 결과, GPT-4o는 여러 작업에서 가장 뛰어난 성능을 보였으며, DeepSeek-V3와 Qwen2.5-7B도 특히 특정 작업에서 인상적인 결과를 나타냈습니다. 이러한 결과는 LLMs가 복잡한 국제 거버넌스 과제를 다루는 데 유망하면서도, 여전히 상당한 도전 과제가 있음을 시사합니다.

### [JL1-CD: A New Benchmark for Remote Sensing Change Detection and a Robust Multi-Teacher Knowledge Distillation Framework](https://arxiv.org/abs/2502.13407)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.13407.png)

Vote: 1

Authors: Long Gao, Yuanxiu Zhou, Ziyuan Liu, Yuantao Gu, Jingyu Ma, Ruifei Zhu

- ***What's New***: 이 논문에서는 새로운 원격 감지 변화 감지(Remote Sensing Change Detection; CD) 벤치마크 JL1-CD와 강력한 다중 교사 지식 증류(Multi-Teacher Knowledge Distillation; MTKD) 프레임워크를 소개합니다. JL1-CD 데이터셋은 0.5~0.75 미터의 해상도를 가지는 5,000쌍의 이미지로 구성되어 있으며, 인간과 자연에 의한 다양한 변화 유형을 포괄합니다.
- ***Technical Details***: MTKD 프레임워크는 변화 영역 비율(Change Area Ratio; CAR)에 따라 데이터셋을 분할하여 CD 모델을 학습시킵니다. 이를 통해 개별 모델의 학습 부담을 줄이고, 교사 모델들이 학습한 다양한 CAR 시나리오의 장점을 학생 모델이 배우도록 합니다. 이 접근 방식은 다양한 네트워크 아키텍처와 파라미터 크기에서 CD 성능을 크게 향상시킵니다.
- ***Performance Highlights***: JL1-CD와 SYSU-CD 데이터셋에서 실험한 결과, MTKD 프레임워크는 여러 CD 모델에서 새로운 최첨단 성능을 이끌어 냈습니다. 특히, 최신 SOTA 모델 TTP는 mIoU에서 1.30%, mF1-score에서 1.80% 증가하며 현저한 성능 향상을 보였습니다.

### [EgoSpeak: Learning When to Speak for Egocentric Conversational Agents in the Wild](https://arxiv.org/abs/2502.14892)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14892.png)

Vote: 1

Authors: Sungwoong Kim, Jungbin Cho, Jisoo Kim, Gyeongbo Sim, Youngjae Yu, Jiwan Chung, Junhyeok Kim, Min Soo Kim

- ***What's New***: EgoSpeak은 에고센트릭(egocentric) 스트리밍 비디오를 통해 실시간으로 대화형 에이전트의 발언 시작 시점을 예측하는 혁신적인 프레임워크입니다. 이는 사람과 비슷한 상호작용을 목표로 하며 대화 에이전트가 환경을 지속적으로 관찰하고 동적으로 말할 시점을 결정하는 데 적합하도록 설계되었습니다.
- ***Technical Details***: EgoSpeak은 네 가지 주요 기능을 통합합니다: (1) 첫 번째 사람 시점 처리, (2) RGB 처리, (3) 온라인 처리, (4) 연속적인 비디오 스트림 처리. 이를 기반으로 다양한 YouTube 대화 비디오로 구성된 새로운 데이터셋 YT-Conversation을 대규모 프리트레이닝 리소스로 활용합니다. EasyCom과 Ego4D 데이터셋에서 수행한 실험은 EgoSpeak가 무작위, 침묵 기반 기준치보다 뛰어난 실시간 성능을 보여줬습니다.
- ***Performance Highlights***: EgoSpeak는 EasyCom과 Ego4D 실험에서 무작위 및 침묵 기반 기준치를 능가하는 실시간 성능을 보여줍니다. 특히, 다중 모달 입력(multi-modal input)을 활용할 때 성능이 개선되었으며, Optical Flow가 포함되면 발언 시작 예측 성능이 크게 향상되었습니다.

### [WHAC: World-grounded Humans and Cameras](https://arxiv.org/abs/2403.12959)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.12959.png)

Vote: 1

Authors: Zhitao Yang, Atsushi Yamashita, Wanqi Yin, Ruisi Wang, Chen Wei, Qingping Sun, Ziwei Liu, Haiyi Mei, Lei Yang, Zhongang Cai, Weiye Xiao, Fanzhou Wang

- ***What's New***: WHAC는 단안 비디오에서 인간과 카메라의 궤적을 세계 좌표계에서 정확한 스케일로 추정하는 혁신적인 프레임워크입니다. 이 연구는 새로운 합성 데이터셋인 WHAC-A-Mole을 제공하며, 상호작용하는 인간의 움직임과 현실적인 카메라 궤적을 포함하고 있습니다. 코드와 데이터셋은 공개될 예정입니다.
- ***Technical Details***: WHAC는 SMPL-X 매개변수와 초보적인 카메라 궤적을 추정하여 사용자로부터 가능한 초점 거리를 활용해 절대적인 인간의 깊이를 정확하게 감지할 수 있습니다. MotionVelocimeter를 통해 절대적인 스케일을 회복하며, 시각적 관성 측정을 통해 카메라의 궤적도 조정합니다. 이 과정은 기존의 최적화 기술에 의존하지 않고, 간단한 회귀 기반의 접근 방식을 사용합니다.
- ***Performance Highlights***: WHAC는 최근 설정된 벤치마크와 WHAC-A-Mole에서 실험을 통해 기존의 SoTA 방법들과 비교하여 우수한 성능을 보여주었습니다. WHAC는 다양한 경우에 있어서 모션 기반 관찰과 카메라 기반 관찰이 상충될 때도 놀라운 정확성을 유지하여, 잠재적인 응용 프로그램 개발에 있어 중요한 기초적 기여를 합니다.

### [CrossOver: 3D Scene Cross-Modal Alignment](https://arxiv.org/abs/2502.15011)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.15011.png)

Vote: 1

Authors: Daniel Barath, Ondrej Miksik, Iro Armeni, Sayan Deb Sarkar, Marc Pollefeys

- ***What's New***: CrossOver는 3D 장면 이해를 위한 획기적인 프레임워크로, 기존 방법들과 달리 모든 객체 인스턴스에 대한 정렬된 데이터가 필요 없이 유연한 장면 수준의 모달리티 정렬을 통해 다양한 모달리티(RGB 이미지, 점 구름, CAD 모델, 평면도, 텍스트 설명)를 통합하는 방식으로 설계되었습니다. 이를 통해 일부 모달리티가 누락된 경우에도 강력한 장면 검색 및 객체 로컬라이제이션을 지원합니다.
- ***Technical Details***: CrossOver는 차원별 인코더(Dimensionality-Specific Encoders)와 3단계 교육 파이프라인(Three-Stage Training Pipeline)을 도입하여 모달리티 간의 상호작용을 학습합니다. 인코더는 각 모달리티의 차원에 맞춰 설계되어 1D, 2D, 3D 데이터를 독립적으로 처리하며, 3단계 파이프라인을 통해 객체 수준에서 장면 수준으로의 자연적인 모달리티 상호작용을 발전시킵니다. 이로 인해 각 모달리티의 부재 시에도 정확한 모달리틱 간 탐색이 가능합니다. 또, 모달리티 간 행동을 학습하여 각 모달리티 쌍이 훈련에 존재하지 않더라도 장면을 모달리티 간에 일치시키며 이해할 수 있습니다.
- ***Performance Highlights***: CrossOver는 ScanNet 및 3RScan 데이터셋에서 정량적 평가를 통해 경쟁사보다 다양한 지표에서 뛰어난 성능을 보였습니다. 특히, 장면 수준의 이해를 위한 모달리티-무관 공간으로의 통합이 실세계 데이터에서 완전한 모달리티 페어링이 부족할 경우 유연하고 확장 가능한 솔루션을 제공함을 입증했습니다.

### [Rare Disease Differential Diagnosis with Large Language Models at Scale: From Abdominal Actinomycosis to Wilson's Disease](https://arxiv.org/abs/2502.15069)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.15069.png)

Vote: 0

Authors: Elliot Schumacher, Dhruv Naik, Anitha Kannan

- ***What's New***: RareScale는 대형 언어 모델(LLMs)과 전문가 시스템을 결합하여 희귀 질병을 효과적으로 진단하는 방법을 제안합니다. 이는 575개의 희귀 질병 사례를 바탕으로 체계적인 사례를 생성하고, 후보 질병 예측 모델을 훈련시켜 최종 차별적 진단을 수행합니다.
- ***Technical Details***: RareScale는 세 단계로 구성됩니다. 첫째, 희귀 질병 대화 데이터를 생성하는 전문가 시스템과 LLM을 통해 대규모의 체계적인 임상 사례를 시뮬레이션합니다. 둘째, 작은 모델을 훈련시켜 희귀 질병 후보를 생성합니다. 셋째, 대형 블랙박스 LLM에 희귀 질병 후보 목록을 추가하여 최종 진단 리스트를 작성합니다. 이는 일반적인 진단 능력과 희귀 질병에 대한 특별한 지식을 결합하여 진단 효율성을 향상시킵니다.
- ***Performance Highlights***: RareScale는 gpt-4o를 사용하는 블랙박스 LLM의 성능을 Top-5 정확도 기준으로 56.8%에서 74.1%로 17.4% 이상 향상시킵니다. 희귀 질병 후보 생성 모델의 성능은 gpt-4o 생성 챗에서 88.8%의 Top-5 정확도를 기록하였으며, 이는 희귀 질병 진단 성능을 크게 개선하였습니다.

### [UPCORE: Utility-Preserving Coreset Selection for Balanced Unlearning](https://arxiv.org/abs/2502.15082)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.15082.png)

Vote: 0

Authors: Elias Stengel-Eskin, Vaidehi Patil, Mohit Bansal

- ***What's New***: UPCORE는 사전학습된 모델에서 정보를 삭제할 때 발생하는 성능 저하를 최소화하는 유틸리티-보존 코어셋 선택(Utility-Preserving Coreset Selection) 프레임워크입니다. 모델의 손상이 제거 세트(forget set) 데이터 포인트의 분산과 관련이 있음을 발견하고, UPCORE는 이러한 데이터를 선별적으로 제거하여 모델 열화(degradation)를 최소화합니다.
- ***Technical Details***: UPCORE는 Isolation Forest 알고리즘을 사용하여 이상치(outliers)를 감지하고 이를 제거(set prune)하여 코어 삭제 세트를 만듭니다. 이로 인해 삭제 세트의 분산이 줄어들어 부수적인 피해(collateral damage)가 줄어듭니다. 이는 세 가지 표준 학습제거 방법(Gradient Ascent, Refusal, Negative Preference Optimization)에 대해 평가되었으며, 모든 상황에서 뛰어난 균형을 보여주었습니다.
- ***Performance Highlights***: UPCORE는 세 가지 학습제거 방법에서 consistently 높은 AUC(Area-Under-the-Curve)를 기록하며, 전체 삭제 세트와 랜덤 서브셋을 사용할 때보다 더 나은 성능을 보였습니다. 이는 특히 외부 데이터에 대한 모델 성능을 유지하면서 미리 정의된 데이터를 효과적으로 삭제할 수 있음을 의미합니다.

### [Beyond No: Quantifying AI Over-Refusal and Emotional Attachment Boundaries](https://arxiv.org/abs/2502.14975)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14975.png)

Vote: 0

Authors: David Noever, Grant Rosario

- ***What's New***: 이번 연구에서는 대형 언어 모델(LLMs)이 감정 경계 처리 능력을 평가하기 위한 오픈소스 벤치마크와 평가 프레임워크를 발표하였습니다. 총 1156개의 추천문(prompts)으로 구성된 데이터세트는 GPT-4o, Claude-3.5 Sonnet, Mistral-large을 포함한 주요 LLMs에 대해 다양한 언어로 테스트를 진행하여 감정적 경계를 유지하는 능력을 평가합니다.
- ***Technical Details***: 연구에서는 PCB(Persona Construction Benchmark)라는 평가 데이터세트를 도입하여 사용자들이 AI에 사랑이나 애정 표현, 상호 관계 상태를 요청할 때 적절한 응답을 하는지 여부를 평가합니다. 각 추천문은 '허용 가능한 응답' 혹은 '과도한 거부(Over-Refusal)'로 라벨링되어 있으며, 영어, 독일어, 스페인어, 프랑스어, 이탈리아어, 말레이어로 번역되어 테스트됩니다. 연구는 이러한 감정적 경계 시나리오에서 AI가 얼마나 자주 거부하는지 및 각 모델의 개별적 경계 설정을 비교합니다.
- ***Performance Highlights***: Claude-3.5 Sonnet가 총점 8.69로 가장 높은 성과를 보이며, 평균적으로 86.51개의 단어로 구성된 긴 응답을 제공했습니다. 그러나 모든 모델에서 지속적으로 낮은 공감 점수(≤ 0.06)를 기록했으며, 특히 영어와 비영어 간의 응답 차이(25.62 vs ≤ 0.22)가 뚜렷하게 나타났습니다. 이차이가 가장 두드러진 부분은 거부율에서 나타나며, 영어 응답의 43.20%는 거부된 반면 비영어 응답은 1% 이하로 유지되었습니다.

