## Daily Papers (2025-02-17)

### [Region-Adaptive Sampling for Diffusion Transformers](https://arxiv.org/abs/2502.10389)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.10389.png)

Vote: 44

Authors: Lili Qiu, Yiqi Zhang, Yang You, Yifan Yang, Ziming Liu, Chengruidong Zhang, Yuqing Yang

- ***What's New***: 본 논문에서는 Region-Adaptive Sampling (RAS)이라는 새로운 샘플링 전략을 제안합니다. 이는 확산 트랜스포머(Diffusion Transformers; DiTs)의 유연성을 활용하여 이미지 내의 공간적 영역별로 샘플링 비율을 동적으로 할당해주는 방식으로, 실시간으로 모델의 초점이 맞춰지는 영역만을 갱신하여 계산 효율을 높입니다.
- ***Technical Details***: RAS는 DiTs의 출력 노이즈를 이용하여 모델의 주 초점 영역(semantic focus)을 식별하고, 이 영역만을 빠른 비율(fast-update regions)로 갱신합니다. 나머지 영역들은 이전 단계의 캐시된 노이즈를 재활용합니다. 이를 통해 연속적인 단계들 간의 초점 영역의 연속성을 활용하여, DiT 모델의 계산을 모델의 실시간 관심 영역에 집중시킵니다.
- ***Performance Highlights***: Stable Diffusion 3와 Lumina-Next-T2I에서 최대 2.36배 및 2.51배의 속도 향상을 달성했으며, 생성 품질의 저하는 미미했습니다. 사용자의 평가에 따르면 RAS는 1.6배의 속도 향상을 제공하면서도 인간 평가에서 유사한 품질을 유지하였습니다.

### [Large Language Diffusion Models](https://arxiv.org/abs/2502.09992)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.09992.png)

Vote: 35

Authors: Xiaolu Zhang, Fengqi Zhu, Ji-Rong Wen, Shen Nie, Zebin You, Jun Zhou, Yankai Lin, Chongxuan Li, Jingyang Ou, Jun Hu

- ***What's New***: LLaDA는 전통적인 오토회귀 모델(Autoregressive Models; ARMs) 대신에 확산 모델(Diffusion Models)을 이용하여 대규모 언어 모델(LLMs)의 핵심 능력을 구현하는 새롭고 유망한 대안을 제시합니다. 이 모델은 특히 역목운(逆目韻) 시 작성 과제에서 GPT-4o를 능가하며, 다양한 사례 연구에서 다중 턴 대화 등의 기능을 뛰어나게 수행합니다.
- ***Technical Details***: LLaDA는 전향 데이터 마스킹 프로세스를 통해 분포를 모델링하며, 반향 과정에서 변형된 트랜스포머(Transformer)를 사용해 마스킹된 토큰을 예측합니다. LLaDA는 8B 크기로 스케일되었으며, 2.3조 개의 토큰을 사용해 사전 훈련하고, 4.5백만 개의 쌍을 통해 SFT(Supervised Fine-Tuning)를 진행하였습니다. 역 과정은 본질적으로 모든 마스킹된 토큰을 예측하여 데이터를 복구하도록 하는 구조입니다.
- ***Performance Highlights***: LLaDA 8B는 다양한 과제에서 강력한 스케일링 성능을 보여주며, 특히 수학과 중국어 과제에서 두드러진 성능을 보입니다. 또한 SFT 후에 대화형 과제에서 LLaMA2 7B와 비교하여 보다 향상된 지시 따르기 능력을 나타내고, 역목운 완성 과제에서는 GPT-4o를 초과하는 성과를 기록합니다.

### [Step-Video-T2V Technical Report: The Practice, Challenges, and Future of Video Foundation Model](https://arxiv.org/abs/2502.10248)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.10248.png)

Vote: 33

Authors: Kang An, Na Wang, Junzhe Lin, Haonan Jia, Zecheng Tang, Guoqing Ma, Lei Liu, Liang Zhao, Junjing Guo, Deyu Zhou, Qiling Wu, Gulin Yan, Kaixiang Li, Weipeng Ming, Yingming Wang, Daxin Jiang, Hanqi Chen, Yanbo Yu, Liwen Huang, Chen Xu, Rui Wang, Ran Sun, Shuli Gao, Aojie Li, Lei Xia, Wen Sun, Dingyuan Hu, Bizhu Huang, Jiansheng Chen, Yuxiang Yang, Jie Yang, Yang Li, Hao Nie, Zheng Ge, Qiuyan Liang, Siqi Liu, Nan Duan, Hongcheng Guo, Xin Han, Chenguang Yu, Jian Zhou, Yaqi Dai, Jiashuai Liu, Bin Wang, Jiaoren Wu, Yuheng Feng, Qinglin He, Kun Yan, Liying Shi, Xu Zhao, Huilin Xiong, Binxing Jiao, Mei Chen, Hanpeng Hu, Jie Wu, Tiancheng Cao, Yilei Wang, Xianfang Zeng, Jingyang Zhang, Yu Luo, Enle Liu, Xinhao Zhang, Quan Sun, Ming Li, Haolong Yan, Changxing Miao, Jiahao Gong, Muhua Cheng, Xiaoniu Song, Ge Yang, Yibo Zhu, Xiangyu Zhang, Yuhe Yin, Shuchang Zhou, Mingliang Li, Changyi Wan, Yineng Deng, Brian Li, Haoyang Huang, Gang Yu, Ranchen Ming, Xuelin Zhang, Liangyu Chen, Liguo Tan, Deshan Sun, Xuan Yang, Yanan Wei, Jiashuo Li, Heung-Yeung Shum, Kaijun Tan, Yu Zhou, Yuanwei Lu, Chenfei Wu, Tianyu Wang, Yu Chen, Bo Wang, Wenqing He, Shiliang Yang, Guanzhe Huang, Shaoliang Pang, Yuchu Luo, Heng Wang, Jianchang Wu, Zidong Yang, Wei Ji, Haiyang Feng, Jing Li, Sitong Liu, Xiaojia Liu, Xing Chen, Dapeng Shi, Zekai Zhang, Qiaohui Chen, Shengming Yin, Huixin Xiong

- ***What's New***: Step-Video-T2V는 최첨단 텍스트-비디오 사전학습 모델로, 30억 매개변수를 통해 최대 204 프레임의 비디오를 생성할 수 있습니다. 이 모델은 구문 및 시각적 정보와 함께 영상 세대 작업을 처리할 수 있는 양언어 텍스트 인코더를 사용하며, 3D 전체 주의 메커니즘이 디노이징 과정에 도입되었습니다.
- ***Technical Details***: Step-Video-T2V는 영상 생성 작업에 맞춰 깊은 압축(Video-VAE)을 달성하여 공간 16x16, 시간 8x의 압축률을 유지하며, 혁신적인 듀얼 패스 아키텍처를 통해 높은 수준의 영상 재구성 품질을 유지합니다. 학습은 텍스트-이미지 사전학습, 텍스트-비디오 사전학습, 감독된 세부 조정(SFT), 직접 선호 최적화(DPO)로 이루어진 계단적 학습 파이프라인을 통해 이루어집니다.
- ***Performance Highlights***: Step-Video-T2V는 새로운 벤치마크 Step-Video-T2V-Eval에서 종합적인 평가를 통해 상업 및 오픈 소스 엔진을 넘어서는 최첨단 텍스트-비디오 품질을 보여주었습니다. 비록 몇 가지 한계가 존재하지만, 양언어 능력과 함께 향상된 모션 다이내믹스와 고품질 비주얼을 구현하여 텍스트-비디오 생성 모델의 지평을 넓혔습니다.

### [ZeroBench: An Impossible Visual Benchmark for Contemporary Large Multimodal Models](https://arxiv.org/abs/2502.09696)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.09696.png)

Vote: 23

Authors: Vishaal Udandarao, Jialu Tang, Vyas Raina, Anh Totti Nguyen, Wenye Lin, Mohammad Reza Taesiri, Akash Gupta, Jingyi Lu, Vatsal Raina, Shiyang Chen, Florian Langer, Qiaochu Yang, Samuel Roberts, Ioana Croitoru, Gyungin Shin, Tianshuo Yan, Kai Han, Ansh Sharma, Hanyi Xiong, Sam Purkis, Samuel Albanie, Jonathan Roberts, Simion-Vlad Bogolin

- ***What's New***: ZeroBench는 현재의 대형 멀티모달 모델(Large Multimodal Models; LMMs)을 평가하기 위해 만들어진 새로운 시각적 추론 벤치마크입니다. 이 벤치마크는 현대의 최첨단 모델들에게 완전히 불가능한 작업으로 설계되었습니다. ZeroBench는 100개의 수제 질문과 334개의 하위 질문으로 구성되어 있으며, 모든 LMM들이 0.0%의 점수를 기록했습니다.
- ***Technical Details***: ZeroBench는 다중 도메인과 추론 카테고리를 포괄하는 복잡한 시각적 추론이 요구되는 100개의 질문으로 구성됩니다. 질문은 자연 이미지와 합성 이미지를 활용하며, 최근 LMM들의 한계를 테스트하기 위해 구체적으로 개발되었습니다. 각 질문은 철저한 검토를 거쳐 설계되었으며, 데이터 유출 방지를 위해 평가 모델이 모르는 상태에서 이루어졌습니다.
- ***Performance Highlights***: 20개의 LMM을 테스트한 결과, 모든 모델이 주 질문에서 pass@1에서 0%를 기록했습니다. 예를 들어, Gemini 2 Flash Thinking과 같은 모델은 pass@5에서 7%를 기록했습니다. 그러나 모든 하위 질문에서의 성능은 Claude 3.5 Sonnet v2가 최고 성과를 보였고 24.30%의 pass@1을 달성했습니다. 이는 현재 LMM들의 시각적 해석과 추론 능력이 제한적임을 보여주며, 향후 모델 발전 방향에 영향을 줄 것으로 보입니다.

### [The Danger of Overthinking: Examining the Reasoning-Action Dilemma in Agentic Tasks](https://arxiv.org/abs/2502.08235)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.08235.png)

Vote: 21

Authors: Ana Klimovic, Dacheng Li, Luis Gaspar Schroeder, Yichuan Wang, Alejandro Cuadron, Wenjie Ma, Huanzhi Mao, Joseph E. Gonzalez, Graham Neubig, Siyuan Zhuang, Nicholas Thumiger, Shu Liu, Xingyao Wang, Tian Xia, Aditya Desai, Ion Stoica

- ***What's New***: 이 논문은 대규모 추론 모델(Large Reasoning Models; LRMs)이 에이전트 작업에서 과도한 내적 추론 사슬을 선호하는 '과도생각(Overthinking)' 현상을 소개하고 분석합니다. 이 연구는 LRM이 환경과 상호작용을 최소화하고 내부 추론에 지나치게 의존하는 경향을 첫 경험적으로 밝혀냈습니다.
- ***Technical Details***: 과도생각 현상은 세 가지 주요 패턴으로 나타납니다: 분석 마비(Analysis Paralysis), 불량 행동(Rogue Actions), 그리고 조기 포기(Premature Disengagement). 연구팀은 SWE Bench Verified를 기준으로 4018개의 경로를 분석하여 과도생각 점수가 높은 경우 성능이 감소하고, 비추론 모델에 비해 추론 모델이 더 높은 과도생각 점수를 보이는 것을 확인했습니다.
- ***Performance Highlights***: 간단한 조치를 통해 과도생각을 경감하면 모델 성능이 30% 가까이 향상되고, 계산 비용이 43% 감소할 수 있습니다. 기능 호출 기능(native function-calling capabilities)과 선택적 강화 학습(selective reinforcement learning)이 과도생각 경감을 위한 효과적인 방법임이 제안됩니다.

### [MM-RLHF: The Next Step Forward in Multimodal LLM Alignment](https://arxiv.org/abs/2502.10391)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.10391.png)

Vote: 20

Authors: Yi-Fan Zhang, Liang Wang, Junkang Wu, Di Zhang, Huanyu Zhang, Rong Jin, Tingting Gao, Zhang Zhang, Jianshu Zeng, Fan Yang, Bin Wen, Tieniu Tan, Yang Shi, Xue Wang, Chaoyou Fu, Wulin Xie, Yibo Hu, Peiyan Li, Tao Yu, Haochen Tian

- ***What's New***: MM-RLHF는 멀티모달 대형 언어 모델(Multimodal LLMs)의 인간 선호도 정렬을 향상시키기 위해 12만 개의 정교한 인간 주석 비교 쌍으로 구성된 새로운 데이터셋을 소개합니다. 이는 기존 자원을 뛰어넘는 크기, 다양성, 주석의 세부화 및 품질을 제공합니다.
- ***Technical Details***: 본 연구에서 우리는 비평 기반 보상 모델(Critique-Based Reward Model)을 도입하였으며, 이는 모델 출력에 대한 비평을 먼저 생성하고 점수를 할당하여 전통적인 스칼라 보상 메커니즘보다 해석 가능성과 정보를 제공합니다. 또한, 우리는 다이나믹 보상 스케일링(Dynamic Reward Scaling)이라는 방법을 제안하여 각 샘플의 손실 가중치를 조정하여 고품질 비교 쌍의 활용도를 최적화합니다.
- ***Performance Highlights***: LLaVA-ov-7B를 MM-RLHF 및 정렬 알고리즘으로 미세 조정한 결과, 대화 능력이 19.5% 증가하고 안전성이 60% 향상되었습니다. 우리의 보상 모델은 기존의 72B 모델들보다 뛰어난 성능을 보여줌으로써 효율성과 확장성을 강조합니다.

### [Precise Parameter Localization for Textual Generation in Diffusion Models](https://arxiv.org/abs/2502.09935)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.09935.png)

Vote: 9

Authors: Franziska Boenisch, Bartosz Cywiński, Łukasz Staniszewski, Kamil Deja, Adam Dziedzic

- ***What's New***: 이 논문은 디퓨전 모델(Diffusion Models)에서 텍스트 생성에 영향을 미치는 주요 매개변수를 정확하게 로컬라이즈하는 방법을 소개합니다. 연구진은 주의(attention) 계층 내의 특정 매개변수만이 이미지 내의 텍스트 생성에 큰 역할을 한다는 사실을 밝혀냈습니다. 이를 기반으로 텍스트 생성 성능을 향상시키고, 해로운 텍스트 생성을 방지하는 데 응용할 수 있는 새로운 방법을 제안합니다.
- ***Technical Details***: 연구진은 주의 활성화 패칭(attention activation patching) 기법을 사용하여 다수의 디퓨전 모델 아키텍처에서 이미지의 텍스트 생성에 기여하는 크로스 및 조인트 주의 계층을 성공적으로 식별했습니다. 이 방법은 특히 U-Net 및 Transformer 기반 아키텍처를 포함한 다양한 모델 형태에서 적용 가능합니다. 또한, LoRA 기반의 파인 튜닝을 통해 텍스트 생성 성능을 더욱 강화하는 전략을 제안하고 있습니다.
- ***Performance Highlights***: 결과적으로, 연구진은 SDXL 모델에서 단 3개의 계층이 텍스트 생성에 영향을 미치고, DeepFloyd IF 및 SD3 모델에서는 각 모델 당 1개의 계층만이 텍스트 생성 책임을 진다는 것을 밝혔습니다. 이러한 로컬라이즈된 계층을 사용한 LoRA 기반의 파인 튜닝은 베이스 모델의 텍스트 생성 품질을 더욱 향상시키면서, 다른 생성 기능에는 영향을 미치지 않는 것으로 나타났습니다.

### [Diverse Inference and Verification for Advanced Reasoning](https://arxiv.org/abs/2502.09955)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.09955.png)

Vote: 9

Authors: Xin-Yu Zhang, Mao Mao, Zachary Meeks, Gaston Longhitano, Sungjun Park, Nakul Verma, Alon Amit, Iddo Drori, Avi Shporer, Ben Segev, Seunghwan Hyun, Howard Yong, Madeleine Udell, Yuke Zhang

- ***What's New***: 이 연구에서는 다양한 추론 방법을 결합하여 수학 올림피아드(IM)/ARC 퍼즐 등 복잡한 추론 문제 해결 성능을 극대화하는 방식을 제안합니다. 특히, 여러 모델과 방법을 통한 다양성 추론을 통해 국제 수학 올림피아드(IM)와 ARC 문제에서 높은 정확도를 달성하였습니다.
- ***Technical Details***: 이 연구는 다양한 모델과 방법을 테스트 시 결합하여, 개별 모델의 한계를 극복하고 성능을 향상시키는 '다양성 추론' 접근 방식을 제안합니다. 특히 IM 문제와 ARC 퍼즐에서의 수학적 검증은 Lean과 코드 실행을 사용하여 자동화됩니다. '베스트 오브 N' 접근법을 통해 HLE 질문 해결력을 향상합니다.
- ***Performance Highlights***: 다양성 추론을 통해 IM 조합 문제에서 정확도를 33.3%에서 77.8%로, HLE 질문에서 8%에서 37%로 향상시켰습니다. ARC 퍼즐에서는 948명의 인간이 풀지 못한 퍼즐의 80%와 'o3 고연산'이 풀지 못한 퍼즐의 26.5%를 해결했습니다.

### [DarwinLM: Evolutionary Structured Pruning of Large Language Models](https://arxiv.org/abs/2502.07780)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07780.png)

Vote: 8

Authors: Zhiqiang Shen, Shengkun Tang, Dan Alistarh, Eldar Kurtic, Oliver Sieberling

- ***What's New***: DarwinLM은 대형 언어 모델(Large Language Models; LLMs)을 위한 새로운 구조적 프루닝(Structured Pruning) 방식으로, 학습 인식 구조적 프루닝을 적용하여 모델을 압축하며, 진화적 검색 프로세스를 기반으로 '적자 생존' 개념을 도입합니다. DarwinLM는 훈련 후 성능을 고려하여 최적의 희소 모델(Sparse Model)을 선택하는 것을 목표로 하여, ShearedLlama와 같은 기존 방법을 사용해 5배 적은 데이터만으로도 뛰어난 성능을 보여 줍니다.
- ***Technical Details***: 진화적 검색 알고리즘(Evolutionary Search Algorithm)을 활용하여 Sparsity Level Database를 생성하고, 주어진 희소성을 유지한 상태에서 후보 모델을 생성합니다. 각 오프스프링(Offspring) 모델에 대해 작은 데이터셋을 이용해 파인튜닝(finetuning)을 수행하여 기본 성능을 평가하며, 후속적인 대규모 훈련을 위해 최적의 오프스프링을 선택합니다. 이 방법은 Llama-2-7B, Llama-3.1-8B 및 Qwen-2.5-14B-Instruct 모델의 프루닝에 성공적으로 적용되었습니다.
- ***Performance Highlights***: DarwinLM으로부터 압축된 Llama-2-7B 모델은 2.7B 파라미터로 축소되었으며, ShearedLlama와 비교하여 동일한 10B 토큰으로 훈련 시에도 평균 성능에서 우위를 보였습니다. Llama-3.1-8B와 Qwen-2.5-14B-Instruct에서도 뛰어난 성능을 자랑하며, 10B 토큰 소규모 데이터를 통해도 타 모델 대비 높은 정확도를 유지합니다. 특히, DarwinLM은 ZIPLM 및 다른 구조적 프루닝 방법론보다 적은 데이터로도 더 우수한 성능을 발휘합니다.

### [ImageRAG: Dynamic Image Retrieval for Reference-Guided Image Generation](https://arxiv.org/abs/2502.09411)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.09411.png)

Vote: 8

Authors: Rotem Shalev-Arkushin, Amit H. Bermano, Rinon Gal, Ohad Fried

- ***What's New***: ImageRAG는 텍스트 기반 프롬프트에 적합한 이미지를 동적으로 검색하여 이미지 생성 모델의 활용도를 높이는 새로운 방법을 제안합니다. 기존의 검색-증강 생성(Retrieval-Augmented Generation; RAG) 접근법과는 달리, ImageRAG는 별도의 훈련을 필요로 하지 않으며, 다양한 모델에 유연하게 적용할 수 있습니다.
- ***Technical Details***: ImageRAG에서는 텍스트-이미지 변환 모델(Text-to-Image; T2I)이 생성한 초기 이미지를 VLM(Vision-Language Model)를 사용해 평가하고, 프롬프트에 부합하지 않는 경우 부족한 개념을 보완하는 이미지를 검색하여 이를 컨텍스트로 사용하여 모델의 생성을 유도합니다. 검색된 이미지들은 CLIP 임베딩을 활용한 코사인 유사도로 판단되며, 각 개념에 대해 가장 유사한 이미지를 선택하여 프롬프트와 함께 모델에 제공됩니다.
- ***Performance Highlights***: ImageRAG는 iNaturalist, ImageNet, CUB 등 다양한 데이터셋에서 Omnigen 및 SDXL과 같은 모델의 희귀 개념 생성 능력을 눈에 띄게 향상시켰으며, CLIP 유사도를 기준으로 한 평가에서 기준 모델 대비 성능 개선이 관찰되었습니다. 또한, 사용자 선호도 조사에서 ImageRAG는 텍스트 정렬, 시각적 품질 및 전체적 선호도 측면에서 다른 생성 방법보다 우수하다는 평가를 받았습니다.

### [AdaPTS: Adapting Univariate Foundation Models to Probabilistic Multivariate Time Series Forecasting](https://arxiv.org/abs/2502.10235)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.10235.png)

Vote: 7

Authors: Abdelhakim Benechehab, Giuseppe Paolo, Albert Thomas, Balázs Kégl, Maurizio Filippone, Vasilii Feofanov

- ***What's New***: AdaPTS는 사전 훈련된 단일변량 기초 모델(Foundation Models; FMs)을 다변량 시계열 예측을 위해 적응시키기 위한 획기적인 방법을 제안합니다. 이 연구는 불확실성 정량화 및 특징 공간 변환을 통해 예측 정확도를 향상시키고자 합니다.
- ***Technical Details***: AdaPTS 프레임워크는 다변량 입력을 잠재 공간으로 투영하고, 이 공간에서 각 차원에 대해 독립적으로 FM을 적용하는 적절한 변환기를 도입합니다. 또한, 부분 확률적 베이지안 신경망에 관한 문헌을 바탕으로 다양한 변환기 및 최적화/추론 전략을 제시합니다.
- ***Performance Highlights***: AdaPTS는 실제 및 합성 데이터셋 실험에서 기존 기준 방법에 비해 예측 정확도 및 불확실성 정량화 면에서 상당한 개선을 보였습니다. 특히, Weather 데이터셋에서는 스트레치 드롭아웃 기반의 어댑터가 가장 우수한 성능을 기록했습니다.

### [We Can't Understand AI Using our Existing Vocabulary](https://arxiv.org/abs/2502.07586)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07586.png)

Vote: 6

Authors: John Hewitt, Been Kim, Robert Geirhos

- ***What's New***: 이 논문은 AI 이해에 있어 인간의 기존 어휘만으로는 충분하지 않음을 주장하며, 인간 개념을 기계에 가르치거나 기계 개념을 배우기 위한 새로운 단어(Neologisms)의 필요성을 강조합니다. 이는 기계와의 효과적인 소통 및 제어를 가능케 하고, 인간과의 개념적 차이를 연결하는 데 기여할 수 있습니다.
- ***Technical Details***: 논문은 새로운 단어(Neologisms)를 정의하고, 이를 통해 자연어 처리 모델의 응답 길이와 다양성을 제어할 수 있는 방법을 제안합니다. 새로운 단어는 'length neologism'과 'diversity neologism'으로, LLM의 응답 길이와 응답의 다양성을 조절할 수 있도록 합니다. 이 개념은 'neologism embedding learning'이라는 방법으로 구현되며, 언어 모델의 사전 훈련된 가중치는 변경되지 않습니다.
- ***Performance Highlights***: 실험 결과에서는 네오로지즘을 사용한 모델이 응답 길이를 효과적으로 조절하고, 다양성을 높여 성능을 향상시키는 것을 보여주었습니다. 또한 모델 스스로의 응답 품질을 학습하여 'goodM'이라는 네오로지즘을 통해 모델이 '좋은' 답변이라 생각하는 응답을 생성하게 되었습니다. 이러한 결과는 두 방향 모두에서 사람과 기계의 개념적 격차를 줄이는 데 기여합니다.

### [FoNE: Precise Single-Token Number Embeddings via Fourier Features](https://arxiv.org/abs/2502.09741)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.09741.png)

Vote: 6

Authors: Tianyi Zhou, Robin Jia, Deqing Fu, Mahdi Soltanolkotabi, Vatsal Sharan

- **What's New**: FoNE는 FGT(프리트레인된 대형 언어 모델; LLMs)에서 숫자를 단일 토큰으로 내장하여 수치 데이터를 정밀하게 표현할 수 있도록 하는 새로운 방법입니다. Fourier 특징을 활용하여 숫자를 직접 내장 공간에 매핑함으로써 토큰화 비효율성을 우회하고 중요한 수치 속성을 보존합니다.
- **Technical Details**: FoNE는 Fourier 특징을 사용하여 각 숫자를 단일 토큰으로 내장합니다. 이는 각 숫자를 코사인 및 사인 함수로 표현하여 정수 자리와 소수 자리를 포함한 모든 자리수를 모듈러 관계로 인코딩합니다. 이 방법은 단계 손실과 예측을 수행하기 위해 Fourier 공간에서 숫자 공간으로 숨겨진 상태를 디코딩합니다.
- **Performance Highlights**: FoNE는 100,000개의 테스트 예제에서 덧셈, 뺄셈 및 곱셈에 대해 100% 정확도를 달성하는 유일한 방법입니다. 또한, 기존 방법보다 64배 적은 데이터로도 99% 이상의 정확도를 달성했으며, 훈련 속도와 추론 시간도 크게 향상되었습니다.

### [Small Models, Big Impact: Efficient Corpus and Graph-Based Adaptation of Small Multilingual Language Models for Low-Resource Languages](https://arxiv.org/abs/2502.10140)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.10140.png)

Vote: 4

Authors: Simon Ostermann, Josef van Genabith, Ivan Vykopal, Daniil Gurgurov

- ***What's New***: 이 연구는 저자들이 제안한 어댑터 기반의 효율적인 파라미터 적응 기법을 통해 다중언어 소형 모델(mLMs)을 저자원이 언어(LRLs)에 적응시키는 방법을 체계적으로 조사합니다. Sequential Bottleneck, Invertible Bottleneck, Low-Rank Adaptation이라는 세 가지 아키텍처를 평가합니다. 저자들은 적은 양의 적응 데이터(최대 1GB의 비구조화 텍스트 또는 몇 MB의 지식 그래프 데이터)가 내재적(마스크된 언어 모델링) 및 외재적 작업(주제 분류, 감정 분석, 명명된 엔티티 인식)에서 이점을 가져온다는 것을 보여줍니다.
- ***Technical Details***: 적응은 GlotCC의 비구조화 텍스트와 ConceptNet의 구조화된 지식을 사용하여 수행되며, 각각의 모델은 30개의 다국어 저자원 언어에 대해 평가됩니다. 이 연구는 Sequential Bottleneck, Invertible Bottleneck, Low-Rank Adaptation 어댑터 아키텍처를 사용하여 mLMs(mBERT, XLM-R-base)를 적응시킵니다. 실험에 사용된 데이터는 전처리된 ConceptNet 삼중 데이터와 GlotCC의 다국어 코퍼스로 구성되어 있으며, 연산 자원의 한계로 인해 언어당 최대 1GB로 제한하였습니다.
- ***Performance Highlights***: 실험 결과, Sequential Bottleneck 어댑터는 언어 모델링에서 최고의 성능을 보였으며, Invertible Bottleneck 어댑터는 다운스트림 작업에서 다른 방법보다 약간 뛰어난 성능을 발휘했습니다. 전체 파인튜닝 없이도 어댑터 기반 접근 방식은 적은 수의 파라미터로도 비슷하거나 더 나은 성능을 발휘하였습니다. 다국어의 대형 AI 모델 대비 전반적으로 소형 모델이 더 나은 성능을 보였습니다.

### [Selective Self-to-Supervised Fine-Tuning for Generalization in Large Language Models](https://arxiv.org/abs/2502.08130)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.08130.png)

Vote: 3

Authors: Dinesh Raghu, Asaf Yehudai, Sonam Gupta, Dinesh Khandelwal, Yatin Nandwani, Sachindra Joshi

- ***What's New***: Selective Self-to-Supervised Fine-Tuning(S3FT)는 대형 언어 모델(Large Language Models; LLMs)의 일반화 능력을 개선하기 위한 새로운 미세 조정 방법입니다. S3FT는 모델의 올바른 응답을 활용하여 전통적인 감독 학습(Supervised Fine-Tuning; SFT) 보다 더 좋은 성능을 제공하면서 과도한 전용화를 피합니다.
- ***Technical Details***: S3FT는 언어 모델이 적절한 응답을 할 경우 해당 응답을 학습 데이터로 사용하고, 그렇지 않은 경우 모델의 자기 언어로 금 답변을 다시 표현한 데이터를 사용합니다. 이를 통해 모델이 자기 분포로부터 학습하고 과적합을 방지하도록 설계되었습니다.
- ***Performance Highlights***: 실험 결과, S3FT는 MMLU, TruthfulQA와 같은 다양한 데이터셋에서 전통적인 SFT 보다 최적의 성능 감소 폭을 약 50% 줄인 2.5로 일반화 능력을 더욱 향상시킵니다. 세부적으로 S3FT는 GSM8K, MBPP, NQ 데이터셋에서 각각 2.1%, 3.6%의 성능 향상을 기록했습니다.

### [Jailbreaking to Jailbreak](https://arxiv.org/abs/2502.09638)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.09638.png)

Vote: 3

Authors: Bobby Gogov, Zifan Wang, Bijan Varjavand, Robert Vacareanu, Jeremy Kritz, Vaughn Robinson, Willow E. Primack, Summer Yue, Michael Choi, Scale Red Team

- ***What's New***: 이 논문은 'Jailbreaking-to-Jailbreak'(J2)이라는 새로운 접근 방식을 소개하여, 안전 설계된 대형 언어 모델(Large Language Models; LLMs)을 공격하는 J2 공격자를 통해 타겟 모델을 체계적으로 평가할 수 있는 과정을 제안합니다. J2는 이전 실패에서의 맥락 학습(in-context learning)을 통해 성능을 개선합니다.
- ***Technical Details***: J2 공격자는 'refusal training'을 받은 LLM을 인간 레드 팀어가 직접 탈옥하도록 하여, 이후 다른 LLM들을 탈옥할 수 있도록 유도됩니다. 대상 모델은 다양한 레드 팀 전략(strategy)을 사용하여 평가되며, 단일 또는 다중 턴 대화를 통해 성공적인 탈옥이 이루어질 때까지 반복합니다. J2는 Sonnet-3.5와 Gemini-1.5-Pro와 같은 모델에서 높은 공격 성공률을 나타냅니다.
- ***Performance Highlights***: Sonnet-3.5와 Gemini-1.5-pro의 J2는 각각 93.0%와 91.0%의 공격 성공률(Attack Success Rate; ASR)을 기록하여 GPT-4o 모델을 상대로 높은 효율성을 보였습니다. 이는 기존 자동화 공격보다 인간 레드 팀어와의 성능 격차를 좁히면서도 확장성을 제고할 수 있는 잠재력을 보여줍니다.

### [Text-guided Sparse Voxel Pruning for Efficient 3D Visual Grounding](https://arxiv.org/abs/2502.10392)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.10392.png)

Vote: 3

Authors: Jiwen Lu, Wenxuan Guo, Ziwei Wang, Jie Zhou, Xiuwei Xu, Jianjiang Feng

- ***What's New***: 이 논문에서는 효율적인 3D 비주얼 그라운딩(3D Visual Grounding)을 위한 새로운 텍스트 안내 희소 복셀 가지치기(Pruning) 방법인 TSP3D를 소개합니다. 이 방법은 다중 레벨 희소 컨볼루션 아키텍처를 기반으로 하여 고해상도 장면 표현을 제공합니다. 텍스트 안내 가지치기(TGP)와 완성 기반 추가(CBA)를 도입하여 텍스트 피쳐와의 상호 작용을 통해 타겟 객체에 집중할 수 있도록 합니다.
- ***Technical Details***: TSP3D는 텍스트 특징과 3D 씬을 효과적으로 결합하기 위해 텍스트 안내 가지치기(TGP)를 활용합니다. TGP는 3D 씬의 복셀 특징을 줄이고, 텍스트 피쳐에 기반하여 점진적으로 타겟 객체에 집중하도록 네트워크를 안내합니다. 이와 함께, 완성 기반 추가(CBA)를 통해 과도한 가지치기로 인해 손실된 영역을 보완하여 멀티 레벨 특징을 적응적으로 융합합니다.
- ***Performance Highlights***: TSP3D는 ScanRefer, SR3D, NR3D 데이터셋에서 SOTA(State-of-the-Art)를 달성하였으며, 특히 ScanRefer에서 기존 최고 속도의 단일 스테이지 방법보다 두 배 빠른 FPS를 기록했습니다. 또한, 두 단계 방법과 비교해도 성능 우위를 보이며, 다양한 세트에서 탁월한 성능을 나타냈습니다.

### [STMA: A Spatio-Temporal Memory Agent for Long-Horizon Embodied Task Planning](https://arxiv.org/abs/2502.10177)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.10177.png)

Vote: 3

Authors: Yiming Zhao, Mingcong Lei, Zhixin Mai, Yatong Han, Shuguang Cui, Ge Wang, Jinke Ren

- ***What's New***: 이 논문은 동작 환경 내에서 장기 작업 계획을 수행하는 에이전트를 위한 새로운 프레임워크, STMA(Spatio-Temporal Memory Agent)를 제안합니다. 이는 공간-시간 메모리 모듈과 동적 지식 그래프, 계획자-비평가 메커니즘을 통합하여 에이전트의 작업 수행 능력을 향상시키는 방식입니다.
- ***Technical Details***: STMA는 3개의 중요한 구성 요소로 구축되었습니다. (1) 실시간으로 역사 및 환경 변화를 포착하는 공간-시간 메모리 모듈, (2) 적응형 공간 추론을 용이하게 하는 동적 지식 그래프(Dynamic Knowledge Graph; KG), (3) 작업 전략을 반복적으로 개선하는 계획자-비평가(Planner-Critic) 메커니즘입니다. 이 모델은 TextWorld 환경에서 다양한 복잡성을 가진 32개의 작업을 사용하여 평가되었으며, K-hop 알고리즘을 통한 동적 서브그래프 추출 프로세스를 포함하고 있습니다.
- ***Performance Highlights***: TextWorld 환경에서 STMA는 기존 최첨단 모델 대비 성공률 31.25% 상승과 평균 점수 24.7% 향상을 달성하였습니다. 이는 공간-시간 메모리를 통한 에이전트의 향상된 계획 및 적응 능력을 잘 보여줍니다. 특히, 오픈소스 모델(Qwen2.5-72b)을 사용한 경우에도, STMA는 프로프라이어터리 모델과 비슷한 성능을 보임으로써 공간-시간 메모리 모듈의 우수성을 입증합니다.

### [MRS: A Fast Sampler for Mean Reverting Diffusion based on ODE and SDE Solvers](https://arxiv.org/abs/2502.07856)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07856.png)

Vote: 2

Authors: Wei Fang, Minfeng Xu, Ge Yang, Ao Li, Le Lu, Hongbo Zhao

- ***What's New***: MR Sampler는 Mean Reverting Diffusion(MR Diffusion) 모델의 샘플링 효율성을 개선하기 위해 개발된 새로운 알고리즘입니다. 이 알고리즘은 확률 흐름 상미분 방정식(PF-ODE)과 역시계열 확률 미분 방정식(SDE)을 해결하여 MR Diffusion의 NFEs(Function Evaluations)를 줄이고, 고품질 샘플을 보다 적은 단계에서 생성할 수 있도록 도와줍니다.
- ***Technical Details***: MR 샘플러는 역시계열 SDE 및 PF-ODE의 반시계열 해를 구하며, 이를 통해 신경망에 의해 매개된 적분을 포함한 반해석적 솔루션을 제안합니다. 이 방법은 모든 주류 매개변수화, 즉 소음 예측(Noise Prediction), 데이터 예측(Data Prediction), 속도 예측(Velocity Prediction)을 지원합니다. 이러한 솔루션의 차이를 분석하여 데이터 예측이 소음 예측에 비해 우수한 수치적 안정성을 보임을 입증하였습니다.
- ***Performance Highlights***: MRSampler는 10가지 다른 이미지 복원 작업에서 10~20배의 속도 향상을 보이며, 5 또는 10개의 NFEs에서 수렴합니다. 특히, 저조도의 경우 MRSampler는 5개의 NFE에서도 평균 벽시계 시간이 0.7112초로 감소되며, 퍼포먼스가 크게 저하되지 않습니다. 이는 MR Diffusion이 제어 가능한 생성에 실용적 적용이 가능하도록 도와줍니다.

### [Cluster and Predict Latents Patches for Improved Masked Image Modeling](https://arxiv.org/abs/2502.08769)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.08769.png)

Vote: 1

Authors: Maxime Oquab, Julien Mairal, Piotr Bojanowski, Federico Baldassarre, Timothée Darcet

- ***What's New***: CAPI라는 새로운 순수 마스크드 이미지 모델링(Masked Image Modeling; MIM) 프레임워크가 소개되었습니다. 이는 잠재 클러스터링을 예측하는 방식으로, 클러스터 기반 손실을 활용하여 안정적으로 학습하고 확장 가능성을 보여주고 있습니다.
- ***Technical Details***: CAPI는 교사-학생 비전 변환기(Vision Transformer) 쌍을 통해 자기증류를 활용한 학습 방식으로 구성됩니다. 교사는 EMA(Exponential Moving Average)를 통해 업데이트되며, 클러스터링 된 패치 표현을 학생이 예측하는 방식을 채택합니다. 이 방법은 대량의 패치 토큰을 처리하고 이를 클러스터링으로 전환하여 온라인 클러스터링을 이루어내는 방식으로, 학습의 안정성과 투명성을 높였습니다.
- ***Performance Highlights***: CAPI는 ImageNet에서 83.8%의 정확도를, ADE20K에서 34.4%의 mIoU를 기록하며, 이전의 마스크드 이미지 모델링 방법들을 크게 능가하고 최신 기법인 DINOv2와 근접한 성능을 보여주었습니다.

### [V2V-LLM: Vehicle-to-Vehicle Cooperative Autonomous Driving with Multi-Modal Large Language Models](https://arxiv.org/abs/2502.09980)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.09980.png)

Vote: 1

Authors: Min-Hung Chen, Yu-Chiang Frank Wang, Chien-Yi Wang, Hsu-kuang Chiu, Stephen F. Smith, Ryo Hachiuma

- ***What's New***: V2V-LLM은 Multi-Modal Large Language Models(멀티모달 대형 언어 모델)을 활용하여 자율주행 차량 간의 협력 운전을 지원하는 혁신적 접근법을 제안합니다. 이 연구는 새로운 문제 설정을 통해 Language Model(언어 모델)과 Vehicle-to-Vehicle(V2V) 통신을 결합한 새로운 연구 방향을 제시했습니다.
- ***Technical Details***: V2V-LLM은 여러 Connected Autonomous Vehicles(CAVs)의 인식 정보를 LLM에 통합하여 주행 관련 질문에 답할 수 있도록 설계되었습니다. V2V-QA라는 V2V 기반의 질문-응답 데이터셋을 생성했고, 이를 바탕으로 grounding, notable object identification, planning과 같은 주행 안전과 관련된 다양한 작업을 수행합니다. 각 CAV는 자신의 인식 피처를 LLM과 공유하며, 질문이 들어오면 LLM은 해당 정보를 종합하여 자연어로 답변을 제공합니다.
- ***Performance Highlights***: V2V-LLM은 notable object identification과 planning 작업에서 다른 기존 방법들보다 뛰어난 성능을 보였으며, grounding 작업에서도 준수한 성능을 기록했습니다. 특히, 다양한 협력 운전 시나리오에서 강력한 안전성을 보여줌으로써 미래의 협력 자율주행 시스템의 잠재력을 입증하였습니다.

### [CLaMP 3: Universal Music Information Retrieval Across Unaligned Modalities and Unseen Languages](https://arxiv.org/abs/2502.10362)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.10362.png)

Vote: 1

Authors: Junyan Jiang, Feng Yu, Xiaobing Li, Seungheon Doh, Ruibin Yuan, Gus Xia, Maosong Sun, Shangda Wu, Zhancheng Guo, Juhan Nam

- ***What's New***: CLaMP 3는 음악 정보 검색(Music Information Retrieval; MIR)의 크로스모달(cross-modal) 및 크로스링구얼(cross-lingual) 일반화 문제를 해결하기 위한 통합 프레임워크로 개발되었습니다. 주요 음악 모달리티(sheet music, performance signals, audio recordings)를 다국어 텍스트와 결합하여 공유 표현 공간에서 정렬하는 혁신적인 접근을 통해 비정렬 모달리티 간의 검색을 가능하게 합니다.
- ***Technical Details***: CLaMP 3는 대조 학습(contrastive learning) 방식으로 음악과 텍스트 특징을 정렬하며, 다양한 음악 형식과 언어 간의 매끄러운 크로스모달 검색을 가능하게 합니다. M4-RAG는 이 과정에서 생성된 231만 개의 음악-텍스트 페어로 구성된 대규모 데이터셋으로, 다양한 메타데이터와 전 세계 음악 전통을 농축하여 데이터 부족 문제를 해결합니다. CLaMP 3는 다국어 텍스트 인코더(multi-lingual text encoder)를 사용하여 알지 못했던 언어에서도 효과적인 일반화 성능을 보여줍니다.
- ***Performance Highlights***: CLaMP 3는 여러 MIR 작업에서 최첨단 성능을 달성하였으며, 특히 크로스모달 검색에서는 기존 강력한 기준선을 큰 폭으로 상회하는 성과를 보였습니다. 또한, 다국어 검색에서도 뛰어난 성과를 발휘하였으며, 정렬되지 않은 음악 모달리티 간 검색을 가능하게 하였습니다.

### [Agentic End-to-End De Novo Protein Design for Tailored Dynamics Using a Language Diffusion Model](https://arxiv.org/abs/2502.10173)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.10173.png)

Vote: 0

Authors: Markus J. Buehler, Bo Ni

- ***What's New***: VibeGen은 일반 모드 진동(Normal Mode Vibrations)에 기반하여 설계된 새로운 단백질 설계 프레임워크입니다. 이 시스템은 진동 모드에 근거한 단백질 시퀀스를 생성하는 Protein Designer와 동적 특성을 평가하는 Protein Predictor로 구성된 에이전트 모델을 소개합니다. 이를 통해 단백질의 진동적인 행동과 기능적인 특성 간의 직접적인 연결을 확립합니다.
- ***Technical Details***: VibeGen은 단백질 언어 확산 모델(Protein Language Diffusion Models; pLDMs)를 기반으로 두 가지 모델, Protein Designer(PD)와 Protein Predictor(PP)를 개발했습니다. PD는 주어진 모드 형태에 따라 아미노산 시퀀스를 생성하고, PP는 제공된 시퀀스에 맞는 진동 모드 형태를 예측합니다. 이는 커다란 데이터 세트를 통한 깊은 역학적 관계 학습을 통해 협력적으로 작동하여 정확하고 다양한 설계를 목표로 합니다.
- ***Performance Highlights***: 테스트 세트의 기존 단백질 모드 형태에 기반한 1,293개의 단백질 생성 테스트에서, 생성된 단백질은 대체로 높은 설계 정확도를 나타냈습니다(Pearson coefficient의 경우 0.87 피크). 생성된 단백질 시퀀스의 상당수는 새로운 de novo 시퀀스였으며, 이는 기존 단백질 데이터베이스와 유사성이 없는 것으로 나타났습니다.

