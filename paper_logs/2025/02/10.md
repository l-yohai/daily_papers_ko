## Daily Papers (2025-02-10)

### [VideoRoPE: What Makes for Good Video Rotary Position Embedding?](https://arxiv.org/abs/2502.05173)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.05173.png)

Vote: 50

Authors: Yuhang Cao, Jiaqi Wang, Xiaoran Liu, Xiaoyi Dong, Dahua Lin, Qipeng Guo, Yuhang Zang, Haodong Duan, Jian Tong, Pan Zhang, Xilin Wei, Xipeng Qiu

- ***What's New***: VideoRoPE는 복잡한 시공간적 구조를 가진 비디오 데이터에 로타리 포지션 임베딩(Rotary Position Embedding; RoPE)을 확장시킨 새로운 방법론입니다. VideoRoPE는 저주파수 시간 할당(Low-frequency Temporal Allocation; LTA)과 대각 배열(Diagonal Layout; DL), 그리고 조절 가능한 시간 간격(Adjustable Temporal Spacing; ATS)을 도입하여 RoPE의 기존 한계를 극복하며 비디오 내 위치 정보의 효과적인 표현을 지원합니다.
- ***Technical Details***: VideoRoPE는 3D 스페이스-타임 구조로 설계되어 있으며, 저주파수의 시간 할당을 통해 주기적 진동을 완화하고 공간 대칭성을 유지하는 대각 배열을 특징으로 합니다. 또한 조절 가능한 시간 간격을 통해 시간 및 공간 인덱싱을 분리하여 다양한 복잡한 비디오 데이터에서의 효율적인 인코딩을 가능하게 합니다. 특히 독창적인 V-NIAH-D(Visual Needle-In-A-Haystack with Distractors) 과제를 도입하여 기존 방식들이 쉽게 혼란스럽게 되는 주기적 방해 요소를 극복할 수 있음을 보여줍니다.
- ***Performance Highlights***: VideoRoPE는 V-NIAH-D와 같은 난이도 높은 과제에서 기존 RoPE 변형 기법들을 능가하며, 긴 영상 검색, 영상 이해, 영상 환각 등의 다양한 다운스트림 작업에서 일관된 성능 향상을 보여줍니다. 예를 들어, 긴 비디오 검색 작업에서는 M-RoPE 대비 최대 12.44%의 성능 향상을 보여주며, 이는 VideoRoPE의 견고함을 입증합니다.

### [Fast Video Generation with Sliding Tile Attention](https://arxiv.org/abs/2502.04507)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04507.png)

Vote: 39

Authors: Peiyuan Zhang, Runlong Su, Ion Stoica, Hao Zhang, Yongqi Chen, Hangliang Ding, Zhenghong Liu

- ***What's New***: 이 논문은 비디오 생성에서 최대의 연산 비용을 줄이기 위해 슬라이딩 타일 어텐션(Sliding Tile Attention; STA)을 도입하였습니다. 이 방법은 3D 비디오 확산 모델에서 주로 나타나는 지역적 어텐션 패턴을 효과적으로 활용하여 기존의 3D 전체 어텐션의 중복성을 제거합니다.
- ***Technical Details***: STA는 하드웨어를 고려한 슬라이딩 윈도우 디자인으로 타일 하나 단위로 작동하며, 타일의 크기는 FlashAttention의 블록 크기에 의해 결정됩니다. STA는 모든 블록을 별도의 마스크 없이 계산하여 하드웨어 효율성을 높이고, 복잡한 마스크 계산 오버헤드를 피합니다. 또한 STA는 최적의 윈도우 크기를 자동으로 구성할 수 있는 머리(Head) 특수화 패턴을 발견하여 연산 효율성을 유지합니다.
- ***Performance Highlights***: STA는 FlashAttention-3 대비 어텐션을 2.8배에서 17배, FlashAttention-2 대비 1.6배에서 10배 가속화합니다. 대표적인 비디오 DiT인 HunyuanVideo에서는 품질 감소 없이 945초에서 685초까지 지연 시간을 줄일 수 있으며, 추가 미세 조정 시 268초까지 감소할 수 있습니다. 이렇게 STA는 기존 방법보다 최대 3.53배의 전체적인 속도 향상을 달성하며, 비디오 생성의 품질을 유지합니다.

### [Goku: Flow Based Video Generative Foundation Models](https://arxiv.org/abs/2502.04896)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04896.png)

Vote: 34

Authors: Yifei Hu, Yanghua Peng, Hongxiang Hao, Fu Li, Ting-Che Lin, Yida Zhang, Yuqi Zhang, Xiaobing Liu, Peize Sun, Ping Luo, Chuan Li, Xing Wang, Shoufa Chen, Yi Jiang, Chongjian Ge, Shilong Zhang, Hao Yang, Zhichao Lai, Bingyue Peng, Hui Wu, Zehuan Yuan, Fengda Zhu

- ***What's New***: Goku는 사진과 비디오를 함께 생성할 수 있는 최신의 VAE 기반의 변환기 모델을 소개합니다. Goku는 정류된 플로우 변환기(rectified flow Transformer)를 활용하여 새로운 산업 표준의 성능을 이끌어내며, 이미지와 비디오 공동 생성에서 우수한 품질을 보장합니다.
- ***Technical Details***: Goku는 3D 이미지-비디오 VAE와 'rectified flow' 접근을 통한 확장 가능한 변환기 아키텍처를 사용합니다. 데이터 파이프라인은 OCR, 미적 분석을 통한 엄격한 데이터 필터링을 포함하며, 36M 비디오-텍스트 쌍과 160M 이미지-텍스트 쌍 수집으로 대규모 훈련을 지원합니다. 플로우 기반 훈련에서는 정류된 플로우 알고리즘을 사용하여 클래스 조건 생성의 수렴 속도를 빠르게 합니다.
- ***Performance Highlights***: Goku는 GenEval에서 0.76, DPG-Bench 텍스트-이미지 생성을 위한 벤치마크에서 83.65, 그리고 VBench 텍스트-비디오 태스크에서 84.85 점수를 기록하였습니다. 특히 텍스트-비디오 생성에서 UCF-101 테스트에서 기존의 상업 모델들을 뛰어넘는 성능을 보여주며 비디오 생성 품질의 새로운 기준점을 설정합니다.

### [QuEST: Stable Training of LLMs with 1-Bit Weights and Activations](https://arxiv.org/abs/2502.05003)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.05003.png)

Vote: 28

Authors: Jiale Chen, Roberto L. Castro, Andrei Panferov, Mahdi Nikdan, Soroush Tabesh, Dan Alistarh

- ***What's New***: QuEST는 대형 언어 모델(LLMs)을 1-bit 가중치와 활성화로 안정적으로 훈련할 수 있는 새로운 Quantization-Aware Training(QAT) 방법입니다. 이 방법은 FP16 과 비교하여 낮은 모델 크기로도 더 높은 정확도를 제공합니다.
- ***Technical Details***: QuEST는 Hadamard 정규화와 MSE-optimal fitting을 통해 가중치와 활성화의 연속 분포를 정확하게 양자화하는 것을 특징으로 하며, 신뢰도(trust) 그라디언트 추정기를 새로운 관점에서 제시합니다. 이 방법은 양자화된 상태에서 계산된 noisy gradient와 정확한 full-precision gradient 간의 오차를 최소화하는 것을 목표로 합니다. 이러한 절차는 모델을 확장할 때에도 안정적인 성능을 유지하게 합니다.
- ***Performance Highlights***: QuEST는 Llama 유형의 아키텍처에 대해 실험을 실시한 결과, 1-bit 가중치와 활성화에서의 안정적인 훈련이 가능함을 보였고, 이로 인해 4-bit 설정이 현재의 BF16보다 더 효율적인 부하 크기 대비 낮은 손실을 기록했습니다. 또한 이 방법이 Sparse 표현으로도 확장 가능함을 증명했습니다. QuEST로 생성된 모델은 상용 하드웨어에서 효율적으로 실행이 가능하도록 GPU 커널 지원을 제공합니다.

### [Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth Approach](https://arxiv.org/abs/2502.05171)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.05171.png)

Vote: 26

Authors: Brian R. Bartoldson, John Kirchenbauer, Sean McLeish, Siddharth Singh, Neel Jain, Abhinav Bhatele, Tom Goldstein, Jonas Geiping, Bhavya Kailkhura

- ***What's New***: 이 연구는 테스트 시점에서 컴퓨팅 확장을 위한 새로운 접근 방식을 제안합니다. 본 연구의 모델은 잠재 공간(Latent Space)에서의 추론을 반복적인 블록을 통해 수행하며, 이는 기존의 체인 오브 쌓기(Chain-of-Thought) 방식과 달리 특별한 트레이닝 데이터는 필요하지 않습니다. 이 모델은 35억 개의 매개변수와 8000억 개의 토큰으로 확장이 가능하며, 테스트 시점에서 500억 매개변수에 해당하는 컴퓨팅 부하에서 성능을 향상시킵니다.
- ***Technical Details***: 제안된 아키텍처는 잠재 공간에서 반복적 추론을 수행하는 심층 재귀 블록을 기반으로 합니다. 트레이닝 중에는 무작위 반복 횟수가 사용되며, 주어진 잠재 상태와 확률적으로 생성된 매개 변수를 통해 성능 향상을 목표로 합니다. 이 모델은 입력 데이터에 여러 층이 포함되어 있어 심층적인 반복적 연산을 수행하고, 자가(KV)캐시 공유, 자가 추측 디코딩 등 다양한 기능을 내장하고 있어 타 모델 대비 효율적입니다.
- ***Performance Highlights***: 제안된 모델은 오픈소스 모델들과 비교했을 때, 수학 및 코딩 기준에서 상당한 성능 향상을 보였습니다. 특히 수학적 추론에서 중요한 부분인 GSM8K 및 Minerva Math 벤치마크에서 높은 점수를 기록했으며, 이는 더 많은 컴퓨팅 자원을 활용할수록 그 성능이 꾸준히 향상됨을 보여줍니다. 본 논문의 결과는 잠재 공간에서의 재귀적 추론 접근법이 테스트 시점의 컴퓨팅 확장을 통한 효율적인 성능 향상에 유망한 방향임을 시사합니다.

### [AuraFusion360: Augmented Unseen Region Alignment for Reference-based 360° Unbounded Scene Inpainting](https://arxiv.org/abs/2502.05176)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.05176.png)

Vote: 23

Authors: Jie-Ying Lee, Chun-Wei Tuan Mu, Yu-Lun Liu, Yen-Yu Lin, Yi-Chuan Huang, Bo-Hsu Ke, Min-Hung Chen, Chung-Ho Wu, Yang-Jung Chen, Ying-Huan Chen, Chin-Yang Lin

- ***What's New***: AuraFusion360는 360° 무제한 장면 보완(Reference-based 360° Unbounded Scene Inpainting)에 최적화된 새로운 기법으로, 3D 장면에서의 객체 제거와 구멍 채우기를 고품질로 처리할 수 있습니다. 이 연구는 깊이 인지사(mask generation), 적응적 지도 깊이 확산(Adaptive Guided Depth Diffusion), SDEdit 기반 디테일 향상을 도입하여 다중 뷰 일관성을 유지하며 인페인팅 정확도를 높였습니다. 또한 360-USID라는 첫 번째 포괄적인 데이터셋을 제안했습니다.
- ***Technical Details***: AuraFusion360는 입력 이미지, 카메라 매개변수, 객체 마스크, 참조 이미지로부터 Gaussian Splatting 표현으로 새로운 뷰를 생성합니다. 깊이 인지사 생성 및 적응적으로 정렬된 시작 지점으로 새로운 Gaussian을 초기화하고, SDEdit을 통해 다중 뷰 일관성을 유지하며 세부사항을 향상시킵니다.
- ***Performance Highlights***: AuraFusion360은 기존 방법들에 비해 인페인팅 품질 및 기하학적 정확도가 향상되었으며, 특히 복잡한 형상 및 큰 뷰포인트 변화에 강력함을 보였습니다. PSNR 및 LPIPS 점수에서 최고 성능을 발휘했으며, 360-USID 데이터셋의 다양한 장면에서 일관된 우수한 성능을 입증했습니다.

### [DuoGuard: A Two-Player RL-Driven Framework for Multilingual LLM Guardrails](https://arxiv.org/abs/2502.05163)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.05163.png)

Vote: 17

Authors: Bo Li, Yihe Deng, Wei Wang, Junkai Zhang, Yu Yang

- ***What's New***: DuoGuard는 다국어 대형 언어 모델(LLM; Large Language Models)을 위한 가드레일(Guardrails) 모델 훈련을 위해 두 명의 플레이어가 참여하는 강화학습(Reinforcement Learning; RL) 기반의 새로운 프레임워크를 제안합니다. 이는 생성자와 가드레일 모델이 대립적으로 공동 진화하여 다국어 가드레일 훈련을 위한 고품질의 합성 데이터를 생성하는 혁신적인 접근 방식입니다.
- ***Technical Details***: DuoGuard는 두 플레이어 게임으로서의 상호작용을 이론적으로 형식화하고, Nash 평형으로의 수렴을 증명했습니다. 데이터 생성기와 가드레일 분류기가 포함된 반복적 두 플레이어 RL 프레임워크를 제안하여 합성 데이터 생성과 분류기 훈련의 지속적인 개선을 가능케 합니다. 이 프레임워크는 29개 언어를 지원하는 Qwen-2.5 모델을 기반으로 개발되었으며, 특히 자원이 적은 언어 간 데이터 불균형 문제를 해결하는 데 중점을 두고 있습니다.
- ***Performance Highlights***: DuoGuard는 영어 벤치마크에서 LlamaGuard3 (8B) 대비 거의 10%의 개선을 보여주었으며, 0.5B의 훨씬 작은 모델로 추론 속도가 4.5배 빠릅니다. 또한, 다양한 데이터셋에서 유사한 규모의 최첨단 모델을 평균적으로 20% 이상 능가하였으며, 더 큰 모델 대비 약 10%의 평균 개선을 달성했습니다. 이는 DuoGuard의 다국어 안전성 작업에서의 상당한 발전을 보여주는 결과입니다.

### [Agency Is Frame-Dependent](https://arxiv.org/abs/2502.04403)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04403.png)

Vote: 13

Authors: Razvan Pascanu, Satinder Singh, Doina Precup, Shi Dong, Georgios Piliouras, Clare Lyle, Mark Rowland, André Barreto, David Abel, Steven Hansen, Jonathan Richens, Khimya Khetarpal, Michael Bowling, Anna Harutyunyan, Will Dabney, Tom Schaul

- ***What's New***: 이번 연구는 시스템의 에이전시(Agency)가 근본적으로 참조 프레임(reference frame)에 의존한다는 점을 재조명합니다. 철학적 논증을 통해 에이전시의 각 중요한 특성들이 프레임 의존적이며, 이를 통해 강화 학습(Reinforcement Learning; RL)의 기초과학에서 프레임 의존성이 불가피함을 주장합니다.
- ***Technical Details***: 에이전시를 네 가지 특성으로 나누어 설명합니다: 개별성(Individuality), 행동의 근원(Source of Action), 목표 지향성(Normativity), 적응성(Adaptivity). 각 특성을 평가할 때, 이를 결정짓는 외부 참조물이 필요하며 이는 임의의 선택에 따라 다를 수 있습니다. 예를 들어, 개별성을 결정할 때 경계선을 설정하는 방식이 다양할 수 있으며, 이는 다양한 참조 프레임에 따라 다르게 해석될 수 있습니다.
- ***Performance Highlights***: 본 논문은 에이전시가 참조 프레임에 의존한다는 주장을 철학적으로 실증하는 데 중점을 두었습니다. 구체적인 성능지표는 제공되지 않았으나, 에이전시의 프레임 의존적 성격이 강화 학습 및 인공지능 시스템 연구에 중요한 통찰을 줄 수 있음을 시사합니다.

### [Generating Symbolic World Models via Test-time Scaling of Large Language Models](https://arxiv.org/abs/2502.04728)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04728.png)

Vote: 12

Authors: Tim Z. Xiao, Ge Zhang, Weiyang Liu, Zhouliang Yu, Yuhuan Yuan, Jie Fu, Ge Lin, Fuxiang Frank Xia

- ***What's New***: 이 논문은 대형 언어 모델(LLM; Large Language Models)을 통해 테스트 시간(Test-time)에서 계산 능력을 확장하여 PDDL 기반의 상징적 세계 모델을 생성하는 새로운 방법을 제안합니다. 이 방법은 추가적인 교육 없이 LLM의 PDDL 추론 능력을 강화하여 고품질의 PDDL 도메인을 생성할 수 있도록 설계되었습니다.
- ***Technical Details***: 제안된 방법은 Best-of-N 샘플링을 사용하여 초기 해를 생성한 후 인스턴스 가시화 머신 러닝(iVML)을 통해 솔루션을 미세하게 조정합니다. 이 프로세스는 다양한 가능성을 탐색하여 초기 솔루션을 찾아내고, 그 해결책을 iVML 기술을 통해 점진적으로 개선합니다. 이는 솔루션 스페이스를 탐험하고, 주변 해를 최적화함으로써 가장 최적의 PDDL 기반 세계 모델을 얻습니다.
- ***Performance Highlights***: 우리의 방법은 NL2Domain 작업에서 85.2%의 성공률을 기록하며, Prob2Domain 작업에서는 71.4%의 성공률을 기록했습니다. 이는 상태 추상화를 활용하여 기존의 LLM 계획 방법보다 더욱 강력한 성능을 보여줍니다. 또한, PDDL 기반의 공식적인 추상화는 보다 강력한 계획을 가능하게 하며, 복잡한 도메인을 처리하는 데 성공적이었습니다.

### [FlashVideo:Flowing Fidelity to Detail for Efficient High-Resolution Video Generation](https://arxiv.org/abs/2502.05179)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.05179.png)

Vote: 10

Authors: Shoufa Chen, Zehuan Yuan, Wenbo Li, Yida Zhang, Yi Jiang, Peize Sun, Chongjian Ge, Shilong Zhang, Ping Luo, Binyue Peng

- ***What's New***: FlashVideo는 텍스트-비디오 생성의 효율성을 높이기 위해 이단계 형태(two-stage framework)를 도입한 새로운 방법입니다. 첫 번째 단계는 저해상도 동영상을 생성하여 사용자의 프롬프트와의 일치를 우선시하며, 두 번째 단계에서는 흐름 맞춤(flow matching)을 통해 저해상도와 고해상도 간의 조화를 이루어 세부 사항을 효율적으로 생성합니다.
- ***Technical Details***: 첫 번째 단계에서는 큰 모델 파라미터와 충분한 함수를 평가하여 270p 해상도에서 비디오를 효율적으로 생성합니다. 두 번째 단계에서는 2억 파라미터 모델과 최소한의 함수 평가를 통해 고해상도(1080p) 비디오를 생성합니다. 또한, 두 단계 설계는 사용자가 결과를 미리 확인한 후 전체 해상도 생성으로 전환할 수 있도록 하여 상업적 효용성을 높입니다.
- ***Performance Highlights***: FlashVideo는 VBench-Long에서 82.99의 점수를 기록하며 최상위 성능을 발휘하고 있습니다. 전체 기능 평가 시간은 약 102초로, 기존 방법보다 훨씬 효율적이며 고해상도 비디오 생성을 지원합니다. 이 방법은 사용자가 30초 내에 초기 출력을 보고 다음 단계로 이동할지 결정할 수 있도록 하여 사용자 경험을 크게 향상시킵니다.

### [No Task Left Behind: Isotropic Model Merging with Common and Task-Specific Subspaces](https://arxiv.org/abs/2502.04959)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04959.png)

Vote: 8

Authors: Andrew D. Bagdanov, Bartłomiej Twardowski, Joost van de Weijer, Sebastian Cygert, Daniel Marczak, Simone Magistri

- ***What's New***: Isotropic Model Merging은 다중 태스크 모델 합병에서 태스크 특화 모델들의 가중치를 하나의 다중 태스크 모델로 통합하며, 공통 및 태스크 별 서브스페이스를 활용하여 모델의 성능 격차를 줄이는 새로운 프레임워크입니다.
- ***Technical Details***: 이 연구에서는 태스크 매트릭스(Singular Value Spectrum)의 정렬이 사전 훈련 모델 대비 성능 향상과 강하게 관련됨을 밝혀냈습니다. 이를 바탕으로 개별 태스크 및 합병된 매트릭스의 싱귤러 성분의 정렬을 개선하고 성능 격차를 줄이는 Isotropic Merging 프레임워크를 제안합니다. 또한, 공통 및 태스크 특화 서브스페이스를 포함하여 모델 성능을 더욱 향상시킵니다.
- ***Performance Highlights***: 최신 기법들과 비교할 때, 제안된 Iso-C 및 Iso-CTS 모델은 다양한 태스크 및 모델 스케일에서 state-of-the-art 성능을 기록하였으며, 특히 태스크 수가 많은 경우 두드러진 성능 향상을 보였습니다.

### [Linear Correlation in LM's Compositional Generalization and Hallucination](https://arxiv.org/abs/2502.04520)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04520.png)

Vote: 8

Authors: Shibo Hao, Chengyu Dong, Jingbo Shang, Chenyang An, Letian Peng

- ***What's New***: 이 논문은 언어 모델(LM)의 조합적 일반화(Compositional Generalization)와 환상(Hallucination)에서 발생하는 선형 상관관계를 밝혀냈습니다. 관련 지식 간의 다음 토큰 예측 출력 로그잇(Logits) 사이에 선형 변환이 존재하는 현상을 처음으로 분석하였으며, 이는 인간의 지식 구성에서도 발견되는 선형성을 반영합니다.
- ***Technical Details***: 지식 조합에서 전국적 상관관계의 존재를 확인하기 위해, 도시와 국가 간의 로그잇 변환에 적합한 선형 변환(W, b)을 학습했습니다. 이 선형성은 대규모 미세 조정 후에도 유지되며, 이는 언어 모델의 지식 일반화에 중요한 역할을 합니다. 우리의 연구는 이와 같은 선형 상관관계가 어휘 표현에 의해 크게 좌우된다고 결론지었습니다.
- ***Performance Highlights***: 선형 상관관계는 고도로 상관된 지식(pair)에서는 일반적으로 높은 상관계수를 나타내며, 이는 관계가 적절히 조정될 경우 정확한 일반화를 가능하게 합니다. 반면, 적절하지 않은 관계에서는 이러한 상관관계가 '조합적 환상'을 유발할 수 있습니다.

### [Step Back to Leap Forward: Self-Backtracking for Boosting Reasoning of Language Models](https://arxiv.org/abs/2502.04404)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04404.png)

Vote: 7

Authors: Wen-Da Wei, Xuan-Yi Zhu, Yu-Feng Li, Lan-Zhe Guo, Zhi Zhou, Xiao-Wen Yang, Jie-Jing Shao, Ding-Chu Zhang

- ***What's New***: 이 논문은 대형 언어 모델(LLMs)의 추론 능력을 강화하기 위해 자가 백트래킹(Self-Backtracking) 메커니즘을 제안합니다. 이 메커니즘은 LLMs가 학습 및 추론 중 백트래킹을 수행할 수 있도록 하여, 비효율적인 깊이 사고를 빠른 사고로 전환하도록 설계되었습니다.
- ***Technical Details***: Self-Backtracking은 LLMs가 백트래킹이 필요한 시점과 위치를 학습하도록 훈련합니다. 이 기법은 LLM이 초기 예측이 최적이 아님을 인식하고, 초기 상태로 돌아가 다른 가능성을 탐색하도록 합니다. 실험에서는 Countdown 작업을 사용하여 다양한 규모의 모델에서 방법의 효과를 평가하였으며, 이로 인해 LLMs의 추론 성능이 크게 향상되었습니다.
- ***Performance Highlights***: Self-Backtracking 기법은 기본 최적 경로 기반 비교 방식인 SFT(Supervised Fine-Tuning) 방법 대비 40% 이상의 성능 향상을 이루었습니다. 특히, 이 기법은 간단한 문제에서 불필요한 백트래킹을 피하면서 외부 보상 모델에 대한 의존성을 줄이고, LLMs의 추론 효율성을 향상시키는 것으로 나타났습니다.

### [CMoE: Fast Carving of Mixture-of-Experts for Efficient LLM Inference](https://arxiv.org/abs/2502.04416)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04416.png)

Vote: 7

Authors: Hui-Ling Zhen, Lancheng Zou, Xianzhi Yu, Zehua Pei, Sinno Jialin Pan, Mingxuan Yuan, Bei Yu, Wulong Liu

- ***What's New***: CMoE(Carved Mixture-of-Experts)는 대형 언어 모델(LLM)로부터 Mixture-of-Experts(MoE)를 빠르고 효율적으로 추출하는 새로운 프레임워크입니다. 이는 높은 활성화 희소성을 이용해 Dense 모델에서 MoE 구조를 훈련 없이 형성할 수 있게 합니다. CMoE는 전문가 그룹화와 경량화된 적응을 통해 높은 성능 회복 능력을 보여주며, 코드도 공개되어 있습니다.
- ***Technical Details***: CMoE는 활성화 희소성을 이용하여 Dense LLM에서 FFN의 뉴런을 공유 전문가(Shared Experts)와 라우팅 전문가(Routed Experts)로 그룹화합니다. 각 전문가 집단은 Jonker-Volgenant 알고리즘을 사용한 균형적 선형 할당 문제로 형성되며, 라우터는 기존 활성화 통계를 통해 차별화 없이 직접 생성됩니다. 추가로, 라우팅 기능은 미세한 조정과 성능 회복을 위해 차별화 가능하고 로드 밸런싱이 적용됩니다.
- ***Performance Highlights***: CMoE는 퍼플렉시티가 25%의 활성화 비율에서도 적절하게 유지되며, 2,048개의 예제에서 진행된 경량화된 미세 조정 후 다운스트림 벤치마크에서 Dense 모델의 76.59%에 해당하는 정확도를 기록했습니다. 특히 SciQ 데이터셋에서는 미세 조정 없이도 Dense 모델의 56.3%, 미세 조정 후에는 76.59%의 정확도를 달성하며, 기존 LLaMA-MoE와 비교해 뛰어난 성능을 보였습니다.

### [Scaling Laws in Patchification: An Image Is Worth 50,176 Tokens And More](https://arxiv.org/abs/2502.03738)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.03738.png)

Vote: 6

Authors: Yaodong Yu, Feng Wang, Alan Yuille, Yuyin Zhou, Guoyizhe Wei, Wei Shao, Cihang Xie

- ***What's New***: 본 논문은 비전 트랜스포머(Vision Transformer; ViT)와 같은 이미지 변환 아키텍처에서 패치화(Patchification)의 정보 손실을 분석하고, 패치의 크기를 줄이면 예측 성능이 향상된다는 새로운 스케일링 법칙(Scaling Laws)을 설명합니다. 이는 다양한 시각적 작업에 적용 가능하며, Pixel 형태의 토큰화를 통해 압축을 최소화합니다.
- ***Technical Details***: 본 연구에서는 ViT 및 최근의 Mamba 기반의 효율적 아키텍처를 사용하여, 패치 크기를 16×16에서 1×1까지 줄임으로써 이미지의 시퀀스를 50,176 토큰 길이로 확장했습니다. 또한, 특정 작업을 위한 디코더 헤드의 중요성이 작은 패치에서 감소함을 발견하며, 범용 인코더 기반 비전 모델 개발의 가능성을 열었습니다.
- ***Performance Highlights***: ImageNet-1k 벤치마크에서 224×224 해상도로 입력한 모델은 패치 크기를 줄일 때 모델의 테스트 정확도가 82.6%에서 84.6%로 향상되었습니다. 이는 패치화를 줄임으로써 정보 손실을 최소화한 결과로, 독립적으로 평가된 다른 비시각적 작업들도 유사한 성능 향상을 보였습니다.

### [On-device Sora: Enabling Diffusion-Based Text-to-Video Generation for Mobile Devices](https://arxiv.org/abs/2502.04363)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04363.png)

Vote: 6

Authors: Seulki Lee, Bosung Kim, Jungmin Cheon, Yeojin Lee, Isu Jeong, Kyuhwan Lee

- ***What's New***: On-device Sora는 확산 기반 텍스트-비디오 생성(diffusion-based text-to-video generation)을 스마트폰과 같은 제한된 자원 환경에서도 가능하게 만든 최초의 솔루션입니다. 이는 Open-Sora를 기반으로 세 가지 기술을 적용하여 모바일 장치에서의 비디오 생성의 계산 및 메모리 한계를 극복합니다: Linear Proportional Leap(LPL), Temporal Dimension Token Merging(TDTM), 그리고 Concurrent Inference with Dynamic Loading(CI-DL)입니다.
- ***Technical Details***: On-device Sora는 비디오 확산의 과도한 디노이징 단계를 줄이기 위해 LPL을 사용하고, STDiT 내 attention layers에서 연속되는 토큰을 병합하여 TDTM을 수행합니다. 또한, 메모리 제약 문제를 해결하기 위해 동적 로딩과 모델 블록의 동시 추론을 제공하는 CI-DL을 제안합니다. iPhone 15 Pro에서의 구현을 통해 제한된 자원을 가진 모바일 장치에서도 높은 품질의 비디오 생성이 가능함을 보여주었습니다.
- ***Performance Highlights***: 실험 결과, On-device Sora는 Open-Sora와 유사한 수준의 비디오 품질을 제공하면서도, 최대 1.94배 더 빠른 비디오 생성 속도를 보였습니다. 이는 모바일 장치에서도 고성능 비디오 생성이 가능하다는 것을 시사합니다. VBench 벤치마크를 통해 대부분의 평가 지표에서 높은 점수를 기록하였으며, 일부 프레임 수준 품질에서 약간의 감소를 확인할 수 있었습니다.

### [QLIP: Text-Aligned Visual Tokenization Unifies Auto-Regressive Multimodal Understanding and Generation](https://arxiv.org/abs/2502.05178)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.05178.png)

Vote: 6

Authors: Yuke Zhu, Yue Zhao, De-An Huang, Philipp Krähenbühl, Jan Kautz, Linxi Fan, Zhiding Yu, Scott Reed, Fuzhao Xue

- ***What's New***: 이 논문에서는 QLIP(Quantized Language-Image Pretraining)이라는 새로운 시각적 토큰화 방법을 제안합니다. 이는 최신 복원 품질과 제로샷(Zero-shot) 이미지 이해 능력을 결합합니다. QLIP는 시각-언어 정렬 목표와의 결합을 통해 다양한 멀티모달 이해 및 텍스트에 조건부된 이미지 생성을 하나의 모델로 수행할 수 있습니다.
- ***Technical Details***: QLIP는 Binary Spherical Quantization(BSQ)를 기반으로 한 오토인코더를 사용하여 시각적-언어적 조정 목표를 달성합니다. 훈련은 두 단계로 진행되며, 첫 번째 단계는 MSE 손실과 메모리 효율적인 Transformer 구조를 통한 조정으로 구성됩니다. 두 번째 단계에서 텍스트 인코더를 삭제하고, 시각적 인코더를 고정하여 세부 조정을 수행합니다. 이렇게 함으로써 멀티모달 정렬을 초기 단계에서 수행하여, 효과적인 시각 언어모델링을 구현합니다.
- ***Performance Highlights***: QLIP는 기존의 시각적 토큰화 방식과 비교하여 경쟁력 있는 복원 결과를 보여주며, 광범위한 멀티모달 이해 및 생성 벤치마크에서도 효과적인 성능을 입증하였습니다. LLaVA 기반 멀티모달 모델에서, QLIP는 유사한 입력 해상도 하에 CLIP-단독 기본과 비교했을 때 마진 손실이 적습니다. 이로써 시각적 토큰화가 VLM(Vision-Language Models)에서 성능 저하를 일으킨다는 기존의 믿음과 대조되는 결과를 보여줍니다.

### [Lost in Time: Clock and Calendar Understanding Challenges in Multimodal LLMs](https://arxiv.org/abs/2502.05092)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.05092.png)

Vote: 5

Authors: Pasquale Minervini, Rohit Saxena, Aryo Pradipta Gema

- ***What's New***: 이 연구는 멀티모달 대형 언어 모델(MLLMs)이 아날로그 시계와 연간 달력을 해석하는 능력을 어떻게 평가할 수 있는지를 조사하며, 새로운 데이터셋인 ClockQA와 CalendarQA를 통해 MLLMs가 시간과 날짜 관련 시각적 데이터를 처리하는 능력을 분석합니다.
- ***Technical Details***: ClockQA는 다양한 아날로그 시계 이미지와 시간 관련 질문으로 구성되어 있으며, CalendarQA는 연간 달력 이미지와 질문으로 구성됩니다. 연구는 시각적 인식, 수치적 추론, 시간 추론을 평가하기 위해 설계되었습니다. 여섯 가지 시계 스타일(로마 숫자, 초침 없음 등)과 10년의 달력 이미지를 포함한 소규모 테스트 세트를 구축하고, 최신의 닫힌 소스와 오픈 소스 모델들을 평가하였습니다.
- ***Performance Highlights***: ClockQA에서 Gemini-2.0은 가장 낮은 시간 및 분 오류를 기록하며 상대적으로 강력한 시계 읽기 능력을 보여 주었지만, 전체적인 EM 점수는 여전히 낮아 MLLMs의 시계 읽기 어려움을 나타냅니다. CalendarQA에서는 GPT-o1이 80%의 높은 정확도를 달성하여 강력한 날짜 산술 및 추론 능력을 보여주었습니다. 그러나 MLLMs는 전체적으로 시간 및 날짜 정보 해석에서 어려움을 겪고 있음을 확인했습니다.

### [CodeSteer: Symbolic-Augmented Language Models via Code/Text Guidance](https://arxiv.org/abs/2502.04350)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04350.png)

Vote: 4

Authors: Yang Zhang, Yueying Liu, Yilun Hao, Yongchao Chen, Chuchu Fan

- ***What's New***: CodeSteer는 대형 언어 모델(LLMs)의 코드/텍스트 생성 경로를 효율적으로 안내하는 새로운 방법론입니다. 이 논문은 37개의 상징적 작업을 포함하는 섬벤치(SymBench)라는 종합적인 벤치마크를 구축하여 다양한 복잡도를 가진 12,000개의 다단계 가이드 생성 경로와 5,500개의 가이드 비교 쌍의 데이터셋을 합성합니다.
- ***Technical Details***: CodeSteer는 Llama-3-8B 모델을 다단계 지도형 미세 조정(SFT)과 직접 선호 최적화(DPO)로 미세 조정하여 과제 LLM(TaskLLM)을 통해 다중 라운드의 상호 작용을 통해 코드/텍스트 생성을 안내합니다. 또한, 상징적 검사기와 자기-답변 검사기라는 새로운 도구를 도입하여 상징 계산 능력을 완전히 활용하는 것을 지원합니다.
- ***Performance Highlights***: CodeSteer를 도입한 GPT-4o 모델은 평균 성능 점수를 53.3에서 86.4로 크게 향상시켰으며, 기존의 최고 성능을 기록한 OpenAI o1과 DeepSeek R1 모델을 능가했습니다. CodeSteer's 적용은 다른 큰 모델들에서도 일반화되어, 클로드(Claude), 미스트랄(Mistral), GPT-3.5에서도 평균 41.8의 성능 향상을 가져왔습니다.

### [YINYANG-ALIGN: Benchmarking Contradictory Objectives and Proposing Multi-Objective Optimization based DPO for Text-to-Image Alignment](https://arxiv.org/abs/2502.03512)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.03512.png)

Vote: 4

Authors: Aman Chadha, Gurpreet Singh, Yaswanth Narsupalli, Suranjana Trivedy, Amitava Das, Vasu Sharma, Amit Sheth, Vinija Jain

- ***What's New***: YinYang-Align은 텍스트-이미지 정렬(Text-to-Image Alignment) 시스템의 충돌하는 목표들을 평가하기 위한 고급 벤치마크 프레임워크입니다. 이는 총 여섯 가지 상충하는 설계 목표를 해결합니다. 또한, DPO(Direct Preference Optimization)를 확장한 새로운 Contradictory Alignment Optimization (CAO) 프레임워크를 소개하여, 충돌하는 목표들을 다중 목표 최적화로 해결합니다.
- ***Technical Details***: YinYang-Align 프레임워크는 인간의 프롬프트, 정렬된 응답, 잘못 정렬된 AI 생성 출력 및 상충의 설명을 특징으로 하는 데이터셋을 활용합니다. CAO는 상충되는 목표를 명시적으로 모델링하고 해결하기 위한 각주 단위 손실 설계를 도입하며, 효율적인 다중 목표 최적화를 위해 상호 작용 기반 글로벌 선호도 및 참신한 시너지 야코비안을 포함합니다. 또한, Sinkhorn 정규화된 Wasserstein Distance를 사용하여 안정성과 확장성을 확보합니다.
- ***Performance Highlights***: CAO는 모든 여섯 가지 상충되는 정렬 목표에서 새로운 성능 벤치마크를 설정하였습니다. 실험적으로, 각 단일 축을 최적화한 DPO 모델과 비교하여 CAO는 모든 목표 간의 균형 잡힌 정렬을 달성하며, 단일 축 최적화가 다른 축에 미치는 부정적 영향을 줄였습니다. 실험 결과에 따르면, DPO-CAO는 DPO에 비해 조화를 이루며 대운영적인 정렬을 달성하였습니다.

### [Value-Based Deep RL Scales Predictably](https://arxiv.org/abs/2502.04327)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04327.png)

Vote: 3

Authors: Charlie Snell, Pieter Abbeel, Preston Fu, Michal Nauman, Aviral Kumar, Oleh Rybkin, Sergey Levine

- ***What's New***: 이 논문은 가치 기반 오프 폴리시 강화 학습(Value-Based Off-Policy Reinforcement Learning; RL) 방법론이 대규모 데이터 및 계산에서는 예측 가능하게 동작함을 보여줍니다. 특히 이 논문에서는 패레토 프런티어(Pareto Frontier)와 업데이트 대 데이터 비율(Updates-to-Data Ratio; UTD)을 활용하여 리소스 할당을 최적화하고 성능을 최대화할 수 있는 방법을 제시합니다.
- ***Technical Details***: 논문에서는 의료 및 데이터 요구 사항이 패레토 프런티어 상에 위치하고, 이러한 관계는 예측 가능하다는 것을 증명합니다. 이를 통해 작은 규모의 실험 데이터로부터 대규모 실험의 성능을 예측할 수 있습니다.  세 가지 알고리즘(SAC, BRO, PQL)을 사용하여 다양한 플랫폼(DeepMind Control, OpenAI Gym, IsaacGym)에서 검증하였습니다. 또한, 최적의 UTD 비율이 리소스 할당 및 데이터를 바탕으로 예측될 수 있는 확장 가능한 기능 형태를 도입합니다.
- ***Performance Highlights***: 테스트 결과, 알고리즘의 성능은 적은 예산과 데이터 상황에서도 최적의 하이퍼파라미터 설정을 통해 높은 효율성을 달성할 수 있었음을 보여주었습니다. 이 모델은 적은 규모에서 얻은 법칙으로도 큰 규모까지 확장 가능한 예측을 가능하게 하였습니다.

### [MEETING DELEGATE: Benchmarking LLMs on Attending Meetings on Our Behalf](https://arxiv.org/abs/2502.04376)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04376.png)

Vote: 3

Authors: Saravan Rajmohan, Xiaoting Qin, Lingxiang Hu, Dongmei Zhang, Qi Zhang, Shurun Yuan, Qingwei Lin, Jue Zhang

- ***What's New***: 이 연구는 대형 언어 모델(LLMs)을 활용하여 회의 참석자 역할을 대신하는 시스템을 개발하고, 실제 회의 대본을 활용한 포괄적인 벤치마크를 구축한 것입니다. GPT-4/4o는 적극적이고 신중한 참여 전략 사이에서 균형 잡힌 성능을 유지한 반면, Gemini 1.5 Pro는 더 신중하고 Gemini 1.5 Flash와 Llama3-8B/70B은 더 적극적인 경향을 보였습니다.
- ***Technical Details***: LLM 기반 회의 대리 시스템은 참가자의 역할에 초점을 맞춰 개발되었습니다. 실제 회의 대본에서 생성된 벤치마크는 명시적 단서(Explicit Cue), 암묵적 단서(Implicit Cue), 참견(Chime In), 침묵 유지(Keep Silence)와 같은 네 가지 일반적인 시나리오를 포괄합니다. 이러한 시스템의 효과를 평가하기 위해, 몇 가지 데모 시나리오와 벤치마크를 활용하여 실험적 평가를 수행했습니다.
- ***Performance Highlights***: 실험 결과, GTP-4/4o는 응답율과 침묵율에서 0.7에서 0.8 사이의 균형 잡힌 성능을 보였습니다. Gemini 시리즈 중에서는 Gemini 1.5 Pro가 가장 높은 침묵율(약 0.9)을 기록했지만 반응율은 낮았습니다. 이는 수행 전략의 신중성을 반영합니다. 실험에서는 다양한 LLMs가 사용되었으며, 약 60%의 답변이 적어도 하나의 핵심 포인트를 다루었습니다. 그러나 현재 모델들은 실질적인 설정에서 자주 발생하는 전사 오류에 대한 내성을 높이는 개선이 필요합니다.

### [ARR: Question Answering with Large Language Models via Analyzing, Retrieving, and Reasoning](https://arxiv.org/abs/2502.04689)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04689.png)

Vote: 2

Authors: Yuwei Yin, Giuseppe Carenini

- ***What's New***: 이 연구는 대형 언어 모델(Large Language Models; LLMs)을 위한 새로운 제로샷 프롬프트 기법인 ARR을 소개합니다. ARR은 질문의 의도를 분석하고, 관련 정보를 회수하며, 단계별로 추론하는 세 가지 주요 단계를 명시적으로 포함합니다. 이는 CoT(연쇄적 사고) 프롬프트 기법보다 일관되게 우수한 성능을 보여줍니다.
- ***Technical Details***: ARR 방식은 '질문 의도를 분석하고, 관련 정보를 찾고, 단계별로 추론하여 답변 합시다'라는 트리거 문장을 활용합니다. 다양한 QA(질문 답변) 작업 세트에서 이 방법의 효과를 평가하기 위해 10개의 다중 선택 QA 데이터셋에서 실험을 수행했습니다. ARR은 Baseline 및 CoT 방법에 비해 일관되게 성능을 개선합니다.
- ***Performance Highlights***: ARR 방법은 모든 QA 데이터셋에서 Baseline 대비 평균 4.1% 성능을 개선했으며, 특히 '의도 분석'의 중요성을 강조하여 CoT 방법에 비해 우수한 성능을 보였습니다. 다양한 모델 크기, LLM 시리즈, 생성 구성 설정에서도 ARR이 높은 효과성과 일반성을 확인했습니다.

### [Continuous 3D Perception Model with Persistent State](https://arxiv.org/abs/2501.12387)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.12387.png)

Vote: 1

Authors: Yifei Zhang, Alexei A. Efros, Aleksander Holynski, Angjoo Kanazawa, Qianqian Wang

- ***What's New***: 지속 상태를 가진 연속 3D 인식 모델은 새로운 입력 이미지 스트림에 따라 지속적으로 상태 표현을 업데이트하여 다양한 3D 작업을 해결하는 통합 프레임워크를 소개합니다. 이 모델은 부정합 카메라 움직임과 같은 전통적인 방법에서의 어려움을 다루고, 새로운 관측으로 상태를 온라인으로 업데이트할 수 있습니다.
- ***Technical Details***: 제안하는 모델, CUT3R(Continuous Updating Transformer for 3D Reconstruction)은 재발생 모델(recurrent model)을 사용하여 지속적인 상태 표현(persistent state representation)을 업데이트합니다. 이미지 스트림이 주어지면 각 입력에 대한 메트릭 스케일의 포인트맵(pointmaps)을 생성하며, 이 결과를 누적하여 일관된 밀도 높인 장면 재구성을 가능하게 합니다. 상태 입력 상호 작용 메커니즘은 Visual Transformer(ViT)와 관련된 두 개의 상호 연결된 트랜스포머 디코더(transformer decoders)를 사용하여 구현됩니다.
- ***Performance Highlights***: 본 모델은 다양한 3D 작업에서 경쟁력 있는 성능을 보여주었으며, 특히 동적 및 정적 장면에서 우수한 3D 재구성 성능을 나타냅니다. 본 연구는 온라인 방식으로 기존 많은 최적화 기반 접근법과 동등하거나 더 나은 성능을 보이며, 거의 50배 이상의 속도 향상을 보입니다.

