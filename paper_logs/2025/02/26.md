## Daily Papers (2025-02-26)

### [OmniAlign-V: Towards Enhanced Alignment of MLLMs with Human Preference](https://arxiv.org/abs/2502.18411)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.18411.png)

Vote: 53

Authors: Zicheng Zhang, Kai Chen, Shengyuan Ding, Wenhai Wang, Guangtao Zhai, Weiyun Wang, Hua Yang, Haodong Duan, Jiaqi Wang, Xinyu Fang, Xiangyu Zhao, Haian Huang, Maosong Cao

- ***What's New***: OmniAlign-V는 MLLMs(Multi-Modal Large Language Models)의 인간 선호도 정렬을 향상시키기 위한 종합 데이터셋으로, 다양한 이미지와 복잡한 질문, 그리고 다양한 응답 형식을 포함한 200K의 고품질 학습 샘플을 제공합니다. 또한, MM-AlignBench라는 새로운 벤치마크를 소개하여, MLLMs의 인간 가치 정렬 능력을 평가합니다.
- ***Technical Details***: OmniAlign-V는 자연 이미지와 인포그래픽 이미지를 포함하는 다양한 소스에서 수집된 이미지와, 복잡한 지식 기반 질의응답, 창의적 과제, 추론 작업이 포함된 205K의 멀티모달 지도 학습 튜닝 샘플로 구성됩니다. 지도 학습 튜닝은 SFT(Supervised Fine-Tuning) 및 DPO(Direct Preference Optimization)를 통해 수행되며, 이미지 선택 전략을 도입하여 풍부한 의미를 지닌 이미지를 필터링합니다. 이 외에도, MM-AlignBench는 252개의 인간이 주석을 단 고품질 샘플로 구성되며, 다양한 이미지 소스와 질문으로 MLLMs의 인간 선호도 정렬을 포괄적으로 평가하도록 설계되었습니다.
- ***Performance Highlights***: OmniAlign-V를 사용한 MLLMs는 MM-AlignBench와 같은 다수의 멀티모달 벤치마크에서 탁월한 인간 정렬 성능을 보였습니다. 특히 LLaVA-Next와 같은 모델은 OmniAlign-V 데이터로 학습했을 때 기존 밴치마크에서 보다 높은 정렬 성능을 발휘했으며, 일반적인 VQA 벤치마크에서도 동등하거나 더 나은 성능을 보여주었습니다. DPO 트레이닝을 통해서는 MM-AlignBench에서의 성능이 더욱 향상되었습니다.

### [SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference](https://arxiv.org/abs/2502.18137)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.18137.png)

Vote: 40

Authors: Jintao Zhang, Jia Wei, Haocheng Xi, Chendong Xiang, Haofeng Huang, Jianfei Chen, Jun Zhu

- ***What's New***: SpargeAttn은 모든 모델의 추론을 가속화할 수 있는 범용 희소(Universal Sparse) 알고리즘을 제안합니다. 이 방법은 단계별 온라인 필터링을 통해 주의(attention) 맵을 빠르고 정확하게 예측하여 매트릭스 곱셈을 일부 생략할 수 있으며, 추가적인 오버헤드 없이 온라인 softmax-aware 필터를 설계하여 더욱 많은 연산을 생략하게 합니다.
- ***Technical Details***: SpargeAttn은 주의 맵의 희소성(sparsity)을 활용하여 연산을 최적화합니다. 첫 번째 단계에서는 Q, K의 각 블록을 하나의 토큰으로 압축하여 희소 마스크를 예측하며, 두 번째 단계에서는 GPU의 warp 수준에서 일부 연산을 생략할 수 있는 sparse online softmax 알고리즘을 구현합니다. 이 방법은 8비트 양자화된 SageAttention 프레임워크에 통합되어 있으며, CUDA를 통해 구현되어 있습니다.
- ***Performance Highlights***: SpargeAttn은 언어, 이미지 및 비디오 생성 모델에서 기존의 밀집 및 희소 주의 모델보다 2.5배에서 5배 더 빠르게 실행하며, 개별 모델의 최종 성능을 유지합니다. 여러 시퀀스 길이를 가진 모델에서 테스트한 결과, Llama3.1 모델에서의 SpargeAttn는 708.1 TOPS의 속도를 달성하며, 이는 기존 주의 메커니즘보다 훨씬 더 빠른 속도를 보여줍니다.

### [SWE-RL: Advancing LLM Reasoning via Reinforcement Learning on Open Software Evolution](https://arxiv.org/abs/2502.18449)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.18449.png)

Vote: 34

Authors: Quentin Carbonneaux, Gabriel Synnaeve, Yuxiang Wei, Rishabh Singh, Jade Copet, Olivier Duchenne, Daniel Fried, Lingming Zhang, Sida I. Wang

- ***What's New***: SWE-RL은 RL(강화 학습)을 기반으로 하여 소프트웨어 공학(SW) 작업을 개선하는 최초의 방법론으로, PR(풀 리퀘스트)과 같은 소프트웨어 진화 데이터를 사용하여 LLM(대형 언어 모델)의 추론 능력을 향상시킵니다.
- ***Technical Details***: SWE-RL은 GitHub PR 데이터를 이용해 씨드 RL 데이터셋을 생성합니다. 정책 LLM은 PR 기술 설명과 코드 컨텍스트에 기반하여 코드 변경을 생성합니다. 올바른 형식의 응답에 대해서는 예측한 패치와 실제 패치의 유사도에 기반하여 보상이 주어집니다. 이를 통해 코드 변경 과정에서 세밀한 결함 위치를 추론하도록 모델을 교육합니다. Llama3-SWE-RL-70B 모델은 GRPO(그룹 상대적 정책 최적화)를 사용한 최적화로 학습됩니다.
- ***Performance Highlights***: Llama3-SWE-RL-70B는 SWE-bench Verified 기준 41.0%의 해결률을 보이며, 중형 모델(<100B) 중 최고 수준의 성능을 자랑합니다. 특히, RL을 오직 소프트웨어 문제 해결에만 사용했음에도 불구하고, 함수 수준 코딩, 수학, 일반 언어 이해 등 다양한 도메인 외 작업에서도 높은 성능을 보입니다.

### [KV-Edit: Training-Free Image Editing for Precise Background Preservation](https://arxiv.org/abs/2502.17363)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.17363.png)

Vote: 26

Authors: Tianrui Zhu, Jiawei Shao, Yansong Tang, Shiyi Zhang

- ***What's New***: KV-Edit는 이미지 편집에서 배경 일관성을 완벽히 유지할 수 있는 새로운 훈련이 필요 없는 접근 방법을 제안합니다. 이 방법은 복잡한 메커니즘 대신, DiTs(Diffusion Transformer)에서의 KV cache를 이용하여 배경의 키-값 쌍을 보존함으로써 배경을 재생산하지 않고도 새로운 콘텐츠가 자연스럽게 통합될 수 있도록 합니다.
- ***Technical Details***: KV-Edit는 이미지의 배경과 전경을 구분하여 KV cache를 이용, 배경의 키-값을 보존하고 전경만 선택적으로 편집합니다. 이 방법은 디퓨전 모델의 역방향과 비디오화 과정을 이용하며, 추가적인 훈련 없이 다양한 DiT 기반 생성 모델에 쉽게 적용할 수 있습니다. 또한, 거대한 모델에 적합하도록 메모리 최적화를 통해 메모리 복잡성을 O(1)로 줄이는 혁신적인 인버전-프리(inversion-free) 방법을 도입하였습니다.
- ***Performance Highlights***: KV-Edit는 기존의 훈련 기반 및 비훈련 기반 방법을 능가하는 성능을 보였으며, 실험 결과 배경 일관성을 완벽히 유지하면서도 직접적인 텍스트-이미지 생성(T2I synthesis)과 비교할 만한 생성 품질을 유지했습니다. 특히, Cloud Compute에서 높은 수준의 이미지 품질과 텍스트 정렬에서 우수한 평가를 받았으며, 다양한 편집 시나리오에서 탁월한 활용성을 입증하였습니다.

### [ART: Anonymous Region Transformer for Variable Multi-Layer Transparent Image Generation](https://arxiv.org/abs/2502.18364)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.18364.png)

Vote: 21

Authors: Ruihong Yin, Dong Chen, Lin Liang, Lijuan Wang, Xiu Li, Yuhui Yuan, Zhouhui Lian, Gao Huang, Baining Guo, Ji Li, Sirui Zhang, Yifan Pu, Haoxing Ye, Zhicong Tang, Yiming Zhao, Yanbin Wang, Jianmin Bao

- ***What's New***: 이 논문에서는 텍스트 프롬프트(global text prompt)와 익명 영역 레이아웃(an anonymous region layout)을 기반으로 가변 멀티레이어 투명 이미지를 직접 생성할 수 있는 Anonymous Region Transformer(ART)를 소개합니다. 이는 이미지 생성 작업에 있어 기존의 의미론적 레이아웃을 대체하며, 인간의 노동력을 줄이고 모델의 시각적 계획 능력을 향상시킵니다.
- ***Technical Details***: ART의 핵심은 익명의 직사각형 영역 세트로 구성된 익명 영역 레이아웃입니다. 이 레이아웃은 영역별 프롬프트 주석이 없으며, 글로벌 프롬프트와 상호작용을 통해 각층에 대한 시각적 토큰과 텍스트 토큰을 자동으로 연결합니다. 또한, 레이어별 컷껍 매커니즘(layer-wise region crop mechanism)이 적용되어 주의 연산을 크게 줄이고 수십 개 이상의 레이어를 효율적으로 생성할 수 있습니다. ART는 또한 다층 투명 이미지의 직접적인 인코딩 및 디코딩을 지원하는 고품질 멀티레이어 투명 이미지 오토인코더(autoencoder)를 제안합니다.
- ***Performance Highlights***: ART는 기존의 멀티레이어 이미지 생성 방법에 비해 12배 이상 빠르면서도 레이어 간 충돌을 줄이는 데 성공하였습니다. 실험 결과, LayerDiffuse 및 COLE과 같은 기존의 최첨단 방법들에 비해 포토리얼리스틱 도메인과 그래픽 디자인 도메인 모두에서 사용자 연구를 통해 우월한 성능을 입증했습니다.

### [Unveiling Downstream Performance Scaling of LLMs: A Clustering-Based Perspective](https://arxiv.org/abs/2502.17262)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.17262.png)

Vote: 14

Authors: Ke Shen, Chenggang Li, Kaiyuan Chen, Chengyin Xu, Xiao Li

- ***What's New***: 이 논문은 대규모 언어 모델(Large Language Models; LLMs) 훈련 전에 다운스트림 작업의 성능을 예측하는 문제를 해결하기 위해 Clustering-On-Difficulty(COD)를 제안합니다. COD는 작업을 난이도 기반으로 클러스터링하여 예측 가능한 하위 집합을 구축하고, 이를 통해 전체 평가 세트의 성능을 예측합니다.
- ***Technical Details***: COD는 작업을 난이도 특징에 따라 클러스터링하며, 각 클러스터의 성능-컴퓨트 관계를 파악하여 대규모 모델의 성능을 추정합니다. 예측 가능한 하위 집합의 성능을 이용하여 전체 세트의 성능을 맵핑하는 함수도 제시합니다. 이러한 과정은 소형 모델을 활용해 진행됩니다.
- ***Performance Highlights***: COD는 70B 파라미터의 LLM에 대한 예측에서 8개의 주요 벤치마크 세트 전반에 걸쳐 절대평균편차 1.36%를 기록하며, 기존 방법들보다 우수한 예측 정확성을 나타냅니다. 이는 LLMs에 대한 훈련 자원 할당 및 훈련 과정 모니터링에 실질적인 인사이트를 제공합니다.

### [Scale-Distribution Decoupling: Enabling Stable and Effective Training of Large Language Models](https://arxiv.org/abs/2502.15499)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.15499.png)

Vote: 12

Authors: Xun Zhou, Zhijian Zhuo, Ya Wang, Xiaoqing Li, Jian Yang, Yutao Zeng

- ***What's New***: Scale-Distribution Decoupling(SDD)는 대형 언어 모델(Large Language Models; LLMs)의 안정적인 훈련을 가능하게 하기 위한 혁신적인 접근 방식입니다. SDD는 완전 연결층의 가중치 행렬에서 규모(Scale)와 분포(Distribution)를 명시적으로 분리하여 훈련의 효율성을 높입니다.
- ***Technical Details***: SDD는 완전 연결층을 재구조화하여 가중치 행렬의 규모와 분포를 명시적으로 분리합니다. 이 방법은 활성화를 규제하는 정규화 메커니즘과 학습 가능한 스케일링 벡터를 도입하여 그라디언트 폭발과 소멸을 방지합니다. 또한 SDD는 기존 프레임워크와의 경량 통합이 가능하며, 다양한 모델 구성에서도 사용 가능합니다.
- ***Performance Highlights***: SDD는 다양한 LLM 아키텍처에서 훈련의 안정성을 크게 향상시켰으며, 기존 정규화 기법을 뛰어넘는 성능을 입증했습니다. 특히, SDD-1B(Post-Norm)는 OLMo2-1B(Pre-Norm)보다 우수한 수렴성과 일반화를 보여주었으며, 다양한 벤치마크에서 평균 정확도 54.04%를 달성하여 경쟁 모델을 능가했습니다.

### [Curie: Toward Rigorous and Automated Scientific Experimentation with AI Agents](https://arxiv.org/abs/2502.16069)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.16069.png)

Vote: 9

Authors: Jiachen Liu, Yibo Huang, Jayanth Srinivasa, Myungjin Lee, Ang Chen, Patrick Tser Jern Kon, Mosharaf Chowdhury, Yiming Qiu, Qiuyi Ding, Zhenning Yang

- ***What's New***: Curie는 AI 에이전트를 활용하여 과학적 실험을 자동화하고 엄격함을 구현할 수 있는 프레임워크입니다. 이 연구에서는 실험 과정에서 신뢰성과 해석 가능성을 높이기 위한 3개의 주요 모듈로 구성된 Curie를 제안합니다. 기존의 ad-hoc 방식에서 벗어나 과학적 실험을 구조적이고 체계적으로 진행할 수 있도록 설계되었습니다.
- ***Technical Details***: Curie는 실험의 엄격성을 보장하기 위해 세 가지 모듈로 구성됩니다: (1) Intra-Agent Rigor Module은 개별 에이전트의 신뢰성을 강화하고, (2) Inter-Agent Rigor Module은 에이전트 간의 협업을 통해 체계적 절차를 유지하며, (3) Experiment Knowledge Module은 잘 구조화된 문서를 유지하여 해석 가능성을 높입니다. 실험 벤치마크는 46개의 질문으로 구성되어 다양한 컴퓨터 과학 분야를 다루고 있으며, 이는 실질적인 연구 논문 및 널리 사용되는 오픈 소스 프로젝트에서 파생되었습니다.
- ***Performance Highlights***: Curie는 실험적 질문을 올바르게 답변하는데 있어서 3.4배의 향상을 보여주며, 가장 강력한 기준선 대비 현저한 성능 향상을 기록했습니다. 이는 복잡하고 엄격한 실험 과제를 자동화할 수 있는 Curie의 능력을 보여주며, 과학적 연구를 가속화하기 위한 유망한 단계를 제시합니다.

### [K-LoRA: Unlocking Training-Free Fusion of Any Subject and Style LoRAs](https://arxiv.org/abs/2502.18461)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.18461.png)

Vote: 8

Authors: Zhen Li, Qibin Hou, Ziheng Ouyang

- ***What's New***: K-LoRA는 학습 없이 LoRA를 융합하여 주제와 스타일을 함께 결합하는 혁신적인 방법을 소개합니다. 기존의 방법들이 추가 훈련을 요구하거나 스타일과 주제를 동시에 효과적으로 유지하지 못했던 문제를 해결하여, 훈련 없는 효율적인 융합을 가능케 합니다.
- ***Technical Details***: K-LoRA는 각 attention layer에서 두 LoRA의 Top-K 요소를 비교하여 최적의 융합을 위해 어떤 LoRA를 선택할지를 결정하는 메커니즘을 사용합니다. 이를 통해 주제와 스타일의 가장 대표적인 특성을 융합 과정에서 유지하며, 스케일링 팩터를 사용해 diffusion 과정 동안 스타일과 주제의 역할을 강조합니다.
- ***Performance Highlights***: 실험 결과 K-LoRA는 학습된 주제 및 스타일 정보를 효과적으로 통합하여 최첨단 학습 기반 접근방식보다 질적 및 양적으로 우수한 결과를 나타냅니다. 특히, 객체와 스타일의 고급 융합을 사용하여 기존 방법보다 높은 수준의 정밀도를 제공합니다.

### [WebGames: Challenging General-Purpose Web-Browsing AI Agents](https://arxiv.org/abs/2502.18356)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.18356.png)

Vote: 6

Authors: Wenqi Wu, Filippos Christianos, Alex J. Chan, Fraser Greenlee, Marvin Purtorab, Jikun Kang, Andy Toulis, George Thomas

- ***What's New***: WebGames는 50개 이상의 상호작용 도전 과제를 통해 일반적인 웹 브라우징 AI 에이전트를 평가하기 위한 포괄적인 벤치마크 스위트입니다. 이는 인간에게는 간단하지만, 현재 AI 시스템의 제한을 사용자 상호작용, 고급 입력 처리, 인지 작업, 워크플로 자동화 및 상호작용 오락에 대해 체계적으로 테스트합니다.
- ***Technical Details***: WebGames는 클라이언트 측에서 완전히 작동하는 싱글 페이지 JavaScript 아키텍처를 사용하여 경량 구현을 제공합니다. 각 도전 과제는 결정론적 검증 시스템을 구현하여 고유한 완료 토큰을 생성합니다. 도전 과제는 기본 브라우저 작업, 고급 입력 처리, 인지 및 메모리 작업, 워크플로 자동화, 상호작용 엔터테인먼트 시스템의 5개의 주요 카테고리를 포함합니다.
- ***Performance Highlights***: 인간 참여자와 비교했을 때, GPT-4o가 인간의 95.7% 성공률에 비해 41.2%를 달성하여 AI 시스템과 인간 간의 상당한 능력 차이가 드러났습니다. 이는 현재 AI 시스템이 인간이 직관적으로 인식하는 일반적인 웹 상호작용 패턴 처리에 근본적인 한계가 있음을 보여줍니다.

### [Introducing Visual Perception Token into Multimodal Large Language Model](https://arxiv.org/abs/2502.17425)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.17425.png)

Vote: 5

Authors: Xinchao Wang, Runpeng Yu, Xinyin Ma

- ***What's New***: 이 논문에서는 멀티모달 대형 언어 모델(Multimodal Large Language Model; MLLM)에 시각적 지각 토큰(Visual Perception Token; VPT)을 도입하여, 모델이 시각적 지각 과정을 자율적으로 제어할 수 있게 하는 새로운 메커니즘을 제안합니다. 이를 통해 MLLM은 특정 이미지의 영역을 선택하거나 추가적인 시각적 정보를 통합함으로써 정확성을 높일 수 있습니다.
- ***Technical Details***: 시각적 지각 토큰은 두 가지 유형으로 나뉘는데, 첫째는 영역 선택 토큰(Region Selection Token)으로, MLLM이 이미지의 특정 영역을 자르도록 지시하여 그 중요도를 반영합니다. 둘째는 재인코딩 토큰(Vision Re-Encoding Token)으로, 추가적인 DINO나 SAM 모델을 활용해 이미지를 재인코딩하고, 이로부터 얻은 시각적 특징을 MLLM에 입력해 원본의 시각 정보를 보완합니다. 각 시각적 지각 토큰이 출력될 때마다 추가적인 시각적 지각과정이 시작되고, 이러한 정보는 LLM에 연결됩니다.
- ***Performance Highlights***: VPT 도입으로 인해 2B 모델의 성능이 평균적으로 30.9% 향상되어, 점수가 0.572에서 0.749로 증가하였고, 이는 7B 모델 대비 20.0% (0.624에서) 더 높은 성과를 보였습니다. 실험 결과, VPT가 장착된 모델들은 시각적 추론과 미세한 이해에서 유리한 성능을 보이며, 특히 복잡한 장면에서 물체 식별이나 개체 수 계산에 있어 그 효과가 두드러졌습니다.

### [The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve?](https://arxiv.org/abs/2502.17535)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.17535.png)

Vote: 4

Authors: Bingsheng He, Zhenheng Tang, Xiaowen Chu, Qian Wang, Bo Li, Xiang Liu, Peijie Dong

- ***What's New***: 이 논문은 LLM(Large Language Models)의 압축 시 합리적으로 유지해야 할 능력에 대해 새로운 'Lottery LLM 가설'을 제시하고 있습니다. 이 가설은 다단계 추론(multi-step reasoning)과 외부 도구들의 도움을 받을 경우, 원래의 LLM과 동일한 성능을 발휘할 수 있는 더 작은 LLM이 존재한다는 것입니다.
- ***Technical Details***: 프루닝(pruning)과 양자화(quantization), KV 캐시 압축(KV cache compression) 등의 기존 모델 압축 방법들이 흔히 단순한 정확성이나 난이도가 낮은 작업들에만 초점을 맞추고 있으며, 복잡한 추론이나 외부 도구와의 상호작용이 필요한 중요한 기능을 간과하고 있습니다. 이 논문에서는 검색 보강 생성(retrieval-augmented generation), 외부 도구의 사용, 다단계 추론 등이 LLM의 성능을 크게 향상시키는 방법으로 제안됩니다.
- ***Performance Highlights***: 소형 LLM이라도 적절한 다단계 추론 알고리즘을 통해 대형 LLM 못지않은 성능을 발휘할 수 있음을 강조합니다. 특히, 외부 메모리 및 도구와의 상호작용을 통해 복잡한 문제를 해결하는 데 있어서 높은 효율성을 보이는 것으로 나타났습니다.

### [AAD-LLM: Neural Attention-Driven Auditory Scene Understanding](https://arxiv.org/abs/2502.16794)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.16794.png)

Vote: 4

Authors: Nima Mesgarani, Ashesh Mehta, Guy M McKhann, Adeen Flinker, Xilin Jiang, Daniel Friedman, Stephan Bickel, Sukru Samet Dindar, Vishal Choudhari

- **What's New**: AAD-LLM은 청취자 주의(attention)를 추론하기 위해 뇌 신호를 통합하여 청취자의 의도(Intention)를 반영한 청각 장면 이해를 위한 새로운 패러다임을 제시합니다. 이 새로운 시스템은 의도에 기반한 청각 장면 이해(II-ASU)를 구현하여 기계가 청취자의 인식을 기반으로 소리를 해석하도록 지원합니다.
- **Technical Details**: AAD-LLM은 청취자의 주의(attentional state)를 추론하기 위해 두뇌 신호와 청각 큰 언어 모델(auditory large language model; LLM)을 통합합니다. 이 모델은 두 청취 신호를 입력으로 받아 주청취 화자를 예측하고, 그 정보를 기반으로 청각 장면을 처리하고 대응을 생성합니다. 신경 주의(decoding)는 intracranial EEG (iEEG)를 통해 수행되며, 청취자의 인식과 맞춰진 응답 생성을 가능하게 합니다.
- **Performance Highlights**: AAD-LLM은 다수의 청각 과제에서 시간을 줄여 개선된 성능을 보이며, 특히 청취자의 주의를 해석하여 응답을 생성하는 데 있어 기존 모델들보다 뛰어난 성능을 보여줍니다. 인간 평가 실험에서 응답이 청취자의 주의를 잘 반영한다는 주관적 평가를 받았습니다. 이를 통해 청취자 주의에 맞춰 응답을 생성하는 성능을 입증하였습니다.

### [Finding the Sweet Spot: Preference Data Construction for Scaling Preference Optimization](https://arxiv.org/abs/2502.16825)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.16825.png)

Vote: 3

Authors: Xiaoli Li, Roy Ka-wei Lee, Hwee Tou Ng, Hai Ye, Lidong Bing, Yao Xiao, Linyao Chen

- ***What's New***: 이번 연구는 Direct Preference Optimization (DPO)을 통해 대규모 언어 모델(LLMs)의 Human Preference Alignment를 개선하는 새로운 방법론을 제시합니다. 기존 방법이 특정 샘플 크기 이상에서 성능 저하를 초래할 수 있음을 밝히며, 이를 해결하기 위한 스케일링 가능한 Preference Data Construction 전략을 제안합니다.
- ***Technical Details***: 연구는 우선 기존의 Preference Pair를 최대 보상과 최소 보상을 기반으로 선택하는 기존 방식을 개선하고자 했습니다. 제안된 방법은 보상의 정규 분포를 활용하여 샘플들을 구분하고, 특히 'µ -2σ' 지점에서 거부 회신을 선택하는 것이 최적의 성능을 제공하는 것을 발견했습니다. 또한 다양한 조합의 Preference Data를 구성하여 DPO를 통한 정책 모델 정책을 체계적으로 평가했습니다.
- ***Performance Highlights***: 새로운 Preference Data Construction 전략을 사용한 모델은 기존의 최대/최소 보상 기반 전략에 비해 일관된 성능 향상을 보여주었으며, Llama-3-8B-Instruct의 경우, length-controlled win rate에서 3% 포인트 증가를 기록했습니다. 이는 잘 구분된 보상 간격을 활용한 Preference Pair가 더욱 효과적인 성능 최적화를 가능케 함을 시사합니다.

### [Prompt-to-Leaderboard](https://arxiv.org/abs/2502.14855)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14855.png)

Vote: 2

Authors: Joseph Tennyson, Ion Stoica, Wei-Lin Chiang, Connor Chen, Tianle Li, Evan Frick, Anastasios N. Angelopoulos

- ***What's New***: Prompt-to-Leaderboard (P2L)는 대형 언어 모델(Large Language Models; LLM) 평가에서 개별 사용자와 프롬프트의 다양성을 더 잘 포착하기 위해 개발된 방법론입니다. 기존의 평균적 리더보드가 제공하던 단점을 보완하고, 구체적인 프롬프트에 따라 개인화된 리더보드를 제공합니다.
- ***Technical Details***: P2L은 자연어 프롬프트를 입력으로 받아 Bradley-Terry 계수를 예측하여 인간의 선호도를 추정합니다. 이 정보로 각 프롬프트에 따라 모델의 성능을 측정하는 리더보드를 생성합니다. 이 방법론은 인간의 선호 피드백을 기반으로 한 LLM 훈련을 통해 프롬프트 별 모델의 승률을 평가할 수 있습니다.
- ***Performance Highlights***: Chatbot Arena에서의 테스트 결과, P2L 기반의 라우터는 기존 모델 대비 25점 점수가 향상되어 #1 자리를 차지하였습니다. P2L의 프롬프트-특정 평가 능력은 대형 언어 모델과 유사한 분포를 따라 성과가 향상되었음을 보여줍니다.

### [LaTIM: Measuring Latent Token-to-Token Interactions in Mamba Models](https://arxiv.org/abs/2502.15612)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.15612.png)

Vote: 2

Authors: Hugo Pitorro, Marcos Treviso

- ***What's New***: LATIM는 Mamba 모델(Mamba Models)의 미세한 해석 가능성을 제공하는 새로운 토큰 수준 분해 방법입니다. 이를 통해 Mamba-1 및 Mamba-2에서 토큰 간의 상호작용 패턴을 드러낼 수 있습니다.
- ***Technical Details***: LATIM은 토큰 간 기여를 세밀하게 분해하여 Mamba의 계산을 재구성하는 방법입니다. 이는 ALTI와 같은 주의 기반의 해석 가능성 기법을 Mamba 구조에 맞게 조정합니다. 다양한 태스크에서 실험을 통해 Mamba의 토큰 간 상호작용을 효과적으로 드러냅니다.
- ***Performance Highlights***: LATIM은 Mamba 모델이 긴 문맥을 효율적으로 처리하는 데 있어 해석 가능성을 크게 향상시킵니다. 실험 결과, LATIM은 Mamba 모델에서 기존 방법들보다 더욱 자세하고 정확한 토큰 간 상호작용을 파악할 수 있음을 보여줍니다.

### [Shakti-VLMs: Scalable Vision-Language Models for Enterprise AI](https://arxiv.org/abs/2502.17092)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.17092.png)

Vote: 2

Authors: Kruthika KR, Kartik Basavaraj Angadi, Syed Abdul Gaffar Shakhadri

- ***What's New***: Shakti-VLMs는 적은 토큰 수로도 강력한 성능을 발휘할 수 있도록 설계된 1B 및 4B 파라미터 용량의 비전-언어 모델(Vision-Language Models)입니다. QK-Normalization, 하이브리드 정규화 전략 및 향상된 위치 인코딩과 같은 아키텍처 혁신을 통해 데이터를 효율적으로 사용하여 경쟁력 있는 성능을 달성하며, 주로 엔터프라이즈 AI 앱을 대상으로 합니다.
- ***Technical Details***: Shakti-VLMs는 QK-Normalization을 통해 주의(attention) 메커니즘의 안정성을 향상시키고, dynamic patch size와 하이브리드 정규화 전략을 사용하여 시각적인 유연성을 제공합니다. 3단계 학습 방법론을 채택하여 데이터 효율성을 극대화하며, OCR 추출 및 문서 이해와 같은 다양한 멀티 모달 작업에서 우수한 성능을 보입니다.
- ***Performance Highlights***: Shakti-VLM-1B는 다양한 멀티 모달 작업에서 뛰어난 성능을 보여주며, 특히 문서 및 차트 이해에서 최첨단 모델을 능가하는 성능을 보였습니다. Shakti-VLM-4B 모델은 OCR, 문서 VQA 및 시각적 질문 답변 등 고도의 시각적 추론 기준에서 경쟁 모델과 비교하여 더 나은 결과를 기록했습니다.

### [MutaGReP: Execution-Free Repository-Grounded Plan Search for Code-Use](https://arxiv.org/abs/2502.15872)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.15872.png)

Vote: 1

Authors: Ali Farhadi, Zaid Khan, Mohit Bansal, Tanmay Gupta, Ranjay Krishna, Luca Weihs

- ***What's New***: MutaGReP는 실행 없이도 코드 저장소 기반의 계획 검색을 통해 LLM이 사용자 요청을 자연어로 분해하여 코드를 작성할 수 있도록 하는 새로운 방식을 제안합니다. 이를 통해 긴 컨텍스트를 사용하는 전통적인 접근 방식을 벗어나 효과적으로 코드 저장소를 활용하는 가능성을 시사합니다.
- ***Technical Details***: MutaGReP는 LLM 기반의 트리 검색(Neural Tree Search) 방식으로, 기호 검색기(Symbol Retriever)를 사용하여 코드 저장소의 심볼을 통한 구체화를 통해 사용자 요구를 해결하는 실행 가능한 계획을 검색합니다. 검색 과정에서는 코드 실행이 필요 없으며, 각 스텝은 자연어 의도와 코드베이스의 심볼이 결합된 구조를 가집니다.
- ***Performance Highlights***: LongCodeArena 벤치마크에서, MutaGReP의 계획은 GPT-4o가 전체 코드 저장소를 컨텍스트로 사용할 때와 견줄 만한 성능을 보여주었으며, 유사한 성능을 위해 필요한 컨텍스트 길이의 5% 미만을 사용합니다. 또한 Qwen 2.5 Coder 32B 및 72B 모델이 MutaGReP의 계획을 통해 GPT-4o와 동일한 성능을 발휘할 수 있음을 입증했습니다.

### [MLLMs Know Where to Look: Training-free Perception of Small Visual Details with Multimodal LLMs](https://arxiv.org/abs/2502.17422)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.17422.png)

Vote: 1

Authors: Mahyar Khayatkhoei, Prateek Chhikara, Filip Ilievski, Jiarui Zhang

- ***What's New***: 본 연구는 MLLM(Multimodal Large Language Models)이 작은 시각적 세부사항을 인식할 때의 한계를 이해하고 이를 개선하기 위한 방법을 탐구합니다. 주목할 만한 점은 MLLM이 어떤 질문이 주어졌을 때, 항상 시각적으로 볼 필요가 있는 곳을 알고 있다는 사실을 발견한 것입니다. 이 연구에서는 학습 없이 시각적 개입을 통해 MLLM의 세부사항 인지 능력을 향상시키는 방안을 제안합니다.
- ***Technical Details***: MLLMS의 내부 지식인 attention과 gradient maps를 활용하여 작은 시각적 세부사항 인식을 향상시키기 위한 세 가지 학습 없이 가능한 자동 시각적 개입 방법을 제안합니다. 이 방법들은 두 가지 인기 있는 MLLMs과 일곱 가지 시각적 질문 응답(Visual Question Answering; VQA) 벤치마크에서 평가되었으며, 별도의 학습 없이도 MLLM의 정확도를 크게 향상시킬 수 있음을 보여줍니다.
- ***Performance Highlights***: TextVQA와 V* 등의 디테일에 민감한 벤치마크에서 원래의 MLLM들의 성능이 작은 시각적 개념을 인식하는 데 어려움을 겪음이 확인되었습니다. 제안된 방법을 적용한 결과, MLLM의 정확도가 특히 세부사항 인지가 중요한 벤치마크에서 유의미하게 향상되었습니다.

### [An Overview of Large Language Models for Statisticians](https://arxiv.org/abs/2502.17814)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.17814.png)

Vote: 1

Authors: Weizhe Yuan, Linjun Zhang, Weijie J. Su, Jing Xu, Song Mei, Kyunghyun Cho, Jason E Weston, Emily Getzen, Wenlong Ji, Michael I. Jordan

- ***What's New***: 이 논문은 통계학자들이 대형 언어 모델(LLMs)의 설계와 배포를 안내하는 데 어떻게 기여할 수 있는지 탐구합니다. 통계적인 방법을 통해 불확실성 측정, 해석 가능성, 공정성, 개인정보 보호, 워터마킹과 모델 적응 등에 주목하며, 인간 사용자에게 신뢰성과 투명성을 제공하는 방향으로 발전시킬 수 있는 가능성을 제시합니다.
- ***Technical Details***: 이 논문은 LLMs의 역사적 발전과 기본 원리에 대해 소개하며, 사전 학습(pre-training), 프롬프트 사용(prompting), 정밀 조정(fine-tuning), 정렬 기술에 관한 교육 파이프라인, 통계적 방법을 통한 신뢰할 수 있는 LLM 설계에 관한 논의를 포함합니다. 특히, 설명 가능성, 불확실성 측정, 워터마킹, 프라이버시와 알고리즘 공정성을 다루고 있습니다.
- ***Performance Highlights***: 통계적 방법을 통해 LLMs의 정렬 및 유연성이 강화될 수 있으며, 데이터 수집 및 청소, 의학 연구 분야에서 적절한 성능을 발휘하도록 지원할 수 있는 잠재력에 대해 논의합니다. 통계 원칙이 LLMs의 구축과 다양한 응용에서 신뢰성과 신뢰성있는 배포를 보장하는 데 중요한 역할을 할 수 있음을 강조합니다.

### [WiCkeD: A Simple Method to Make Multiple Choice Benchmarks More Challenging](https://arxiv.org/abs/2502.18316)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.18316.png)

Vote: 0

Authors: Ahmed Elhady, Mikel Artetxe, Eneko Agirre

- ***What's New***: WiCkeD는 기존의 객관식 벤치마크에 '위의 어느 것도 아님(None of the Above)' 옵션을 추가하여 난이도를 높이는 간단한 방법을 제안합니다. WiCkeD는 자동으로 적용할 수 있어 새로운 벤치마크를 만들지 않고도 기존의 벤치마크를 더 도전적으로 만들 수 있습니다.
- ***Technical Details***: WiCkeD 알고리즘은 주어진 객관식 질문의 선택지 중 하나를 임의로 삭제하고 'None of the Above' 옵션을 추가합니다. 만약 정답이 삭제된 경우, 정답은 'None of the Above'가 됩니다. 검사, Qwen2.5, Llama-3.1과 같은 다양한 비중의 오픈소스 LLM 모델에서 WiCkeD를 적용하여 각 벤치마크의 변형을 평가했습니다.
- ***Performance Highlights***: WiCkeD를 적용했을 때 모델 성능은 원래 벤치마크에 비해 평균 12.1 포인트 하락했습니다. 예를 들어, Qwen2.5-7B 모델은 원래보다 19.7% 성능이 떨어졌고, DeepSeek-R1 의 Qwen7B는 7.3% 하락했습니다. 이는 WiCkeD가 더 높은 수준의 추론 능력을 요구하기 때문입니다.

### [LDGen: Enhancing Text-to-Image Synthesis via Large Language Model-Driven Language Representation](https://arxiv.org/abs/2502.18302)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.18302.png)

Vote: 0

Authors: Xudong Rao, Tao Wei, Pengzhi Li, Zide Liu, Xuhao Pan, Wei Chen, Pengfei Yu, Wei He

- ***What's New***: 이 논문에서는 LDGen이라는 새로운 방법을 소개하며, 이는 대형 언어 모델(LLM; Large Language Models)을 기존의 텍스트-이미지 변환 확산 모델에 통합하여 계산 요구를 최소화하고자 합니다. LDGen은 LLM의 고급 기능을 사용하여 다국어 이미지를 생성할 수 있는 방식으로, 새로운 언어 표현 전략을 통해 텍스트와 이미지의 정렬을 개선합니다.
- ***Technical Details***: LDGen은 LLM을 T5/CLIP 텍스트 인코더 기반의 현재 확산 모델에 효율적으로 통합합니다. 이를 위해 계층적 캡션 최적화 및 인간 교육 전략을 사용하여 텍스트 정보의 정확한 의미론적 정보를 도출합니다. 경량 어댑터와 크로스-모달 리파이너(Cross-modal Refiner)를 도입하여 LLM과 이미지 특징들 간의 효율적인 피처 정렬 및 상호작용을 가능하게 합니다. 이 접근법은 제로샷 다국어 이미지 생성을 지원하며, 이미지 제너레이션 성능을 향상시킵니다.
- ***Performance Highlights***: LDGen의 실험 결과는 프롬프트 준수성 및 이미지 미학적 품질에서 최신 모델들을 능가한다는 것을 보여주며, 여러 언어를 매끄럽게 지원합니다. 우리의 모델은 DPG-Bench에서 베이스라인 모델과 비교했을 때 약 13%의 성능 향상을 달성했습니다. 다국어 이미지 생성에 관해서도, 영어 텍스트로만 훈련된 상태에서도 여러 언어에서 제로샷으로 작동하며 양질의 결괏값을 제공합니다.

