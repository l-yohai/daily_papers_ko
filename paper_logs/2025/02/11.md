## Daily Papers (2025-02-11)

### [SynthDetoxM: Modern LLMs are Few-Shot Parallel Detoxification Data Annotators](https://arxiv.org/abs/2502.06394)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06394.png)

Vote: 69

Authors: Alexander Panchenko, Daniil Moskovskiy, Elena Tutubalina, Sergey Pletenev, Nikita Sushko

- ***What's New***: 이 논문은 새로운 다국어 병렬 해독 데이터 생성 파이프라인과 최신 대형 언어 모델(LLMs)을 사용한 병렬 해독데이터세트인 SynthDetoxM을 소개합니다. SynthDetoxM은 독일어, 프랑스어, 스페인어, 러시아어에서 생성된 16,000개의 고품질 해독 문장 쌍을 포함하며, 데이터 부족 문제를 해결하는 데 목적이 있습니다.
- ***Technical Details***: SynthDetoxM 데이터세트는 기존의 독성 평가 데이터세트에서 출발하여, 최신 9가지 오픈소스 LLMs를 사용하여 소수의 샘플(few-shot) 학습 환경에서 생성되었습니다. 이는 데이터의 다양성과 품질을 보장하기 위해 각 모델에서 가장 우수한 답변을 제공하는 수작업 기준법을 통해 결합되었습니다.
- ***Performance Highlights***: 제안된 데이터세트를 기반으로 학습된 모델들이 인간 주석 데이터세트인 MultiParaDetox에서 학습된 모델을 능가하는 성능을 보여주었습니다. SynthDetoxM으로 학습된 모델은 평가된 모든 LLMs를 소수 샷 환경에서 능가했으며, 전체 데이터세트를 통한 학습에서 최고의 성능을 나타내었습니다.

### [Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling](https://arxiv.org/abs/2502.06703)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06703.png)

Vote: 61

Authors: Bowen Zhou, Wanli Ouyang, Runze Liu, Junqi Gao, Jian Zhao, Kaiyan Zhang, Biqing Qi, Xiu Li

- ***What's New***: 2025년 2월에 발표된 이 논문에서는 Test-Time Scaling (TTS) 전략을 통해 작은 언어 모델(1B 파라미터)이 훨씬 더 큰 언어 모델(405B 파라미터)을 능가할 수 있는지 탐구했습니다. 다양한 policy 모델, Process Reward Models(PRMs), 그리고 문제의 난이도에 따라 최적의 TTS 전략을 적용하여 LLM의 성능을 향상시킬 수 있으며, 특히 MATH-500과 AIME24와 같은 복잡한 작업에서 작은 모델이 더 큰 모델을 능가할 수 있음을 보였습니다.
- ***Technical Details***: 이 연구는 MATH-500 및 AIME24와 같은 경쟁 수준의 수학 데이터셋에서 다양한 PRMs를 이용하여 TTS 방법을 평가했습니다. TTS 방법으로는 Best-of-N (BoN), beam search, Diverse Verifier Tree Search (DVTS)가 사용되었습니다. 각 방법은 특정 problem에서 최적의 계산을 할당하는 방식으로 작동하며, 특히 PRMs와 policy 모델의 조합에 따라 최적의 성능을 발휘하도록 설계되었습니다.
- ***Performance Highlights***: MATH-500 및 AIME24 실험에서, 1B 파라미터 크기의 모델이 405B 파라미터 모델을 넘어서는 결과를 보여주었습니다. 예를 들어, 0.5B LLM이 GPT-4o를 능가했으며, 3B LLM이 405B LLM을 상회하는 성능을 기록했습니다. 더 나아가, DeepSeek-R1-Distill-Qwen-7B는 o1과 DeepSeek-R1를 능가하는 결과를 보였으며, 높은 추론 효율성을 보였습니다.

### [Exploring the Limit of Outcome Reward for Learning Mathematical Reasoning](https://arxiv.org/abs/2502.06781)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06781.png)

Vote: 36

Authors: Ziyi Wang, Dahua Lin, Jiangning Liu, Qian Zhao, Jianfei Gao, Junnan Liu, Yuzhe Gu, Songyang Zhang, Chengqi Lyu, Shuaibin Li, Kai Chen, Hongwei Liu, Weihan Cao, Haian Huang, Kuikun Liu, Songyang Gao, Wenwei Zhang

- ***What's New***: 이 논문은 수학적 추론 과제를 위한 새로운 강화학습(RL) 프레임워크인 OREAL을 제안합니다. OREAL은 Outcome Reward 기반의 강화학습을 활용하여 수학적 추론에서 달성할 수 있는 성능의 한계를 탐구합니다. 초기로서 7B 모델이 OREAL을 통해 MATH-500에서 94.0의 pass@1 정확도를 달성하여, 32B 모델과 동등한 성능을 거둔 최초의 사례가 되었습니다.
- ***Technical Details***: OREAL은 최선의 N개(BoN) 샘플링에서 얻은 긍정적인 경로를 통해 KL 정규화 최적 정책을 학습할 수 있도록 행동 복제를 사용합니다. 부정적인 샘플의 보상은 경로 간의 일관성과 보상 형상을 위해 조정되어야 합니다. 희박한 보상 문제를 해결하기 위해, OREAL은 토큰 수준 보상 모델을 사용하여 학습을 위한 중요한 토큰을 샘플링합니다.
- ***Performance Highlights***: OREAL-7B는 MATH-500에서 91.0의 pass@1 정확도를 달성했으며, 94.0으로 개선된 OREAL-DSR1-Distill-Qwen-7B는 모든 7B 모델 중 새로운 최고 기록을 세웠습니다. OREAL-32B는 MATH-500에서 95.0의 pass@1 정확도로 모든 이전의 RL 및 증류 기반 모델을 능가하는 새로운 최고 성과를 보여주었습니다.

### [Training Language Models for Social Deduction with Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2502.06060)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06060.png)

Vote: 21

Authors: C. Karen Liu, Bidipta Sarkar, Warren Xia, Dorsa Sadigh

- ***What's New***: 이 논문은 다중 에이전트 강화 학습(Multi-Agent Reinforcement Learning; MARL)을 통해 인간 시연 없이 자연어로 소셜 디덕션 게임 내에서의 언어 모델의 대화 능력을 향상시키는 방법을 소개합니다. 기존 연구와 달리, 이 방법은 대화의 듣기와 말하기 부분을 나누어 각 대화를 통해 얻은 정보를 활용하여 환경에 대한 예측을 통해 에이전트가 자율적으로 학습하도록 설계되었습니다.
- ***Technical Details***: 이 연구는 소셜 디덕션 게임 'Among Us'를 모델링하여 모든 에이전트가 자연어로만 상호작용하도록 환경을 설정하였습니다. 에이전트는 RWKV 언어 모델을 사용하여 정책을 구성하였으며, 주어진 환경 관찰 및 대화 기록에 기초하여 행동을 취하는 방식을 학습했습니다. 듣기 향상을 위해, 대화 중 메시지를 듣고 배후의 진실을 예측하는 보조 학습 과제를 제공하여 에이전트가 진정한 임포스터를 예측하도록 도왔습니다. 말하기 향상은 메시지가 다른 에이전트의 임포스터에 대한 믿음을 어떻게 변화시키는지를 보상으로 반영한 강화 학습 기법을 통해 이루어졌습니다.
- ***Performance Highlights***: 훈련된 에이전트들은 표준 강화 학습을 사용한 모델에 비해 두 배 이상의 성공률을 기록했습니다. 이 모델은 1.5B 파라미터의 RWKV 모델을 활용하며, 기본적으로 7B 파라미터를 사용하는 모델 보다도 성능이 우세했습니다. 에이전트는 강력한 의사소통 전략을 형성해, 인간 플레이어와의 실제 게임 속 상황과 유사한 행동(예: 용의자 지목, 증거 제공)을 보였습니다.

### [CODESIM: Multi-Agent Code Generation and Problem Solving through Simulation-Driven Planning and Debugging](https://arxiv.org/abs/2502.05664)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.05664.png)

Vote: 15

Authors: Md Rizwan Parvez, Mohammed Eunus Ali, Md. Ashraful Islam

- ***What's New***: CODESIM은 시뮬레이션 기반 계획 및 디버깅을 통해 다중 에이전트 코드 생성 및 문제 해결을 수행하는 새로운 프레임워크입니다. 이 시스템은 사람처럼 단계별 시뮬레이션을 활용해 계획을 검증하고 디버깅을 수행하며, 외부 자원 없이 복잡한 코딩 솔루션을 합성하는 독창적인 기능을 갖추고 있습니다.
- ***Technical Details***: CODESIM은 계획(Planning), 코딩(Coding), 디버깅(Debugging) 단계로 구성된 다중 에이전트 프레임워크입니다. 계획 에이전트는 문제에 대한 계획을 시뮬레이션을 통해 검증하고, 코딩 에이전트는 이 계획을 바탕으로 문제를 해결할 코드를 작성합니다. 디버깅 에이전트는 실패한 테스트 케이스를 시뮬레이션하여 오류를 찾아내고 이를 수정합니다.
- ***Performance Highlights***: CODESIM은 HumanEval에서 95.1%, MBPP에서 90.7%, APPS에서 22%, CodeContests에서 29.1%로 새로운 최신 성능을 달성했습니다. 특히 MBPP와 APPS와 같은 복잡한 프로그래밍 문제에서도 탁월한 성능을 보여주며, 외부 디버거와 결합 시 더욱 높은 성능 향상을 나타냈습니다.

### [LM2: Large Memory Models](https://arxiv.org/abs/2502.06049)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06049.png)

Vote: 14

Authors: George Thomas, Alex J. Chan, Marvin Purtorab, Jikun Kang, Filippos Christianos, Fraser Greenlee, Wenqi Wu, Andy Toulis

- ***What's New***: 이번 연구에서는 대형 메모리 모델(Large Memory Model; LM2)을 소개합니다. 이 모델은 디코더 전용 Transformer 구조를 보완하여 보조 메모리 모듈을 추가하여 다단계 추론, 관계 논증, 긴 문맥에서 분포된 정보를 종합하는 데 필요한 성능을 향상시켰습니다.
- ***Technical Details***: LM2는 Transformer 디코더 블록에 메모리 모듈을 추가한 구성입니다. 메모리 모듈은 입력 임베딩과 교차 주의 기법을 통해 상호작용하여 성능을 향상시키며 게이팅 메커니즘을 통해 정보를 동적으로 업데이트합니다. 이 구조는 긴 컨텍스트 및 복잡한 추론이 필요한 작업에 적합하며, 기존 주의 정보 흐름을 유지하면서 메모리 정보를 통합합니다.
- ***Performance Highlights***: LM2는 BABILong 벤치마크에서 기존 메모리 강화 모델인 RMT보다 평균 37.1%, 일반 Llama-3.2 모델보다 86.3% 우수한 성능을 보였습니다. 또한, MMLU 데이터셋에서는 일반 과제를 수행하면서도 성능 저하 없이 5.0%의 향상을 보여주며, 다단계 추론, 수리적 추론, 대량 문맥 질문응답에서 뛰어난 성능을 입증했습니다.

### [Show-o Turbo: Towards Accelerated Unified Multimodal Understanding and Generation](https://arxiv.org/abs/2502.05415)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.05415.png)

Vote: 13

Authors: Yishun Li, Chenkai Xu, Zhijie Deng, Xu Wang, Tianqi Hou, Zhenyi Liao

- ***What's New***: Show-o Turbo는 Show-o의 비효율성을 해결하기 위한 새로운 모델입니다. 텍스트 토큰의 병렬 디코딩(Parallel Decoding) 알고리즘을 도입하여 이미지와 텍스트의 생성 과정을 하나의 디노이징(Denoising) 관점으로 통합하였습니다. 이는 멀티모달 디노이징 경로를 단축하고, 일관성 증류(Consistency Distillation; CD)를 확장하는 방법을 제안합니다.
- ***Technical Details***: Show-o Turbo는 텍스트-이미지 생성을 일관된 디노이징 관점에서 진행합니다. 여기에는 멀티모달 디노이징 경로를 단축하기 위해 일관성 증류(CD) 기술을 적용하였으며, 경로 분할(Trajectory Segmentation) 및 커리큘럼 학습을 도입하여 학습 수렴성을 개선하였습니다. 모델은 사전 학습된 Show-o를 초기화하여, 분포의 불균형을 바탕으로 디노이징 과정을 개선합니다.
- ***Performance Highlights***: 텍스트-이미지 생성에서, Show-o Turbo는 4개 샘플링 스텝에서 분류기 자유 가이드(Classifier-Free Guidance; CFG)를 사용하지 않고 GenEval 점수 0.625를 기록하며, 8개 스텝의 CFG를 사용하는 원래 Show-o를 앞섰습니다. 이미지-텍스트 생성에서는 1.5배 가속을 이루었으며 성능 저하가 미세하게 나타났습니다. 이러한 결과는 Show-o Turbo가 빠른 속도로 의미 있는 콘텐츠를 생성할 수 있음을 보여줍니다.

### [Lossless Acceleration of Large Language Models with Hierarchical Drafting based on Temporal Locality in Speculative Decoding](https://arxiv.org/abs/2502.05609)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.05609.png)

Vote: 12

Authors: Huije Lee, Jeongyeon Seo, Sangjin Choi, Taeho Hwang, Hoyun Song, Sukmin Cho, Jong C. Park, Soyeong Jeong, Youngjin Kwon

- ***What's New***: 이 논문에서는 대규모 언어 모델(LLM)의 추론 속도를 향상시키기 위한 혁신적인 손실 없는 드래프팅(Drafting) 방법인 계층적 드래프팅(Hierarchy Drafting; HD)을 제안합니다. HD는 시간적 지역성(Temporal Locality)에 기반한 계층적 프레임워크를 통해 다양한 토큰 소스를 조직하여, 다양한 작업에서 일관된 가속을 제공하고 드래프팅 지연 시간을 최소화합니다.
- ***Technical Details***: 계층적 드래프팅(HD)은 다양한 토큰 소스를 시간적 지역성에 기반하여, 문맥 의존성 데이터베이스(Context-Dependent Database), 모델 의존성 데이터베이스(Model-Dependent Database), 통계 의존성 데이터베이스(Statistics-Dependent Database)로 구성하여 조직합니다. 추론 과정에서 HD는 시간적 지역성이 높은 순서로 데이터베이스에 접근하여, 점진적으로 드래프팅 토큰을 확보한 후 검증 과정을 거칩니다. 이로 인해 드래프팅의 정확성이 향상되고, 드래프팅 지연 시간을 줄이는 두 가지 이점을 얻을 수 있습니다.
- ***Performance Highlights***: HD는 다양한 크기의 LLM(Vicuna-7B/13B 및 Llama-2-7B/13B)과 작업에서 기존의 손실 없는 드래프팅 방법보다 일관되게 높은 추론 속도를 보여주었습니다. 예를 들어 Vicuna-7B 모델에서 HD는 단일 드래프팅 방법과 비교하여 지속적으로 1.5배 이상의 가속을 달성했습니다. 또한, HD의 채택 비율은 다른 방법들보다 높은 수준인 70% 이상을 기록하였으며, 드래프트 과정의 지연 시간이 상대적으로 짧았습니다.

### [ReasonFlux: Hierarchical LLM Reasoning via Scaling Thought Templates](https://arxiv.org/abs/2502.06772)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06772.png)

Vote: 11

Authors: Ling Yang, Zhaochen Yu, Mengdi Wang, Bin Cui

- ***What's New***: ReasonFlux는 복잡한 추론 작업에서도 OpenAI o1-preview와 DeepSeek V3를 능가하는 효율적인 문제 해결을 위한 계층적 LLM 추론 프레임워크입니다. 이 모델은 사용 가능한 GPU 자원을 최소한으로 사용하여 32B짜리 모델을 훈련하며, 새로운 추론 확장 시스템, 계층적 강화학습(hierarchical reinforcement learning), 및 높은 수준의 사고 템플릿(thought templates) 라이브러리를 도입했습니다. 특히 MATH 벤치마크에서 91.2%의 정확도를 달성하였고, 이는 o1-preview보다 6.7% 더 높은 성과입니다.
- ***Technical Details***: ReasonFlux의 기술 구조는 약 500개의 고수준 사고 템플릿으로 구성된 템플릿 라이브러리를 포함합니다. 대량의 원래 복잡한 CoT 데이터를 대체하여 사고 템플릿 시퀀스에 대해 계층적 강화학습을 수행합니다. 이를 통해 복잡한 문제를 점차적으로 다룰 수 있습니다. 추론 단계에서 사고 템플릿을 적응적으로 확장할 수 있는 새로운 추론 확장 시스템을 개발하여 복잡한 추론 작업 시 최적의 템플릿 경로를 계획합니다.
- ***Performance Highlights***: ReasonFlux-32B 모델은 다양한 수학 추론 벤치마크에서 뛰어난 성과를 보였습니다. MATH 벤치마크에서는 91.2%의 정확도를 기록했으며, AIME 벤치마크에서는 56.7%의 문제를 해결해 o1-preview와 DeepSeek-V3를 각각 27%, 45% 능가했습니다. OlympiadBench와 중국 대학입학시험에서도 각각 63.3%, 83.6%의 높은 정확도를 보이며, 프레임워크의 강력한 일반화 능력을 입증했습니다.

### [Matryoshka Quantization](https://arxiv.org/abs/2502.06786)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06786.png)

Vote: 10

Authors: Pranav Nair, Jeff Dean, Prateek Jain, Puranjay Datta, Aditya Kusupati

- ***What's New***: Matryoshka Quantization (MatQuant)은 큰 모델의 통신 및 추론 비용을 줄이기 위한 새로운 다중 스케일 양자화 기법입니다. 이 방법은 중첩된 (Matryoshka) 구조를 활용하여 다양한 정밀도 수준에서 모델을 훈련 및 유지할 수 있으며, 특히 매우 낮은 정밀도(int2)에서도 기존 양자화 기법보다 최대 10% 더 정확한 모델을 제공합니다.
- ***Technical Details***: MatQuant는 int8, int4, int2와 같은 다중 정밀도 수준을 동시에 최적화하는 방식으로, 모델 파라미터를 다양한 정밀도 수준에서 공통된 중요한 비트(MSB)를 공유하여 최적의 성능을 달성합니다. 이는 Quantization Aware Training (QAT) 및 OmniQuant 같은 인기 있는 학습 기반 양자화 방법과 호환됩니다.
- ***Performance Highlights***: MatQuant는 int2 모델에서 기존 기법보다 최대 10% 더 높은 정확도를 보여주며, 특히 Gemma-2 9B 모델에서는 int8 양자화된 모형보다 더 우수한 성능을 발휘합니다. 또한 층별 Mix'n'Match 기능을 통해 메모리와 비용 간의 트레이드오프를 최적화하여 다양한 하드웨어 환경에서 효율적으로 대규모 모델을 배포할 수 있습니다.

### [The Hidden Life of Tokens: Reducing Hallucination of Large Vision-Language Models via Visual Information Steering](https://arxiv.org/abs/2502.03628)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.03628.png)

Vote: 9

Authors: Yuxiao Chen, Zhuowei Li, Haizhou Shi, Dimitris N. Metaxas, Hao Wang, Di Liu, Long Zhao, Ting Liu, Yunhe Gao, Zhenting Wang

- ***What's New***: 이 연구는 대형 비전-언어 모델(LVLMs)의 환각(hallucination)을 완화하기 위한 VISTA(Visual Information Steering with Token-logit Augmentation)를 제안합니다. VISTA는 훈련을 필요로 하지 않는 연구자로, 시각 정보를 활성화 공간에서 강화하고, 초기 레이어 활성화를 활용하여 의미 있는 토큰 생성을 촉진합니다.
- ***Technical Details***: VISTA는 두 가지 상호 보완적인 모듈로 구성됩니다. 첫 번째는 Visual Steering Vector(VSV)로, 시각적 단서를 활성화 공간에서 추출하고 강화하여 점진적인 시각 정보 손실을 방지합니다. 두 번째 모듈은 Self-Logits Augmentation(SLA)로, 최종 레이어 이전의 레이어 활성화를 통해 의미 있는 생성이 우선되도록 합니다. 이러한 조합은 LVLM에서 환각을 효과적으로 감소시킬 수 있습니다.
- ***Performance Highlights***: VISTA는 다양한 방법을 사용하는 네 가지 LVLM 아키텍처에서 환각을 평균 약 40% 감소시키고, 세 가지 디코딩 전략을 통해 일관되게 기존 방법보다 우수한 성능을 보였습니다. LLAVA, Shikra, MiniGPT-4, 및 InstructBLIP 등 아키텍처에서의 실험 결과, VISTA는 모든 디코딩 전략에서 뛰어난 성능을 발휘합니다.

### [EVEv2: Improved Baselines for Encoder-Free Vision-Language Models](https://arxiv.org/abs/2502.06788)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06788.png)

Vote: 9

Authors: Ting Pan, Xiaotong Li, Haiwen Diao, Yueze Wang, Xinlong Wang, Wenxuan Wang, Yufeng Cui, Haoge Deng, Huchuan Lu

- ***What's New***: EVEv2.0는 encoder-free 시각-언어 모델(Vision-Language Models; VLMs)의 새로운 기준을 제시합니다. EVEv2.0은 modality-wise sparsity를 도입하여 다양한 모듈을 디코더-온리 구조로 완벽히 분리합니다. 이는 통합된 모델에서 비전-언어 간의 간섭을 최소화하며, 데이터 확장 효율을 극대화합니다.
- ***Technical Details***: EVEv2.0는 dense transformer를 sparse하고 modality-aware한 디코더-온리 구조로 변환하여 모든 모달리티 사이의 상호작용을 개선합니다. 시각적 임베딩을 위해, patch embedding layer를 설정하여 이미지 정보를 추출합니다. 이후, 텍스트 토크나이저를 사용하여 텍스트를 임베딩하여 두 모달리티를 통합합니다. 또한, 전처리된 이미지-텍스트 데이터셋을 활용하여 단계별 학습 전략을 통해 모달리티 간의 효과적인 정렬을 달성합니다.
- ***Performance Highlights***: EVEv2.0는 encoder-free 측면에서 이전 버전 및 다른 대조군들과 비교하여 다양한 벤치마크에서 월등한 성능을 보입니다. 공개 데이터 100M만으로도 기존 encoder-free 모델을 능가하며, 유사한 용량의 encoder 기반 모델들과 성능 격차를 지속적으로 좁혀갑니다. 이는 lossless한 시각 인코딩 방식의 우수성을 입증합니다.

### [History-Guided Video Diffusion](https://arxiv.org/abs/2502.06764)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06764.png)

Vote: 8

Authors: Vincent Sitzmann, Max Simchowitz, Russ Tedrake, Yilun Du, Boyuan Chen, Kiwhan Song

- ***What's New***: 이 논문에서는 다양한 길이의 과거 비디오 프레임 히스토리(History)를 가이드(Guidance)로 활용하여 조건부 비디오 생성의 품질을 향상시키는 방법을 제안합니다. 이를 위해 새로운 비디오 확산(비디오 Diffusion) 아키텍처인 Diffusion Forcing Transformer (DFoT)를 소개하며, 이 아키텍처는 유연한 조건부 비디오 생성을 지원합니다.
- ***Technical Details***: DFoT 아키텍처는 각각의 비디오 프레임에 독립적인 노이즈 레벨을 적용하여 훈련되며, 마스킹으로서의 노이즈를 통해 과거 프레임을 유연하게 지원합니다. 비 선형 Transformer 구조와 결합하여 Code Generation 및 시각적 추론을 강화하는 History Guidance라는 새로운 가이드 기법을 지원합니다. 이러한 가이드는 다양한 히스토리 구간이나 주파수 축에 따른 조건부 점수들을 조합하여 생성 품질을 향상시킵니다.
- ***Performance Highlights***: DFoT와 관련된 History Guidance 기법은 기존의 DiT 확산 모델보다 뛰어난 품질과 일관성을 보여줍니다. 실험 결과 DFoT는 학습 집합 내 최대 길이를 넘는 800 프레임 이상의 비디오를 생성할 수 있으며, 산업 모델과 견줄만한 성능을 갖추고 있음을 보여주었습니다.

### [Lumina-Video: Efficient and Flexible Video Generation with Multi-scale Next-DiT](https://arxiv.org/abs/2502.06782)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06782.png)

Vote: 8

Authors: Qibin Hou, Bin Fu, Peng Gao, Ziwei Liu, Yuewen Cao, Dongyang Liu, Qi Qin, Kai Wang, Yufei Liu, Hongsheng Li, Yi Xin, Xinyue Li, Chenyang Si, Zhongyu Li, Conghui He, Zhen Li, Yutong Liu, Shicheng Li, Yu Qiao

- ***What's New***: Lumina-Video는 최신 Diffusion Transformers(DiTs)를 활용하여 비디오 생성을 효과적으로 수행하는 새로운 프레임워크입니다. 특히, Multi-scale Next-DiT 아키텍처를 도입하여 패치화(patchification)를 통해 효율성과 유연성을 동시에 강화합니다. 또한, 동작 점수(motion score)를 조건 입력으로 포함시켜 생성된 비디오의 동적 정도를 직접 조절할 수 있습니다.
- ***Technical Details***: Lumina-Video는 여러 패치 크기와 공통 DiT 백본을 공유하는 Multi-scale Next-DiT 아키텍처를 통해 다양한 컴퓨팅 예산에 맞게 비디오 구조를 학습합니다. Optical Flow를 사용하여 얻은 동작 점수를 통해 DiT를 추가 조절하고, 긍정 및 부정의 classifier-free guidance(CFG) 샘플의 동작 조건화를 정밀하게 조작하여 생성된 비디오의 역동성을 효과적으로 제어합니다.
- ***Performance Highlights***: Lumina-Video는 VBench 벤치마크에서 뛰어난 성능을 보였으며, 높은 해상도와 동작 부드러움을 제공하는 동시에 효율적인 학습 및 추론을 달성했습니다. 특히 Multi-scale Next-DiT를 활용한 설계로 효율성과 품질 사이의 균형을 잘 유지하면서 다양한 해상도의 고품질 비디오 생성에 성공하였습니다. 또한, Lumina-V2A라는 비디오-오디오 모델을 설계하여 무음 비디오에 동기화된 소리를 생성할 수 있습니다.

### [MetaChain: A Fully-Automated and Zero-Code Framework for LLM Agents](https://arxiv.org/abs/2502.05957)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.05957.png)

Vote: 7

Authors: Tianyu Fan, Chao Huang, Jiabin Tang

- ***What's New***: MetaChain은 LLM Agents를 위한 완전 자동화된 제로코드(Low-Code) 프레임워크로, 자연어(Natural Language)만으로 에이전트를 생성하고 배포할 수 있도록 합니다. 이를 통해 코딩 경험이 없는 비전문가도 자신만의 AI 에이전트와 워크플로우를 손쉽게 생성, 커스터마이즈 할 수 있습니다.
- ***Technical Details***: MetaChain 프레임워크는 에이전틱 시스템 유틸리티(Agentic System Utilities), LLM 기반 태스크 실행 엔진(LLM-powered Actionable Engine), 셀프 매니징 파일 시스템(Self-Managing File System), 셀프 플레이 에이전트 커스터마이제이션(Self-Play Agent Customization) 모듈로 구성되어 있습니다. 이러한 구조는 코딩 없이 에이전트 작성 및 도구 통합을 자동화하여 강화된 액션 자동 생성과 정보 접근을 가능하게 합니다.
- ***Performance Highlights***: MetaChain은 GAIA 벤치마크에서 오픈소스 에이전트 시스템 중 상위 2위를 기록했으며, Retrieval-Augmented Generation (RAG) 성능에서 기존 SOTA를 능가하는 성과를 보였습니다. 다양한 실제 시나리오에서의 사례 연구를 통해 자동화된 에이전트 개발에 있어 강력한 자기 발달 능력을 확인하였습니다.

### [Dual Caption Preference Optimization for Diffusion Models](https://arxiv.org/abs/2502.06023)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06023.png)

Vote: 7

Authors: Yiran Luo, Bimsara Pathiraja, Shamanthak Hegde, Amir Saeidi, Agneet Chatterjee, Chitta Baral, Yezhou Yang

- ***What's New***: 이 연구는 Dual Caption Preference Optimization (DCPO)라는 새로운 방법을 제안하여, 확산 모델(Diffusion Models)을 선호하는 이미지와 덜 선호하는 이미지에 각각 서로 다른 두 개의 캡션을 활용하여 조정하는 방법입니다. Pick-Double Caption이라는 데이터셋을 도입하여 선호 이미지와 덜 선호 이미지에 대한 별도의 캡션을 제공함으로써, 데이터 충돌 배포 문제와 무관한 프롬프트 문제를 해결하려 합니다.
- ***Technical Details***: DCPO는 텍스트 생성 프레임워크와 새로운 목적함수를 통해 두 단계로 구성됩니다. 이 방법은 기존 데이터셋의 충돌 배포 문제를 완화하고자 합니다. 첫 번째 캡션 방법은 이미지와 원래 프롬프트를 기반으로 새로운 캡션을 생성합니다. 두 번째 방법은 대규모 언어 모델(LLM)을 이용해 원래의 프롬프트를 변형하여 덜 선호하는 이미지에 대한 프롬프트를 생성합니다. DCPO는 선호하는 이미지의 가능성을 증가시키고 덜 선호하는 이미지의 가능성을 감소시키는 방식으로 최적화를 수행합니다.
- ***Performance Highlights***: 실험 결과, DCPO는 Stable Diffusion (SD) 2.1, SFTChosen, Diffusion-DPO, MaPO 등 기존의 방법을 여러 평가 지표에서 능가했으며, 특히 Pickscore, HPSv2.1, GenEval, CLIPscore, 그리고 ImageReward에서 두드러진 성과를 보여줍니다. 예를 들어 Pickscore에서 +0.21, HPSv2.1에서 +0.45, ImageReward의 경우 normalized 기준으로 +1.8, CLIPscore에서 +0.15, 그리고 GenEval에서 +3%의 개선을 기록하였습니다.

### [The Curse of Depth in Large Language Models](https://arxiv.org/abs/2502.05795)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.05795.png)

Vote: 6

Authors: Lu Yin, Wenfang Sun, Yefeng Zheng, Pengxiang Li, Shiwei Liu, Xinyuan Song

- ***What's New***: 이 논문에서는 대형 언어 모델(Large Language Models; LLMs)에서 깊이가 깊은 층의 비효율성을 강조하는 '깊이의 저주(Curse of Depth)' 개념을 소개합니다. 이러한 문제를 해결하기 위해 층 정규화(Layer Normalization)의 출력 분산을 깊이에 의해 역비례로 스케일링하는 'LayerNorm Scaling'을 제안했습니다.
- ***Technical Details***: 깊이의 저주는 LLM의 깊은 층이 학습과 표현에 충분히 기여하지 못하는 현상을 설명하며, Pre-Layer Normalization(Pre-LN)을 사용해 층의 입력을 정규화할 때 발생하는 출력 분산의 폭발로 인한 것임을 밝혀냈습니다. LayerNorm Scaling은 각 층의 정규화 출력을 스케일링하여 이러한 분산 폭발을 막고, 더 깊은 층이 학습에 효과적으로 기여하도록 합니다.
- ***Performance Highlights***: 실험 결과 LayerNorm Scaling은 기존의 Pre-LN 방법에 비해 LLM의 사전 학습 성능을 크게 향상시켰으며, 같은 손실을 달성하기 위한 학습 토큰의 수도 절반으로 줄였습니다. 이는 더 깊은 층이 학습에 효과적으로 기여할 수 있도록 했기 때문입니다.

### [Efficient-vDiT: Efficient Video Diffusion Transformers With Attention Tile](https://arxiv.org/abs/2502.06155)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06155.png)

Vote: 6

Authors: Zhijie Deng, Hangliang Ding, Hao Zhang, Peiyuan Zhang, Dacheng Li, Runlong Su, Ion Stoica

- ***What's New***: Efficient-vDiT는 Attention Tile이라는 새로운 개념을 도입하여 비디오 생성에서 기존에 사용된 3D 풀 어텐션의 비효율성을 극복하는 혁신적인 방법을 제시합니다. 이는 비디오 데이터 내 중복성을 기반으로 어텐션을 정돈하여 선형 복잡도를 유지하면서 성능을 개선하는 새로운 경량의 조밀한 어텐션 마스크를 갖춘 방법입니다.
- ***Technical Details***: Efficient-vDiT는 주어진 모델에서 3단계의 프로세스를 거쳐 생성 속도를 크게 높입니다. 먼저, 다중 단계의 일관성 증류(Multi-Step Consistency Distillation; MCD)를 통해 각 단계의 일관성 증류를 수행하며 간격별로 일관성을 보장합니다. 그런 다음 층별 희소 어텐션 마스크를 결정하고 지식 증류를 통해 성능을 유지합니다. 특히, Attention Tile 패턴의 데이터 독립적 특성을 활용하여 고정된 집합의 어텐션 마스크를 사용하며, 계산 복잡성을 선형으로 줄입니다.
- ***Performance Highlights***: Open-Sora-Plan 모델을 기반으로 한 실험에서, Efficient-vDiT는 29 프레임 비디오 생성에서 최대 7.8배, 93 프레임에서 최대 7.4배의 속도 향상을 보여주며, VBench에서 성능 저하는 미미했습니다. 또한 4개의 GPU에서 실행 시, 시퀀스 병렬 처리(sequence parallelism)를 통합하여 추가로 3.91배의 속도 향상을 달성하였습니다.

### [CustomVideoX: 3D Reference Attention Driven Dynamic Adaptation for Zero-Shot Customized Video Diffusion Transformers](https://arxiv.org/abs/2502.06527)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06527.png)

Vote: 5

Authors: Zhen Yang, Guanghao Zhang, Haobin Tang, Yi Wang, Yunlong Yu, Jingxuan Pang, D. She, Qihan Huang, Siming Fu, Mushui Liu, Jin Wang, Wanggui He

- ***What's New***: CustomVideoX는 새로운 Zero-Shot 개인화 비디오 생성 프레임워크로, 3D Reference Attention을 통해 참조 이미지와 비디오 생성 간의 상호작용을 쉽게 합니다. 이 시스템은 텍스트-비디오 변환 확산 모델 (Text-to-Video Diffusion Transformer) 아키텍처를 기반으로 하며, 참조 이미지에서 개인화된 비디오를 생성합니다.
- ***Technical Details***: CustomVideoX는 사전 학습된 비디오 네트워크를 활용하여, LoRA(Linearized Region of Attention) 파라미터만 훈련하여 참조 특징을 추출해 효율성과 적응성을 유지합니다. 3D Reference Attention 메커니즘을 제안하여, 참조 이미지 특징이 공간적 및 시간적 차원에서 모든 비디오 프레임과 직접 및 동시 상호작용할 수 있도록 하며, Time-Aware Reference Attention Bias (TAB) 전략을 통해 참조 이미지 특징의 영향력을 시간 단계별로 동적으로 조절합니다.
- ***Performance Highlights***: CustomVideoX는 VideoBench 및 DreamBench와 같은 기존 및 제안된 벤치마크에서 비디오 일관성과 품질 면에서 기존 방법들보다 훨씬 우수한 성능을 보였으며, 개인화된 비디오 생성 분야에서 새로운 최고 성능을 달성했습니다.

### [APE: Faster and Longer Context-Augmented Generation via Adaptive Parallel Encoding](https://arxiv.org/abs/2502.05431)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.05431.png)

Vote: 5

Authors: Beidi Chen, Xinyu Yang, Tianqi Chen

- ***What's New***: APE(Adaptive Parallel Encoding)는 다중 컨텍스트를 효과적으로 병렬로 인코딩하여 빠르고 긴 컨텍스트를 활용하는 새롭고 획기적인 방법론입니다. APE는 RAG(Retrieval-Augmented Generation) 및 ICL(In-Context Learning)에서 기존의 병렬 인코딩(Parellel Encoding)이 갖는 성능 저하 문제를 해결하며, 위치 재사용을 통해 더 많은 컨텍스트를 수용할 수 있게 합니다.
- ***Technical Details***: APE는 각 컨텍스트의 KV(Key-Value) 상태를 독립적으로 사전 계산 및 캐싱이 가능하도록 설계되었습니다. APE는 순차적 인코딩과 병렬 인코딩 간의 주의(distribution)를 공유하는 prefix, 주의 온도, 및 scaling factor를 통해 정렬합니다. 이러한 요소는 초기 위치에서 발생하는 이상치를 피하고, 적절한 폴링을 통해 주의 분포를 날카롭게 하여, 관련 정보에 집중할 수 있도록 합니다.
- ***Performance Highlights***: RAG와 ICL에서 APE는 순차적 인코딩의 98% 및 93%의 성능을 보존하면서 각각 3.6% 및 7.9% 더 성능이 향상되었음을 보여주었습니다. APE는 수백 개의 컨텍스트를 병렬로 인코딩할 수 있으며, 128k 길이의 컨텍스트에 대해 28배의 사전 채우기 시간을 단축하여 전체 속도를 4.5배까지 가속할 수 있습니다.

### [Steel-LLM:From Scratch to Open Source -- A Personal Journey in Building a Chinese-Centric LLM](https://arxiv.org/abs/2502.06635)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06635.png)

Vote: 4

Authors: Shu Li, Zhaoxiang Zhang, Tianyu Zheng, Qingshui Gu

- ***What's New***: Steel-LLM은 제한된 컴퓨팅 자원으로 개발된 중국어 중심의 오픈 소스 대형 언어 모델(LLM)로, 자원 효율성을 강조하며 완전한 투명성을 제공하는 프로젝트입니다.
- ***Technical Details***: Steel-LLM의 모델 아키텍처는 Soft Mixture of Experts (Soft MOE)와 강화된 Feed-Forward Network를 포함하여 Qwen 계열의 개선된 트랜스포머 구조를 기반으로 합니다. 1억 개의 파라미터와 8개의 GPU만을 사용하여 개발된 이 모델은 Mixed-Precision Training, FlashAttention과 같은 최적화를 통해 실행 효율성을 높였습니다.
- ***Performance Highlights***: Steel-LLM은 CEVAL과 CMMLU 같은 중국어 벤치마크에서 초기 대형 기관의 모델을 능가하는 성능을 입증했습니다. CEVAL에서 41.90%의 정확도를 기록했으며, CMMLU에서는 36.08%의 성능을 보였습니다.

### [DreamDPO: Aligning Text-to-3D Generation with Human Preferences via Direct Preference Optimization](https://arxiv.org/abs/2502.04370)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04370.png)

Vote: 4

Authors: Yi Yang, Tat-Seng Chua, Fan Ma, Hehe Fan, Zhenglin Zhou, Xiaobo Xia

- ***What's New***: DreamDPO는 3D 생성 과정에서 인간의 선호를 직접 최적화(Direct Preference Optimization)로 통합하여 텍스트에서 3D로의 생성품질을 향상시킨 새로운 접근법입니다. 이 프레임워크는 기존 방법들이 부정확한 평가에 의존하는 문제를 해결하고 인간의 세밀한 선호에 맞춘 3D 컨텐츠를 생성하는데 집중하고 있습니다.
- ***Technical Details***: DreamDPO는 NeRF와 같은 초기 3D 표현을 이용하고, 세 가지 단계적 과정을 통해 이를 최적화합니다: (1) 가우시안 노이즈를 적용하여 쌍별 예제 생성; (2) 보상 모델이나 대형 멀티모달 모델(Large Multimodal Models; LMM)을 이용한 쌍별 예제 비교; (3) 선호에 기반한 최적화를 통해 3D 표현을 갱신합니다. 이 과정에서 피드백을 받은 예제는 가까이 끌어당기고, 선호도가 낮은 예제는 배제하는 최적화를 수행하여 품질 향상과 컨트롤을 도모합니다.
- ***Performance Highlights***: DreamDPO는 GPTEval3D와 같은 다양한 평가에서 텍스트-3D 생성 벤치마크에서 기존 13가지 최첨단 방법들을 능가하는 성능을 발휘했습니다. 구체적으로 텍스트와 3D 자산의 정렬에서 28.4, 3D 개연성에서 24.4, 텍스처 세부사항에서는 48.4 등에서 향상된 점수를 기록하며, 전반적인 성능면에서 가장 우수한 결과를 보여주었습니다.

### [Adaptive Semantic Prompt Caching with VectorQ](https://arxiv.org/abs/2502.03771)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.03771.png)

Vote: 3

Authors: Joseph E. Gonzalez, Stephan Krusche, Matei Zaharia, Alfons Kemper, Alejandro Cuadron, Shu Liu, Luis Gaspar Schroeder, Mark Zhao

- ***What's New***: 이 논문은 VectorQ라는 프레임워크를 제안하여, 현재의 정적 임계값(static threshold)이 가진 한계를 극복하고 임베딩에 특화된 임계값 구역(threshold regions)을 학습하여, 의미적 프롬프트 캐시(semantic prompt cache)에서 정확성과 효율성을 향상시킵니다. VectorQ는 베이시안 샘플링(Bayesian sampling)을 사용하여 캐시 적중(cache hit)의 올바름을 재평가하고, 임베딩에 따라 동적으로 임계값을 조정합니다.
- ***Technical Details***: VectorQ는 각 임베딩에 대해 세 가지 임계값 구역 R1, R2, R3을 정의합니다. 이 구역은 캐시 적중 여부를 결정하는데 사용되며, 임베딩의 유사도 점수에 따라 구역이 확장 또는 축소됩니다. 베이시안 샘플링을 통해 특정 유사도 값에 대한 올바름의 불확실성을 줄임으로써 캐시 적중의 정확성을 보장합니다.
- ***Performance Highlights***: VectorQ는 최대 12배 더 높은 캐시 적중률(cache hit rate)을 달성하고, 오류율을 최대 92%까지 감소시켜 기존의 정적 임계값 시스템보다 우수한 성능을 보여줍니다. 다양한 데이터셋을 사용하는 벤치마크 실험에서 VectorQ의 임베딩 특화 임계값 구역이 정적 임계값보다 더 효과적임을 입증했습니다.

### [SPARC: Subspace-Aware Prompt Adaptation for Robust Continual Learning in LLMs](https://arxiv.org/abs/2502.02909)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.02909.png)

Vote: 2

Authors: Dinithi Jayasuriya, Amit Ranjan Trivedi, Ranganath Krishnan, Davide Ettori, Sina Tayebati

- ***What's New***: SPARC는 대형 언어 모델(LLMs)을 위한 경량의 지속적인 학습 프레임워크로, 프롬프트튜닝(prompt tuning)을 통해 저차원 공간에서 효율적인 작업 적응을 가능하게 합니다. 주요 기술적 혁신은 주성분 분석(principal component analysis; PCA)을 활용하여 훈련 데이터의 컴팩트한 부분 공간을 식별하고, 적응 과정에서 저장된 지식을 손상시키지 않으면서 효율적인 학습을 지원한다는 점입니다.
- ***Technical Details***: SPARC 프레임워크는 각 작업을 입력 임베딩 공간의 하위 공간으로 표현합니다. 학습 중 새로운 작업이 기존 프롬프트를 재사용할 수 있는지를 결정하기 위해 하위 공간의 겹침 정도를 측정하며, 최소한의 겹침을 갖는 작업에 대해서는 직교 공간에서 새로운 프롬프트를 초기화합니다. 이는 작업 간의 간섭을 방지하며, 모델의 구조적 변화를 요구하지 않으면서도 적은 가중치 업데이트로 대형 모델에서의 확장성을 보장합니다.
- ***Performance Highlights***: SPARC는 GPT-2와 DeBERTa-base 모델을 기반으로 한 실험에서 두각을 나타냈습니다. 프롬프트의 재사용을 통해 거의 모든 이전 지식(97% 유지)을 유지하면서 새로운 작업을 효과적으로 학습할 수 있었습니다. 도메인 증분 학습에서 평균의 망각 비율은 3%에 불과하며, 태스크 증분 학습에서는 망각 없이 성과를 냈습니다. 이는 기존 접근법에 비해 학습 파라미터 0.04%만 미세 조정하여 달성된 결과입니다.

### [Towards Internet-Scale Training For Agents](https://arxiv.org/abs/2502.06776)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06776.png)

Vote: 1

Authors: Ruslan Salakhutdinov, Brandon Trabucco, Robinson Piramuthu, Gunnar Sigurdsson

- ***What's New***: InSTA 파이프라인은 인간 주석 없이 에이전트를 인터넷 규모로 훈련시키는 자동 데이터 생성 방식을 최초로 제안합니다. 우리는 사전 훈련된 언어 모델(LLM)을 활용하여 150,000개 웹사이트에서 웹 내비게이션 작업을 생성, 시도, 평가하며, LLM의 정확한 판단을 통해 안전하고 다양한 웹사이트를 선택하여 데이터를 생성합니다.
- ***Technical Details***: InSTA 파이프라인은 세 단계로 구성됩니다. 첫째, LLM이 웹 내비게이션 작업을 제안합니다. 둘째, LLM 에이전트가 웹 브라우저를 사용하여 작업을 시도합니다. 셋째, LLM이 경로를 평가하여 작업 성공 가능성을 판단합니다. Llama 3.1 70B 기반의 에이전트는 Playwright API를 통해 작업을 수행하며, LLM 심판은 작업 성공 여부를 93.1%의 정확도로 평가합니다.
- ***Performance Highlights***: InSTA 파이프라인을 통해 생성된 데이터로 훈련된 에이전트는 Mind2Web과 WebLINX에서 각각 89.5%, 122.1%의 스텝 정확도 개선을 보였습니다. 모든 가용한 인간 데이터로 훈련된 에이전트가 다양한 실제 웹사이트에 일반화하지 못할 때, InSTA 데이터를 추가하면 WebLINX에서 149.0%, Mind2Web에서 156.3% 일반화 능력이 향상되었습니다.

### [Intelligent Sensing-to-Action for Robust Autonomy at the Edge: Opportunities and Challenges](https://arxiv.org/abs/2502.02692)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.02692.png)

Vote: 0

Authors: Saibal Mukhopadhyay, Nastaran Darabi, Dinithi Jayasuriya, Adarsh Kumar Kosta, Yeshwanth Venkatesha, Nethmi Jayasinghe, Priyadarshini Panda, Amit Ranjan Trivedi, Hemant Kumawat, Kaushik Roy, Divake Kumar, Sina Tayebati

- ***What's New***: 이 연구는 엣지(Edge) 자율 컴퓨팅을 위한 'Sensing-to-Action' 루프의 통합과 혁신 기회를 강조하며, 이를 통해 로봇 공학, 스마트 시티, 자율 주행 차량 등에서의 실시간 의사 결정을 강화할 수 있는 방법을 제안합니다. 특히, 신경모방 컴퓨팅(bio-inspired neuromorphic computing)을 활용하여 에너지 효율을 높이고, 현장 제어에서 효율적인 자원 사용을 지원할 수 있는 방법을 제시합니다.
- ***Technical Details***: 본 논문에서는 폐회로 방식의 'Sensing-to-Action' 루프를 엣지 컴퓨팅 환경에서 개선하는 다양한 전략을 탐구합니다. 특히, R-MAE (Radially Masked Autoencoder)와 같은 생성적 감지(generative sensing) 방식이 LiDAR와 같은 감지 장치의 데이터를 효율적으로 활용하는 방법을 소개합니다. 이를 통해 데이터의 중복을 줄이고 처리 효율성을 높이며, 적응형 제어를 통한 시스템의 지속 가능성을 향상시킵니다. 또한, 신경모방 컴퓨팅(neuromorphic computing)을 통해 이벤트 기반(event-driven)의 스파이크 네트워크 구조를 활용하여, 전력 소모를 줄이고, 실시간 응답성을 높이는 방법을 제시합니다.
- ***Performance Highlights***: 생성적 감지 기법인 R-MAE는 LiDAR 데이터에서 평균 8%의 장면만을 적극적으로 감지하면서도 성능을 유지하여 에너지 소비를 9배 이상 감소시킵니다. 또한, STARNet 프레임워크를 통해 센서 데이터의 신뢰성을 향상시켜, 복잡한 데이터 세트에서 10% 이상의 예측 정확도 향상을 달성했습니다. 여러 에이전트가 협력하는 다중 에이전트(multi-agent) 시스템에서는 효율적인 자원 할당을 통해 에너지 소비를 3배까지 줄이는 성과를 보여, 자율 시스템의 신뢰성을 높입니다.

### [Embodied Red Teaming for Auditing Robotic Foundation Models](https://arxiv.org/abs/2411.18676)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.18676.png)

Vote: 0

Authors: Rahul Gupta, Christophe Dupuy, Yen-Chen Lin, Sathwik Karnik, Nishant Abhangi, Tsun-Hsuan Wang, Pulkit Agrawal, Zhang-Wei Hong

- ***What's New***: 이 논문에서는 로봇의 언어 조건 모델을 평가하는 새로운 방법인 Embodied Red Teaming(ERT)을 소개합니다. ERT는 다양하고 도전적인 명령어를 생성하여 로봇 모델의 안전성과 성능을 테스트하며, 이는 기존 벤치마크가 실제 환경에서의 안전성을 충분히 평가하지 못함을 보여줍니다.
- ***Technical Details***: ERT는 비전-언어 모델(Vision Language Models, VLM)을 사용하여 환경에 기반한 명령어를 생성하고, 로봇이 실패하도록 유도합니다. 이 프로세스는 반복적인 인컨텍스트 러닝(in-context learning)을 통해 명령어를 개선하며, VLM은 다양한 명령어 세트를 샘플링하여 가장 다양한 세트를 선택합니다.
- ***Performance Highlights***: 최신 언어 조건 로봇 모델은 ERT 가 생성한 명령어에서 실패하거나 비정상적인 행동을 보였으며, 이는 현재의 벤치마크가 실제 환경에서 로봇의 성능과 안전성을 평가하는 데 한계가 있음을 나타냅니다. 이러한 결과는 미래 연구 방향에 중요한 시사점을 제공합니다.

