## Daily Papers (2025-02-21)

### [MLGym: A New Framework and Benchmark for Advancing AI Research Agents](https://arxiv.org/abs/2502.14499)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14499.png)

Vote: 121

Authors: Dieuwke Hupkes, Jakob Foerster, Ajay Menon, Lovish Madaan, Tatiana Shavrina, Yoram Bachrach, William Yang Wang, Vladislav Vorotilov, Gaurav Chaurasia, Vincent Moens, Deepak Nathani, Ricardo Silveira Cabral, Nicholas Roberts, Despoina Magka, Nikolay Bashlykov, Amar Budhiraja, Roberta Raileanu

- ***What's New***: MLGym은 AI 연구 태스크에서 LLM 에이전트를 평가하고 개발할 수 있는 새로운 프레임워크와 벤치마크인 MLGym-Bench를 소개합니다. 이는 머신 러닝 태스크를 위한 최초의 Gym 환경으로써, 강화 학습 알고리즘을 연구하는 데에 유용합니다.
- ***Technical Details***: MLGym-Bench는 컴퓨터 비전, 자연어 처리, 강화 학습, 게임 이론 등 다양한 분야에서 UX를 평가하기 위한 13개의 열린 결말 연구 태스크로 구성되어 있습니다. 이 프레임워크는 새로운 태스크를 추가하고, 모델이나 에이전트를 통합하고 평가하며, 대규모 합성 데이터를 생성하고, AI 연구 태스크에 대한 학습 알고리즘을 개발하는 데 용이합니다.
- ***Performance Highlights***: O1-preview 모델은 Best Attempt와 Best Submission 점수 모두에서 비교적 우수한 성능을 보였습니다. 그러나, Gemini-1.5-Pro와 Claude-3.5-Sonnet도 비용 대비 높은 효율성을 제공하여 OpenAI의 O1 모델과 비교했을 때 비슷한 수준으로 성과를 냈습니다. Gemini-1.5-Pro는 비용 대비 가장 효율적인 모델로, O1 모델보다 약 9배 저렴하면서 O1 모델의 99%의 AUP를 달성했습니다.

### [SuperGPQA: Scaling LLM Evaluation across 285 Graduate Disciplines](https://arxiv.org/abs/2502.14739)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14739.png)

Vote: 78

Authors: Yiyan Liao, Qian Liu, Qunshu Lin, Yuelin Bai, Tianyang Zhan, Wangchunshu Zhou, Ge Zhang, Shian Jia, Shiwen Ni, Liang Chen, Ruibin Yuan, Jiajun Xu, Yuanhao Yue, Siming Huang, Bingli Wang, Min Yang, Wenbo Su, Sirun Li, Shi Qiu, Yuansheng Ni, Zhoufutu Wen, Yue Zhang, Xiang Yue, Qige Qi, Hao Wang, Wenhao Huang, Chengtuo Cheng, Haoran Que, M-A-P Team, Xiangyu Zheng, Yunwen Li, Zili Wang, Junting Zhou, Kaijing Ma, Zhaoqun Li, Zifan Peng, Jingyang Zhang, Xingyuan Bu, Zekun Moore Wang, Shi Wang, Qiyao Wang, Chenqing Wang, Jiaheng Liu, Tianshun Xing, Kang Zhu, Junran Peng, Chenghua Zhong, Yizhe Li, Yizhi Li, Minghao Liu, Xingjian Zhang, Yun Huang, Yiming Liang, Yaoru Li, Meng Cao, Yiya Wang, Guoyin Wang, Siwei Wu, Yinghao Ma, Chenglin Cai, Zhenzhu Yang, Yifan Yao, Chengdong Lin, Xiyue Zhang, Xingwei Qu, Qinrui Li, Keyi Ding, Hongquan Lin, Zhenlin Wei, Zhoujun Li, Dehua Ma, Yujia Qin, Yifan Chen, Jian Yang, Tianyu Liu, Ming Xu, Dayiheng Liu, Zhongyuan Peng, Kaixing Deng, Xiaolong Jin, Yongchi Zhao, Yang Gao, Yizhou Tan, Xinrun Du, Rui Li, Chujie Zheng, Tianhao Cheng, Shuyue Guo, Zhaoxiang Zhang, Tianhao Liang, Chun Zhang, Tianyu Zheng, Sichao Jiang, Yubo Wang, Kexin Yang

- ***What's New***: SuperGPQA는 대형 언어 모델(LLMs)의 인간 지식 경계를 측정하기 위해 285개의 대학원 수준의 분야에서 평가를 수행하는 포괄적 벤치마크입니다. 기존 벤치마크의 범위를 넘어서는 다양한 전문 분야를 포함하며, Human-LLM 협력 필터링 메커니즘을 사용하여 LLM 답변과 전문가 피드백을 통해 질문을 반복적으로 정제합니다.
- ***Technical Details***: SuperGPQA는 285개의 분야마다 최소 50개의 질문을 포함하며, 데이터 검수는 3단계의 품질 점검 과정을 통해 이루어집니다. 단계에는 데이터 수집, 전사 및 엄격한 품질 점검 프로세스가 포함됩니다. 협업 시스템은 전문가, 크라우드소싱 주석자 및 SOTA LLMs로 구성됩니다.
- ***Performance Highlights***: SuperGPQA에서 좋은 성능을 보인 이유 추론 모델(예: DeepSeek-R1)의 최고 정확도는 61.82%를 기록했으며, 이는 현재의 LLMs가 상당한 개선 여지가 있음을 의미합니다. 또한, 신뢰성 있는 데이터 수집 관리에 대한 종합적인 통찰력을 제공하며, 비슷한 규모의 향후 연구 이니셔티브에 유용한 방법론적 지침을 제안합니다.

### [SigLIP 2: Multilingual Vision-Language Encoders with Improved Semantic Understanding, Localization, and Dense Features](https://arxiv.org/abs/2502.14786)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14786.png)

Vote: 76

Authors: Basil Mustafa, Ibrahim Alabdulmohsin, Muhammad Ferjad Naeem, Michael Tschannen, Olivier Hénaff, Alexey Gritsenko, Talfan Evans, Jeremiah Harmsen, Xiao Wang, Nikhil Parthasarathy, Lucas Beyer, Ye Xia, Xiaohua Zhai, Andreas Steiner

- ***What's New***: SigLIP 2는 기존 SigLIP의 성공을 바탕으로 한 새로운 다국어 비전-언어 인코더(multilingual vision-language encoders)로, 개선된 시맨틱 이해(semantic understanding), 로컬라이제이션(localization), 밀집 표현(dense features)을 제공합니다. 이 모델은 캡셔닝 기반의 프리트레이닝과 자기 지도 학습 손실(self-supervised losses)을 통합하여 다양한 해상도를 지원하며, 데이터의 공정성을 개선하기 위한 디바이싱(De-biasing) 기법이 포함되었습니다.
- ***Technical Details***: SigLIP 2는 CLIP 스타일의 벤치마크를 초과하는 성능을 위해 여러 기술을 통합한 트레이닝 레시피를 사용합니다. 여기에는 LocCa를 사용한 캡셔닝 기반의 프리트레이닝과 SILC(AP TIPS처럼)에서 가져온 자기 증류(self-distillation)와 마스크 예측(masked prediction)이 포함됩니다. 다양한 시퀀스 길이를 지원하는 NaFlex 변종도 도입되었습니다. WebLI 데이터셋을 사용하여 10억 개 이상 이미지와 12억 개 이상의 설명 텍스트로 학습되었습니다.
- ***Performance Highlights***: SigLIP 2는 다양한 벤치마크에서 뛰어난 성능을 보였습니다. 제로샷 이미지 분류와 이미지-텍스트 검색에서 기존 모델보다 월등히 우수한 성능을 나타내며, 다국어 테스트에서 mSigLIP과 유사한 결과를 보였습니다. 특히 지역 및 지리적 다양성 벤치마크에서 월등히 개선된 성과를 보여주었으며, SigLIP 2는 문화적 다양성과 공정성을 개선하였습니다.

### [How Much Knowledge Can You Pack into a LoRA Adapter without Harming LLM?](https://arxiv.org/abs/2502.14502)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14502.png)

Vote: 50

Authors: Vasily Konovalov, Daniil Moskovskiy, Maria Marina, Mikhail Salnikov, Alexander Panchenko, Pavel Braslavski, Sergey Pletenev

- ***What's New***: 이 연구는 대규모 언어 모델(LLMs)의 성능을 손상시키지 않고 새로운 지식을 LoRA 어댑터에 통합할 수 있는 방법을 조사합니다. LoRA는 LLMs의 파라미터를 효율적으로 조정하기 위한 기법으로, 새로운 지식을 학습하면서도 기존의 학습된 지식을 손상시키지 않는 최적의 방법을 탐색합니다.
- ***Technical Details***: 이 연구에서는 Llama-3.1-8B-Instruct 모델을 LoRA를 이용해 다양한 양의 새로운 지식으로 미세 조정했습니다. 이 과정에서 알려진 사실과 새로운 사실의 혼합이 포함된 훈련 데이터가 최상의 성과를 보인다는 것을 밝혔습니다. LoRA는 사전 학습된 모델의 가중치를 고정하고 각 Transformer 레이어에 훈련 가능한 저랭크(low-rank) 분해 행렬을 주입하여 후속 작업의 훈련 가능한 파라미터 수를 크게 줄여주기 때문입니다.
- ***Performance Highlights***: LoRA를 사용하여 새로운 지식을 추가하면 기존 세계 지식이 손상될 수 있으며, 이는 TruthfulQA 벤치마크에서 성능의 하락으로 나타났습니다. 예를 들어, 모델이 학습한 새 데이터의 양이 증가할수록 외부 질문-응답 벤치마크 성능이 감소했습니다. 이는 도메인 내 지식 학습과 목표 기반 학습의 균형을 맞추는 것이 얼마나 중요한지를 시사합니다. 또한, 새로운 지식을 학습하면서 모델은 질문에 대한 대답을 제공하는 능력을 잃고, 확신을 가지는 경우가 증가합니다.

### [S*: Test Time Scaling for Code Generation](https://arxiv.org/abs/2502.14382)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14382.png)

Vote: 38

Authors: Xiuyu Li, Dacheng Li, Jiarong Xing, Shiyi Cao, Chengkun Cao, Ion Stoica, Kurt Keutzer, Shangyin Tan, Joseph E. Gonzalez

- ***What's New***: S*는 코드 생성(Code Generation) 분야에서 최초로 하이브리드 테스트 타임 스케일링 프레임워크(Test-Time Scaling Framework)를 제안하여, 생성된 코드의 커버리지(Coverage)와 선택 정확도(Selection Accuracy)를 크게 향상시킵니다. 기존의 병렬 스케일링 패러다임을 확장하여 순차적 스케일링을 도입하였으며, 짝을 이룬 샘플들을 구별하기 위해 적응형 입력 생성 메커니즘을 활용하여 올바른 솔루션을 식별합니다.
- ***Technical Details***: S*는 두 가지 주요 단계로 구성됩니다. 첫 번째 단계인 생성 단계에서는 병렬 샘플링(Parallel Sampling)을 순차적 디버깅(Iterative Debugging)과 함께 사용하여 커버리지를 개선하고, 공개 테스트 케이스에 기반한 실행 결과를 통해 샘플을 반복적으로 수정합니다. 두 번째 단계인 선택 단계에서는 LLM을 사용하여 각 샘플 쌍의 구별 가능한 입력을 생성하고 실행 결과를 통해 최적의 샘플을 선택합니다. 사용된 주요 알고리즘은 LLM과 실제 실행 결과를 결합하여 보다 정확하고 신뢰성 있는 샘플 선택을 달성합니다.
- ***Performance Highlights***: S*는 12개의 대형 언어 모델(Large Language Models)과 대형 추론 모델(Large Reasoning Model)에서 일관되게 성능을 향상시킵니다. S*는 작은 모델이 GPT-4o-mini를 능가할 수 있고, 비추론 모델이 추론 모델을 초과할 수 있도록 하며, DeepSeek-R1-Distill-Qwen-32B 모델과 함께 기존의 최첨단 모델 수준에 가까운 성과인 85.7%를 LiveCodeBench에서 달성합니다.

### [On the Trustworthiness of Generative Foundation Models: Guideline, Assessment, and Perspective](https://arxiv.org/abs/2502.14296)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14296.png)

Vote: 36

Authors: Xiang Li, Siyuan Wu, Huan Zhang, Huaxiu Yao, Andy Zou, Kai Shu, Bryan Hooi Kuen-Yew, Furong Huang, Yujun Zhou, Yue Zhao, Chujie Gao, Zhihao Jia, Yu Su, Or Cohen Sasson, Jianfeng Gao, Chaowei Xiao, Nitesh V. Chawla, Xiyang Hu, Jieyu Zhao, Philip S. Yu, Zhaoyi Liu, Max Lamparth, Pin-Yu Chen, Jaehong Yoon, Han Bao, Zhengzhong Tu, Hongzhi Yin, Weijia Shi, Jieyu Zhang, Michael Backes, Yuexing Hao, Yanbo Wang, Yue Huang, Jiayi Ye, Swabha Swayamdipta, Neil Zhenqiang Gong, Jian Pei, Qihui Zhang, Zhize Li, Ruoxi Chen, Hongyang Zhang, Jiawen Shi, Yuan Li, Elias Stengel-Eskin, Lichao Sun, Nouha Dziri, Mohit Bansal, Taiwei Shi, Xiangliang Zhang, Heng Ji, Xiangqi Wang, Kaijie Zhu, Tianyi Zhou, Bo Li, Anka Reuel, Ranjay Krishna, Huan Sun, Haoran Wang, Xiuying Chen, Caiming Xiong, Kehan Guo, Dongping Chen, Prasanna Sattigeri, Tianrui Guan, Yiwei Li

- ***What's New***: 이 논문에서 소개된 TrustGen은 생성적 모델의 신뢰성을 다차원적으로 평가하기 위한 동적 평가 플랫폼으로, 전통적으로 고정된 벤치마크와는 달리 지속적으로 업데이트되며 진화하는 모델의 능력을 평가할 수 있습니다.
- ***Technical Details***: TrustGen은 Metadata Curator, Test Case Builder, Contextual Variator 등 3개의 모듈로 구성되어 있으며, 텍스트-이미지 모델, 대형 언어 모델, 비전-언어 모델을 평가합니다. 이 시스템은 모델의 진화에 따른 평가 데이터를 지속적으로 생성하고, 평가 모듈을 통해 다양한 시나리오에 걸쳐 신뢰성을 테스트합니다.
- ***Performance Highlights***: TrustGen 벤치마크에서 Dall-E 3는 텍스트-이미지 모델 중 높은 안전 점수(94)를 기록하였으며, LLM 중에서는 GPT-4o가 높은 전반적 신뢰성을 보여주었습니다. 또한 BERT, GPT 모델 시리즈와 같은 설치형 모델들이 자주 사용되는 언어 모델에서 신뢰성, 적합성 및 안전점수 평가에서 두각을 나타냈습니다.

### [Logic-RL: Unleashing LLM Reasoning with Rule-Based Reinforcement Learning](https://arxiv.org/abs/2502.14768)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14768.png)

Vote: 24

Authors: Chong Luo, Haoming Luo, Zhirong Wu, Tian Xie, Yuqian Hong, Joey Zhou, Bryan Dai, Kai Qiu, Qingnan Ren, Zitian Gao

- ***What's New***: Logic-RL은 규칙 기반 강화 학습(Rule-Based Reinforcement Learning)을 통해 대형 언어 모델(LLM)의 추론 능력을 향상시키는 혁신적인 방법론을 제시합니다. 이 연구는 논리 퍼즐을 훈련 데이터로 사용하여 효과적이고 안정적인 RL 훈련을 이루고, 복잡한 수학 벤치마크 AIME와 AMC로의 일반화 능력을 시연합니다.
- ***Technical Details***: Logic-RL 프레임워크는 REINFORCE++ 알고리즘을 활용하여 규칙 기반의 보상 시스템을 구현하였습니다. 훈련 데이터는 절차적으로 생성된 Knights and Knaves(K&K) 논리 퍼즐을 사용하여 다양한 난이도를 조절하고 보상 검증을 용이하게 하였습니다. 형식 포맷 보상 함수와 시스템 프롬프트를 통해 모델이 추론과정을 정확하게 따르도록 설계되었습니다.
- ***Performance Highlights***: Logic-RL은 단 5,000개의 논리 퍼즐 훈련만으로 AIME 벤치마크에서 125%, AMC에서 38%의 성능 향상을 보여주었습니다. 모델은 복잡한 추론 행동들, 예를 들어 탐색, 검증, 요약 등을 자연적으로 학습하며, 이는 초기 훈련 데이터에 없는 행동이었습니다. 이러한 성과는 RL 훈련 과정에서 획득한 추론 휴리스틱이 도메인에 구애받지 않고 추상적 문제 해결 스키마를 개발함을 시사합니다.

### [LongWriter-V: Enabling Ultra-Long and High-Fidelity Generation in Vision-Language Models](https://arxiv.org/abs/2502.14834)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14834.png)

Vote: 19

Authors: Lei Hou, Yucheng Wang, Daniel Zhang-Li, Yuhao Wu, Juanzi Li, Shangqing Tu, Yushi Bai, Bin Xu, Zhiyuan Liu, Huiqin Liu, Jifan Yu

- ***What's New***: LongWriter-V는 초대형 비전-언어 모델(Large Vision-Language Models; LVLMs)의 출력 길이 제약을 해결하기 위해 22,158개의 예를 포함한 LongWriter-V-22k 데이터셋을 소개했습니다. 또한 Direct Preference Optimization(DPO)을 활용하여 모델의 고충실도(High-Fidelity) 출력을 보장하는 IterDPO 방법을 제안했습니다.
- ***Technical Details***: LongWriter-V-22k은 다중 이미지 입력과 상세한 지시를 포함하여 0에서 10,000 단어에 이르는 여러 출력을 생성하는 22,158개의 예로 구성된 SFT 데이터셋입니다. IterDPO는 긴 출력을 세그먼트로 나누고 반복적인 수정 과정을 통해 인간의 피드백 효율성을 높였습니다. 또한, MMLongBench-Write라는 벤치마크를 개발하여 VLM의 긴 생성 능력을 평가했습니다.
- ***Performance Highlights***: 7B 파라미터 모델이 LongWriter-V-22k와 IterDPO로 훈련되어 MMLongBench-Write에서 인상적인 성능을 나타냈으며, GPT-4o와 같은 더 큰 프라프라이어터리 모델을 능가했습니다. 특히, 모델의 출력 길이가 3,000 단어 이상으로 확장되었습니다.

### [Does Time Have Its Place? Temporal Heads: Where Language Models Recall Time-specific Information](https://arxiv.org/abs/2502.14258)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14258.png)

Vote: 18

Authors: Yein Park, Minbyul Jeong, Jaewoo Kang, Chanwoong Yoon, Jungwoo Park

- ***What's New***: 이 연구는 Temporal Heads를 밝히며, 언어 모델이 시점에 맞춘 정보(time-specific information)를 처리하는 방법을 탐구하고 있습니다. 이는 여러 모델에서 다르게 위치하지만 공통적으로 시시간 지식을 처리하는 헤드들이 존재함을 밝힙니다.
- ***Technical Details***: Temporal Heads는 주로 시간에 맞춘 정보를 처리하는 주의 헤드를 Circuit Analysis를 통해 발견합니다. 이러한 헤드를 비활성화하면 시간에 특정된 지식을 회상하는 모델의 능력이 약화되지만, 시간에 무관한 지식과 기본 QA 성능은 유지됩니다. 이 헤드들은 숫자 조건과 텍스트 별칭을 양쪽 모두에서 활성화됩니다.
- ***Performance Highlights***: 시간에 특정된 정보를 처리하는 Temporal Heads의 역할을 나타내기 위해 특정 주의 헤드를 비활성화하는 실험을 합니다. 그 결과, 맞춤형 Attention 으로 Temporal Heads를 보강하면 해당 연도에 맞춘 사실 회상이 개선됩니다.

### [Discovering highly efficient low-weight quantum error-correcting codes with reinforcement learning](https://arxiv.org/abs/2502.14372)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14372.png)

Vote: 18

Authors: Austin Yubo He, Zi-Wen Liu

- ***What's New***: 이 논문은 강화 학습(Reinforcement Learning; RL)을 사용하여 효율적인 저-무게 양자 오류 정정 코드(Quantum Error-Correcting Codes; QEC)를 발견하는 새로운 방법을 제안합니다. 이는 기존 접근법을 대체하여 양자 오류 정정 코드 설계의 복잡한 문제를 해결하고, 거리와 무게 제약을 동시에 만족시키며 더 큰 규모의 안정자 코드를 효과적으로 설계할 수 있게 합니다.
- ***Technical Details***: RL 기반의 알고리즘은 강화 학습 에이전트가 주어진 상태에서 '엣지 추가/제거'라는 행동을 통해 그래프 상태를 업데이트하고, 코드의 새로운 거리와 무게에 기반하여 보상을 제공합니다. 주요 알고리즘으로 Proximal Policy Optimization(PPO)를 사용하여 정책을 안정적으로 업데이트합니다. 초거래(Hypergraph Product) 코드를 시작점으로 하여 모든 액션 마스킹을 적용하여 특정 행동에 대한 제한을 둡니다.
- ***Performance Highlights***: 이 알고리즘은 기존의 국가첨단 수준(SOTA) 방법과 비교하여 물리적 큐빗 오버헤드를 1-2 자릿수 줄이는 데 성공했으며, 최대 73배의 오버헤드 감소를 달성하였습니다. 또, 이전 접근법이 단일-자릿수 거리로 제한되는 반면, 실험적 방향성으로 중요한 수십의 거리까지 코드를 설계할 수 있습니다. 이는 근미래의 양자 장치에 적합한 거리 30-40 수준의 코드도 포함합니다.

### [S^2R: Teaching LLMs to Self-verify and Self-correct via Reinforcement Learning](https://arxiv.org/abs/2502.12853)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.12853.png)

Vote: 15

Authors: Ruotian Ma, Peisong Wang, Jiaqi Chen, Cheng Liu, Xingyan Liu, Xin Zhou, Bang Zhang, Nan Du, Jia Li

- ***What's New***: S2R는 강화 학습(Reinforcement Learning)을 통해 언어 모델(LLMs)을 스스로 검증(Self-verify)하고 자가 수정(Self-correct)할 수 있도록 교육하는 새로운 프레임워크를 제안합니다. 이 연구는 기존 데이터나 대규모 훈련이 필요 없이 모델의 추론 능력을 효율적으로 향상시킬 수 있는 방법을 제공합니다.
- ***Technical Details***: S2R 프레임워크는 두 단계로 구성됩니다. 첫 번째 단계에서는 정확하게 선별된 데이터로 지도 학습을 통해 초기 검증 및 자기 수정 행동을 모델에 주입합니다. 두 번째 단계에서는 결과 수준과 프로세스 수준의 강화 학습을 통해 이러한 능력을 강화합니다. 특히, Qwen2.5-math-7B 모델은 3.1k 샘플만으로도 모델의 정확도가 기존의 51.0%에서 81.6%로 증가했습니다.
- ***Performance Highlights***: 3개의 기초 모델을 사용한 실험 결과, S2R는 내부 및 외부 벤치마크 모두에서 경쟁 모델을 능가했습니다. 특히, Qwen2.5-7B 모델은 S2R를 통해 수학적 추론에서 다른 최첨단 모델들보다 높은 성능을 보였습니다. 이 모델은 높은 에러 탐지 및 수정 능력으로 인해 다양한 문제에서 더욱 우수한 정확도를 기록했습니다.

### [PC-Agent: A Hierarchical Multi-Agent Collaboration Framework for Complex Task Automation on PC](https://arxiv.org/abs/2502.14282)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14282.png)

Vote: 12

Authors: Yuyang Wanyan, Ji Zhang, Chunfeng Yuan, Xi Zhang, Haowei Liu, Changsheng Xu, Weiming Hu, Fei Huang, Haiyang Xu, Junyang Wang, Ming Yan

- ***What's New***: PC-Agent는 복잡한 PC 작업 자동화를 위해 설계된 계층적 멀티 에이전트 협업 프레임워크입니다. 이 시스템은 복잡한 사용자 지침과 상호 종속적인 하위 작업을 보다 효과적으로 처리하기 위해 Instruction-Subtask-Action 레벨로 의사결정 과정을 분해합니다. 이 프레임워크는 새로운 벤치마크인 PC-Eval을 도입하여 25개의 실제 복잡한 지침을 제시하며, PC 작업의 성공률이 이전 최고 성능 방법 보다 32% 향상되었음을 보여줍니다.
- ***Technical Details***: PC-Agent는 세 가지 핵심 설계로 구성됩니다: (1) 능동적 인식 모듈(APM)은 상호작용 요소와 텍스트의 고급 인식 및 조작을 가능하게 하며, 화면의 위치와 의미를 추출합니다. (2) 계층적 멀티 에이전트 협업은 의사결정 과정을 Instruction-Subtask-Action 3단계로 나누고, 사용자 지침을 매개변수화된 하위 작업으로 분해하여 처리합니다. (3) 반영 기반 동적 의사결정 메커니즘은 실행 결과의 오류를 탐지하여 피드백 후 즉각적인 조정을 가능하게 합니다.
- ***Performance Highlights***: PC-Eval에서 PC-Agent는 기존 최첨단 방법보다 작업 성공률이 32% 개선되었습니다. GPT-4o를 기반으로 하는 PC-Agent는 단계별로 화면 변화 감지를 통해 비효율적이거나 중복된 작업을 피하고, 복잡한 PC 작업을 수행하는 데 있어 탁월한 성능을 발휘하였습니다.

### [Dynamic Concepts Personalization from Single Videos](https://arxiv.org/abs/2502.14844)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14844.png)

Vote: 10

Authors: Daniel Cohen-Or, Aliaksandr Siarohin, Sergey Tulyakov, Willi Menapace, Rameen Abdal, Or Patashnik, Kfir Aberman, Ivan Skorokhodov

- ***What's New***: 이 논문에서는 단일 비디오에서 동적 개념(Dynamic Concepts)을 개인화할 수 있는 새로운 프레임워크인 Set-and-Sequence를 소개합니다. 이는 텍스트-비디오 모델(Text-to-Video Models)을 개인화하여 다양한 동적 개념을 캡처하는 획기적인 접근 방식입니다.
- ***Technical Details***: Set-and-Sequence 프레임워크는 스테이지 1에서 LoRA(Low-Rank Adaptation) 레이어를 사용하여 비디오의 프레임 집합을 통해 외형을 학습하고, 스테이지 2에서는 이 LoRA 기반을 고정한 후 Motion Residual을 사용해 전체 비디오의 모션 동역학을 캡처합니다. 이 방법은 DiT(Diffusion Transformers) 아키텍처 기반으로 스페이셜-템포럴(Spatio-Temporal) 가중치 공간을 도입하여 동적 개념을 효과적으로 모델링합니다.
- ***Performance Highlights***: 우리의 프레임워크는 실험을 통해 고품질의 씬 컴포지션(Scene Composition)과 동작 보존을 유지하면서 텍스트 프롬프트에 따라 비디오를 편집할 수 있는 능력을 보여주었습니다. 이는 기존의 방법들을 초월하여 보다 높은 수준의 수정 가능성과 조합성을 제공합니다.

### [Scaling Text-Rich Image Understanding via Code-Guided Synthetic Multimodal Data Generation](https://arxiv.org/abs/2502.14846)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14846.png)

Vote: 9

Authors: Ajay Patel, Yue Yang, Chris Callison-Burch, Ranjay Krishna, Aniruddha Kembhavi, Mark Yatskar, Tanmay Gupta, Christopher Clark, Andrew Head, Luca Weihs, Matt Deitke

- ***What's New***: 이 연구는 CoSyn이라는 새로운 프레임워크를 소개하여, 대형 텍스트 전용 언어 모델(LLMs)의 코딩 능력을 활용해 합성 텍스트 기반의 멀티모달 데이터를 자동으로 생성하는 방법을 제안합니다. 이를 통해 VLMs(비전-언어 모델)를 효과적으로 훈련시켜 텍스트-리치 이미지 분석을 가능하게 합니다.
- ***Technical Details***: CoSyn 프레임워크는 파이썬(Python), HTML, LaTeX 등 다양한 코딩 언어를 활용하여 다양한 도메인의 합성 이미지를 렌더링합니다. 이렇게 생성된 코드가 합성 이미지의 텍스처적 표현으로 작용하며, 이를 통해 비전-언어 인스트럭션 튜닝 데이터를 생성할 수 있습니다. CoSyn을 통해 총 40만 개의 이미지와 270만개의 인스트럭션 튜닝 데이터를 생성하였고, 이를 이용하여 훈련된 모델은 여러 기존 벤치마크에서 우수한 성능을 보였습니다.
- ***Performance Highlights***: CoSyn을 통해 생성된 합성 데이터로 훈련된 모델은 7개의 벤치마크에서 경쟁력 있는 오픈 소스 모델들 사이에서 최신 성능을 달성하였으며, GPT-4V 및 Gemini 1.5 Flash와 같은 독점 모델도 능가했습니다. 특히, 새로운 도메인에 대한 제로샷(Zero-shot) 학습 설정에서도 우수한 적응력을 보이며, 이는 CoSyn 데이터의 효율성과 새로운 도메인에 대한 적응성을 나타냅니다.

### [How to Get Your LLM to Generate Challenging Problems for Evaluation](https://arxiv.org/abs/2502.14678)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14678.png)

Vote: 9

Authors: Siva Reddy, Dzmitry Bahdanau, Arkil Patel

- ***What's New***: 이 연구에서는 CHASE라는 통합 프레임워크를 소개하여 사람이 개입하지 않고 LLMs을 사용해 도전적인 문제를 합성적으로 생성할 수 있도록 합니다. 이 접근 방식은 주어진 작업에 대해 간단한 구성 요소로부터 상향식으로 새로운 문제를 구축함으로써 도전적인 문제를 만드는 방법을 제안합니다. 이를 통해 독립적으로 검증 가능한 하위 작업으로 생성 프로세스를 분해함으로써 높은 품질과 정확성을 보장합니다.
- ***Technical Details***: CHASE는 세 가지 영역에서 벤치마크를 구현하였습니다: (1) 문서 기반 질문 응답(Document-based Question Answering), (2) 저장소 수준의 코드 완성(Repository-level Code Completion), (3) 수학 추론(Math Reasoning). 각 작업은 CHASE에서 정의한 방법론에 따라 문제를 생성하고 검증합니다. 예를 들어, 각 생성 단계에서 간단하고 개별적으로 검증 가능한 하위 작업으로 작업을 분할하여 세부적인 단계별 검증을 수행합니다.
- ***Performance Highlights***: 최신 LLM들이 CHASE가 생성한 합성 벤치마크에서 40-60%의 정확도를 달성함으로써, CHASE 프레임워크의 도전적인 문제 생성 능력을 시연합니다. 여러 LLM 간의 성능 차이를 명확히 드러내어, 기존의 표준 벤치마크에서는 구별하기 어려운 모델 간의 성능 차이를 발견할 수 있습니다. 또한 다양한 문맥 크기에 대해 성능이 어떻게 변하는지를 연구하여, 문맥 크기를 증가시키면 성능이 일관되게 감소하는 것을 보여줍니다.

### [NAVIG: Natural Language-guided Analysis with Vision Language Models for Image Geo-localization](https://arxiv.org/abs/2502.14638)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14638.png)

Vote: 8

Authors: Jordan Boyd-Graber, Zheyuan Zhang, Tasnim Kabir, Runze Li

- ***What's New***: NAVIG 프레임워크는 이미지를 기반으로 장소를 추론하는 새로운 방식으로, GeoGuessr 게임에서 영감을 받아 전문가의 언어 기반 추론을 데이터세트(NAVICLUES)로 제공하여 정확도를 높였습니다. 새로운 데이터세트를 활용하여 평균 거리 오류를 14% 줄이면서도 1000개 이하의 훈련 샘플만을 사용했으며, NAVIG 프레임워크는 전 세계적으로 이미지를 소규모 모델 기반으로 효율적으로 지리적 위치를 추정할 수 있게 합니다.
- ***Technical Details***: NAVICLUES는 GeoGuessr 게임에서 수집한 데이터로, 전문가의 데이터와 게임을 분석하여 2000개 이상의 사례가 포함된 고품질 데이터세트를 만듭니다. NAVIG 프레임워크는 세 가지 주요 구성 요소로 구성되어 있습니다: 이미지를 기반으로한 일반적인 추론을 생성하는 REASONER, 추가적인 정보를 활용하여 세부사항을 탐색하는 SEARCHER, 그리고 최종 예측을 생성하는 GUESSER입니다. NAVIG는 이를 통해 시각적 요소와 외부 정보 활용을 결합하여 세분화된 이미지 분석을 수행합니다.
- ***Performance Highlights***: NAVIG는 이전 최첨단 모델들과 비교하여 평균 거리 오류를 14% 줄였고, GeoGuessr 게임을 통해 수집한 데이터를 사용하여 인간 전문가의 전략을 모방하는 추론을 생성합니다. Qwen2-VL을 사용한 NAVIG는 750km 이내에서의 예측 정확도에서 가장 높은 성과를 보이며, GeoGuessr 점수와 거리에서 상당한 개선을 이루었습니다.

### [From RAG to Memory: Non-Parametric Continual Learning for Large Language Models](https://arxiv.org/abs/2502.14802)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14802.png)

Vote: 6

Authors: Yiheng Shu, Sizhe Zhou, Weijian Qi, Bernal Jiménez Gutiérrez, Yu Su

- ***What's New***: 이 논문은 대형 언어 모델(LLMs)의 비파라메트릭 지속 학습을 위한 혁신적인 프레임워크인 HippoRAG 2를 소개합니다. 새로운 정보 통합에서 기존의 RAG(Retrieval-Augmented Generation)의 한계를 극복하고 인간의 장기 기억을 보다 효과적으로 모방하기 위해 설계되었습니다.
- ***Technical Details***: HippoRAG 2는 Personalized PageRank 알고리즘을 기반으로 하여, 구문과 구절을 KG(Knowledge Graph)에 통합하고, 온라인에서 LLMs를 효율적으로 활용하여 컨텍스트 기반 검색을 수행합니다. 이 설계는 HippoRAG의 OpenIE와 PPR(Methodologies)을 활용하면서, 구문의 깊은 통합 및 트리플 필터링을 통해 보다 포괄적인 성능을 제공합니다.
- ***Performance Highlights***: 실험 결과, HippoRAG 2는 최신 임베딩 모델을 상회하는 동시적 메모리 태스크에서 7% 향상된 성능을 보여주었으며, 사실적 지식과 의미 해석 능력에서도 우수한 성능을 입증하였습니다. 이는 LLMs의 인간 유사 비파라메트릭 지속 학습에 대한 중요한 발전을 제시합니다.

### [AlphaMaze: Enhancing Large Language Models' Spatial Intelligence via GRPO](https://arxiv.org/abs/2502.14669)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14669.png)

Vote: 4

Authors: Alan Dao, Dinh Bach Vu

- ***What's New***: AlphaMaze는 대형 언어 모델(Large Language Models; LLMs)에 시각적 추론 능력을 부여하여 미로 탐색과 같은 공간적 과제를 해결하기 위한 새로운 두 단계 훈련 프레임워크를 소개합니다. 이 연구는 체계적인 시각적 정보 처리를 통해 LLMs의 시각적 공간 추론을 강화하는 것을 목표로 합니다.
- ***Technical Details***: AlphaMaze의 훈련 프레임워크는 두 단계로 이루어져 있습니다. 첫 번째 단계는 '지도형 세부 튜닝(Supervised Fine-Tuning; SFT)'으로, 토큰화된 미로 표현을 사용하여 LLM이 이동 명령을 예측하도록 합니다. 이후 '그룹 상대 정책 최적화(Group Relative Policy Optimization; GRPO)'를 적용하여 모델의 순차적 의사 결정 능력을 강화하고 체계적 사고 과정을 유도합니다. GRPO는 잘 설계된 보상 함수를 사용하여 모델의 시각적 추론 능력을 더욱 정교하게 만듭니다.
- ***Performance Highlights***: 실험 결과, SFT만 적용한 모델은 86%의 정확도로 미로 탐색 성능을 보였으며, GRPO를 추가로 적용한 후에는 정확도가 93%까지 향상되었습니다. 이 결과는 LLMs가 강화학습을 통해 시각적 공간 추론 능력을 더욱 효과적으로 발전시킬 수 있음을 시사합니다.

### [LLM-based User Profile Management for Recommender System](https://arxiv.org/abs/2502.14541)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14541.png)

Vote: 4

Authors: Seunghwan Bang, Hwanjun Song

- ***What's New***: 이 연구는 LLM(대형 언어 모델)을 활용한 새로운 추천 시스템 프레임워크인 PURE를 도입했습니다. PURE는 사용자의 리뷰로부터 주요 정보를 추출하고 요약하여 지속적으로 진화하는 사용자 프로필을 작성하고 관리합니다. 이는 전통적인 훈련 없이도 제로샷(Zero-shot) 방식으로 추천을 가능하게 합니다.
- ***Technical Details***: PURE는 '리뷰 추출기(Review Extractor)', '프로필 업데이트(Profile Updater)', '추천자(Recommender)'의 세 가지 주요 구성 요소로 이루어져 있습니다. 리뷰 추출기는 사용자 리뷰를 분석하여 '좋아하는 것', '싫어하는 것', 및 '핵심 특성'을 식별하여 추출하며, 프로필 업데이트는 중복성과 충돌을 제거하여 사용자 프로필을 세련되게 업데이트합니다. 마지막으로 추천자는 최신 사용자 프로필을 기반으로 개인화된 추천을 생성합니다.
- ***Performance Highlights***: 아마존 데이터세트를 사용한 실험 결과, PURE는 기존의 LLM 기반 추천 방법들보다 우수한 성능을 보였습니다. 이는 장기간의 사용자 정보를 효과적으로 활용함과 동시에 입력 토큰의 제한을 관리할 수 있는 능력을 보여줍니다.

### [RelaCtrl: Relevance-Guided Efficient Control for Diffusion Transformers](https://arxiv.org/abs/2502.14377)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14377.png)

Vote: 4

Authors: Jiasong Feng, Shanyuan Liu, Dawei Leng, Zhanjie Zhang, Jie Zhang, Xuanhua He, Ke Cao, Jing Wang, Ao Ma, Yuhui Yin, Bo Cheng

- ***What's New***: RelaCtrl은 제어 정보의 관련성을 효율적으로 분석하여 Diffusion Transformer의 제어 성능을 최적화하는 새로운 프레임워크를 제안합니다. 이는 제어 신호를 효율적이고 자원 최적화된 방식으로 통합하여 매개 변수와 계산 복잡성을 상당히 줄이는 것을 목표로 합니다.
- ***Technical Details***: RelaCtrl은 각 Transformer 레이어의 제어 정보와의 관련성을 평가하는 'ControlNet Relevance Score'를 사용합니다. 이를 바탕으로 제어 레이어의 위치, 매개 변수 규모, 모델링 능력을 최적화하여 불필요한 매개 변수 및 중복 계산을 줄입니다. 또한, 컴퓨팅 효율성을 높이기 위해 '이차원 Shuffle Mixer (Two-Dimensional Shuffle Mixer; TDSM)'를 사용하여 토큰 믹서와 채널 믹서 역할을 수행합니다.
- ***Performance Highlights***: RelaCtrl은 PixArt-δ 대비 매개 변수와 계산 복잡성을 15% 수준으로 줄이면서 우수한 성능을 기록했습니다. 네 가지 조건부 가이드 작업과 두 가지 텍스트-이미지 생성 모델에서 실험한 결과, 높은 제어 정확성과 텍스트-이미지 일치를 유지하면서 탁월한 효율성과 성능을 입증했습니다.

### [LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention](https://arxiv.org/abs/2502.14866)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14866.png)

Vote: 4

Authors: Haotian Tang, Shang Yang, Guangxuan Xiao, Yujun Lin, Zhijian Liu, Qinghao Hu, Song Han, Jiaming Tang, Junxian Guo, Yao Lu

- ***What's New***: LServe는 긴 시퀀스를 효과적으로 처리하는 대형 언어 모델(LLM)의 효율적인 서비스 시스템을 소개합니다. 하이브리드 스파스 어텐션(hybrid sparse attention)을 활용하여, 프리필링(prefilling)과 디코딩(decoding) 단계 모두에서 계산을 가속화합니다. 특히, 중요도가 낮은 토큰에 대한 처리를 블록 단위로 건너뛰는 방식으로 최적화되었습니다.
- ***Technical Details***: LServe는 하드웨어 친화적인 구조적 스파스 패턴을 하나의 프레임워크로 통합하여 긴 문맥을 가진 모델에서의 정적(Static) 및 동적(Dynamic) 스파시티를 활용합니다. 프리필링 및 디코딩 단계 모두에서 어텐션 헤드의 절반을 스트리밍 헤드로 변환하여 연산을 거의 무료로 처리하며, 쿼리 중심의 유사성에 기반한 계층적 KV 페이지 선택 정책을 설계합니다. 또한 KV 페이지의 수는 문맥의 길이에 관계없이 일정하게 유지되어야 함을 발견했습니다.
- ***Performance Highlights***: LServe는 프리필링 속도를 최대 2.9배, 디코딩 속도를 1.3~2.1배 향상시켰습니다. 최신 메모리 관리 시스템 vLLM과 비교하여 LServe는 다중 모델(Llama-3-8B, Minitron-4B, Llama-2-7B)에서 테스트되어 뛰어난 성능을 보여주었습니다. 더욱이, LServe는 오픈 소스 코드로 제공되어 지속적인 발전과 연구를 지원합니다.

### [Enhancing Cognition and Explainability of Multimodal Foundation Models with Self-Synthesized Data](https://arxiv.org/abs/2502.14044)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14044.png)

Vote: 4

Authors: Xiang Li, Quanzheng Li, Jin Sun, Yucheng Shi, Ninghao Liu

- ***What's New***: 이 논문에서는 셀프 합성 데이터(self-synthesized data)를 이용하여 멀티모달 기초 모델(Large Multimodal Models; LMMs)의 인지 능력과 설명 가능성을 높이는 새로운 프레임워크를 제안합니다. 특히, 시각적 거부 샘플링(visual rejection sampling) 기법을 적용하여, 특정한 드메인에 맞는 시각적 분류에서 높은 정확도와 논리적인 설명을 생성할 수 있는 능력을 개선합니다.
- ***Technical Details***: 제안된 프레임워크는 전문가가 정의한 개념을 기반으로 시각적으로 해석 가능한 답변을 합성하는 과정을 포함합니다. 정보 병목(Information Bottleneck) 기법을 통해 이미지 내의 관련 시각적 개념을 선택하고, 이러한 개념을 텍스트 형식의 설명으로 변환하여 초기 데이터셋으로 사용합니다. 보상 모델이 없는 거부 샘플링 기법을 통해 낮은 품질의 출력이 제거되고, 이후 라운드에서 고품질 답변을 중심으로 세부 튜닝을 진행합니다.
- ***Performance Highlights***: 실험 결과, 제안된 방법은 다양한 데이터셋에서 인지 능력과 설명의 질을 효과적으로 개선합니다. 예시로, Stanford Dogs 데이터셋에서 86.91%의 정확도를 달성하였으며, 이는 기존 방법보다 높은 성능입니다. 또한 설명의 일관성과 논리성도 개선되어 모델의 해석 가능성이 증가하였습니다.

### [CLIPPER: Compression enables long-context synthetic data generation](https://arxiv.org/abs/2502.14854)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14854.png)

Vote: 3

Authors: Chau Minh Pham, Yapei Chang, Mohit Iyyer

- ***What's New***: CLIPPER는 복잡한 장문 문맥에서의 추론 과제를 위해 합성 데이터를 생성하는 새로운 압축 기반 접근법입니다. 이는 서사적 주장 검증 작업을 위한 합성 데이터를 생성하도록 설계되었으며, CLIPPER는 책의 원문에서 직접 주장을 생성하는 대신, 챕터 개요와 책 요약을 통해 복잡하고 신뢰성 있는 주장을 만들어냅니다.
- ***Technical Details***: CLIPPER는 두 단계로 구성된 데이터 생성 파이프라인을 통해 작동합니다. 첫 단계에서는 LLM이 긴 문서를 요약과 개요로 압축하고, 두 번째 단계에서는 이 압축된 정보를 활용하여 복합적인 주장과 연관된 사고 과정을 생성합니다. 여기서 생성된 주장과 사고 과정은 감독된 미세 조정을 위해 사용됩니다.
- ***Performance Highlights***: CLIPPER를 통해 생성된 데이터셋을 기반으로 미세 조정된 모델은 서사적 주장 검증에서 획기적인 결과를 이뤘습니다. 예를 들어, 최고 성능 모델은 NoCha leaderboard의 극복 모델 중에서 기존의 28%에서 76%로 테스트 세트에서의 정확성을 크게 끌어올렸습니다. 이러한 모델은 다른 서사 이해 관련 작업에서도 성능 향상을 보여줍니다.

### [Unstructured Evidence Attribution for Long Context Query Focused Summarization](https://arxiv.org/abs/2502.14409)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14409.png)

Vote: 3

Authors: Lu Wang, Dustin Wright, Isabelle Augenstein, Zain Muhammad Mujahid, David Jurgens

- ***What's New***: 이 논문은 장문 컨텍스트 내에서 사용자 쿼리에 초점을 맞춘 요약 작업에서 비구조화된 증거 인용을 처음으로 연구하였습니다. 특히, 기존 모델들이 비구조화된 텍스트 범위에서 증거를 올바르게 인용하지 못하는 문제를 해결하기 위해, 범용적인 합성 데이터셋인 SUnsET을 제공하여 모델이 이러한 작업에 적응할 수 있도록 지원합니다.
- ***Technical Details***: SUnsET 데이터셋은 새로운 도메인 독립적 파이프라인을 사용하여 생성된 합성 데이터셋으로, 롱 컨텍스트 쿼리 요약 작업에서 비구조화된 증거 인용을 개선하기 위해 활용됩니다. 총 5개의 다른 모델과 4개의 데이터셋에서 실험을 통해, 이 데이터로 파인튜닝된 모델들이 더욱 관련성 있고 사실적으로 일관된 증거를 생성하며 다양한 위치에서 증거를 추출할 수 있음을 보였습니다.
- ***Performance Highlights***: 기존 모델이 비구조화된 증거를 효과적으로 발췌하고 사용할 수 없는 반면, SUnsET을 통해 학습된 모델은 증거의 인용 정확도를 높이고 요약의 품질을 향상시키는 데 성공하였습니다. 특히, 보다 큰 모델들은 SUnsET을 통해 더욱 강력하게 적응하여 성능을 크게 개선하였습니다.

### [How Much Do LLMs Hallucinate across Languages? On Multilingual Estimation of LLM Hallucination in the Wild](https://arxiv.org/abs/2502.12769)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.12769.png)

Vote: 3

Authors: Anne Lauscher, Saad Obaid ul Islam, Goran Glavaš

- ***What's New***: 이 연구는 LLM(Large Language Models; 대형 언어 모델)의 'in the wild' 착시 현상(hallucinations)을 다양한 언어로 최초로 추정하는 시도를 보여줍니다. 30개의 언어에 대해 다중 언어 착시 검출 모델의 효과를 평가하고, 각 언어에서 발견된 착시 현상의 정도를 측정하는 포괄적인 프레임워크를 제안합니다.
- ***Technical Details***: FAVA 데이터셋의 영어 착시 검출 데이터를 기초로, 기계 번역을 통해 30개 언어로의 번역을 통해 훈련 데이터를 생성합니다. 다중 언어 착시 검출 모델은 Llama-3-8B와 같은 대형 LLM을 기반으로 양방향 문맥화(Bidirect) 기법을 사용해 학습되며, 6가지 LLM 계열을 통해 다양한 착시율을 측정합니다.
- ***Performance Highlights***: 작은 모델일수록 더 많은 착시를 발생시키며, 많은 언어를 지원하는 모델들이 보다 높은 착시율을 보입니다. 예를 들어, 3B 파라미터의 Qwen-2.5는 큰 모델들에 비해 착시 현상을 더 많이 나타냈습니다. 평균적으로, 착시율은 약 7%에서 12%의 범위로 나타났습니다. 이는 모델의 크기와 언어 지원 정도가 큰 영향을 미치는 것을 보여줍니다.

### [Geolocation with Real Human Gameplay Data: A Large-Scale Dataset and Human-Like Reasoning Framework](https://arxiv.org/abs/2502.13759)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.13759.png)

Vote: 3

Authors: Yuan Huang, Jingpu Yang, Iryna Gurevych, Xiuying Chen, Zirui Song, Jonathan Tonglet, Meng Fang, Tao Cheng, Zeyu Zhang

- ***What's New***: 이 논문은 대규모 게임플랫폼의 실제 지리위치 데이터를 활용하여 소재지(Geolocation)를 향상시키기 위한 새로운 데이터셋 GeoComp와 인간과 유사한 추론 프레임워크 GeoCoT를 소개합니다. GeoComp는 2년에 걸쳐 740K 명의 사용자로부터 수집된 3M 지리태그 위치와 25M 메타데이터 항목으로 구성되며, 이는 다양한 난이도 분석을 가능하게 하고 현재 모델의 한계를 드러냅니다.
- ***Technical Details***: GeoCoT(Geographical Chain-of-Thought)는 인간의 지리적 추론을 모방하는 다단계 추론 체계입니다. 사진 속에서 지리적 단서를 단계적으로 분석해 대륙부터 도시에 이르는 구체적인 위치를 예측하며, 이는 기존의 격자 기반 분류나 광범위 이미지 데이터베이스 구축의 한계를 극복하고자 합니다. 평가를 위해 GeoEval이라는 측정 기준을 도입해 모델의 전체 성능 뿐만 아니라 추론 체계의 정교함을 검증합니다.
- ***Performance Highlights***: GeoCoT는 GPT-4o(CoT) 대비 최대 25% 향상된 소재지 정확도를 보여주며, 항목별 추론 정확성, 논리적 일관성에서 도드라진 성능을 나타냈습니다. 특히 도시 수준의 예측에서 가장 높은 성능을 나타내며 전반적인 지리적 추론 능력에서 우위를 점했습니다.

### [Which of These Best Describes Multiple Choice Evaluation with LLMs? A) Forced B) Flawed C) Fixable D) All of the Above](https://arxiv.org/abs/2502.14127)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14127.png)

Vote: 2

Authors: Nishant Balepur, Rachel Rudinger, Jordan Lee Boyd-Graber

- ***What's New***: 이 논문은 대형 언어 모델(LLMs)의 평가 방법으로 널리 사용되는 다지선다형 질문 평가 방식(Multiple Choice Question Answering; MCQA)의 문제점을 지적하고, 교육적 통찰을 바탕으로 평가 포맷을 개선할 수 있는 방안을 제안합니다. 기존의 단순한 정답 선택 방식에서 벗어나, 생성적인 형식으로의 변화를 주장하며, 이를 통해 LLM의 실제 용도와 지식 테스트를 더욱 잘 반영할 수 있도록 합니다.
- ***Technical Details***: MCQA의 기본 포맷은 한 문제와 선택지들로 구성되어 있으며, 단 한 개의 정답과 나머지 오답 선택지(방해 요소)를 포함합니다. 논문은 이 포맷의 문제로 주제적/생성적 평가에 적합하지 않고, 실제 LLM 용도와 잘 맞지 않으며, 지식을 완벽하게 테스트하지 못함을 지적합니다. 제안된 개선 방안은 '짧은 답 생성'과 '설명 MCQA' 방식으로, 학생들이나 LLM이 단순히 선택지만 고르는 것이 아닌, 답안을 구체적으로 생성하고 해설할 수 있게 합니다.
- ***Performance Highlights***: 논문은 MCQA 데이터셋의 문제로 데이터 누출, 비답변성, 지름길, 그리고 데이터 포화 같은 사례를 제시하며, 교육적 관행을 적용하여 이러한 문제들을 극복할 수 있는 방안을 제공합니다. 특히 데이터 누출 문제를 해결하기 위해 실시간으로 업데이트되는 질문 세트를 제안합니다.

### [Multimodal RewardBench: Holistic Evaluation of Reward Models for Vision Language Models](https://arxiv.org/abs/2502.14191)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14191.png)

Vote: 1

Authors: Michihiro Yasunaga, Marjan Ghazvininejad, Luke Zettlemoyer

- ***What's New***: Multimodal RewardBench는 Vision-Language Models(VLMs)의 보상 모델을 평가하기 위한 최초의 포괄적이고 전문가 주석이 포함된 벤치마크입니다. 이 벤치마크는 일반적 정확성, 선호도, 지식, 추론, 안전성, 시각적 질문-답변(VQA) 등 6가지 주요 영역을 포함하며, VLM의 보상 모델 개발을 위한 새로운 과제를 제공합니다.
- ***Technical Details***: Multimodal RewardBench는 5,211 개의 (질문, 선택된 응답, 거부된 응답) 삼중항으로 구성되어 있으며, 다양한 VLM이 생성한 응답을 사용하여 응답의 정확성과 인간 선호도를 평가합니다. 전문가 주석을 통해 높은 품질의 데이터 및 주석자 간 높은 일치를 보장하며, 모든 테스트 사례는 사람에 의해 작성되었습니다.
- ***Performance Highlights***: Gemini 1.5 Pro와 Claude 3.5 Sonnet 등 가장 성능이 좋은 모델들도 전체 정확도가 72%에 불과하여 인간 수준의 성능에 한참 미치지 못합니다. 일반적인 영역이나 긴 형식의 생성 작업에서는 비교적 우수한 성능을 보였으나, 지식 및 코딩 관련 추론 및 안전성(특히 편향 및 독성 탐지) 영역에서는 여러 모델이 어려움을 겪었습니다. 이 결과는 VLM 보상 모델이 여러 분야에서 더욱 개선되어야 함을 시사합니다.

### [Symmetrical Visual Contrastive Optimization: Aligning Vision-Language Models with Minimal Contrastive Images](https://arxiv.org/abs/2502.13928)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.13928.png)

Vote: 1

Authors: Shengguang Wu, Nick Haber, Fan-Yun Sun, Kaiyue Wen

- **What's New**: 이 논문은 이미지-언어 모델(VLMs)이 시각적 정보를 충분히 활용하지 못하고 언어 모델에 지나치게 의존하는 문제를 해결하기 위해 새로운 미세 조정 양상인 대칭 시각 대조 최적화(S-VCO)를 제안합니다. 또한, 모델의 시각적 세부 사항을 도전하는 최소 대조 이미지 데이터셋(MVC)을 소개하여 VLM의 시각적 의존성 과제를 해결합니다.
- **Technical Details**: S-VCO는 이미지와 대응되는 텍스트 토큰과의 정밀한 상응을 강화하는 시각 대조 목표를 설정합니다. 또한, S-VCO는 텍스트와 대응되는 부정적인 이미지를 활용하여 모델이 부정확한 시각 정보를 강하게 거부하도록 유도합니다. MVC 데이터셋은 자동 필터링과 시각적 반사실 데이터의 증강을 통해 구성되며, 쌍을 이루는 텍스트 응답과 함께 최소 시각적 대조를 가진 이미지로 구성됩니다.
- **Performance Highlights**: 제안된 방법론은 다양한 벤치마크에서 VLM 성능을 일관되게 향상시켰으며, 특히 시각적 비중이 높은 과제에서 최대 22%의 환각 감소를 달성하였습니다. S-VCO는 기존의 선호 조정 방법보다 더 높은 성능 향상을 제공하며, 특히 시각적 의존성이 높은 벤치마크에서 현저한 이점을 나타냈습니다.

### [Generating π-Functional Molecules Using STGG+ with Active Learning](https://arxiv.org/abs/2502.14842)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14842.png)

Vote: 0

Authors: Yan Zhang, Boris Knyazev, Aristide Baratin, Alexia Jolicoeur-Martineau, Cheng-Hao Liu

- ***What's New***: 이 연구에서는 STGG+와 액티브 러닝(Active Learning) 기법을 통합하여 π-기능성 분자(π-Functional Molecules)를 생성하는 새로운 접근법인 STGG+AL을 제안합니다. 이 방법은 기존의 강화학습(Reinforcement Learning) 기법이 가지는 '보상 해킹' 문제를 해결하고 화학적으로 합성 가능한 분자를 생성할 수 있도록 돕습니다.
- ***Technical Details***: STGG+는 계량이 가능한 그래프 생성방식인 확장 트리 기반의 자가 회귀 생성 모델(Autoregressive Generative Model)입니다. 이 모델은 290만 개의 π-고리 결합 분자를 포함한 Conjugated-xTB 데이터세트로 사전 학습되며, 목표 특성을 가진 분자를 생성하고 평가를 통해 세부 조정됩니다. 액티브 러닝 내에서는 주어진 특성 범위에 따라 새로운 분자를 생성하고 이를 평가하여 점진적으로 학습 모델을 개선해 나갑니다.
- ***Performance Highlights***: 실험 결과, STGG+AL은 전통적인 가상 스크리닝보다 현저히 더 높은 oscillator 강도를 가진 분자를 발견하는데 성공했으며, 이는 기존 기법으로 발견된 최대 값보다 약 세 배 가까이 향상된 수치입니다. 특히, NIR 흡수 범위를 가지는 바이오메디컬 이미징에 적합한 분자를 설계하는데 유리한 결과를 보여주었습니다.

