## Daily Papers (2025-02-07)

### [Analyze Feature Flow to Enhance Interpretation and Steering in Language Models](https://arxiv.org/abs/2502.03032)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.03032.png)

Vote: 41

Authors: Nikita Balagansky, Daniil Gavrilov, Yaroslav Aksenov, Daniil Laptev

- ***What's New***: 이 논문은 데이터가 없는 코사인 유사성(data-free cosine similarity) 기법을 사용하여 대형 언어 모델(Large Language Models; LLMs)의 연속적 계층에서 발견되는 특징들을 체계적으로 매핑하는 새로운 접근법을 소개합니다. 이 방법은 모델 행위의 직접적인 조정을 가능하게 함으로써 텍스트 생성에서 특정 테마의 제어를 실현합니다.
- ***Technical Details***: 다층에서 자기 회귀 계층(sparse autoencoder; SAE) 특징을 정렬하기 위해 코사인 유사성 기법이 사용됩니다. 특정 특징이 각 스테이지에서 어떻게 지속되거나 변화하거나 처음 등장하는지를 추적할 수 있는 상세한 흐름 그래프(flow graphs)가 생성됩니다. MLP와 주의(attention) 모듈이 이미 존재하는 특징에 새로운 것을 도입하거나 변화시키는 방식도 설명됩니다.
- ***Performance Highlights***: 제안된 흐름 그래프(flow graph) 방식은 모델의 행동을 조정하는 질을 향상시키며, 이는 구조 투명성이 높은 LLM 조작에 새로운 가능성을 엽니다. 또한 전형적인 다양한 특징을 목표로 하여 다계층 모델 조정을 실현하는 데 성공했습니다.

### [DynVFX: Augmenting Real Videos with Dynamic Content](https://arxiv.org/abs/2502.03621)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.03621.png)

Vote: 18

Authors: Omer Bar-Tal, Rafail Fridman, Danah Yatim, Tali Dekel

- ***What's New***: DynVFX는 사용자가 제공하는 간단한 텍스트 지시에 따라 새롭게 생성된 동적 콘텐츠를 실제 동영상에 통합시키는 방법을 제시합니다. 본 연구에서는 텍스트를 비디오로 변환하는 사전 훈련된 확산 변환기(Diffusion Transformer)와 비전 언어 모델(Vision Language Model; VLM)을 활용하여 새로운 콘텐츠를 생성하고 이를 비디오에 무훈련으로 쉽게 통합합니다.
- ***Technical Details***: DynVFX는 주어진 비디오와 사용자가 원하는 콘텐츠를 설명하는 짧은 지시만으로 자동으로 동영상을 편집합니다. 사전 훈련된 텍스트-비디오 모델과 VLM을 활용하여 원본 비디오의 잠재 표현(Latent Representation)에 잔여치를 추정, 이를 사용하여 손쉽게 합성된 비디오에서 고유의 공간 및 동적 요소를 유지하여 콘텐츠를 적절히 통합합니다. 편집의 로컬리제이션(Localization)을 위해 'Anchor Extended Attention'을 사용하고, 새로운 콘텐츠의 조화를 위해 잔여치를 반복적으로 정제하여 더 나은 결과를 도출합니다.
- ***Performance Highlights***: DynVFX는 경쟁 방법론들과 비교하여 새로운 동적 요소 생성과 원본 콘텐츠 유지 간의 최적 균형을 이루는 뛰어난 결과를 보여주었습니다. 주어진 편집 지침에 얼마나 잘 맞는지, 시각적 품질, 새로운 콘텐츠와 원본 콘텐츠의 조화 및 동작의 사실성을 평가하여 높은 점수를 기록했습니다. 사용자가 직접 판단한 연구에서도 DynVFX는 우수한 결과를 나타냈습니다.

### [UltraIF: Advancing Instruction Following from the Wild](https://arxiv.org/abs/2502.04153)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04153.png)

Vote: 15

Authors: Shuzheng Si, Yu Cheng, Kaikai An, Li Sheng, Baobao Chang, Ning Ding, Ganqu Cui

- ***What's New***: ULTRAIF는 대형 언어 모델(LLMs)이 복잡한 지시를 따를 수 있도록 오픈 소스 데이터를 활용하여 개발된 새로운 접근법입니다. 복잡한 사용자 지시를 간단한 질의, 제약 및 평가 질문으로 분해하고 UltraComposer를 사용하여 복합적인 지시를 합성합니다.
- ***Technical Details***: ULTRAIF는 실제 사용자 지시를 간단한 구성 요소와 제약으로 분해하며, 각 제약에 대해 응답이 요구 사항을 충족하는지 확인할 평가 질문을 생성합니다. UltraComposer 모델은 이러한 구성 요소를 사용하여 복합적이고 다양한 지시를 생성할 수 있도록 훈련되며, 생성된 데이터의 품질을 높이기 위해 평가 질문을 통해 응답을 필터링합니다.
- ***Performance Highlights***: ULTRAIF는 LLaMA-3.1-8B-Base 모델을 기준으로 하여 5개의 지시 추종 벤치마크에서 그 효과를 입증했으며, 이 모델을 지시 버전과 동일한 수준으로 맞추는 중요한 성과를 달성했습니다. 더 나아가서, 자기 정렬을 통해 LLaMA-3.1-8B-Instruct 모델의 성능을 향상시키는 가능성을 보여주었습니다.

### [Gold-medalist Performance in Solving Olympiad Geometry with AlphaGeometry2](https://arxiv.org/abs/2502.03544)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.03544.png)

Vote: 14

Authors: Vikas Verma, Trieu H. Trinh, Junehyuk Jung, Xiaomeng Yang, Quoc V. Le, Yuri Chervonyi, Miroslav Olšák, Marcelo Menegali, Thang Luong, Hoang Nguyen

- ***What's New***: AlphaGeometry2는 국제 수학 올림피아드(International Math Olympiad; IMO)의 기하학 문제 해결에서 이전 버전인 AlphaGeometry의 성능을 크게 향상시킨 업그레이드입니다. 기하학적 문제의 해결 비율을 66%에서 88%로 증가시켰으며, 평균 금메달리스트 수준을 초과하는 성과를 달성했습니다.
- ***Technical Details***: AlphaGeometry2는 이동하는 객체나 각도, 비율, 거리의 선형 방정식을 포함한 더 복잡한 문제를 다룰 수 있도록 언어를 확장했습니다. Gemini 기반의 언어 모델과 다중 검색 트리를 사용하는 새로운 지식 공유 메커니즘을 도입하였으며, 심볼릭 엔진에서도 최적화를 진행하여 성능을 향상시켰습니다.
- ***Performance Highlights***: AlphaGeometry2는 지난 25년간의 모든 IMO 기하학 문제 중 84%를 해결하여 이전의 54% 해결 성능에 비해 크게 향상되었습니다. 이는 평균 IMO 금메달리스트의 성능을 능가하는 것으로 증명되었습니다.

### [Great Models Think Alike and this Undermines AI Oversight](https://arxiv.org/abs/2502.04313)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04313.png)

Vote: 14

Authors: Douwe Kiela, Shashwat Goel, Ilze Amanda Auzina, Karuna K Chandra, Joschka Struber, Ameya Prabhu, Matthias Bethge, Jonas Geiping, Ponnurangam Kumaraguru

- ***What's New***: 이번 연구는 AI 감독(AI Oversight)에서의 모델 유사성이 평가 및 교육에 미치는 영향을 조사하고, 모델의 오류가 증가함에 따라 유사성이 증가하는 경향을 초래한다고 보고합니다. 연구는 '우연 조정 확률적 일치도(Chance Adjusted Probabilistic Agreement; CAPA)'라는 새로운 모델 유사성 메트릭을 제안합니다.
- ***Technical Details***: CAPA는 모델의 오류가 중첩되는지를 기반으로 하는 메트릭으로, 이는 모델의 정밀도를 감안한 우연 일치를 수정합니다. 이 메트릭을 통해 대형 언어 모델(Large Language Models; LLMs)의 판단에서 자신과 유사한 모델에 유리한 점수를 매기는 경향이 있음을 발견했습니다. 또한, 유사성이 큰 경우 '약한 감독자에서 강한 학생으로의 학습(weak-to-strong generalization)'에서 성능 향상이 더욱 높음을 확인했습니다.
- ***Performance Highlights***: LLMs의 오류가 점점 더 유사하게 나타나고 있으며 모델 기능이 향상됨에 따라 이 문제는 더욱 두드러집니다. 이는 AI 감독 과정에서의 위험성을 보여주며, 다양한 모델 접근법의 필요성을 강조합니다. CAPA를 활용한 실험은 음성, 이미지, 텍스트 등의 여러 분야에서 이와 같은 경향이 발견되고 있음을 나타냅니다.

### [Ola: Pushing the Frontiers of Omni-Modal Language Model with Progressive Modality Alignment](https://arxiv.org/abs/2502.04328)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04328.png)

Vote: 14

Authors: Jiahui Wang, Zuyan Liu, Yuhao Dong, Jiwen Lu, Ziwei Liu, Yongming Rao, Winston Hu

- ***What's New***: Ola는 진보적 모달리티 정렬(Progressive Modality Alignment) 전략을 통해 각 모달리티에서 뛰어난 성능을 이룬 최초의 옴니-모달 언어 모델(Omni-Modal Language Model)입니다. 이미지, 비디오, 오디오 이해 벤치마크에서 그 성능을 비교하고 있으며, 기존의 개방형 멀티모달 모델과 비교해 더 높은 성능을 제공합니다.
- ***Technical Details***: Ola 모델의 핵심은 각 모달리티에서 학습 능력을 단계적으로 확장할 수 있는 진보적 모달리티 정렬 전략입니다. 초기에는 이미지와 텍스트 모달리티를 기반으로 기초 지식을 구축하고, 이어서 오디오 및 비디오 데이터를 점진적으로 활용함으로써 언어와 오디오, 비디오 사이의 관계를 확장시킵니다. 이를 통해 복잡한 학습 과정을 작은 단계로 나누어 크로스-모달리티 정렬 데이터 크기를 줄일 수 있습니다.
- ***Performance Highlights***: Ola는 이미지 벤치마크에서 OpenCompass에서 72.6%의 평균 정확도를 기록하였으며, 비디오 벤치마크인 VideoMME에서 68.4%의 정확도를 달성했습니다. 또한, LibriSpeech에서는 3.1의 평균 WER(Word Error Rate)과 AIR-Bench에서 6.41의 GPT 평가 점수를 기록하면서 현재의 옴니-모달 모델보다 더 뛰어난 성능을 보여줍니다.

### [MAGA: MAssive Genre-Audience Reformulation to Pretraining Corpus Expansion](https://arxiv.org/abs/2502.04235)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04235.png)

Vote: 13

Authors: Chenggang Li, Xintong Hao, Ke Shen

- ***What's New***: MAGA는 대용량 장르-청중(Genre-Audience) 재구성을 통한 사전 학습 코퍼스의 확장을 목표로 합니다. 기존 코퍼스를 이용해 다양한 문맥적으로 풍부한 데이터셋을 합성하는 경량형 확장 방법을 제안하였으며, 이를 통해 770B 토큰 규모의 MAGACorpus를 구축했습니다.
- ***Technical Details***: MAGA 접근 방식은 (Genre, Audience) 쌍을 활용하여 원시 문서를 입력으로 사용해 데이터셋을 확장하며, 3.3B MoE 모델을 이용해 경량형 및 확장 가능한 형태로 설계되었습니다. 각 문서는 5개의 새로운 문서로 재구성되어 토큰 수를 3.9배로 확장하며, 다양성을 유지합니다. 또한, 'Limited Consistency'라는 품질 평가 메커니즘을 도입하여 정보 보존과 텍스트 변이 사이의 균형을 유지합니다.
- ***Performance Highlights***: MAGACorpus는 다양한 모델 크기에서 기존 코퍼스에 비해 성능 향상을 보였습니다. 예를 들어, 모델 크기가 커질수록 MAGA-Mix 그룹은 적은 성능 향상(+0.26/+0.95/+2.15)을 보였으며, TriviaQA와 GSM8K에서도 상당한 향상을 보였습니다.

### [MotionLab: Unified Human Motion Generation and Editing via the Motion-Condition-Motion Paradigm](https://arxiv.org/abs/2502.02358)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.02358.png)

Vote: 13

Authors: Na Zhao, Ziyan Guo, Zeyu Hu, De Wen Soh

- ***What's New***: MotionLab은 인간의 모션 생성과 편집을 통합하여 처리할 수 있는 새로운 패러다임인 Motion-Condition-Motion을 소개합니다. 기존의 고립된 해결책의 비효율성을 극복하고, 다양한 모션 관련 작업을 통일된 형식으로 처리할 수 있는 플랫폼을 제공합니다.
- ***Technical Details***: MotionLab은 MotionFlow Transformer를 사용하여 정의된 조건에 따라 소스 모션에서 타겟 모션으로의 매핑을 학습합니다. 이 프레임워크는 Aligned Rotational Position Encoding을 도입하여 소스 모션과 타겟 모션 사이의 시간 동기화를 보장하며, 다양한 태스크를 효과적으로 학습하기 위해 Task Instruction Modulation과 Motion Curriculum Learning 전략을 포함합니다.
- ***Performance Highlights***: MotionLab은 여러 태스크에서의 일반화 능력과 추론 효율성에서 유망한 성능을 보여주었습니다. 특히, 텍스트 기반 모션 생성에서 경쟁력 있는 성능을 보유하며, 궤적 기반 태스크와 모션 편집 작업에서 평균 오류를 낮추었습니다. 이러한 성능 향상은 공간적 및 시간적 동기화를 보장하는 사전 학습과 Aligned ROPE의 효과 덕분입니다.

### [ScoreFlow: Mastering LLM Agent Workflows via Score-based Preference Optimization](https://arxiv.org/abs/2502.04306)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04306.png)

Vote: 11

Authors: Guohao Li, Yinjie Wang, Mengdi Wang, Bryon Aragam, Ling Yang

- ***What's New***: ScoreFlow는 최신 대형 언어 모델(Large Language Models; LLM)의 에이전트 워크플로우를 최적화하기 위해 설계된 프레임워크로, 수동 개입 없이도 자동화된 워크플로우 생성 및 최적화를 구현합니다. 특히, ScoreFlow는 정량적 피드백을 활용하는 새로운 최적화 방법인 Score-DPO를 도입하여 작업 효율성을 높이고, 기존의 불연속 최적화 기술의 한계를 극복하였습니다.
- ***Technical Details***: ScoreFlow는 매번 주어진 작업에 대해 코드 기반으로 워크플로우를 생성하며, 평가 점수로부터 선호 데이터를 수집하여 워크플로우 생성기를 정밀하게 튜닝합니다. 특히, Score-DPO는 기존의 선호 최적화 방식과 달리 평가 점수를 직접 통합하여 최적화 과정을 개선하였습니다. 이를 통해 워크플로우 생성에서 속도를 높이고 데이터의 불확실성을 줄였습니다.
- ***Performance Highlights***: ScoreFlow는 질문 응답, 코드 생성, 수학적 추론 등 6개의 벤치마크에서 기존 기준선 기법들보다 평균 8.2% 높은 성능을 보였습니다. 특히 Score-DPO의 도입으로 인해 소규모 모델이 대규모 모델보다 성능이 높은 경우도 관찰되었으며, 이는 소요되는 API 비용을 줄여 비용 효율성을 크게 향상시켰습니다.

### [Llasa: Scaling Train-Time and Inference-Time Compute for Llama-based Speech Synthesis](https://arxiv.org/abs/2502.04128)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04128.png)

Vote: 10

Authors: Zheqi DAI, Lei Xie, Zhen Ye, Xinsheng Wang, Chi-Min Chan, Yunlin Chen, Jianyi Chen, Xingjian Du, Xu Tan, Yike Guo, Jiahe Lei, Wei Xue, Yizhu Jin, Haohe Liu, Qiuqiang Kong, Hongzhan Lin, Liumeng Xue, Zhifei Li, Yi Peng, Xinfa Zhu

- ***What's New***: Llasa는 Llama 기반의 음성 합성(Speech Synthesis) 시스템을 위한 새로운 접근 방식을 소개합니다. 이 시스템은 단일 레이어 벡터 양자화기(VQ) 코덱과 단일 변환기(Transformer) 아키텍처를 사용하여 표준 대형 언어 모델(LLM)과의 완전한 정렬을 목표로 합니다. Llasa는 훈련 시점 및 추론 시점 컴퓨팅의 스케일 조정을 통해 음성 합성의 자연스러움과 복잡한 운율 패턴 생성 능력을 향상시킵니다.
- ***Technical Details***: Llasa는 코덱 모듈과 단계별 탐색 전략을 통해 텍스트와 음성 토큰의 공동 분포를 학습합니다. 표준 LLM과 연결되는 테스트 시 컴퓨팅에서 음성 이해 모델(Speech Understanding Models)을 검증기로 사용하여 감정적 표현력과 음성 일관성, 내용 정확성을 향상시킬 수 있습니다. X-codec2라는 음성 토크나이저를 도입하여 음성 신호의 내용, 운율, 음색 등을 포착할 수 있도록 설계되었습니다.
- ***Performance Highlights***: Llasa 모델은 LibriSpeech와 Seed-TTS-Eval 및 ESD 데이터셋에서 최첨단 성능을 입증했습니다. 자연어 이해와 생성 과제에서 더 나은 성능을 제공하며, 특히 훈련 시점의 컴퓨팅 자원 증가가 자연스러운 음성합성과 감정표현 정확성을 향상시키는 효과가 확인되었습니다. 공개된 체크포인트와 훈련 코드를 통해 더 많은 연구와 개발을 촉진하는 것을 목표로 하고 있습니다.

### [ConceptAttention: Diffusion Transformers Learn Highly Interpretable Features](https://arxiv.org/abs/2502.04320)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04320.png)

Vote: 9

Authors: Pinar Yanardag, Duen Horng Chau, Alec Helbling, Tuna Han Salih Meral, Ben Hoover

- **What's New**: ConceptAttention은 다중 모달 디퓨전 트랜스포머(Multi-modal Diffusion Transformer; DiT)의 표현력을 활용하여 이미지 내에 텍스트 개념을 정확히 찾는 고품질 살리언시 맵(Saliency Map)을 생성하는 새로운 방법입니다. 추가 훈련 없이 DiT의 주의 레이어(Attention Layers)를 재사용하여 더 날카로운 살리언시 맵을 생성하며, ImageNet-Segmentation과 PascalVOC의 벤치마크에서 11개의 다른 방법보다 뛰어난 성능을 보였습니다. 이는 DiT 모델의 표현이 시각적 작업에 상당히 전이 가능함을 보여주는 첫 증거입니다.
- **Technical Details**: ConceptAttention은 DiT 주의 레이어의 출력 공간에서 선형 투영을 수행하여 높은 품질의 살리언시 맵을 생성합니다. 이는 이미지 패치 표현과 개념 임베딩을 결합하여 이루어집니다. 우리의 방법은 UNet 기반 모델의 교차 주의 메커니즘을 능가하며, 다중 모달 DiT 모델에서만 가능한 텍스트 개념 임베딩을 통해 가능한 것입니다. 우리는 ConceptAttention이 개념 토큰을 생성하고 이미지에 정보를 주입하지 않도록 하여, 이미지와 다른 토큰 간의 독립성을 유지합니다.
- **Performance Highlights**: ConceptAttention은 zero-shot 이미지 분할 작업에서 ImageNet-Segmentation과 PascalVOC 벤치마크에서 높은 정확도와 mIoU를 기록하며, 다양한 해석 방법들을 능가하는 성과를 거두었습니다. 특히, 복수 객체 이미지를 분할할 때에도 다른 방법들과 비교해 월등한 성능을 보이며, 이는 DiT 기반 모델이 후속 비전 과제에 전이 가능함을 시사합니다.

### [MotionCanvas: Cinematic Shot Design with Controllable Image-to-Video Generation](https://arxiv.org/abs/2502.04299)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04299.png)

Vote: 8

Authors: Feng Liu, Jinbo Xing, Long Mai, Cusuh Ham, Tien-Tsin Wong, Chi-Wing Fu, Jiahui Huang, Aniruddha Mahapatra

- ***What's New***: MotionCanvas는 이미지-비디오(I2V) 생성 과정에서 시네마틱샷 디자인을 도입하여 사용자가 장면 인식 방식으로 카메라 움직임과 객체 운동을 모두 제어할 수 있게 하는 영상 합성 시스템입니다.
- ***Technical Details***: MotionCanvas는 사용자 정의 3D 공간에서의 운동 설계 모듈을 기반으로 동작 신호 변환(Motion Signal Translation) 모듈을 통해 장면 공간의 운동 의도를 스크린 공간의 운동 신호로 변환합니다. 이를 통해 고급 사용자의 운동 설계를 비디오 확산 모델의 화면 신호로 변환하여 영상 합성을 진행하며, DiT 기반의 비디오 확산 모델을 통해 카메라 및 객체 운동을 따르는 전용 운동 조건 메커니즘을 설계합니다.
- ***Performance Highlights***: MotionCanvas는 다양한 실제 이미지 콘텐트와 샷-디자인 시나리오에서의 효과를 입증하며, 디지털 콘텐츠 제작에 창의적 워크플로우를 향상시키고 다양한 이미지 및 비디오 편집 애플리케이션에 적응 가능성을 보여줍니다.

### [Weak-to-Strong Diffusion with Reflection](https://arxiv.org/abs/2502.00473)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.00473.png)

Vote: 8

Authors: Zeke Xie, Lichen Bai, Masashi Sugiyama

- ***What's New***: 이 논문은 Weak-to-Strong Diffusion (W2SD)라는 새로운 프레임워크를 제안합니다. 이는 약한 모델과 강한 모델 간의 차이(weak-to-strong difference)를 추정하여 강한 모델을 이상적인 모델에 가까워지도록 유도합니다. 이를 통해 생성된 데이터와 실제 데이터 분포 간의 차이를 줄이고자 합니다.
- ***Technical Details***: W2SD는 약한 모델과 강한 모델 간의 차이를 추정하여 이상적인 모델에 접근하는 약한 모델과 강한 모델 쌍을 전략적으로 선택함으로써 다방면에서 개선을 달성할 수 있습니다. 이 방법은 다양한 데이터 모달리티(예: 이미지, 비디오)와 아키텍처(예: UNet 기반, DiT 기반)에서 광범위하게 적용 가능합니다.
- ***Performance Highlights***: W2SD는 여러 벤치마크에서 최첨단(SOTA) 성능을 달성하며, 인체 선호도, 미학적 품질, 프롬프트 준수 측면에서 큰 개선을 보여줍니다. 예를 들어, Juggernaut-XL 모델에 W2SD를 적용할 경우 HPSv2 승률을 원본 결과보다 최대 90%까지 개선할 수 있습니다.

### [BOLT: Bootstrap Long Chain-of-Thought in Language Models without Distillation](https://arxiv.org/abs/2502.03860)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.03860.png)

Vote: 7

Authors: Hanze Dong, Caiming Xiong, Yingbo Zhou, Silvio Savarese, Bo Pang, Jiacheng Xu

- ***What's New***: BOLT는 긴 체인 오브 소트(Long Chain-of-Thought, LongCoT) 능력을 기존의 LongCoT 모델이나 비용이 많이 드는 인간 주석 없이 LLM에서 개발할 수 있는 새로운 방법을 소개합니다. 이 방법은 ShortCoT 모델로부터 LongCoT를 부트스트랩하는 과정을 통해 이루어집니다.
- ***Technical Details***: BOLT 방법론은 세 가지 단계로 구성되어 있습니다: (1) 기존 ShortCoT LLM을 사용하여 컨텍스트 내 학습을 통한 LongCoT 데이터 부트스트래핑, (2) LongCoT 감독 하에 파인 튜닝, (3) LongCoT 능력을 더욱 정밀하게 하기 위한 온라인 훈련. Llama-3.1-70B-Instruct 모델을 부트스트랩 모델로 사용하여 다양한 모델 크기에서 BOLT를 적용하였습니다.
- ***Performance Highlights***: BOLT는 Arena-Hard, MT-Bench, WildBench, ZebraLogic, MATH500와 같은 다양한 벤치마크에서 뛰어난 성능을 보여줍니다. 이 벤치마크들은 정보 탐색, 창의적 글쓰기, 코딩, 수학 등을 포함한 실제 사용자 요청 및 논리 퍼즐 해결 능력을 평가합니다.

### [ChartCitor: Multi-Agent Framework for Fine-Grained Chart Visual Attribution](https://arxiv.org/abs/2502.00989)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.00989.png)

Vote: 6

Authors: Puneet Mathur, Ryan Rossi, Kanika Goswami, Franck Dernoncourt

- ***What's New***: ChartCitor는 생성적 AI를 활용한 ChartQA의 신뢰성을 높이기 위한 시각적 증거 기반의 멀티 에이전트 프레임워크를 제시합니다. 이는 기존의 답변 인용 방식이 시각 요소와 텍스트 정렬의 복잡성으로 인한 어려움을 극복하며, 차트 이미지를 통해 지지 증거를 식별하고 미세한 바운딩 박스(Bounding Box) 인용을 제공합니다.
- ***Technical Details***: ChartCitor는 여러 전담 LLM 에이전트(Agents)를 활용하여 차트-테이블 추출, 답변 재구성, 테이블 증대, 증거 검색(Pre-filtering 및 Re-ranking), 테이블-차트 매핑을 수행합니다. Chart2Table 추출 에이전트는 주어진 PDF 이미지에서 테이블 형식(예: CSV, HTML)으로 데이터를 변환하는 역할을 하며, GPT-4V를 통한 시각적 셀프 리플렉션(Self-Reflection) 기법을 사용하여 데이터의 일관성을 검증합니다. 답변 재구성 에이전트는 정성적 논리 단계로 답변을 재구성하여 정확한 인용을 보장합니다. 엔티티 캡셔닝 에이전트는 로우, 컬럼, 셀의 맥락을 파악하여 다층적 설명을 생성합니다. 마침내, 셀 로컬라이제이션 에이전트는 DETR(Detector) 모델을 활용하여 차트 이미지의 데이터 마크(bar, line segment, pie slice)를 식별하고 인용된 셀이 차트의 시각적 요소로 정확히 매핑되도록 보장합니다.
- ***Performance Highlights***: ChartCitor는 IoU(Intersection over Union) 기준으로 기존 대비 27.4%의 성능을 보여주며, 더욱 신뢰성 있는 시각적 증거를 제공합니다. 사용자 평가에서 ChartCitor는 GPT-4o보다 완전히 정확한(41% 대 28%) 또는 어느 정도 정확한(17% 대 15%) 인용 비율이 높았으며, 참가자들은 큰 시간 절약 효과를 경험했다고 평가했습니다.

### [PILAF: Optimal Human Preference Sampling for Reward Modeling](https://arxiv.org/abs/2502.04270)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04270.png)

Vote: 6

Authors: Yaqi Duan, Julia Kempe, Ariel Kwiatkowski, Yunzhen Feng, Kunhao Zheng

- ***What's New***: PILAF(PILAF)은 보상 모델링(Reward Modeling)을 위한 최적의 인간 선호 샘플링 기법으로, 인간 피드백을 통한 강화 학습(RLHF)에서 나타날 수 있는 데이터 수집의 비효율성을 해결하기 위한 새로운 접근 방식을 제안합니다. 이는 선호 학습을 오라클 보상을 최대화하는 방향으로 명시적으로 조정함으로써, 현실 세계의 대형 언어 모델과 인간 가치의 조화를 더욱 효과적으로 달성할 수 있도록 설계되었습니다.
- ***Technical Details***: PILAF는 선택적 데이터 수집을 통한 정책 최적화를 구현하며, 응답 샘플링에 관한 이론적 기반을 제공합니다. 구체적으로는 현재의 정책과 참조 정책 간의 보간을 통해 탐색과 착취의 균형을 맞춥니다. 이 방법의 수학적 분석은 PILAF가 정책 네트워크의 매개변수 변화가 오라클 값의 경사와 1차적으로 일치할 수 있음을 보여줍니다. 이는 선호 데이터의 선택이 더욱 정보성이 있도록 돕고, 통계적 변동성을 줄이는데 기여합니다.
- ***Performance Highlights***: PILAF는 대규모 실험을 통해 그 효과성과 강건성을 입증하였으며, 주어진 설정에서 40% 이상의 어노테이션 및 계산 비용을 절감하면서 모든 기준선보다 뛰어난 성과를 기록했습니다. 또한 BENCHMARK와 같은 대형 네트워크 정렬 작업에서도 더 높은 보상 달성 및 작은 KL 발산(KL Divergence)을 나타내었으며, 이는 전문가가 필요한 분야에서의 어노테이션 비용을 크게 절감할 수 있음을 의미합니다.

### [Beyond Prompt Content: Enhancing LLM Performance via Content-Format Integrated Prompt Optimization](https://arxiv.org/abs/2502.04295)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04295.png)

Vote: 4

Authors: Yuanye Liu, Yuqing Yang, Li Lyna Zhang, Cheng Peng, Yang Chen, Zhongxin Guo, Xuan Feng, Jiahang Xu, Qi Chen

- ***What's New***: 이번 연구에서는 Content-Format Integrated Prompt Optimization (CFPO)라는 새로운 방법론을 도입하여, 대형 언어 모델(LLMs)의 성능을 향상시키기 위해 프롬프트의 컨텐츠와 포맷을 통합적으로 최적화하는 방식을 제안합니다. 이는 기존의 컨텐츠에만 집중된 최적화 방법론에서 벗어나 포맷의 중요성을 강조하며, 모델 불가지론적인 접근을 통해 다양한 작업과 오픈 소스 LLMs에서 성능을 측정 가능하게 향상시킵니다.
- ***Technical Details***: CFPO는 자연 언어 변이를 활용하여 컨텐츠 변형을 탐구하며, 다양한 포맷 옵션을 체계적으로 평가하는 동적 포맷 탐사 전략을 채택합니다. 포맷 최적화는 Prompt Renderer와 Query Format의 두 가지 핵심 차원을 활용하여, 프롬프트를 구성하는 모든 요소의 조직적 구조를 통제하고 in-context 학습 예제와 쿼리의 표현을 결정합니다. 이러한 방식으로 콘텐츠와 포맷 타입을 구별하여, 높은 성능의 프롬프트를 효율적으로 식별할 수 있습니다.
- ***Performance Highlights***: CFPO는 다양한 작업과 여러 오픈 소스 LLMs에 걸쳐 도입되어 실질적이고 측정 가능한 성능 향상을 입증하였습니다. 특히, 기초 모델의 경우 CFPO가 최적화한 프롬프트는 긴 텍스트와 더 많은 in-context 예제를 포함하며, 이러한 특성이 모델 최적화 요구에 더욱 부합함을 시사합니다. 결과적으로 LLMs의 성능에 포맷이 미치는 중대한 영향을 강조하며, 콘텐츠와 포맷의 통합 최적화 접근법의 필요성을 분명히 합니다.

### [Enhancing Code Generation for Low-Resource Languages: No Silver Bullet](https://arxiv.org/abs/2501.19085)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.19085.png)

Vote: 4

Authors: Alberto Martin-Lopez, Gabriele Bavota, Alessandro Giagnorio

- ***What's New***: 저자는 저자 Alessandro Giagnorio와 공동 연구자들이 저자한 이 논문에서 로우 리소스 언어(Low-Resource Languages)의 코드 생성(Code Generation) 성능 향상을 위한 다양한 접근 방식을 비교하고 있습니다. 주로 고자원 언어와 로우 리소스 언어 간의 성능 격차를 줄이기 위한 새로운 방법들을 탐구합니다.
- ***Technical Details***: 이 연구는 총 6개의 대형 언어 모델(Large Language Models; LLMs)을 대상으로 수행되었습니다. 연구에서 소개하는 방법으로는 기본적인 파인 튜닝(Fine-Tuning)과 세 가지 변형의 인컨텍스트 학습(In-Context Learning)이 있습니다. 특히 인컨텍스트 학습에서는 몇 가지 샘플 코드를 통해 로우 리소스 언어에 대한 추가 정보를 제공하며, 사전 훈련된 고자원 언어를 로우 리소스 언어로 번역하는 사전 학습 목표도 설정됩니다.
- ***Performance Highlights***: 이 연구는 R과 Racket 두 로우 리소스 언어에 대한 LLM들의 성능을 집중적으로 평가한 결과를 보여줍니다. 연구의 결과에 따르면, 소규모 모델에서는 파인 튜닝이 가장 효과적이며, 모델 크기가 증가할수록 인컨텍스트 학습의 효과가 뛰어납니다. 큰 LLM에서는 충분한 데이터가 없을 시 파인 튜닝이 성능을 저하시킬 수 있습니다. 특히, GitHub Copilot은 인컨텍스트 학습을 통해 R에서 8.4%의 성능 향상을 보였습니다.

### [Towards Physical Understanding in Video Generation: A 3D Point Regularization Approach](https://arxiv.org/abs/2502.03639)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.03639.png)

Vote: 4

Authors: Anil Kag, Sergey Tulyakov, Vidit Goel, Jian Ren, Junli Cao, Yunuo Chen, Sergei Korolev, Chenfanfu Jiang

- ***What's New***: 이 연구에서는 비디오 생성에서 물리적 이해를 향상시키기 위한 3D 포인트 정규화(3D Point Regularization) 접근 방식을 제안합니다. 비디오 생성 모델에 3차원 기하학 정보를 통합함으로써, 기존의 2D 영상에서는 얻을 수 없었던 접촉 및 변형과 같은 복잡한 상호작용을 더 잘 인식할 수 있도록 합니다.
- ***Technical Details***: 이 연구는 2D 비디오에 3D 포인트 궤적(trajectory)을 추가하고, 이를 픽셀 공간(pixel space)에서 정렬하는 새로운 비디오 생성 프레임워크를 제시합니다. 이 접근은 3차원 정보가 없는 전통적인 RGB 비디오 모델을 대체하여 3D 점 클라우드(point cloud)와의 일치성을 촉진합니다. PointVid라는 3D 인식 비디오 데이터셋을 구축하여, 비디오 모델이 3D 형상과 운동을 더 잘 이해하도록 하고자 했습니다. 이 데이터셋은 픽셀 정렬된 3D 좌표를 첫 번째 프레임에 대한 정보로 제공합니다.
- ***Performance Highlights***: 제안된 모델은 VBench 및 VideoPhy 벤치마크에서 테스트되었으며, 물리적 상식과 모션 매끄러움 측면에서 상당한 향상을 보였습니다. 비디오 생성에서는 물체의 형상과 모션의 일관성을 유지하면서도 비물리적 변형과 같은 아티팩트를 최소화합니다. 사용자 연구 결과에서도 물리적인 연결성과 신뢰성이 향상되었다는 평가를 받았습니다.

### [PlotGen: Multi-Agent LLM-based Scientific Data Visualization via Multimodal Feedback](https://arxiv.org/abs/2502.00988)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.00988.png)

Vote: 4

Authors: Puneet Mathur, Ryan Rossi, Kanika Goswami, Franck Dernoncourt

- ***What's New***: PlotGen은 과학 데이터를 시각화하기 위해 멀티모달 LLMs를 활용하여 여러 에이전트를 조율하는 새로운 멀티 에이전트 프레임워크입니다. 이는 사용자 요구에 따라 정확한 과학적 시각화를 생성하도록 설계되어 있으며, 시각, 레이블, 데이터 정확성을 개선하기 위해 자체 반성을 통해 시각적, 문자적, 수치적 피드백 에이전트를 통합하고 있습니다.
- ***Technical Details***: PlotGen은 5개의 에이전트로 구성되어 있습니다: (1) 쿼리 계획 에이전트는 복잡한 사용자 요청을 실행 가능한 단계로 나누고, (2) 코드 생성 에이전트는 사용자 데이터를 실행 가능한 파이썬 코드로 변환합니다. (3) 수치 피드백 에이전트는 데이터와 플롯 유형의 정확성을 확인하며, (4) 문자 피드백 에이전트는 제목, 부제목, 축 레이블 등을 점검합니다. (5) 시각 피드백 에이전트는 차트의 색상, 레이아웃 등을 사용자 요구에 맞게 조정합니다.
- ***Performance Highlights***: PlotGen은 다양한 LLM 설정에서 MatPlotBench 데이터셋에 대해 10-12%의 성능 향상을 달성했습니다. 특히 GPT-4 기반으로 실행될 때 가장 높은 성과를 보였으며, 시각, 문자, 수치 피드백의 통합이 다양한 오류 복구에 효과적임을 입증했습니다.

### [Speak Easy: Eliciting Harmful Jailbreaks from LLMs with Simple Interactions](https://arxiv.org/abs/2502.04322)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04322.png)

Vote: 3

Authors: Narutatsu Ri, Yuxin Xiao, Marzyeh Ghassemi, Yik Siu Chan

- ***What's New***: 이 논문은 간단한 다중단계 및 다중언어 상호 작용을 통해 대형 언어 모델(LLMs)로부터 유해한 행동을 유발할 수 있는 'SPEAK EASY'라는 프레임워크를 제안합니다. 이를 통해, 기술적 전문 지식 없이 일반 사용자도 LLMs의 보안 취약점을 악용할 수 있음을 발견했습니다.
- ***Technical Details***: SPEAK EASY는 악의적인 쿼리를 다중단계의 무해한 하위질문으로 분해하고, 이를 여러 언어로 번역하여 LLMs에 입력합니다. 그런 후, 번역된 응답을 영어로 다시 변환하고, 행동 가능성과 정보성을 기준으로 응답을 최적화하여 최종 응답을 만듭니다. 또한, 'HARMSCORE'라는 새로운 측정기를 도입하여 유해한 함정(Jailbreak) 응답의 행동 가능성과 정보성을 평가합니다.
- ***Performance Highlights***: SPEAK EASY 프레임워크는 GPT-4o와 같은 모델의 공격 성공률(ASR)을 0.092에서 0.555로, HARMSCORE를 0.180에서 0.759로 크게 향상시켰습니다. 이 방법은 기존의 탈옥 기법과 결합할 때도 효과적이며, 특히 글로벌 사용자가 일반적인 질문을 통해 유해한 콘텐츠를 생성할 가능성을 증가시킵니다.

### [Learning Real-World Action-Video Dynamics with Heterogeneous Masked Autoregression](https://arxiv.org/abs/2502.04296)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04296.png)

Vote: 2

Authors: Xinlei Chen, Kevin Zhao, Chaoqi Liu, Lirui Wang

- ***What's New***: 이 연구는 Heterogeneous Masked Autoregression (HMA; 비동질적 마스크드 오토리그레션)을 도입하여 로봇 학습 확장을 위해 고품질 데이터와 평가를 생성하는 첫 번째 시도입니다. HMA는 다양한 로봇 엔보디먼트, 도메인 및 작업에 걸쳐 관찰 및 동작 시퀀스로부터 비동질적 사전훈련을 수행하여 비디오 예측을 위한 정량화된 또는 소프트 토큰을 생성합니다.
- ***Technical Details***: HMA는 비디오 및 동작 시퀀스의 마스킹된 오토리그레션을 사용하며, 벡터 정량화(VQ) 토큰과 소프트 토큰의 두 가지 변종을 탐구합니다. 비동질적 데이터에서의 사전훈련은 40가지의 서로 다른 엔보디먼트를 포함한 3백만 개의 궤적을 사용합니다. 네트워크는 모듈화되어 있어 새로운 엔보디먼트를 추가할 때마다 작은 동작 인코더와 디코더만 추가로 훈련하면 됩니다.
- ***Performance Highlights***: HMA는 이전 로봇 비디오 생성 모델보다 15배 더 빠르게 시각적 충실도와 제어 가능성을 개선하였습니다. 시뮬레이션 벤치마크에서는 높은 충실도의 시뮬레이터로서 정책을 평가하고, 90%의 합성 궤적 데이터를 생성하여 정책 성능을 실제 데이터와 유사한 수준으로 향상시킵니다. IRASim과 비교할 때, HMA는 더 빠른 속도와 향상된 시각적 품질 및 제어 가능성을 나타냅니다.

