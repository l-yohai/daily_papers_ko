## Daily Papers (2025-02-14)

### [InfiniteHiP: Extending Language Model Context Up to 3 Million Tokens on a Single GPU](https://arxiv.org/abs/2502.08910)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.08910.png)

Vote: 58

Authors: Sung Ju Hwang, Heejun Lee, Geon Park, Jaduk Suh

- ***What's New***: InfiniteHiP는 새로운 LLM 추론 프레임워크로, 단일 L40s 48GB GPU에서 최대 300만 토큰까지의 컨텍스트를 처리할 수 있도록 설계되었습니다. 이를 위해 동적 모듈형 계층적 토큰 프루닝 알고리즘을 도입하여 불필요한 컨텍스트 토큰을 제거해 추론 속도를 가속화하고, GPU 메모리 비용을 절감합니다.
- ***Technical Details***: InfiniteHiP는 다중 단계 프루닝 모듈을 사용하여 효율적인 블록 희소 주의(attention)을 구현합니다. 이 프레임워크는 주의 메커니즘의 계산 부담을 줄이기 위해 모듈형 희소 주의 방식을 제안하며, 다양한 RoPE 조정 전략을 적용하여 OOL(out-of-length) 일반화를 달성합니다. 또한, HiP attention의 오프로드 전략을 개선하여 LRU 기반의 캐시 정책을 도입, 효율적인 KV 캐시 오프로드를 통해 GPU 메모리 압박을 완화합니다.
- ***Performance Highlights***: InfiniteHiP는 1백만 토큰 컨텍스트에서 18.95배 빠른 주의 디코딩을 구현하는 데 성공했으며, FA2의 VRAM 사용량 대비 3.34%만으로 3백만 토큰 컨텍스트에서 엔드 투 엔드 디코딩에서 7.24배의 속도 향상을 달성했습니다. 이 프레임워크는 기존의 LLM 성능을 유지하면서 빠른 추론과 사용 가능한 컨텍스트 길이 확장이 가능합니다.

### [Skrr: Skip and Re-use Text Encoder Layers for Memory Efficient Text-to-Image Generation](https://arxiv.org/abs/2502.08690)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.08690.png)

Vote: 28

Authors: Se Young Chun, Hoigi Seo, Wongi Jeong, Jae-sun Seo

- ***What's New***: Skrr는 텍스트-이미지(Text-to-Image; T2I) 확산 모델에서 텍스트 인코더의 메모리 효율성을 개선하기 위한 새로운 레이어 스킵 및 재사용 전략을 제안합니다. 이는 텍스트-이미지 합성을 위한 전용 불일치 메트릭을 활용하여 특정 트랜스포머 블록을 선택적으로 스킵하거나 재사용하여 메모리 사용량을 줄이면서 성능을 유지합니다.
- ***Technical Details***: Skrr는 두 가지 주요 단계로 구성됩니다. 'Skip' 단계에서는 불필요한 블록을 탐지하여 프루닝(Pruning)하고 'Re-use' 단계에서는 이전 스킵되지 않은 블록을 재활용하여 성능 저하를 방지합니다. 이 과정에서 T2I 확산 모델에 맞춰 최적화된 불일치 메트릭을 활용해 제거 심층 항목을 평가하며, 빔 서치(Beam Search)를 사용해 여러 프루닝 경로를 동시에 평가하여 더 나은 결과를 도출합니다.
- ***Performance Highlights***: Skrr는 높은 게으름 수준에서도 원래 모델과 유사한 이미지 품질을 유지하며, 기존의 블록 단위 프루닝 기법을 능가하는 메모리 효율성을 달성합니다. 특히 40% 이상의 높은 희석 상태에서 GenEval 점수가 최대 20.4% 개선되었음을 보여줍니다.

### [SelfCite: Self-Supervised Alignment for Context Attribution in Large Language Models](https://arxiv.org/abs/2502.09604)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.09604.png)

Vote: 22

Authors: Benjamin Cohen-Wang, James Glass, Shannon Zejiang Shen, Zhaofeng Wu, Xi Victoria Lin, Yung-Sung Chuang, Shang-Wen Li, Wen-tau Yih, Hu Xu

- ***What's New***: SelfCite는 대형 언어 모델(Large Language Models; LLMs)이 인용 정보를 생성할 때, 사람이 주석을 달지 않아도 스스로 품질을 향상시킬 수 있도록 하는 새로운 자기 지도 학습 방법(Self-Supervised Approach)을 제공합니다. 이는 인용이 전체 또는 부분적 문맥을 제거해도 동일한 응답이 나오지 않으면 모델이 인용이 필요하다는 신호로 판단함으로써 이루어집니다.
- ***Technical Details***: SelfCite는 LLM의 문장 수준 인용 품질 향상을 위해 보상 신호를 사용하는 방법으로, 문맥 제거를 통해 인용의 필요성과 충분성을 평가합니다. LLM이 특정 인용 문장을 제거하면 동일한 응답을 생성할 확률이 크게 낮아지거나, 반대로 인용 문장만 있을 때 높은 확률로 동일한 응답을 생성할 시, 각각 필요성과 충분성을 평가하여 보상으로 사용합니다.
- ***Performance Highlights***: SelfCite는 LongBench-Cite 벤치마크에서 최대 5.3점의 F1 점수 향상을 이루었으며, 이는 LLM의 인용 생성 품질 향상에 매우 유망한 방향을 제시합니다. GPT-4o와 비교할 때, 더 짧은 인용을 생성하면서도 더 높은 점수를 기록하여, 독점 솔루션의 강력한 대안이 될 수 있음을 보여줍니다.

### [Can this Model Also Recognize Dogs? Zero-Shot Model Search from Weights](https://arxiv.org/abs/2502.09619)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.09619.png)

Vote: 21

Authors: Or Nathan, Jonathan Kahana, Yedid Hoshen, Eliahu Horwitz

- ***What's New***: ProbeLog는 사전훈련 및 온라인 상태의 분류기 모델을 훈련 데이터나 메타데이터 없이 타겟 개념(예: "Dog")을 인식할 수 있는 모델로 검색할 수 있는 새로운 방법입니다. 이는 특히 대규모 모델 리포지토리에서 효과적인 모델 검색을 가능하게 합니다.
- ***Technical Details***: ProbeLog는 모델의 각 출력 차원(logit)에 대해 고정된 입력 집합(probes)의 응답을 관찰하여 설명자를 계산합니다. 로그잇 기반 검색 및 제로샷(zero-shot), 텍스트 기반 검색을 지원하며, 협업 필터링(Collaborative Filtering)을 기반으로 한 방법을 통해 저장소 인코딩의 비용을 3배 줄였습니다.
- ***Performance Highlights***: ProbeLog는 실제 데이터셋 및 미세 검색 작업 모두에서 높은 검색 정확도를 달성하며, 대규모 저장소에 대해 확장 가능합니다. 대표적으로, ImageNet에서 개념을 텍스트로 예측하는 경우, 상위 1 정확도가 40% 이상에 도달하며, 이 성능은 무작위 방법의 성능인 0.1%에 비해 상당한 차이를 보입니다.

### [An Open Recipe: Adapting Language-Specific LLMs to a Reasoning Model in One Day via Model Merging](https://arxiv.org/abs/2502.09056)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.09056.png)

Vote: 20

Authors: Pittawat Taveekitworachai, Kunat Pipatanakul, Potsawee Manakul, Kasima Tharnpipitchai

- ***What's New***: 이 연구는 특정 언어를 위한 대형 언어 모델(LLM)들을 하루 만에 추론 모델로 적응하는 모델 병합 방법론을 소개합니다. 특히, 태국어 LLM에 DeepSeek R1의 고급 추론 능력을 통합하려는 시도입니다. 이 연구는 공개된 데이터셋과 $120의 비용으로 태국어 특화 LLM의 추론 능력을 향상시키면서도 다른 언어 작업에서는 성능 저하 없이 목표를 달성할 수 있음을 증명합니다.
- ***Technical Details***: 두 가지 전문 LLM을 선택하여 동일한 기본 아키텍처를 공유하도록 하고, 모델 병합은 '표현정렬 방식 및 능력 인식' 모델 병합을 사용합니다. 이 과정에서 주요 레이어의 병합 비율을 최적화했습니다. 이 연구에서 사용한 데이터셋은 Bespoke-Stratos 및 DeepSeek R1로부터 생성된 자료를 기반으로 했습니다.
- ***Performance Highlights***: 결과적으로, Typhoon2-R1-70B 모델은 이전에 존재하던 Typhoon2 70B Instruct와 DeepSeek R1 70B Distill의 성능 격차를 약 4%까지 줄였습니다. 평균 성능은 Typhoon2 70B와 비교해 41.6% 향상되었으며, DeepSeek R1에 비해서는 12.8%의 성능 향상을 보여줍니다. 이 연구는 태국어 모델에서 이 방법론이 효과적임을 증명했고, 다른 언어로의 확장 가능성 또한 제시합니다.

### [EmbodiedBench: Comprehensive Benchmarking Multi-modal Large Language Models for Vision-Driven Embodied Agents](https://arxiv.org/abs/2502.09560)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.09560.png)

Vote: 19

Authors: Hanyang Chen, Qineng Wang, Rui Yang, Huan Zhang, Manling Li, Mark Zhao, Cheng Qian, Teja Venkat Koripella, Marziyeh Movahedi, Junyu Zhang, Heng Ji, Tong Zhang, Kangrui Wang

- ***What's New***: EMBODIEDBENCH는 시각적 상호작용 에이전트(Vision-Driven Embodied Agents)를 평가하기 위한 포괄적 벤치마크로 도입되었습니다. 이는 높은 수준의 의미적 작업부터 원자적 행동에 이르는 저수준의 작업까지 4개의 환경에서 1,128개 테스트 작업으로 구성되어 있으며, 공동 추론, 복잡한 설명 이해, 공간 인식, 시각적 인식, 장기계획 등 필수 에이전트 능력을 평가하기 위한 6개의 세부 그룹으로 구체화되었습니다.
- ***Technical Details***: EMBODIEDBENCH는 다양한 작업을 선별하여 높은 수준의 작업 분해와 계획을 포함한 EB-ALFRED와 EB-Habitat 환경, 저수준의 탐색 및 조작이 요구되는 EB-Navigation와 EB-Manipulation 환경으로 구성됩니다. 에이전트는 자아 중심 시각 인식, In-Context 학습, 상호작용 이력, 환경 피드백을 통합하여 의사결정을 해야 합니다.
- ***Performance Highlights***: 다양한 실험을 통해 MLLM은 높은 수준의 작업에서는 우수한 성과를 보이는 반면, 저수준의 조작에서는 어려워하며 최고 모델인 GPT-4o조차 평균 28.9% 정도의 성과를 기록했습니다. 이 벤치마크는 현존하는 문제점을 강조하고 시각 주도 에이전트를 발전시킬 귀중한 통찰을 제공합니다.

### [Exploring the Potential of Encoder-free Architectures in 3D LMMs](https://arxiv.org/abs/2502.09620)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.09620.png)

Vote: 17

Authors: Bin Zhao, Delin Qu, Zhuhao Wang, Ray Zhang, Xuelong Li, Zoey Guo, Qizhi Chen, Zhigang Wang, Dong Wang, Junli Liu, Yiwen Tang

- ***What's New***: 이 논문은 3D 대형 멀티모달 모델(Large Multimodal Models; 3D LMMs)에서 인코더 없는(Encoder-free) 아키텍처의 잠재성을 최초로 체계적으로 조사한 결과를 제시합니다. ENEL이라는 인코더 없는 3D LMM을 개발하여 기존 ShapeLLM-13B의 성능에 맞먹는 성과를 달성했습니다.
- ***Technical Details***: 이 연구는 LLM을 3D 인코더의 역할을 담당하도록 하기 위해 두 가지 주요 전략을 제안합니다: 1) 사전 훈련 단계에서 LLM-임베디드 의미 인코딩(Semantic Encoding) 전략을 제안하여 여러 포인트 클라우드 자가-지도 손실을 탐구하고, 고수준 의미 추출을 위해 하이브리드 의미 손실(Hybrid Semantic Loss)을 소개합니다. 2) 명령 튜닝 단계에서는 계층적 기하학적 집계(Hierarchical Geometry Aggregation) 전략을 제안하여 LLM의 초기 계층에 귀납적 편향을 통합하여 포인트 클라우드의 로컬 세부사항에 중점을 둡니다.
- ***Performance Highlights***: ENEL 모델은 분류, 캡셔닝 및 VQA 작업에서 각각 55.0%, 50.92%, 42.7%의 성과를 달성하여 인코더 기반의 3D LMM들과 비교할 때 인코더 없는 아키텍처의 가능성을 보여줍니다. 특히, ENEL은 PointLLM과 동일한 데이터셋을 사용하면서도 인코더 없이도 훌륭한 성능을 발휘하였습니다.

### [CoSER: Coordinating LLM-Based Persona Simulation of Established Roles](https://arxiv.org/abs/2502.09082)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.09082.png)

Vote: 16

Authors: Jiangjie Chen, Wei Wang, Yanghua Xiao, Shuchang Zhou, Siyu Yuan, Rui Xu, Xintao Wang, Heng Wang, Haoran Guo, Jen-tse Huang, Yifei Zhang, Xinfeng Yuan

- ***What's New***: CoSER는 확립된 캐릭터들의 역할을 효과적으로 모사하기 위한 고품질 데이터셋, 오픈 모델, 그리고 평가 프로토콜을 제시합니다. CoSER 데이터셋은 771개의 유명한 문학 작품에서 17,966명에 이르는 캐릭터의 대화를 담고 있으며, 현실 세계의 복잡성을 반영한 대화와 캐릭터의 경험, 내적 생각 등을 포함합니다. 이 연구는 캐릭터 대화를 구축하고 평가하기 위해 주어진 상황 연기(given-circumstance acting)를 도입합니다.
- ***Technical Details***: CoSER 데이터셋은 LLaMA-3.1 모델을 기반으로 개발된 CoSER 8B 및 CoSER 70B와 같은 고급 오픈 역할 실행 LLMs(LLM-based Persona Simulation Models)를 포함합니다. 이 데이터셋은 인물 대화, 플롯 요약, 캐릭터 경험과 배경 정보를 다루며 각 캐릭터의 실제 발언에 대해 LLMs를 훈련합니다. 평가를 위해서는 멀티 에이전트 시뮬레이션과 페널티 기반의 LLM 비평을 포함한 GCA(Given-Circumstance Acting) 방법을 사용합니다.
- ***Performance Highlights***: CoSER 70B 모델은 InCharacter 벤치마크에서 75.80%, LifeChoice 벤치마크에서 93.47%의 정확도로 기존 GPT-4o 모델의 성능을 초과하거나 동등한 성능을 보여줍니다. 실험 결과는 CoSER 데이터셋이 RPLA(RPLA; Role-Playing Language Agents) 훈련, 평가 및 검색에 있어 상당한 가치를 지님을 보여줍니다.

### [TripoSG: High-Fidelity 3D Shape Synthesis using Large-Scale Rectified Flow Models](https://arxiv.org/abs/2502.06608)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06608.png)

Vote: 15

Authors: Yuan Liang, Zexiang Liu, Wanli Ouyang, Ding Liang, Zi-Xin Zou, Yan-Pei Cao, Dehu Wang, Yangguang Li, Xingchao Liu, Zhipeng Yu, Yuan-Chen Guo

- ***What's New***: TripoSG는 대형 크기 보정 플로우 트랜스포머를 사용하여 고충실도 3D 메쉬를 생성할 수 있는 새로운 형상 확산(paradigm)입니다. 이는 입력 이미지와 정확하게 일치하는 3D 형상을 생성할 수 있으며, 새로운 데이터 구축 시스템을 이용하여 대규모 고품질 데이터로 훈련되었습니다.
- ***Technical Details***: TripoSG에서는 플로우 기반 생성 아키텍처, 스케일링 전략, VAE 구조 및 지도 학습이 결합되어 있습니다. 주요 기술로는 SDF(Signed Distance Function) 표현, 표면 노멀 안내와 에이코날(eikonal) 손실을 결합한 하이브리드 지도 학습 전략이 있으며, 이는 고해상도 세부 사항을 복원하는 데 기여합니다. 또한, 모형 파라미터는 4B(4 billion)로 확장되었고, 시간 단계(timestep)는 선형 샘플링 방식을 통해 조정되어 학습의 효율성과 안정성을 높였습니다.
- ***Performance Highlights***: TripoSG는 다양한 구조, 스타일 및 여러 객체 구성에서 우수한 세부사항을 보유한 고충실도 3D 모형을 생성할 수 있습니다. 실험을 통해, TripoSG는 성능 개선을 위해 대규모 데이터, 대형 모델 크기, 고해상도 활용을 통해 보다 뛰어난 3D 형상 생성 결과를 보여 주었습니다. 성능 메트릭으로는 Normal-FID를 사용하여 기존의 방법과 비교할 때, TripoSG는 모든 측면에서 경쟁 모델보다 뛰어난 성과를 보였습니다.

### [The Stochastic Parrot on LLM's Shoulder: A Summative Assessment of Physical Concept Understanding](https://arxiv.org/abs/2502.08946)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.08946.png)

Vote: 13

Authors: Jiangnan Li, Dit-Yan Yeung, Shunchi Zhang, Jie Zhou, Junjie Wu, Mo Yu, Lemao Liu, Tsz Ting Chung

- ***What's New***: 이 논문은 물리적 개념 이해를 평가하는 새로운 과제인 PHYSICO를 제안하여 LLMs의 이해 수준에 대한 체계적인 평가를 수행하였습니다. PHYSICO는 고등학교 수준의 물리적 개념을 다양한 수준으로 평가하며, LLMs와 인간 간의 성능 차이를 수치적으로 검증합니다.
- ***Technical Details***: PHYSICO는 블룸의 교육 목표 분류에 따라 두 가지 수준의 이해력을 평가합니다. 저수준 이해력 과제는 개념의 기억력을 측정하고, 고수준 과제는 ARC 데이터셋에서 영감을 받은 추상적 표현을 통해 개념의 심층 이해도를 평가합니다. 실험은 상업용 및 오픈소스 LLMs를 대상으로 수행되었습니다.
- ***Performance Highlights***: 최신 LLMs는 저수준 과제에서 95% 이상의 정확도를 기록한 반면, 고수준 과제에서는 인간보다 약 40% 낮은 성능을 보였습니다. 이는 LLMs가 개념을 자연어로 잘 설명할 수 있으나, 추상적 표현을 통한 심층 이해에는 한계를 지님을 나타냅니다.

### [Logical Reasoning in Large Language Models: A Survey](https://arxiv.org/abs/2502.09100)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.09100.png)

Vote: 12

Authors: Yue Zhang, Chaoli Zhang, Xiaozhang Liu, Zhizhang Fu, Mengru Ding, Ruoxi Ning, Hanmeng Liu

- ***What's New***: 이 논문은 대형 언어 모델(Large Language Models; LLMs)에서 논리적 추론의 최근 발전을 살펴봅니다. LLMs의 논리적 추론 능력에 대한 체계적이고 포괄적인 검토를 제공하며, 연역적, 귀납적, 가설 유추 및 유추적 추론을 포함한 핵심 패러다임을 식별하고 평가합니다.
- ***Technical Details***: 논문은 논리적 추론을 강화하기 위해 데이터 중심 튜닝, 강화 학습, 디코딩 전략 및 신경-심볼릭(neuro-symbolic) 접근법을 포함한 전략을 분석합니다. 논리적 추론을 위한 데이터세트와 벤치마크들은 자연어 추론 (Natural Language Inference; NLI) 및 기계 독해(Machine Reading Comprehension; MRC) 등의 과제를 다룹니다. 또한, 증명 작성자(ProofWriter) 및 LogicBench와 같은 논리적 평가 프레임워크를 살펴봅니다.
- ***Performance Highlights***: 논문은 GPT-4와 ChatGPT와 같은 모델이 LogiQA 및 ReClor 벤치마크에서 일관되지 않은 성과를 보여주며, 과제외 데이터(out-of-distribution tasks)에서 고군분투하고 있음을 발견합니다. 이러한 모델들이 높은 논리적 일관성을 유지하며 인간과 유사한 성능을 발휘하는 데에는 현재 한계가 있습니다.

### [MME-CoT: Benchmarking Chain-of-Thought in Large Multimodal Models for Reasoning Quality, Robustness, and Efficiency](https://arxiv.org/abs/2502.09621)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.09621.png)

Vote: 12

Authors: Bo Zhang, Liuhui Wang, Yu Qi, Peng Gao, Hongsheng Li, Shen Yan, Chaoyou Fu, Xinyan Chen, Renrui Zhang, Yanwei Li, Claire Guo, Jianhan Jin, Ziyu Guo, Dongzhi Jiang

- ***What's New***: MME-CoT는 대형 멀티모달 모델(Large Multimodal Models; LMMs)의 사고 연쇄(Chain-of-Thought; CoT) 추론 성능을 평가하기 위한 특화된 벤치마크를 소개합니다. 수학, 과학, OCR, 논리, 시공간, 일반 장면 등 6개 도메인에 걸쳐 CoT를 체계적으로 평가합니다. 이 연구는 해당 분야에서 최초로 CoT의 품질, 견고성 및 효율성을 정교하게 평가하는 세 가지 새로운 메트릭을 통합합니다.
- ***Technical Details***: MME-CoT 벤치마크는 대형 멀티모달 모델에서의 CoT 추론 능력을 평가하기 위해 세밀하게 설계된 평가 프레임워크를 도입합니다. 평가 프레임워크는 세 가지 새로운 메트릭을 통해 CoT의 품질(Quality), 견고성(Robustness), 효율성(Efficiency)을 분석합니다. 특정 중간 CoT 단계의 논리적 유효성 및 신뢰도를 평가하고 CoT가 인식 작업에 미치는 영향을 검토합니다. 또한 CoT의 효율성을 두 가지 주요 메트릭으로 평가합니다: 생성된 콘텐츠가 질문의 답변에 기여하는 비율(Relevance Rate)과 각 반영 단계가 질문 해결에 미치는 영향을 분석하는 반영 품질(Reflection Quality)을 분석합니다.
- ***Performance Highlights***: 반영 메커니즘이 있는 모델은 CoT 품질이 뛰어난 것으로 나타났으며 Kimi k1.5는 GPT-4o보다 우수한 품질 결과를 보여줍니다. 다중모달 모델의 경우 CoT가 인식 중심 작업에서 성능을 저하시킬 수 있으며 과도한 사고가 해로울 수 있음을 시사합니다. CoT 품질은 높지만, 반영 기능이 있는 LMMs는 일반 반응과 자기 수정 단계 모두에서 상당한 비효율성이 나타납니다.

### [Typhoon T1: An Open Thai Reasoning Model](https://arxiv.org/abs/2502.09042)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.09042.png)

Vote: 11

Authors: Pittawat Taveekitworachai, Kunat Pipatanakul, Potsawee Manakul, Kasima Tharnpipitchai

- ***What's New***: Typhoon T1은 태국어로 된 추론 가능성을 제공하는 최초의 오픈 태국어 추론 모델로 소개됩니다. 이 모델은 새로운 형태의 생성적 모델군으로, 주어진 문제에 대해 긴 사유의 흔적을 생성한 후 결론에 도달하는 방식을 취하고 있습니다.
- ***Technical Details***: Typhoon T1은 대규모 강화 학습(RL) 대신 공개 데이터세트를 활용한 감독된 미세 조정(Supervised Fine-Tuning; SFT)을 통해 개발되었습니다. 이는 비용 효율적이며, 모델의 다양한 분야 일반화 및 낮은 자원 언어(태국어)에서 추론 흔적을 생성할 수 있도록 설계되었습니다.
- ***Performance Highlights***: Typhoon T1은 GSM8K, HumanEval+, GPQA 등 다양한 벤치마크에서 눈에 띄는 개선을 보여줍니다. 특히 구조화된 사고 방식(Structured Thinking)을 통해 특정 벤치마크에서 성능 개선을 달성했으며, 태국어 및 다른 언어의 추론 흔적 생성에서도 성능이 입증되었습니다.

### [CoT-Valve: Length-Compressible Chain-of-Thought Tuning](https://arxiv.org/abs/2502.09601)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.09601.png)

Vote: 8

Authors: Gongfan Fang, Guangnian Wan, Xinchao Wang, Xinyin Ma, Runpeng Yu

- ***What's New***: CoT-Valve는 다중 단계 추론을 필요로 하는 어려운 과제에 대해 크기를 줄인 Chain-of-Thought(coT; 연쇄 사고) 스킴을 하나의 모델로 엘라스틱하게 조절 가능하도록 설계된 새로운 튜닝 및 추론 전략입니다. 이 새로운 방법은 각 task의 난이도에 따라 동적으로 추론 경로의 길이를 제어함으로써 추론 모델의 추론 비용을 줄이는 데 목적이 있습니다.
- ***Technical Details***: CoT-Valve는 파라미터 공간 내에서 방향성을 식별하여, 이를 조작하여 CoT의 길이를 효과적으로 조절할 수 있게 합니다. 이를 위해 LoRA(영소랭크적응)를 사용하여 'Valve(밸브)'로써 기능하도록 하며, 모델에 최소한의 추가 파라미터를 부과하며 제어할 수 있습니다. 우리는 MixChain이라는 데이터셋을 구성하여 질문당 긴 것에서 짧은 것으로 다양한 길이의 추론 체인을 생성하고 이를 기존 모델에 적용해 길이를 조절합니다.
- ***Performance Highlights***: CoT-Valve는 GSM8K에서 741토큰에서 225토큰으로, AIME에서 6827토큰에서 4629토큰으로 줄이면서 성능 감소를 최소화(95.07%에서 94.92%)할 수 있었습니다. 이를 통해 짧은 추론 경로가 긴 경로를 능가할 수 있음을 보여주며 모델 효율성을 향상시킵니다.

### [SQuARE: Sequential Question Answering Reasoning Engine for Enhanced Chain-of-Thought in Large Language Models](https://arxiv.org/abs/2502.09390)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.09390.png)

Vote: 7

Authors: Daniel Fleischer, Moshe Wasserblat, Moshe Berchansky, Gad Markovits

- ***What's New***: SQuARE는 대규모 언어 모델(LLMs)의 향상된 Chain-of-Thought(사고의 연쇄)를 실현하기 위한 새로운 프롬프트 기법을 소개합니다. 모델이 주요 질문을 해결하기 전에 스스로 여러 보조 질문을 생성 및 해결하도록 유도하여 주제를 보다 심도 있게 탐구하도록 설계된 self-interrogation(자기-질문) 패러다임을 활용합니다.
- ***Technical Details***: SQuARE는 기존의 CoT(chain-of-thought) 프레임워크를 기반으로 만들어진 프롬프트 기술로, 모델이 N개의 질문과 답변 쌍을 생성한 후 결론을 도출하도록 합니다. N의 값은 탐색의 철저함과 계산 비용 사이의 균형을 맞추기 위해 조정할 수 있으며, 실험에서는 소수의 질문 집합도 최종 답변의 정확성을 크게 높일 수 있음을 보여줍니다.
- ***Performance Highlights***: SQuARE는 다양한 질문-응답(QA) 데이터셋에서 Llama 3 및 GPT-4o 모델을 사용한 평가에서 기존의 CoT와 Rephrase-and-Response(RaR) 방법을 능가했습니다. 예를 들어, TriviaQA에서, SQuARE는 GPT-4o와 함께 사용시 다른 방법들보다 최소 2.0% 더 높은 96.7%의 성능을 나타냈습니다.

### [mmE5: Improving Multimodal Multilingual Embeddings via High-quality Synthetic Data](https://arxiv.org/abs/2502.08468)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.08468.png)

Vote: 6

Authors: Furu Wei, Zhicheng Dou, Haonan Chen, Nan Yang, Liang Wang, Ziliang Zhao, Yutao Zhu

- ***What's New***: 이 논문은 고품질의 합성 데이터를 통해 멀티모달 다국어 임베딩(mmE5)을 개선하는 방법을 제안합니다. 특히, 새로운 데이터 합성 프레임워크를 설계하여 다국어 및 다양한 멀티모달 태스크를 아우르는 데이터를 생성합니다.
- ***Technical Details***: mmE5 모델은 세 가지 주요 기준으로 합성 데이터를 생성합니다: 1) 넓은 범위(broad scope)로 다양한 언어와 모달리티 조합을 포함합니다. 2) 강력한 교차 모달 정렬(robust cross-modal alignment)을 통해 텍스트와 이미지를 일관된 의미로 결합합니다. 3) 높은 충실도(high fidelity)를 유지하도록 실제 이미지와 관련 텍스트를 사용하여 합성 데이터를 생성합니다.
- ***Performance Highlights***: mmE5는 MMEB 벤치마크에서 최신 성능(states-of-the-art)을 기록했으며, 이전 SOTA 모델보다 45배 적은 훈련 데이터를 사용하여 우수한 성능을 달성했습니다. 또한, XTD 벤치마크에서도 다국어 능력을 입증하여 다른 모델을 능가했습니다.

### [VFX Creator: Animated Visual Effect Generation with Controllable Diffusion Transformer](https://arxiv.org/abs/2502.05979)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.05979.png)

Vote: 3

Authors: Yike Guo, Qifeng Liu, Wenhan Luo, Harry Yang, Xinyu Liu, Ailing Zeng, Wei Xue

- ***What's New***: VFX Creator는 제어 가능한 Diffusion Transformer를 이용하여 애니메이션 비주얼 이펙트(VFX)를 생성하는 혁신적인 프레임워크를 제안합니다. 이는 영상 생성에서 공간적 및 시간적 제어 가능성을 제공하여 전통적인 VFX 제작에 비해 효율성과 사용 편의성을 크게 향상시킵니다.
- ***Technical Details***: VFX Creator는 비디오 Diffusion Transformer에 기반한 제어 가능한 VFX 생성 프레임워크로, 최소한의 훈련 데이터로 고품질 비디오 생성이 가능합니다. 공간 제어를 위해, 플러그-앤-플레이 마스크 제어 모듈을 사용해 인스턴스 수준의 공간 조작을 구현하며, 시작-종료 동작 타임스탬프를 토큰화하여 텍스트 인코더와 결합함으로써 시간적 제어를 정밀하게 수행합니다.
- ***Performance Highlights***: VFX Creator는 Open-VFX 시험셋 상에서 실험을 통해 현재 방법을 능가하는 고품질의 시각 효과 생성 능력을 입증했으며, 공간 및 시간 제어에서 최첨단 성능을 발휘했습니다. 또, 사용자 연구에서 다른 방법에 비해 문맥과 잘 맞는, 부드럽고 높은 품질의 비디오를 생성하는 것으로 평가되었습니다.

### [Mathematical Reasoning in Large Language Models: Assessing Logical and Arithmetic Errors across Wide Numerical Ranges](https://arxiv.org/abs/2502.08680)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.08680.png)

Vote: 2

Authors: Safal Shrestha, Keith Ross, Minwu Kim

- ***What's New***: GSM-Ranges는 GSM8K 데이터를 기반으로 만들어진 데이터셋 생성 툴로, 다양한 수치 범위에 걸쳐 모델의 오류를 평가할 수 있도록 설계되었습니다. 이 데이터셋은 문제에 숫자값을 체계적으로 퍼트베이션(perturbation)하여 모델의 견고성을 평가하며, 더욱 정확한 논리적 평가를 제공하는 새로운 채점 방법론을 제안합니다.
- ***Technical Details***: GSM-Ranges는 기존 GSM8K 데이터셋 문제를 여섯 개의 숫자 레벨로 교체하여 문제를 수정합니다. 이 퍼트베이션 방법은 논리적 오류와 비논리적 오류를 구분하며, 모델이 생성한 응답을 Python 코드로 변환하여 올바른 논리적 과정을 포착합니다. 시험에서는 다양한 개방형 및 폐쇄형 LLM 모델을 분석하여, 문제의 수치 복잡성이 커짐에 따라 논리적 오류 비율이 최대 14% 포인트 증가하는 경향을 확인했습니다.
- ***Performance Highlights***: 독립적인 산술 작업에서 높은 정확도를 보여주는 모델들은 문제 내의 산술 계산이 포함될 때 성능이 크게 저하됩니다. GPT-4o 모델은 테스트에서 다른 모델과 비교해 논리 오류가 거의 증가하지 않아 가장 견고한 성능을 보여주었으며, 이 연구는 LLMs의 수치 일반화 및 수학적 추론 성능 향상을 위한 향후 연구 방향성을 제공합니다.

### [3CAD: A Large-Scale Real-World 3C Product Dataset for Unsupervised Anomaly](https://arxiv.org/abs/2502.05761)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.05761.png)

Vote: 1

Authors: Hanyang Sun, Dan Zeng, Zechao Li, Enquan Yang, Wenbo Guo, Peng Xing, Yuanwei Ma

- ***What's New***: 3CAD는 3C 제품 생산 라인으로부터 유도된 실세계 3C 제품의 이상 탐지에 초점을 맞춘 최초의 대규모 데이터셋입니다. 3CAD는 다양한 크기의 이상 영역과 다양한 이상 유형을 포함하며, 단일 이미지에 여러 이상 영역과 유형이 존재할 수 있는 특징을 제공합니다.
- ***Technical Details***: 제안된 프레임워크는 복원 지침을 통한 입체적 탐지 패러다임(CFRG)으로, 전이학습을 통한 멀티스케일 특징 추출과 이방법으로 얻은 이상 영역은 세분화된 로컬라이제이션을 통해 더욱 정확한 이상 탐지를 가능하게 합니다. 특히, 복원 네트워크를 통해 얻은 정상 패턴 정보를 기반으로 이상 패턴을 구별하도록 설계되었습니다.
- ***Performance Highlights***: CFRG 프레임워크는 3CAD 데이터셋에서 93.4%의 P-AUROC, 86.5%의 I-AUROC, 82.0%의 P-PRO, 17.6%의 AP를 달성하며, 이는 기존의 대표적인 이상 탐지 방법들보다 우수한 성능을 보여줍니다. 대부분의 기존 방법들이 평균 90% 이상의 성능을 보이나 우리의 접근법은 3CAD의 복잡성과 다양한 경우에서 뛰어난 탐지 능력을 발휘합니다.

### [DexTrack: Towards Generalizable Neural Tracking Control for Dexterous Manipulation from Human References](https://arxiv.org/abs/2502.09614)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.09614.png)

Vote: 1

Authors: Qianwei Han, Xueyi Liu, Li Yi, Jianibieke Adalibieke, Yuzhe Qin

- ***What's New***: DexTrack는 인간의 참조 동작을 기반으로 한 정교한 조작에 대한 일반화 가능한 신경 추적 제어기(neural tracking controller)를 개발하는 새로운 접근 방식을 소개합니다. DexTrack는 대규모 로봇 추적 시연을 수집하여 신경 제어기를 훈련시키며, 이는 복잡한 환경에서의 제어 성능을 향상시킵니다.
- ***Technical Details***: DexTrack는 인간과 객체 간의 운동학적 상호작용을 분석하여 로봇 손이 이들을 모방하도록 추적 제어기를 개발합니다. Reinforcement Learning(RL)과 Imitation Learning을 결합하여 다양한 움직임을 학습하며, 각 궤적 추적을 위한 최적화 방안을 도입했습니다. 여기에는 경로 추적 단순화를 통한 chain-of-thought과 같은 방법을 활용하여 다양한 시뮬레이션 환경에서도 강력한 추적 능력을 확보합니다.
- ***Performance Highlights***: 제안된 방법은 두 데이터 세트에서 기존의 최신 방법을 초과하였으며, 성공률(Success Rate)에서 10% 이상의 개선을 보였습니다. 뿐만 아니라, 시뮬레이션과 실제 환경 모두에서 복잡한 물체 움직임과 새로운 조작 궤적을 추적하는 데 성공적으로 활용될 수 있음을 입증하였습니다.

