## Daily Papers (2025-02-27)

### [GHOST 2.0: generative high-fidelity one shot transfer of heads](https://arxiv.org/abs/2502.18417)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.18417.png)

Vote: 48

Authors: Denis Dimitrov, Andrey Kuznetsov, Pavel Paramonov, Alexander Groshev, Anastasiia Iashchenko

- ***What's New***: GHOST 2.0는 머리 교체(head swap) 작업에 대해 새로운 접근법을 제시하여 기존의 한계점을 극복합니다. 이 모델은 두 가지 모듈, 즉 Aligner와 Blender를 사용하여 고화질의 머리 합성 및 블렌딩을 수행하여 자연스러운 결과를 제공합니다.
- ***Technical Details***: GHOST 2.0는 Aligner 모듈과 Blender 모듈로 구성됩니다. Aligner는 StyleGAN 기반의 생성기를 사용하여 소스의 정체성을 유지하며 대상의 움직임을 추출하고, Blender는 U-Net을 활용하여 재연된 머리를 대상의 배경에 자연스럽게 융합합니다. 또한, Segformer-B5를 사용한 세분화 모델을 통해 정확한 머리 및 얼굴 부위 구분을 수행합니다.
- ***Performance Highlights***: GHOST 2.0은 기존의 모델들과 비교하여 512x512 및 256x256 해상도에서 대조 실험을 통해 유의미한 참신성을 보여줍니다. LPIPS, SSIM, PSNR 등 다양한 품질 평가 지표에서 높은 성능을 기록하였으며, 특히 대규모 포즈 변형에 대해 강력한 생성을 보였습니다.

### [Kanana: Compute-efficient Bilingual Language Models](https://arxiv.org/abs/2502.18934)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.18934.png)

Vote: 45

Authors: Hyunwoong Ko, Doohae Jung, EungGyun Kim, Daniel Wontae Nam, Hyunho Kim, Shinbok Lee, Byeongil Ko, Hojin Lee, Seungjae Jung, Jaesun Park, Boseop Kim, Eunhwa Kim, Jieun Kang, Changmin Lee, Daniel Lee, Seulye Baeg, Miok Lee, Sunghee Jung, Minho Ryu, Kyoung-Woon On, Kanana LLM Team, Junrae Cho, Nayeon Kim, Gaeun Seo, Taegyeong Eo, Donghun Lee, Minchul Lee, Jiyeon Ham, Yunju Bak

- ***What's New***: Kanana는 한국어와 영어에서 뛰어난 성능을 보여주는 이중언어 모델(bilingual language models) 시리즈로, 기존 동일 크기의 최첨단 모델들보다 계산 비용이 현저히 낮습니다. Kanana는 2.1억 ~ 32.5억 파라미터 모델로 구성되며, 2.1억 모델은 한국어 연구를 촉진하기 위해 공개되었습니다.
- ***Technical Details***: Kanana는 고품질 데이터 필터링, 단계적 사전 훈련(staged pre-training), 깊이 확장(depth up-scaling), 가지치기 및 지식 증류(pruning and distillation)를 이용하여 계산 비용을 절감하면서도 경쟁력을 갖춘 모델을 개발했습니다. 사후 훈련(post-training) 과정에서는 감독된 미세 조정(supervised fine-tuning)과 선호도 최적화가 포함되며, 사용자의 원활한 상호작용을 목표로 합니다.
- ***Performance Highlights***: Kanana Flag 32.5B 모델은 Llama 3.1 70B와 같은 모델들을 능가하는 성능을 보이며, 특히 한국어 평가(KMMLU, HAE-RAE)에서 놀라운 성능을 발휘합니다. 이러한 성능을 도출하는 데 있어 사용된 계산 비용은 Llama 3.1 8B보다 낮으며, Gemma 2 9B와 EXAONE-3.5-7.8B와 비슷합니다.

### [TheoremExplainAgent: Towards Multimodal Explanations for LLM Theorem Understanding](https://arxiv.org/abs/2502.19400)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.19400.png)

Vote: 28

Authors: Max Ku, Thomas Chong, Wenhu Chen, Alvin Yu, Jonathan Leung, Krish Shah

- ***What's New***: TheoremExplainAgent는 큰 언어 모델(LLM)이 멀티모달 방식으로 정리를 이해하고 설명할 수 있는 에이전트 접근 방식을 제시합니다. Manim 애니메이션을 활용하여 긴 형태의 설명 비디오를 생성할 수 있으며, 5가지 자동 평가 지표를 포함한 TheoremExplainBench 벤치마크를 제안합니다.
- ***Technical Details***: TheoremExplainAgent는 Manim을 사용하여 Python 애니메이션 스크립트를 생성하는 코드 에이전트와 스토리 플랜과 내레이션을 생성하는 계획 에이전트로 이루어진 에이전트 파이프라인을 사용합니다. 다양한 STEM 분야의 240개의 정리에 대한 비디오를 생성하고, 5가지의 평가 지표(Accuracy and Depth, Visual Relevance, Logical Flow, Element Layout, Visual Consistency)를 통해 체계적으로 평가합니다. RAG(Retrieval-Augmented Generation)의 접근을 통해 콘텐츠 생성시 관련 문서를 검색하여 코드 생성 능력을 향상시킵니다.
- ***Performance Highlights***: TheoremExplainAgent의 o3-mini 모델은 성공률 93.8%로 가장 높은 퍼포먼스를 보였으며, 특히 어려운 단계의 정리에서도 안정적인 성과를 기록했습니다. GPT-4o 및 Gemini 2.0 Flash 모델은 상대적으로 낮은 성공률을 보였는데, 이는 복잡하고 구조적인 설명 처리에 어려움을 겪기 때문으로 분석됩니다. 평가 지표 중 Accuracy and Depth 및 Logical Flow은 인간 평가자와의 낮은 상관관계를 보였으며, 이는 AI와 인간의 설명 흐름 인식 차이에 기인합니다.

### [Plutus: Benchmarking Large Language Models in Low-Resource Greek Finance](https://arxiv.org/abs/2502.18772)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.18772.png)

Vote: 22

Authors: Jimin Huang, Sophia Ananiadou, Lingfei Qian, Triantafillos Papadopoulos, Efstathia Soufleri, Polydoros Giannouris, Xueqing Peng, Ruoyu Xiang, Yan Wang, Qianqian Xie

- ***What's New***: Plutus는 그리스 금융 분야에서 대형 언어 모델(LLMs)을 첫 평가하며, Plutus-ben 벤치마크 및 Plutus-8B 모델을 도입한 것이 주요 혁신입니다. 이는 그리스 금융 데이터에 대한 특화된 모델을 개발함으로써 기존 모델의 한계를 넘어 새로운 표준을 설정하는 데 기여합니다.
- ***Technical Details***: Plutus-ben은 그리스어로 수치 및 텍스트 기반의 명명된 엔티티 인식(NER), 질문 응답(QA), 추상화 요약 및 주제 분류 등 5가지 핵심 NLP 작업을 포함하여 구성되어 있습니다. 이를 지원하기 위해 세 가지 고품질 그리스 금융 데이터셋이 개발되었으며, 각각은 그리스어와 금융 분야에 정통한 전문가에 의해 철저히 주석 처리되었습니다.
- ***Performance Highlights***: 22개 LLMs에 대한 종합 평가 결과, Plutus-8B는 Plutus-ben 벤치마크에서 최고 성능을 보였으며, 특히 GPT-4 및 기타 기존 모델을 능가하였습니다. Plutus-8B는 그리스 금융 데이터로의 튜닝 덕분에 도메인에 특화된 성능을 발휘하여, 수치 추론 및 엔티티 추출 업무에서 뛰어난 능력을 보였습니다.

### [Language Models' Factuality Depends on the Language of Inquiry](https://arxiv.org/abs/2502.17955)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.17955.png)

Vote: 19

Authors: Hamid Palangi, Kumar Tanmay, Kumar Ayush, Ayush Agrawal, Tushar Aggarwal, Paul Pu Liang

- ***What's New***: 이번 연구에서는 다양한 언어를 사용하는 다국어 언어 모델(Multilingual Language Models; LMs)의 사실적 지식 전달력에서 언어 간 비대칭성을 논의하고, 13개의 언어와 1만 개의 국가 관련 사실로 이루어진 벤치마크를 소개합니다. 특히 언어에 따라 성능이 달라지는 근본적인 약점을 밝혔습니다.
- ***Technical Details***: 벤치마크는 각 국가에 대한 사실적 지식을 802개의 Factual Recall, 156개의 In-context Recall 및 1404개의 Counter-Factual Context Adherence 작업을 통해 평가합니다. 벤치마크는 주로 고자원 언어와 저자원 언어 사이의 지식 전이 능력을 평가하기 위해 설계되었으며, Factual Recall Score (FRS), Knowledge Transferability Score (KTS), Cross-Lingual Factual Knowledge Transferability (X-FaKT) Score라는 세 가지의 새로운 지표를 제안했습니다.
- ***Performance Highlights***: Llama-3-70B 모델은 가장 높은 X-FaKT 점수인 0.848을 기록하며, 사실적 재호출과 지식 전이 모두에서 우수한 성능을 보였습니다. 반면, 모델 크기가 작은 경우나 저자원 언어에서는 높은 오류율을 보이며 성능 차이가 확연히 드러났습니다. 이는 현재의 모델들이 언어 간 사실적 지식 전달에 있어서 상당한 도전 과제를 가지고 있음을 시사합니다.

### [Can Large Language Models Detect Errors in Long Chain-of-Thought Reasoning?](https://arxiv.org/abs/2502.19361)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.19361.png)

Vote: 15

Authors: Yancheng He, Zhongyuan Peng, Bo Zheng, Ge Zhang, Xingyuan Bu, Zhaoxiang Zhang, Weixun Wang, Wenbo Su, Shilong Li, Jiaheng Liu

- ***What's New***: 이 논문에서는 장기 사고의 오류 감지를 위한 새로운 벤치마크인 DeltaBench를 제안합니다. DeltaBench는 다양한 o1-like 모델에 의해 생성된 장기 Chain-of-Thought(CoT) 추론을 분석하고 기존 대형 언어 모델(LLMs)의 비판 능력을 평가하기 위한 최초의 데이터셋입니다.
- ***Technical Details***: DeltaBench는 수학, 프로그래밍, 일반 추론 등 다양한 영역에서 o1-like 모델들이 생성한 장기 CoT를 수집해 각 섹션별로 나눈 후, 각 섹션에 대한 비판 유용성, 정확성, 반영 효율성을 평가했습니다. 이를 통해 프로세스 보상 모델(Process Reward Models; PRMs)와 비판 모델들이 장기 CoT 추론에서 오류를 감지하는 능력을 테스트했습니다.
- ***Performance Highlights***: DeltaBench의 최상위 성능 모델인 GPT-4-turbo-128k는 F1 점수 40.8%에 그쳤고, 이는 현재의 PRM 및 비판 모델들이 장기 CoT 추론에서 오류를 인식하는 능력이 제한적임을 나타냅니다. 또한 o1-like 모델들이 다른 모델 대비 비판 능력에서 특별한 이점을 보이지 않았습니다.

### [Agentic Reward Modeling: Integrating Human Preferences with Verifiable Correctness Signals for Reliable Reward Systems](https://arxiv.org/abs/2502.19328)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.19328.png)

Vote: 14

Authors: Lei Hou, Zijun Yao, Xiaozhi Wang, Hao Peng, Juanzi Li, Yunjia Qi, Bin Xu

- ***What's New***: 이 논문은 인간의 선호도와 검증 가능한 정확성 신호를 통합하여 보다 신뢰할 수 있는 보상 시스템을 제공하는 Agentic Reward Modeling을 소개합니다. 이를 통해 기존 보상 모델이 주로 인간의 주관적 선호에 의존함으로써 발생하는 문제점을 해결하고자 합니다.
- ***Technical Details***: Agentic Reward Modeling은 REWARDAGENT라는 보상 에이전트를 사용하여 인간의 선호도 기반 보상 모델과 두 가지 핵심적인 검증 신호인 사실성(factuality) 및 지시사항 준수(instruction-following)를 통합합니다. REWARDAGENT는 라우터(Router), 검증 에이전트(Verification Agents), 그리고 판단자(Judger)의 세 가지 주요 모듈로 구성되어 있으며, 라우터는 지시에 따라 적절한 검증 에이전트를 선택하고, 검증 에이전트는 응답의 사실성과 지시 준수를 평가한 후 판단자가 최종 보상 점수를 산출합니다.
- ***Performance Highlights***: REWARDAGENT는 RM-Bench 및 JudgeBench에서 기존의 보상 모델보다 높은 성능을 보이며, 특히 여러 제약 조건을 따르는 IFBench에서 탁월한 결과를 보여줍니다. 다양한 NLP 벤치마크에서 DPO 목표를 사용하여 LLM을 훈련한 결과, 기존 보상 모델보다 우수한 성능을 기록하였습니다. 이로써 Agentic Reward Modeling의 신뢰성과 효용성을 입증하였습니다.

### [Can Language Models Falsify? Evaluating Algorithmic Reasoning with Counterexample Creation](https://arxiv.org/abs/2502.19414)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.19414.png)

Vote: 12

Authors: Ponnurangam Kumaraguru, Matthias Bethge, Jonas Geiping, Shashwat Goel, Shiven Sinha, Ameya Prabhu

- ***What's New***: 이 논문에서는 언어 모델(LM)의 알고리즘적 추론 능력을 평가하기 위해 새로운 벤치마크인 REFUTE를 소개합니다. REFUTE는 프로그래밍 대회에서 나온 잘못된 제출물에 대한 반례(counterexample)를 생성하여 LMs의 반증 능력을 테스트하는 최초의 벤치마크입니다.
- ***Technical Details***: REFUTE 벤치마크는 Codeforces와 같은 프로그래밍 대회에서 가져온 문제들로 구성되어 있습니다. 각 샘플에는 문제 설명과 잘못된 솔루션이 포함되어 있으며, LMs는 입력 조건을 만족하면서 솔루션이 실패하는 입력을 생성해야 합니다. 반례는 코드 실행을 통해 자동으로 검증됩니다. REFUTE 데이터셋은 다양한 알고리즘 주제의 샘플 324개를 포함하고 있으며, 지속적으로 업데이트되어 간접적인 데이터 유출을 방지합니다.
- ***Performance Highlights***: 현재 최고의 모델 OpenAI의 o3-mini(high)나 DeepSeek R1은 REFUTE의 잘못된 솔루션 중 < 9%에 대해서만 반례를 생성할 수 있었습니다. 이 실험 결과는 LMs가 여전히 자체적으로 잘못된 코드를 수정하는 데 제한이 있음을 보여줍니다.

### [Towards an AI co-scientist](https://arxiv.org/abs/2502.18864)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.18864.png)

Vote: 12

Authors: Amin Vahdat, Alan Karthikesalingam, Dan Popovici, Vivek Natarajan, Byron Lee, Vikram Dhillon, Anil Palepu, Nenad Tomasev, Petar Sirkovic, Avinatan Hassidim, Alexander Daryin, Tao Tu, Yossi Matias, Yunhan Xu, Burak Gokturk, Yuan Guan, Katherine Chou, Andrew Carroll, Felix Weissenberger, Gary Peltz, Juraj Gottweis, Artiom Myaskovsky, Keran Rong, Jacob Blum, Fan Zhang, Kavita Kulkarni, Khaled Saab, Wei-Hung Weng, José R Penadés, Eeshit Dhaval Vaishnav, Annalisa Pawlosky, Ryutaro Tanno, Tiago R D Costa, Pushmeet Kohli

- ***What's New***: AI 공동과학자(AI co-scientist)는 다중 에이전트 시스템으로서 과학자들과 협력하여 새로운, 독창적인 연구 가설과 제안을 생성하는 AI 시스템입니다. 이 시스템은 연구자들이 설정한 목표와 가이던스를 기반으로 새로운 지식을 발굴하도록 설계되었습니다. 실험적인 평가를 통해 AI 공동과학자는 생물의약 분야에서 신약 재용도(drug repurposing), 새로운 표적 탐색(novel target discovery), 항생제 저항성(antimicrobial resistance)의 메커니즘을 설명하는 데 성공적인 가설들을 제안했습니다.
- ***Technical Details***: AI 공동과학자는 Gemini 2.0을 기반으로 한 다중 에이전트 아키텍처와 비동기 작업 실행 프레임워크를 사용하여 유연한 컴퓨팅 확장을 지원합니다. 하이포시스 생성은 '생성, 토론 및 발전' 접근 방식으로 이루어지며, 과학적 방법론을 반영하여 설계되었습니다. 시스템은 생성 에이전트, 반성(reflection) 에이전트, 랭킹 에이전트, 근접성(proximity) 에이전트, 진화(evolution) 에이전트, 메타 리뷰(meta-review) 에이전트 등 여러 특화 에이전트로 구성되어 있으며, 각 에이전트는 특정 역할을 수행하여 하이포시스를 생성하고 발전시킵니다.
- ***Performance Highlights***: AI 공동과학자는 신약 재용도에서 급성 골수성 백혈병(AML)을 위한 새로운 약물 후보를 인비트로(in vitro) 실험을 통해 검증했으며, 간섬유화(liver fibrosis) 치료를 위한 새로운 에피제네틱 표적을 제안하여 실험적 검증을 받았습니다. 또한 박테리아의 항생제 저항성 메커니즘을 설명하는 새로운 유전자 이동 메커니즘을 유추하는 데 성공했습니다. 이러한 결과는 AI 공동과학자가 생물의약 분야에서 잠재적으로 충족할 수 있는 도전 과제를 해결하는 데 기여할 수 있음을 보여줍니다.

### [Project Alexandria: Towards Freeing Scientific Knowledge from Copyright Burdens via LLMs](https://arxiv.org/abs/2502.19413)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.19413.png)

Vote: 10

Authors: Tawsif Ahmed, Ludwig Schmidt, Sören Auer, Jenia Jitsev, Matthias Bethge, Gollam Rabby, Andreas Hochlehnert, Christoph Schuhmann, Huu Nguyen, Robert Kaczmarczyk, Nick Akinci Heidrich, Ameya Prabhu

- ***What's New***: 프로젝트 알렉산드리아는 대규모 언어 모델(LLMs)을 활용하여 과학적 지식을 저작권의 장애물에서 해방시키는 혁신적인 접근법을 제시합니다. 이 연구는 과학적 텍스트를 구조화된 데이터 형식인 지식 단위(Knowledge Units)로 변환하여, 저작권 문제 없이 지식을 공유할 수 있는 방안을 탐구합니다.
- ***Technical Details***: 지식 단위(Knowledge Units)는 논문의 텍스트를 엔티티, 속성, 관계로 나누어 구조화된 형태로 변환합니다. 여기에는 정보 보존을 위해 문장에서 스타일적 요소를 배제하고, 사실적 내용을 중심으로 데이터를 기록하는 과정이 포함됩니다. 또한, 정보를 효율적으로 추출하기 위해 대규모 언어 모델을 사용하여 문서의 구조를 분석하고, 필요한 정보를 정리합니다.
- ***Performance Highlights***: 지식 단위를 활용한 경우, 원본 텍스트와 비교하여 정보 보존율이 약 95%로 나타났습니다. 이는 생물학, 수학, 물리학, 컴퓨터 과학 등 다양한 연구 영역에서 다중 선택 질문(MCQ) 실험을 통해 평가된 결과입니다. 이러한 결과는 지식 단위가 본질적인 정보를 효과적으로 보존할 수 있음을 시사합니다.

### [Rank1: Test-Time Compute for Reranking in Information Retrieval](https://arxiv.org/abs/2502.18418)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.18418.png)

Vote: 10

Authors: Kathryn Ricci, Benjamin Van Durme, Eugene Yang, Dawn Lawrie, Andrew Yates, Orion Weller

- ***What's New***: RANK1은 정보 검색(Information Retrieval)에서 재랭킹(Reranking)을 위해 테스트 시간의 계산 자원을 사용하는 최초의 모델입니다. 이 모델은 OpenAI의 o1과 Deepseek의 R1 등의 추론 언어 모델(Reasoning Language Models)을 활용하여 작은 모델의 성능을 빠르게 향상시킵니다. 60만 개가 넘는 R1의 추론 데이터세트를 공개하여, 이를 바탕으로 학습한 모델들이 최첨단의 성능을 보이며, 사용자 입력에 유연하게 반응할 수 있는 능력을 갖추고 있음을 보여줍니다.
- ***Technical Details***: RANK1은 MS MARCO에서 수집된 R1의 추론 과정 데이터를 기반으로 모델을 미세 조정하여 강력한 추론 역량을 갖춘 모델을 개발했습니다. 이 모델은 높은 성능 유지를 위해 양자화(Quantization)를 통해 메모리 사용량과 계산 자원을 감소시켰습니다. R1의 추론 체인을 사용하여 학습한 모델은 높은 설명 가능성을 제공하며 다양한 정보 검색 벤치마크에서 우수한 성능을 발휘합니다.
- ***Performance Highlights***: BRIGHT와 같은 복잡한 추론 벤치마크에서 RANK1 모델은 nDCG@10 지표에서 기존 모델보다 두 배 가까운 성능 향상을 보여줍니다. 특히 32B 모델은 작은 데이터셋(예: 600k)으로도 32B 파라미터 모델처럼 우수한 성능을 기록합니다. mFollowIR 데이터셋에서는 멀티언어 환경에서도 높은 점수를 기록하며, 기존의 멀티언어 모델을 능가하는 성능을 발휘했습니다.

### [VEM: Environment-Free Exploration for Training GUI Agent with Value Environment Model](https://arxiv.org/abs/2502.18906)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.18906.png)

Vote: 7

Authors: Chaoyun Zhang, Jiani Zheng, Qingwei Lin, Fangkai Yang, Saravan Rajmohan, Qi Zhang, Wenjie Yin, Lingrui Mei, Lu Wang, Dongmei Zhang

- ***What's New***: VEM (Value Environment Model)는 GUI 에이전트를 훈련하는 환경 없는 환경 강화 학습 프레임워크입니다. 이 모델은 기존의 환경 기반 모델과 달리 미래 상태 예측이나 환경 피드백 없이도 행동의 결과를 예측할 수 있는 VEM을 활용하여 인간과 유사한 GUI 상호작용 결과를 의미적으로 추론합니다. 이는 UI가 변경되는 상황에서도 복원력이 뛰어나며, 환경 상호작용 비용 없이 환경 기반 접근법과 유사한 성능을 달성할 수 있습니다.
- ***Technical Details***: VEM 프레임워크는 두 가지 단계로 운영됩니다. 첫째, VEM을 사전 학습하여 다양한 GUI 문맥에서 행동의 장기적인 유틸리티를 추정합니다. 둘째, VEM 신호를 이용하여 정책 탐색을 안내함으로써 레이아웃 비의존적인 GUI 자동화를 가능하게 합니다. VEM은 오프라인 데이터를 기반으로 상태-행동 값(Q(s, a))을 직접 예측하며, 이는 온라인 상호작용 없이 인간과 유사한 행동 결과를 학습합니다.
- ***Performance Highlights***: VEM은 Android-in-the-Wild 벤치마크에서 환경 없는 베이스라인을 크게 능가하고 환경 기반 접근법과 상호작용 비용 없이 성능을 맞추며, 오프라인 및 온라인 설정 모두에서 최첨단 성능을 달성했습니다. 특히 오프라인 설정에서 General/Webshopping 도메인에서 28.0%/21.0%의 태스크 성공율을 기록하며, 기존의 환경 없는 기법보다 12-28% 높은 성과를 보였고, 온라인 설정에서는 42.4%의 일반 태스크 성공율을 달성해 절차 효율성 면에서 환경 기반 정책과 유사한 성과를 보였습니다.

### [Distill Any Depth: Distillation Creates a Stronger Monocular Depth Estimator](https://arxiv.org/abs/2502.19204)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.19204.png)

Vote: 6

Authors: Hongji Li, Ruibo Li, Xiankang He, Chi Zhang, Dongyan Guo, Ying Cui

- ***What's New***: 이 논문에서는 모노큘러 심도 추정(Monocular Depth Estimation)을 강화하는 새로운 증류(Distillation) 방법을 소개합니다. 기존의 전역 정규화(Global Normalization) 방식의 한계를 극복하고, 크로스 컨텍스트 증류(Cross-Context Distillation)와 다중 교사 프레임워크(Multi-Teacher Framework)를 도입하여 다양한 장면에서 더욱 정확한 심도 추정을 가능하게 합니다.
- ***Technical Details***: 제안된 크로스 컨텍스트 증류는 지역(Local)과 글로벌(Global) 심도 정보를 통합하여 교사 모델(Teacher Model)로부터 효과적으로 지식을 증류합니다. 또한, 다양한 심도 추정 모델의 다양한 장점을 활용하는 다중 교사 증류 프레임워크를 제안하여, 교사 모델들이 생성한 보완적인 가짜 라벨(Pseudo-Labels)을 학생 모델(Student Model)에 학습시킵니다.
- ***Performance Highlights***: 제안된 방법은 도쿄(ETH3D), 다이오드(DIODE), 스캔넷(ScanNet), 키티(KITTI), 뉴욕(2v) 등의 벤치마크 데이터셋에서 계량 및 정량적 관점 모두에서 기존 최첨단 방법들보다 탁월한 성능을 발휘하며, 심도 정확도 및 세부 묘사 능력에서 현저한 향상을 보였습니다.

### [AISafetyLab: A Comprehensive Framework for AI Safety Evaluation and Improvement](https://arxiv.org/abs/2502.16776)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.16776.png)

Vote: 4

Authors: Chengwei Pan, Hao Wang, Zhexin Zhang, Qinglin Zhang, Xianqi Lei, Lei Sha, Hongning Wang, Renmiao Chen, Hao Li, Minlie Huang, Yida Lu, Shiyao Cui, Xinyuan Wang, Leqi Lei, Xijie Huang, Junxiao Yang

- ***What's New***: AISafetyLab는 AI 안전 평가 및 개선을 위해 통합된 프레임워크와 도구 모음을 소개합니다. 이는 대표적인 공격, 방어 및 평가 방법론을 통합하여 AI 안전성을 강화하는 데 중점을 두고 있으며, 특히 Vicuna 모델에 대한 실험을 통해 각기 다른 공격 및 방어 전략의 효과성을 분석합니다.
- ***Technical Details***: AISafetyLab는 공격(Attack), 방어(Defense), 평가(Evaluation) 모듈로 구성되어 있으며, 각 모듈은 모델의 안전성을 평가하거나 강화하기 위한 다양한 기법을 제공합니다. 공격 모듈은 화이트박스 및 블랙박스 기법을 포함한 13가지 대표적 기법을 구현하고 있으며, 방어 모듈은 3가지 학습 기반 방어 전략과 13가지 추론 단계 방어 메커니즘을 지원합니다. 평가 모듈은 2가지 규칙 기반 스코어러와 5가지 모델 기반 스코어러를 통합하여 모델 응답의 안전성 평가를 수행합니다.
- ***Performance Highlights***: Vicuna 모델을 대상으로 한 실험 결과 AutoDAN 공격 방법이 높은 효과를 보였으며, 방어 전략 중에서는 Prompt Guard와 Safe Unlearning이 가장 효과적이었습니다. 이러한 실험은 다양한 방어 전략 하에서 공격 방법의 성능을 강조하며, 방어 메커니즘의 과도한 거부율 문제를 지적하였습니다.

### [CritiQ: Mining Data Quality Criteria from Human Preferences](https://arxiv.org/abs/2502.19279)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.19279.png)

Vote: 4

Authors: Zhiheng Xi, Demin Song, Kai Chen, Qiuyinzhe Zhang, Kai Lv, Xipeng Qiu, Yu Sun, Qipeng Guo, Tianyi Liang, Tao Gui, Honglin Guo

- ***What's New***: CritiQ는 인간의 선호도를 활용하여 데이터 품질 기준을 자동으로 발굴하고, 효율적인 데이터 선택을 수행하는 새로운 방법을 제시합니다. 기존에 수작업으로 설정해왔던 데이터 품질 기준을 단지 약 30개 정도의 인간 주석(pairwise annotations)을 통해 LLM 기반 에이전트를 중심으로 프로그램틱하게 추출합니다.
- ***Technical Details***: CritiQ Flow는 관리 에이전트(manager agent)와 작업자 에이전트(worker agent)를 활용하여 품질 기준(criteria)을 발전시키는 동시에 작업자 에이전트가 쌍(pair)에 대해 비교 판단을 합니다. 초기 품질 기준은 기존 연구에서 발췌한 지식 기반(knowledge base)에서 시작합니다. 최종적으로, CritiQ Scorer라는 경량화된 Bradley- Terry 모델을 훈련하여 대규모 데이터 선택을 효율적으로 수행합니다.
- ***Performance Highlights***: CritiQ Flow는 코드, 수학, 논리 도메인에서 실험을 통해 테스트 세트에서 높은 정확성을 기록하여 인간 선호도를 효과적으로 포착함을 보여줍니다. CritiQ Scorer로 평가된 고품질 하위 집합을 사용하여 모델을 지속적으로 훈련한 결과, 균일한 표본보다 다운스트림 작업에서 더 높은 성능 향상을 확인하였습니다.

### [BIG-Bench Extra Hard](https://arxiv.org/abs/2502.19187)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.19187.png)

Vote: 4

Authors: Disha Jindal, Sanket Vaibhav Mehta, Mehran Kazemi, Silvia Chiappa, Uri Shalit, Bahare Fatemi, Virginia Aglietti, Peter Chen, Chrysovalantis Anastasiou, Nishanth Dikkala, Vinh Q. Tran, Quoc V. Le, Yi Tay, John Palowitch, Lalit K. Jain, Orhan Firat, Xin Liu, Gladys Tyen, Kate Olszewska, Hritik Bansal

- ***What's New***: BIG-Bench Extra Hard (BBEH)는 기존의 BIG-Bench Hard (BBH)보다 훨씬 높은 난이도를 가진 새로운 벤치마크입니다. 각 BBH의 기존 작업을 유사한 추론 능력을 시험하지만 난이도를 크게 높인 새로운 작업으로 대체하여 LLM의 일반적인 추론 역량을 보다 정확하게 평가합니다.
- ***Technical Details***: BBEH는 BBH의 23개 작업을 새롭게 구성하고 난이도를 대폭 증가시킨 23개의 작업으로 구성됩니다. 이러한 작업은 개념적 비약, 즉석 학습, 긴 문맥 입력의 처리, 강한 선입견을 넘어서기, 방해 요소 관리, 예제로부터 패턴 유도, 다단계 추론 등의 다양한 추론 능력을 요하는 작업들입니다.
- ***Performance Highlights***: BBEH의 일반 목적 모델 중 최고 성능은 9.8%의 조화 평균 정확도를 기록하였으며, 추론에 특화된 모델의 최고 성능은 44.8%를 기록했습니다. 이는 여전히 엄청난 개선 여지를 가지고 있음을 보여주며, LLM의 강력한 일반 추론 성과 달성을 위한 지속적인 도전 과제를 강조합니다.

### [FSPO: Few-Shot Preference Optimization of Synthetic Preference Data in LLMs Elicits Effective Personalization to Real Users](https://arxiv.org/abs/2502.19312)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.19312.png)

Vote: 3

Authors: Anikait Singh, Tatsunori Hashimoto, Kyle Hsu, Archit Sharma, Chelsea Finn, Eric Mitchell, Sheryl Hsu, Stefano Ermon

- ***What's New***: FSPO는 LLMs에서 염두에 둔 자동화된 환경을 활용한 메타러닝으로 사용자의 개인화된 보상 함수를 빠르게 학습할 수 있는 새로운 프레임웍입니다. 또한 FSPO는 실제 사용자에게 효과적인 개인화를 이끌어내는 몇 안 되는 LLM 메타러닝 접근법 중 하나로, 기계 학습에서 강조되는 다양성과 구조를 반영한 인공 선호도 데이터를 활용하는 방법을 제시합니다.
- ***Technical Details***: FSPO는 사용자로부터의 극소 수의 선호도 데이터만을 이용하여 개인화된 보상 함수를 학습하는 메타러닝 설계를 사용하여 선호도 최적화를 다루며, 다양한 사용자 간의 개인화를 가능하게 만듭니다. FSPO의 특징은 학습 후에도 새로운 사용자에게 빠르게 적응할 수 있으며, 특히 IPO(Implicit Preference Optimization)와 같은 선호도 학습 알고리즘을 사용하여 사용자별 최적화된 보상 모델을 구축합니다.
- ***Performance Highlights***: FSPO는 다양한 도메인에서 87%의 Alpaca Eval 승률을 기록하며, COT(User Description Chain-of-Thought)을 활용한 방식은 오라클 방법과의 성능 격차를 좁힐 수 있음을 보여 주었습니다. 또한 실제 사용자 대상의 예비 인간 연구에서는 FSPO가 개인화된 응답 생성에 있어 다른 모델 대비 72%의 승률을 보였습니다.

### [MolSpectra: Pre-training 3D Molecular Representation with Multi-modal Energy Spectra](https://arxiv.org/abs/2502.16284)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.16284.png)

Vote: 2

Authors: Liang Wang, Deli Zhao, Shu Wu, Yu Rong, Qiang Liu, Shaozhen Liu

- ***What's New***: MolSpectra는 양자 역학적 관점을 활용하여 분자 시스템의 3D 구조와 에너지 상태 간의 관계를 개선하는 분자 스펙트럼을 3D 분자 표현 사전 학습에 통합한 혁신적인 방법론을 제안합니다. 기존 방법들이 고전 역학에 기반을 두는 것과 달리, MolSpectra는 양자화된 에너지 레벨 구조의 지식을 분자 표현에 주입합니다.
- ***Technical Details***: SpecFormer라는 멀티 스펙트럼 인코더를 도입하여 마스크 패치 재구성 목표(Masked Patch Reconstruction Objective)를 통해 분자 스펙트럼을 인코딩합니다. 그리고 대조적 목표(Contrastive Objective)를 사용하여 3D 인코더와 스펙트럼 인코더의 출력을 정렬하여, 3D 인코더의 분자 이해를 향상시킵니다. 대규모 내외부 데이터셋에서의 유효성을 테스트하였고, 제안한 사전 학습된 표현이 기존 방법들보다 분자의 특정 예측 및 동적 모델링에서 뛰어남을 보여주었습니다.
- ***Performance Highlights***: QM9 데이터셋에서 12개 속성 중 8개 속성에 대해 최첨단 성능을 달성했으며, 10개 속성에서 기존의 Coord 방법보다 성능이 우수했습니다. MD17 데이터셋에서도, 에너지 레벨 전이 패턴을 이해하여 동적 분자 시스템의 진화를 더 잘 포착함으로써 탁월한 성능을 나타냈습니다.

### [Towards Optimal Multi-draft Speculative Decoding](https://arxiv.org/abs/2502.18779)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.18779.png)

Vote: 2

Authors: Tong Zheng, Heng Huang, Ryan A. Rossi, Yihan Wu, Zhengmian Hu, Dinesh Manocha, Ziyi Chen, Vignesh Viswanathan

- ***What's New***: 이 논문에서는 MDSD(Multi-Draft Speculative Decoding)의 이론적 상한을 측정하고, 다양한 초안 샘플링(Draft Sampling) 방법의 최적 수용률(Optimal Acceptance Rate)을 비교하여 현대 대형 언어 모델(LLM)의 성능을 평가하고자 합니다. 특히 초안 샘플링 방법이 최적 수용률에 미치는 강한 영향을 발견하였으며, 대체하지 않고 샘플링(sampling without replacement)하는 것이 대체하여 샘플링(sampling with replacement)하는 것보다 성능이 우수함을 보였습니다.
- ***Technical Details***: 고정된 초안 샘플링 방법에 대해 최적 수용률은 최적 운송 문제의 해로 이루어져 있지만, 문제의 복잡성 때문에 이를 해결하기 어렵습니다. 이를 효율적으로 계산하기 위해 이 논문은 문제의 이중 문제를 논의하고 완전 유니모듈러리티(Total Unimodularity)를 적용하여 클 수용률을 최초로 효율적으로 계산할 수 있는 방법을 제공하였습니다.
- ***Performance Highlights***: 실험 결과, 기존 검증 알고리즘은 이론적 상한에 도달하지 못하는 상당한 갭을 나타냈습니다. 제안된 탐욕적 샘플링 방법은 일부 경우에서 대체 없이 샘플링하는 것보다 더 높은 최적 수용률을 달성할 수 있으며, 제안된 검증 알고리즘은 이론적 상한과 일치합니다. 이러한 결과는 미래 연구에 대한 방향성을 제시하며, LLM의 성능 향상에 기여할 수 있습니다.

### [Drop-Upcycling: Training Sparse Mixture of Experts with Partial Re-initialization](https://arxiv.org/abs/2502.19261)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.19261.png)

Vote: 1

Authors: Jun Suzuki, Kazuki Fujii, Rio Yokota, Yusuke Oda, Taishi Nakamura, Takuya Akiba

- ***What's New***: Drop-Upcycling은 새로운 Mixture of Experts (MoE) 모델을 구축하기 위한 방법으로, 사전 훈련된 밀집(dense) 모델의 지식을 활용하여 전문화(expert specialization)를 촉진합니다. 이 접근법은 기존의 네트워크를 더욱 효율적으로 학습할 수 있도록 하여 장기적인 학습 시 성능을 향상시킵니다.
- ***Technical Details***: Drop-Upcycling은 사전 훈련된 밀집 모델의 가중치를 복제하여 MoE 모델을 초기화하고, 게이트(Gate) 및 FFN(FeedForward Network)의 일부 파라미터를 선택적으로 다시 초기화하여 다양한 전문가의 전문화를 촉진합니다. 이 방법은 기존의 파라미터를 부분적으로 보존하면서 FFN의 중간 차원을 따라 행 또는 열 기반으로 가중치를 드롭(dropped) 및 다시 초기화합니다.
- ***Performance Highlights***: Drop-Upcycling은 5.9B 활성 파라미터의 MoE 모델에서 13B의 밀집 모델과 성능이 동등하며, 훈련 플롭스(FLOPs)의 약 1/4만을 사용합니다. 이전 MoE 모델 구축 방법보다 학습 속도가 더 빠르며, 8×3.7B 설정에서도 효과적임을 보여주었습니다.

### [Scaling LLM Pre-training with Vocabulary Curriculum](https://arxiv.org/abs/2502.17910)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.17910.png)

Vote: 1

Authors: Fangyuan Yu

- ***What's New***: 이 논문에서는 고정된 단어장(static vocabularies)을 넘어서 인간 언어 학습과 유사하게 언어 모델(LLM)의 어휘 획득을 동적으로 향상시키는 '어휘 커리큘럼 학습(vocabulary curriculum learning)'을 소개합니다. 이 접근법은 모델이 다양한 토크나이제이션(tokenization) 세분성에서 전이 가능(representations)한 표현을 학습할 수 있도록 하며, 사전 학습(pre-training) 효율성을 향상시킵니다.
- ***Technical Details***: 이 연구는 엔트로피(entropy)를 가이드로 한 어휘 확장과 모델 최적화를 번갈아 진행하여, 예측 가능한 토큰은 긴 단어로, 예측이 어려운 경우 더 짧은 형태로 모델이 포커스를 맞출 수 있도록 합니다. 실험에서는 작은 규모의 GPT 모델에 대해 enwiki8 데이터셋을 사용하였으며, 고정된 단어 사전을 사용하는 전통적인 방법보다 일관되게 낮은 bits-per-character(BPC)를 달성했습니다.
- ***Performance Highlights***: 점진적으로 증가하는 어휘 커리큘럼 학습을 통해 전통적인 컴퓨팅-매칭(compute-matching) 학습과 비교했을 때 BPC 곡선의 경사가 더 가파른 개선을 보였습니다. 특히, 새로 생성된 긴 토큰은 낮은 BPC를 일관되게 성취하였으며, 이는 엔트로피-인지적인 토큰 추가 접근법의 유효성을 뒷받침합니다.

### [PosterSum: A Multimodal Benchmark for Scientific Poster Summarization](https://arxiv.org/abs/2502.17540)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.17540.png)

Vote: 1

Authors: Rohit Saxena, Pasquale Minervini, Frank Keller

- ***What's New***: PosterSum 벤치마크는 과학 포스터를 연구 논문의 요약으로 변환하는 멀티모달 모델 개발을 촉진하기 위해 설계되었습니다. 포스터Sum은 16,305개의 학회 포스터와 그에 대응하는 요약문으로 구성되어 있으며, 복잡한 레이아웃, 텍스트 영역, 표 및 그림을 포함한 다양한 시각 이해 과제를 제시합니다.
- ***Technical Details***: PosterSum은 대형 학회(개인 예: ICLR, ICML, NeurIPS)에서 수집된 과학 포스터와 요약문 한 쌍으로 구성됩니다. 이 데이터셋은 SEGMENT & SUMMARIZE라는 계층적 방법을 제안했으며, 포스터를 일관된 영역으로 나누고, 각 영역 내에서 텍스트를 추출하여 요약을 생성하며, 이를 결합하여 포스터 전체의 종합 요약을 생성합니다. 모델은 추가적인 학습이나 미세 조정 없이 이 방법을 통해 우수한 성능을 발휘할 수 있습니다.
- ***Performance Highlights***: 현재 대형 멀티모달 모델들은 과학 포스터를 요약하는 데 한계가 있습니다. 예를 들어, GPT-4o 모델은 ROUGE-L 점수 22.30을 기록하며, SEGMENT & SUMMARIZE 방식은 ROUGE-L 점수 24.18로 성능을 개선했습니다. 이는 섬세한 세부 사항을 보존하고 포스터의 복잡성을 효과적으로 처리하는 데 기여합니다.

### [DOEI: Dual Optimization of Embedding Information for Attention-Enhanced Class Activation Maps](https://arxiv.org/abs/2502.15885)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.15885.png)

Vote: 0

Authors: Shimin Wen, Daji Ergu, Xu Wang, Yu Bai, Ying Cai, Hongjie Zhu, Guansong Pang, Zeyu Zhang, Yang Zhao

- ***What's New***: DOEI 기술은 Embedding Information을 이중으로 최적화하는 새로운 접근 방식을 제안하여 Attention-Enhanced Class Activation Maps의 성능을 향상시킵니다. 이 기법은 시각적 변환기 기반의 WSSS(Weakly Supervised Semantic Segmentation) 모델들에 잘 통합되어 PASCAL VOC 및 MS COCO와 같은 인기 있는 벤치마크에서 성능을 크게 개선하는 데 기여합니다.
- ***Technical Details***: DOEI는 이상적인 embedding 표현을 얻기 위해 semantic-aware attention weight matrices를 활용하여 큰 신뢰도의 토큰을 증폭하고 낮은 신뢰도의 토큰을 억제합니다. 또한 RGB 값, embedding 가이드 기능, self-attention weights를 통합하는 hybrid-feature alignment module을 제안하여 candidate tokens의 신뢰성을 높입니다.
- ***Performance Highlights***: DOEI 적용 결과 PASCAL VOC 데이터셋에서 mIoU 측정치가 최대 3.6% 향상되었습니다. MS COCO 데이터셋에서도 baseline 모델 대비 1.6% 향상된 성능을 보였습니다.

