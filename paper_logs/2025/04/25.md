## Daily Papers (2025-04-25)

### [Step1X-Edit: A Practical Framework for General Image Editing](https://arxiv.org/abs/2504.17761)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.17761.png)

Vote: 55

Authors: Jingwei Wu, Zheng Ge, Chunrui Han, Shiyu Liu, Yucheng Han, Daxin Jiang, Yibo Zhu, Lei Xia, Ranchen Ming, Xiangyu Zhang, Wei Cheng, Yingming Wang, Quan Sun, Xianfang Zeng, Jiaqi Liao, Honghao Fu, Fukun Yin, Binxing Jiao, Guopeng Li, Gang Yu, Peng Xing, Rui Wang, Yan Cai, Yuang Peng

- ***What's New***: Step1X-Edit는 대형 멀티모달 모델(Multimodal Large Language Model)과 확산 이미지 디코더(Diffusion Image Decoder)를 결합하여 클로즈드 소스 모델과 유사한 성능을 제공하는 최첨단 오픈 소스 이미지 편집 모델입니다.
- ***Technical Details***: Step1X-Edit는 대규모 데이터셋을 구축하기 위해 20만 이상의 고품질 인스트럭션-이미지 쌍을 생성했습니다. MLLM은 사용자의 편집 지시를 처리하고, 수집된 뉴럴 네트워크 임베딩을 기반으로 확산 모델을 통해 목표 이미지를 생성합니다. 이 모든 과정은 편집 플로우에 실제 환경의 요구사항을 반영하여 사용하는 GEdit-Bench에서 평가됩니다.
- ***Performance Highlights***: Step1X-Edit는 GEdit-Bench에서 기존 오픈 소스 모델보다 우월한 성능을 보여주며, 클로즈드 소스 모델인 GPT-4o와 Gemini2 Flash와 비견할 만한 레벨에 도달했습니다. 특히 스타일 변환 및 색상 변경에서 뛰어난 성능을 입증했습니다.

### [Paper2Code: Automating Code Generation from Scientific Papers in Machine Learning](https://arxiv.org/abs/2504.17192)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.17192.png)

Vote: 47

Authors: Minju Seo, Jinheon Baek, Seongyun Lee, Sung Ju Hwang

- ***What's New***: PaperCoder는 머신러닝 논문을 기능적인 코드 레포지토리로 변환하는 멀티 에이전트 LLM 프레임워크로, 세 단계인 기획(Planning), 분석(Analysis), 생성(Generation) 과정을 통해 논문의 코드를 자동으로 생성합니다. 이는 기존 연구들이 부분 코드 구현에 의존했던 것에 비해, PaperCoder는 논문에서만 출발하여 완전한 구현을 목표로 합니다.
- ***Technical Details***: PaperCoder는 세 단계로 나뉘어 작업을 진행합니다: 첫 번째, 기획 단계에서는 시스템 아키텍처와 다이어그램을 설정하고, 파일 종속성을 확인하여 고수준의 로드맵을 작성합니다. 두 번째, 분석 단계에서는 구체적인 구현 세부사항을 해석하고, 세 번째, 생성 단계에서는 모듈화된 종속성 인식 코드가 생성됩니다. 이 모든 단계는 다양한 전문 에이전트들이 파이프라인을 통해 효과적으로 협업하여 진행됩니다.
- ***Performance Highlights***: PaperCoder는 최근에 출시된 PaperBench 벤치마크에서 기존 강력한 기준선들을 크게 능가했으며, 생성된 코드 레포지토리의 77%가 인간 평가에서 최고 등급으로 평가되었습니다. 평가에 참가한 인간 심판의 85%가 이 레포지토리가 실제로 유용하다고 보고했습니다. 이러한 결과는 PaperCoder가 고품질의 신뢰할 수 있는 구현을 생성하는 데 효과적임을 보여줍니다.

### [RefVNLI: Towards Scalable Evaluation of Subject-driven Text-to-image Generation](https://arxiv.org/abs/2504.17502)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.17502.png)

Vote: 47

Authors: Aviv Slobodkin, Nitzan Bitton Guetta, Michal Sokolik, Almog Gueta, Royi Rassin, Idan Szpektor, Itay Laish, Hagai Taitelbaum, Brian Gordon, Yonatan Bitton, Dani Lischinski

- ***What's New***: REFVNLI는 주어진 텍스트 설명과 참조 이미지에 기반한 이미지 생성의 텍스트 정렬(Textual Alignment) 및 주제 일관성(Subject Consistency)을 평가하는 비용 효율적인 새 메트릭을 소개합니다. 이는 비디오 추론 벤치마크와 이미지 변형으로부터 생성된 대규모 데이터셋에 학습되어 여러 벤치마크와 주제 범주에서 기존 기준보다 우수하거나 동등한 성능을 보입니다.
- ***Technical Details***: REFVNLI는 <이미지 참조, 프롬프트, 이미지 타겟>의 삼중항을 입력으로 받아 한 번의 예측으로 텍스트 정렬 및 주제 일관성을 평가합니다. 주제 보존을 위해 비디오 프레임을 기반으로 동일 주제를 포함하는 긍정 예제와 다른 주제를 포함하는 부정 예제를 만듭니다. 텍스트 정렬을 위해 원본 프롬프트에서 작은 세부 사항을 변경하여 부정 예제를 생성합니다. 총 120만 개 이상의 인스턴스를 활용하여 모델을 학습합니다.
- ***Performance Highlights***: REFVNLI는 DreamBench++, ImagenHub, KITTEN 등의 데이터셋에서 텍스트 정렬과 주제 일관성 모두에서 우수한 성능을 보입니다. 특히 적은 알려진 개체를 포함한 범주에서도 인간 선호도와 87% 이상의 일치도를 보입니다. 이는 현존하는 많은 GPT-4o 기반와 같은 대형 모델들보다도 뛰어난 성능을 제공합니다.

### [Breaking the Modality Barrier: Universal Embedding Learning with Multimodal LLMs](https://arxiv.org/abs/2504.17432)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.17432.png)

Vote: 28

Authors: Yanzhao Zhang, Kaicheng Yang, Ziyong Feng, Jiankang Deng, Tiancheng Gu, Xingjun Wang, Yingda Chen, Weidong Cai, Dingkun Long

- ***What's New***: UniME는 Multimodal LLMs(Large Language Models)을 활용하여 여러 다운스트림 작업에 대해 보편적인 표현 학습을 가능하게 하는 새로운 두 단계 프레임워크입니다. 대규모 언어 모델 기반의 교사 모델로부터 언어 표현력을 향상시키고, 하드 네거티브 샘플링을 사용하여 더욱 구별력 있는 표현 학습을 진행합니다.
- ***Technical Details***: UniME는 두 단계로 구성됩니다. 첫 번째 단계에서 텍스트 기반 구별 학습 지식을 강력한 LLM 기반 교사 모델로부터 증류하여 MLLM의 언어 구성 요소의 임베딩 능력을 향상시킵니다. 두 번째 단계에서는 하드 네거티브 샘플을 배치 내에서 여러 개 선택하여 구별 표현 학습을 더욱 발전시킵니다. MMEB 벤치마크 및 여러 검색 작업에서 UniME의 성능을 평가하며, 실험적으로 뛰어난 성능 향상을 보입니다.
- ***Performance Highlights***: UniME는 모든 평가된 작업에서 꾸준한 성능 향상을 달성했습니다. 예를 들어, MMEB 벤치마크에서 평균 4.1%에서 4.2%까지 성능이 향상되었습니다. 이는 매우 뛰어난 구별성과 구성 능력을 보여주며, 특히 단일 및 복합 검색 작업에서 강력한 결과를 냈습니다.

### [Perspective-Aware Reasoning in Vision-Language Models via Mental Imagery Simulation](https://arxiv.org/abs/2504.17207)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.17207.png)

Vote: 18

Authors: Jihyeon Je, Phillip Y. Lee, Minhyuk Sung, Leonidas Guibas, Mikaela Angelina Uy, Chanho Park

- ***What's New***: 본 연구에서는 멘탈 이미지 시뮬레이션(Mental Imagery Simulation)을 통해 비전-언어 모델(Vision-Language Models; VLMs)에 대한 관점 인식 추론(Perspective-Aware Reasoning) 프레임워크를 제안했습니다. 사람 수준의 시각적 이해가 중요한 관점 전환 능력을 갖춘 이 모델은 새로운 시점에서 장면을 상상할 수 있게 해주며, VLMs의 본질적 자기 중심적 편견을 극복할 수 있습니다.
- ***Technical Details***: 제안된 프레임워크, 즉 추상적 관점 변화(Abstract Perspective Change; APC)는 객체 탐지(Object Detection), 분할(Segmentation), 방향 추정(Orientation Estimation)과 같은 비전 기초 모델(Vision Foundation Models)을 활용하여 장면을 추상화하고 관점의 변화를 가능하게 합니다. 주어진 입력 이미지를 기반으로 장면의 추상 표현을 만들어, 참고 객체의 시점에서 문제를 다시 해석하여 새로운 프롬프트를 생성합니다.
- ***Performance Highlights***: APC는 COMFORT++와 3DSRBench와 같은 합성 및 실제 이미지 벤치마크 실험에서 우수한 성능을 보입니다. 특히, 전통적 비전 모델들과 달리, APC는 다양한 관점에서의 견고한 공간 추론을 보이며, 세심한 논리적 오류를 최소화할 수 있음을 보여줍니다.

### [QuaDMix: Quality-Diversity Balanced Data Selection for Efficient LLM Pretraining](https://arxiv.org/abs/2504.16511)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.16511.png)

Vote: 14

Authors: Yifeng Yu, Fengze Liu, Weidong Zhou, Yong Cao, Binbin Liu, Zhimiao Yu, Yifan Zhang, Taifeng Wang, Haobin Lin, Xiaohuan Zhou

- ***What's New***: QuaDMix는 대규모 언어 모델(LLM) 사전 학습을 위해 품질과 다양성을 균형 있게 조정하는 데이터 선택 프레임워크입니다. 이는 데이터 품질과 다양성을 함께 최적화하는 통합 데이터 선택 방식을 제안하여 고유한 품질-다양성 트레이드오프를 해결합니다.
- ***Technical Details***: QuaDMix는 각 문서의 품질 점수와 도메인 분류를 통해 데이터 품질과 다양성을 평가합니다. 이후, 파라미터로 제어되는 샘플링 함수를 디자인하여 각 데이터 포인트의 샘플링 확률을 결정합니다. 작은 모델에서의 시뮬레이션 실험을 통해 최적의 파라미터 탐색이 이루어지며, LightGBM을 이용해 파라미터를 최적화합니다.
- ***Performance Highlights***: QuaDMix는 여러 벤치마크에서 평균 7.2%의 성능 향상을 보여주었으며, 품질과 다양성을 독립적으로 최적화한 기존의 전략보다 우수한 성과를 거두었습니다. 이는 대규모 언어 모델의 사전 학습에서 데이터 품질과 다양성의 균형이 중요함을 강조합니다.

### [DyMU: Dynamic Merging and Virtual Unmerging for Efficient VLMs](https://arxiv.org/abs/2504.17040)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.17040.png)

Vote: 8

Authors: Zhenhailong Wang, Heng Ji, Silvio Savarese, Senthil Purushwalkam, Caiming Xiong, Ran Xu

- ***What's New***: 이 논문에서는 DYMU(Dynamic Merging and Virtual Unmerging)라는 새로운 프레임워크를 소개합니다. DYMU는 비전-언어 모델(VLMs)의 복잡성에 따라 시각적 토큰 길이를 동적으로 줄여주는 효율적이며 훈련이 필요 없는 방법을 제안합니다. 이를 통해 VLM의 성능을 유지하면서도 계산 부담을 크게 줄일 수 있습니다. 이는 기존의 고정 길이의 시각적 토큰을 사용하는 방법과 달리, 이미지의 복잡성에 적응하여 토큰 압축을 수행합니다.
- ***Technical Details***: DYMU는 두 가지 주된 구성 요소로 이루어져 있습니다. 첫째, Dynamic Token Merging(DToMe)은 시각적 인코더에서 이미지 복잡성에 따라 유사한 토큰들을 합쳐서 시각적 토큰 임베딩 수를 줄입니다. 둘째, Virtual Token Unmerging(VTU)은 대형 언어 모델(LLMs)에서 기대되는 토큰 시퀀스를 가상으로 재구성하여 종단 성능을 유지하면서도 시퀀스를 효율적으로 처리합니다. 이 방법은 기존의 사전 훈련된 VLM에 추가적인 재훈련이 필요 없으며 대부분의 최신 VLM 아키텍처에 적용 가능합니다.
- ***Performance Highlights***: DYMU는 다양한 VLM 아키텍처에서 평균 시각적 토큰 수를 32%-85%까지 줄이면서도 전체 모델과 비슷한 성능을 유지합니다. 특히, LLaVA-OneVision 같은 최신 VLM에서도 성능 저하 없이 토큰 수를 크게 줄일 수 있습니다. 이러한 결과는 DYMU가 VLM의 효율성을 높이면서도 유연하게 토큰 사용의 제어를 가능하게 한다는 것을 보여줍니다.

### [Token-Shuffle: Towards High-Resolution Image Generation with Autoregressive Models](https://arxiv.org/abs/2504.17789)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.17789.png)

Vote: 8

Authors: Yun Fu, Felix Juefei-Xu, Tingbo Hou, Xuan Ju, Hao Tang, Yushi Hu, Yujun Shi, Junjiao Tian, Peizhao Zhang, Zecheng He, Matt Feiszli, Yen-Cheng Liu, Ji Hou, Peter Vajda, Jialiang Wang, Peize Sun, Tao Xu, Zijian He, Xu Ma, Sam Tsai, Kunpeng Li, Haoyu Ma, Chih-Yao Ma, Xiaoliang Dai, Artsiom Sanakoyeu

- ***What's New***: 이 논문에서는 Token-Shuffle이라는 새로운 기법을 제안하여 오토레그레시브 모델(Autoregressive Models)이 고해상도 이미지(High-Resolution Image)를 효율적으로 생성할 수 있도록 합니다. 이 기법은 입력 토큰 수를 감소시켜 학습과 추론의 효율성을 높이면서, 2048x2048 해상도로 이미지를 생성할 수 있는 경지를 처음으로 이끌어냈습니다.
- ***Technical Details***: Token-Shuffle 기법은 다채널 토큰을 공간 내의 국부적(local) 토큰으로 통합(Shuffle)하여 토큰 수를 줄이고, 트랜스포머(Transformer) 블록 이후에는 이를 원래의 형태로 해제(Unshuffle)합니다. 추가적인 사전 학습된 텍스트 인코더가 필요하지 않으며, 기존의 MLLM 멀티모달 거대 언어 모델(Multimodal Large Language Models) 내에서 동작합니다. 이 접근 방식은 텍스트 프롬프트와 공동으로 훈련되며, 차세대 토큰 예측 프레임워크 내에서 매우 고해상도의 이미지 생성을 지원합니다.
- ***Performance Highlights***: 제안된 2.7B 토큰 모델은 높은 품질의 텍스트-이미지 생성 성능을 달성하였으며, GenAI 벤치마크에서 0.77 점수를 기록하여 기존의 LlamaGen과 같은 AR 모델 및 일부 강력한 Diffusion 모델의 성능을 뛰어넘었습니다. 대규모 인간 평가에서도 텍스트 정렬과 시각적 외관에서 우수한 생성 능력을 입증하였습니다.

### [Process Reward Models That Think](https://arxiv.org/abs/2504.16828)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.16828.png)

Vote: 6

Authors: Moontae Lee, Lajanugen Logeswaran, Lu Wang, Jaekyeom Kim, Rishabh Agarwal, Honglak Lee, Muhammad Khalifa, Hao Peng

- ***What's New***: 이 연구에서는 단계별 보상 모델(Process Reward Models; PRMs)의 데이터 효율성을 극대화한 새로운 접근법을 제안했습니다. THINKPRM이라 불리는 이 모델은 기존의 판별적 PRM들보다 훨씬 적은 프로세스 레이블을 사용하면서도 뛰어난 성능을 보입니다.
- ***Technical Details***: THINKPRM은 장문의 Chain-of-Thought(CoT)을 생성하여 문제 해결의 올바름을 단계별로 검증하게 함으로써 훈련됩니다. 기존의 대형 추론 모델을 기초로 가벼운 튜닝을 통해 생성한 것이 특징입니다. 약 8천 개의 프로세스 레이블로 훈련된 THINKPRM은 여러 벤치마크에서 뛰어난 성능을 보였습니다.
- ***Performance Highlights***: THINKPRM은 ProcessBench와 MATH-500 등의 벤치마크에서 기존 판별적 PRM 및 LLM-as-a-Judge보다 뛰어난 성능을 보였습니다. 특히 ProcessBench에서는 8%의 매크로 F1 포인트 향상을, MATH-500에서는 약 5% 정확도 향상을 나타냈습니다. 이 모델은 더 적은 데이터로도 더 나은 성능을 발휘하며, 검증의 효율성을 크게 높였습니다.

### [IberBench: LLM Evaluation on Iberian Languages](https://arxiv.org/abs/2504.16921)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.16921.png)

Vote: 5

Authors: Mara Chinea-Ríos, Ian Borrego Obrador, Angelo Basile, Álvaro Romo Herrero, Areg Mikael Sarvazyan, Marc Franco-Salvador, José Ángel González

- ***What's New***: IberBench는 이베리아 반도 및 이베로-아메리카 언어에서 LLM(Large Language Models)의 성능을 평가하기 위한 종합적이고 확장 가능한 벤치마크로, 스페인어, 포르투갈어, 카탈루냐어, 바스크어, 갈리시아어, 멕시코 스페인어 등의 다양한 언어를 포함합니다. 이는 기존 평가 방식의 언어적 다양성 부족 및 정적 평가 문제를 지속적 업데이트와 커뮤니티 주도의 모델, 데이터셋 기여를 통해 해결합니다.
- ***Technical Details***: IberBench는 101개의 데이터셋과 22개의 작업 카테고리로 구성되며, 감정 분석, 독성 탐지, 요약 등의 주요 작업을 포함합니다. 이는 다국어 모델과 특정 언어 모델의 성능을 평가하며, lm-eval-harness를 통해 모델의 순차적 라벨링 작업을 포함하여 다양한 작업을 평가합니다.
- ***Performance Highlights***: 23개의 LLM을 평가한 결과, 대부분의 모델은 산업적 관련성 있는 작업에서 더 낮은 성능을 보였습니다. 가장 도전적인 언어는 갈리시아어와 바스크어로 나타났으며, 감정 분석, 유머 탐지와 같은 몇몇 작업에서는 무작위 추측을 약간 상회하였으나 다른 작업에서는 대부분의 모델이 무작위 추측에 못 미쳤습니다. 최상위 모델은 가장 최근의 모델인 Qwen-2.5-7B-Instruct와 RigoChat-7b-v2가 차지하였습니다.

### [Boosting Generative Image Modeling via Joint Image-Feature Synthesis](https://arxiv.org/abs/2504.16064)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.16064.png)

Vote: 5

Authors: Theodoros Kouzelis, Nikos Komodakis, Spyros Gidaris, Efstathios Karypidis, Ioannis Kakogeorgiou

- ***What's New***: 이 논문에서 제안한 ReDi 프레임워크는 기존의 라텐트(diffusion) 모델에 고수준의 의미적 피처(semantic features)를 결합하여 이미지 생성 성능을 크게 향상시킵니다. 특히, 보통의 Diffusion Transformer 아키텍처에 최소한의 수정만을 가해 극대화된 생성 품질과 훈련 효율성을 제공합니다. 이 새로운 접근 전략은 Representation Guidance를 통해 학습된 의미 정보를 사용하여 이미지를 세밀하게 생성하도록 합니다.
- ***Technical Details***: ReDi는 다차원적 표상(Representation)의 잠재공간과 의미공간을 동시에 모델링하여 심층적 특징을 학습합니다. 이 접근 방식은 기존의 Distillation Objective를 제거하여 훈련 단계를 간소화하고, Representation Guidance를 도입하여 모델이 학습한 의미를 이용해 이미지를 정교하게 생성합니다. 또한, DINOv2를 이용해 고품질 의미 표상을 소스로 활용하며, VAE(latent)를 통해 저수준 이미지 정보를 모델링합니다. 이 두 정보를 융합하여 통합된 토큰 시퀀스 형태로 DiT나 SiT에 입력합니다.
- ***Performance Highlights***: ReDi를 통해 DiT-XL/2 모델은 400k 반복(iteration)에서 FID 8.7을 달성하며, 이는 7M 단계까지 훈련된 기본 모델보다 우수합니다. SiT-XL/2 모델도 ReDi와 함께 400k 반복에서 FID 7.5를 달성하였으며, 이는 7M 단계에 비해 뛰어난 결과입니다. 또한, Representation Guidance를 적용할 경우 비모조건적 생성에서 성능이 더욱 향상되어 FID가 DiT-XL/2의 경우 8.7에서 5.9로 하락했습니다.

### [Distilling semantically aware orders for autoregressive image generation](https://arxiv.org/abs/2504.17069)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.17069.png)

Vote: 4

Authors: Antoine Poupon, David Vazquez, Rishav Pramanik, Masih Aminbeidokhti, Marco Pedersoli, Zhaozheng Yin, Juan A. Rodriguez, Christopher Pal

- ***What's New***: 이 논문은 시맨틱 인식 주문을 사용하여 패치 기반 AR 이미지 생성의 품질을 향상시키는 방법을 제안합니다. 기존의 순차적 래스터 스캔 접근 방식 대신, 이미지 콘텐츠에 기반한 시맨틱 순서로 패치를 생성하여 이미지 질을 개선할 수 있음을 보여줍니다.
- ***Technical Details***: 이 방법은 특정 순서가 없는 모든-주문 모델(any-given-order model)을 먼저 훈련시킨 후, 이 모델로부터 추출된 주문을 사용하여 자가-지도 세밀 조정을 수행합니다. 이 모델은 패치의 콘텐츠와 위치를 동시에 예측하는 것을 목표로 하며, 패치 생성 시에 가장 유리한 위치를 선택합니다. 이를 위해 상대적 위치 부호화를 사용하고, 모든 가능한 위치 패치를 병렬로 생성하여 가장 가능성이 높은 패치를 순차적으로 선택합니다.
- ***Performance Highlights***: 패션 제품 데이터셋과 멀티모달 CelebA-HQ 데이터셋에서 실험을 통해, 제안한 방법이 정렬된 순서로 생성된 이미지에서 기존 래스터 스캔 접근법보다 더 우수한 품질의 이미지를 생성할 수 있음을 증명했습니다. 특히, Fine-tuned Ordered 모델이 Fréchet inception distance(FID)에서 가장 낮은 값을 기록하여, 비정렬(random) 순서 및 래스터 스캔 모델보다 향상된 성능을 보여주었습니다.

### [3DV-TON: Textured 3D-Guided Consistent Video Try-on via Diffusion Models](https://arxiv.org/abs/2504.17414)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.17414.png)

Vote: 4

Authors: Min Wei, Chaohui Yu, Fan Wang, Jingkai Zhou

- ***What's New***: 3DV-TON은 텍스처드 3D 가이던스를 통해 모션의 일관성을 유지하면서 비디오 시뮬레이션에 일관된 텍스처 모션을 생성할 수 있는 새로운 디퓨전 기반 프레임워크입니다. 새로운 데이터셋 HR-VVT를 도입하여 다양한 의류 종류와 시나리오를 포함한 130개의 고해상도 비디오를 제공합니다.
- ***Technical Details***: 3DV-TON은 애니메이션 가능한 텍스처드 3D 메시를 생성하고 비디오 시뮬레이션과 동기화하여 일관된 텍스처 모션 레퍼런스를 제공하는 적응형 파이프라인을 특징으로 합니다. 또한, 강력한 직사각형 마스킹 전략을 도입하여 동적 인체 및 의류 움직임 중 누출 문제로 인한 아티팩트 전파를 성공적으로 완화합니다.
- ***Performance Highlights***: 제안된 3DV-TON은 기존 방법들보다 우수한 성능을 보여주었으며, 정량적 및 정성적 실험에서 유리한 결과를 나타냈습니다. 특히, VIVID와 HR-VVT 벤치마크에서 가장 높은 사용자 선호도를 기록하면서 뛰어난 시각적 품질과 모션의 일관성을 유지합니다.

### [TimeChat-Online: 80% Visual Tokens are Naturally Redundant in Streaming Videos](https://arxiv.org/abs/2504.17343)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.17343.png)

Vote: 4

Authors: Xu Sun, Yuanxin Liu, Qi Liu, Sida Li, Yicheng Li, Lean Wang, Yuancheng Wei, Shicheng Li, Yuanxing Zhang, Linli Yao, Shuhuai Ren, Lingpeng Kong, Kun Ouyang, Lei Li

- ***What's New***: 이 논문은 실시간 비디오 상호작용을 혁신하는 새로운 온라인 VideoLLM인 TimeChat-Online을 소개합니다. 시각적 중복성을 해결하기 위해 Differential Token Drop(DTD) 모듈을 도입하여 스트리밍 비디오에서 의미 있는 변화만을 유지하고 82.8%의 비디오 토큰을 줄이면서도 성능을 거의 유지합니다.
- ***Technical Details***: Differential Token Dropping(DTD) 모듈은 사람이 시각적으로 느끼는 변화 맹시(Change Blindness) 현상에서 영감을 받아 비디오 프레임 간의 시공간 변화를 선택적으로 구별합니다. TimeChat-Online-139K 데이터셋을 구축하여, 실시간 비디오 처리와 백워드 트레이싱(backward-tracing), 실시간 인식, 미래 반응까지 다양한 패턴을 포함하고 있습니다.
- ***Performance Highlights***: TimeChat-Online은 StreamingBench에서 75.28의 점수를 기록하며, 최신 온라인 모델 Dispider-7B에 비해 7.65 향상된 성능을 보여줍니다. 또한 Qwen2.5VL-7B와 결합할 경우, VideoMME의 30-60분 비디오에서 5.7 포인트 정확도를 향상시키면서 비디오 토큰을 84.6% 줄입니다. 이는 실시간 비디오의 시각적 중복을 효율적으로 활용할 수 있음을 입증합니다.

### [ViSMaP: Unsupervised Hour-long Video Summarisation by Meta-Prompting](https://arxiv.org/abs/2504.15921)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15921.png)

Vote: 4

Authors: Dimitrios Korkinof, Jian Hu, Shaogang Gong, Mariano Beguerisse-Diaz

- ***What's New***: ViSMaP는 메타 프롬프팅(Meta Prompting)을 활용하여 시간 단위의 비디오를 비지도 학습 방법으로 요약하는 시스템입니다. 기존 비디오 이해 모델들은 짧은 비디오에선 잘 작동하지만, 긴 비디오에서는 관련 이벤트가 드문드문 배치되어 있는 경우가 많아 요약이 어렵습니다. ViSMaP는 이러한 한계를 극복하며, 짧은 비디오의 세그먼트 설명을 활용하여 긴 비디오에 대한 최적화된 의사 요약(Pseudo-summaries)을 생성하고 이를 학습 데이터로 사용하여 고성능의 요약을 가능하게 합니다.
- ***Technical Details***: ViSMaP는 세 단계로 구성됩니다. 첫 번째 단계에서는 짧은 비디오 세그먼트로부터 요약 모델을 학습하고, 두 번째 단계에서는 길이가 긴 비디오를 3분 단위로 분할하여 의사 자막을 생성합니다. 마지막 단계에서는 메타 프롬프팅을 통해 의사 요약을 생성하여 장시간 비디오 요약 모델을 학습하게 됩니다. 메타 프롬프팅 전략은 세 개의 LLMs(대형 언어 모델)를 활용하여 의사 요약을 생성, 평가, 최적화합니다.
- ***Performance Highlights***: 비지도 방법으로 실행되는 ViSMaP는 여러 데이터셋에서의 평가 결과, 완전 학습 모델과 비교 가능한 성능을 보였습니다. 주요 성능 지표인 CIDEr, ROUGE-L, METEOR에서 유의미한 점수를 기록하며, 다양한 도메인에 대해 일반화 성능을 보여 줍니다. 특히 Video Recap과 같은 완전 감독된 모델과 비교했을 때에도 성능이 비등하여, 무비용으로 높은 성능을 유지하는 것을 입증하였습니다.

### [Interpretable non-linear dimensionality reduction using gaussian weighted linear transformation](https://arxiv.org/abs/2504.17601)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.17601.png)

Vote: 2

Authors: Erik Bergh

- ***What's New***: 이 논문은 기존의 선형성과 비선형 변환의 조합을 통해 해석 가능하고 표현력이 뛰어난 비선형 차원 축소 알고리즘을 제안합니다. 특정 차원에 가우시안 함수로 가중치를 부여하여 복잡한 패턴을 포착하면서도 각 변환을 독립적으로 분석할 수 있어 해석 가능성을 갖추고 있습니다.
- ***Technical Details***: 고차원과 저차원 공간을 연결하는 비선형 맵핑은 여러 선형 변환을 가우시안 함수로 가중하여 구성됩니다. 입력 벡터 x에 대해 wi(x) Ti(x) 형식의 변환을 정의하며, 이를 통해 공간의 기하학적 관계를 보존 및 수정합니다. 또한, 내장된 최적화 기법을 통해 연산 효율성을 높였습니다.
- ***Performance Highlights***: 제안한 알고리즘은 t-SNE와 같은 기존 방법들과 달리 추가 연산 비용 없이 새로운 데이터 포인트로 확장이 가능하며, 공간에서의 거리 보존이 이뤄집니다. 실험 결과 0.45의 재구성 오류를 통해 거리 보존의 정확성을 확인하였습니다.

### [DiMeR: Disentangled Mesh Reconstruction Model](https://arxiv.org/abs/2504.17670)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.17670.png)

Vote: 1

Authors: Xu Zheng, Lutao Jiang, Yifan Jiang, Yingcong Chen, Yuanhuiyi Lyu, Xin Yang, Jiantao Lin, Wenhang Ge, Kanghao Chen

- ***What's New***: DiMeR는 정상 맵과 3D 형상 간의 귀납적 편향을 활용하여 형상과 텍스처 예측을 별도로 수행하는 분리된 이중 스트림 메시 재구축 모델입니다. 이를 통해 메시 형상은 정상 맵에서, 텍스처는 RGB 이미지에서 예측하며, 3D 감독을 도입하여 훈련의 안정성을 강화했습니다.
- ***Technical Details***: DiMeR 모델에서는 메시 형상을 위한 정상 맵만을 입력으로 사용하며, 텍스처 예측을 위해 RGB 입력을 사용합니다. 기존의 FlexiCubes의 규제 손실을 대체하기 위해 eikonal 손실과 3D 지상 실체(ground truth) 감독을 도입하였습니다. 모델의 효율성을 높이기 위해 네트워크의 불필요한 요소를 간소화하여 더 높은 공간 해상도를 제공합니다. 각 지점의 예측을 위해 DiMeR는 모델의 기하학 브랜치에서 얻어진 정규 맵을 활용하며, 텍스처 필드의 출력은 RGB 손실을 통해 감독됩니다.
- ***Performance Highlights***: DiMeR는 이전 메시 재구축 모델을 크게 능가하여 Chamfer Distance를 30% 이상 줄였습니다. 다양한 태스크에서 견고한 성능을 보였으며, 특히 sparse-view 재구축, single-image-to-3D, and text-to-3D에서 강력한 성능을 입증했습니다. 이러한 성능 개선은 정상 예측 모델의 개선에도 영향을 받을 수 있으며, 앞으로의 연구에서도 기대를 갖게 합니다.

### [Dynamic Camera Poses and Where to Find Them](https://arxiv.org/abs/2504.17788)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.17788.png)

Vote: 1

Authors: Tsung-Yi Lin, Ming-Yu Liu, David F. Fouhey, Chen-Hsuan Lin, Chris Rockwell, Joseph Tung

- ***What's New***: 이 논문에서는 대규모 동적 인터넷 비디오 데이터셋인 DynPose-100K를 소개합니다. 이 데이터셋은 다양한 설정의 동적 콘텐츠를 포함하며, 카메라 포즈가 주석으로 표시되어 있습니다. 이를 통해 사실적인 비디오 생성 및 시뮬레이션 분야의 발전을 도모하고자 합니다.
- ***Technical Details***: DynPose-100K 데이터셋은 대규모 인터넷 비디오 Panda-70M에서 3.2M의 비디오를 필터링하여 수집하였으며, 100,131개의 고품질 카메라 정보를 포함하는 비디오를 제공합니다. 필터링 파이프라인은 전용 모델과 일반 모델(VLM)을 결합하여 비디오의 적합성을 평가합니다. 동적 포즈 추정은 최신의 포인트 트래킹, 동적 마스킹, 구조에서 모션으로(Structure-from-Motion; SfM) 기술을 통합하여 수행됩니다.
- ***Performance Highlights***: DynPose-100K의 실험을 통해 필터링이 실제로 포즈 추정에 적합한 비디오를 높은 정밀도로 선택함이 입증되었습니다. 제안된 마스킹과 트래킹 기법은 뛰어난 성능을 보였으며, 평균적으로 90% 이상의 정확도를 달성하고, 에러를 기존 접근 방법들에 비해 50% 이상으로 감소시켰습니다. 이를 통해 데이터셋의 높은 품질을 확인할 수 있습니다.

