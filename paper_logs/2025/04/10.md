## Daily Papers (2025-04-10)

### [DDT: Decoupled Diffusion Transformer](https://arxiv.org/abs/2504.05741)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.05741.png)

Vote: 48

Authors: Weilin Huang, Shuai Wang, Zhi Tian, Limin Wang

- ***What's New***: DDT(Decoupled Diffusion Transformer)은 기존의 디퓨전 트랜스포머(Diffusion Transformers)의 문제점을 해결하기 위해 설계된 새로운 아키텍처입니다. DDT는 의미 제한(semantic constraint)을 추출하는 '조건 인코더(condition encoder)'와 고주파 세부를 해독하는 '속도 디코더(velocity decoder)'를 분리하여 더 빠르고 높은 품질의 이미지 생성이 가능합니다.
- ***Technical Details***: DDT는 조건 인코더와 속도 디코더로 구성된 디퓨전 트랜스포머 모델로, 조건 인코더는 주어진 입력으로부터 자가 조건(self-condition)을 추출해내고, 속도 디코더는 이 자가 조건과 함께 노이즈가 있는 잠재 공간(latent space)을 사용해 고주파 속도(velocity)를 복원합니다. 인퍼런스 속도를 향상시키기 위해 동적 프로그래밍(Dynamic Programming)을 사용하여 최적의 공유 전략(sharing strategy)을 제안합니다.
- ***Performance Highlights***: DDT-XL/2 모델은 ImageNet 256 × 256 데이터셋에서 1.31 FID의 성능을 기록하며 기존 모델보다 약 4배 빠른 수렴 속도를 보였습니다. ImageNet 512 × 512에서 DDT-XL/2는 1.28의 FID로 종전 기록을 넘으며 모두 최신 상태의 성능을 달성하였습니다. 이러한 성능 향상과 함께, 조건 인코더의 자가 조건을 인접 단계 사이에서 공유함으로써 인퍼런스 속도를 대폭 향상시켰습니다.

### [OLMoTrace: Tracing Language Model Outputs Back to Trillions of Training Tokens](https://arxiv.org/abs/2504.07096)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.07096.png)

Vote: 39

Authors: Sewon Min, Jiacheng Liu, David Albright, Karen Farley, Arnavi Chheda-Kothary, YenSung Chen, Ali Farhadi, Rock Yuren Pang, Hannaneh Hajishirzi, Jon Borchardt, Bailey Kuehl, Evie Cheng, Huy Tran, Luca Soldaini, Yejin Choi, Dirk Groeneveld, Jenna James, Byron Bischoff, Carissa Schoenick, Cassidy Trier, Eric Marsh, Sruthi Sreeram, Sophie Lebrecht, Pang Wei Koh, Noah A. Smith, Taylor Blanton, Yanai Elazar, Taira Anderson, Jesse Dodge, Michael Schmitz, Aaron Sarnat

- ***What's New***: OLMoTrace는 언어 모델(Language Models; LMs)의 출력을 그들의 다중 조 단위(training tokens) 훈련 데이터로 실시간으로 추적할 수 있는 최초의 시스템입니다. 사용자가 언어 모델의 작동 방식을 훈련 데이터의 관점에서 이해할 수 있도록 지원하며, 팩트 체크, 환각 탐지, 창의성 같은 다양한 영역에서 그 유용성이 입증되었습니다.
- ***Technical Details***: OLMoTrace는 인피니그램(Infini-gram)이라는 확장된 버전을 사용하여 모델의 훈련 데이터와 모델 출력 간의 문자 그대로 일치하는 문장을 찾아냅니다. 인덱스를 사전 정렬하고 평행 알고리즘을 적용하여 최대 일치 구간을 빠르게 계산하며, 각 출력의 추적을 평균 4.5초 내에 완료합니다. OLMoTrace는 검색된 문서 및 해당 문맥을 제공합니다.
- ***Performance Highlights***: OLMoTrace는 약 450 토큰 길이의 언어 모델 응답을 4.46초 내에 처리할 수 있으며, 랜덤 디스크 읽기 I/O의 최적화를 통해 높은 처리량을 유지합니다. 모델 출력을 가장 적합하게 설명하는 훈련 문서의 BM25 점수를 기반으로 높은 일관성을 유지하도록 설계되었습니다.

### [A Unified Agentic Framework for Evaluating Conditional Image Generation](https://arxiv.org/abs/2504.07046)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.07046.png)

Vote: 23

Authors: Kaifu Zhang, Min Zhang, Yiyu Wang, Jifang Wang, Zhenran Xu, Xue Yang, Baotian Hu, Weihua Luo, Yaowei Wang, Longyue Wang

- ***What's New***: 이 논문은 조건부 이미지 생성(Conditional Image Generation) 작업에 대한 포괄적인 평가 프레임워크인 CIGEVAL을 소개합니다. CIGEVAL은 대형 멀티모달 모델(Large Multimodal Models; LMMs)을 활용하여 다기능 툴박스와 세밀한 평가 프레임워크를 통합합니다. 이 프레임워크는 이전의 GPT-4o 기반 최첨단 방법을 능가하며 이미지 생성 작업의 자동 평가를 인간 수준의 신뢰도로 수행할 잠재력을 가지고 있습니다.
- ***Technical Details***: CIGEVAL은 다양한 조건부 이미지 생성 작업에 대해 평가하기 위한 LMM 기반 에이전트 프레임워크입니다. 작업의 세밀한 분석을 위해 GPT-4o 모델과 오픈소스 모델을 통합한 다기능 툴박스를 사용하며, 툴의 출력을 바탕으로 세분화된 평가를 수행합니다. 평가를 위한 훈련 데이터로 2,274개의 고품질 궤적(trajectories)을 생성하여 LMM을 미세 조정하며, 이로써 작은 LMM들도 효과적인 평가를 수행할 수 있도록 돕습니다.
- ***Performance Highlights***: CIGEVAL은 GPT-4o를 기반으로 사용했을 때 모든 7가지 조건부 이미지 생성 작업에 대해 최첨단의 성능을 달성했습니다. 인간 평가자와의 평균 스피어만 코릴레이션(Spearman Correlation)이 0.4625이며, 인간 평가자 간의 코릴레이션과 유사한 수준을 보였습니다. 특히, 여러 조건을 포함하는 작업에서 이전의 평가 지표들이 고전한 반면, CIGEVAL은 우수한 성능을 보여주었습니다.

### [Missing Premise exacerbates Overthinking: Are Reasoning Models losing Critical Thinking Skill?](https://arxiv.org/abs/2504.06514)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.06514.png)

Vote: 21

Authors: Ming Li, Tianyi Zhou, Chenrui Fan, Lichao Sun

- ***What's New***: 이 논문은 추론 LLMs의 응답 길이가 미완성 질문(Missing Premise)에서 크게 증가하여 불필요하고 비효율적인 사고를 하게 되는 문제를 지적하며, 이를 MiP-Overthinking이라고 명명하였습니다. 일반적인 오버싱킹 문제를 훨씬 악화시킬 수 있으며, 이는 현재의 추론 모델이 비판적 사고를 충분히 수행하지 못하고 있음을 나타냅니다.
- ***Technical Details***: 저자들은 다양한 난이도의 MiP 질문을 구성하고, SVAMP, GSM8K, MATH500 등 여러 데이터셋을 변형하여 연구를 수행했습니다. 다양한 대규모 언어 모델 (LLMs)을 평가하여 MiP 질문에서 추론 길이, 오버싱킹 패턴, 비판적 사고의 위치를 분석했습니다. 실험은 생성된 응답의 길이, 정확도, 그리고 비정상적인 질문에 대한 포기율을 측정하여 진행했습니다.
- ***Performance Highlights***: MiP 질문은 일반적인 오버싱킹보다 2배에서 4배 더 많은 토큰을 생성하게 하였으며, 이는 테스트 시간 확대 법칙과 상충하는 결과를 나타냈습니다. 비추론 모델은 MiP 상황에서 짧은 응답을 생성하여 높은 포기율을 보였고, 이는 불완전한 정보에 대한 저항력을 보여주었습니다. 대부분의 추론 모델은 MiP 존재를 인식하고 이를 초기 단계에서 식별할 수 있으나, 여전히 비효율적인 사고를 계속하며 판단에 확신을 가지지 못했습니다.

### [GenDoP: Auto-regressive Camera Trajectory Generation as a Director of Photography](https://arxiv.org/abs/2504.07083)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.07083.png)

Vote: 18

Authors: Dahua Lin, Mengchen Zhang, Tong Wu, Jing Tan, Gordon Wetzstein, Ziwei Liu

- ***What's New***: GenDoP는 사진감독(Director of Photography)을 모방하여 대화형 오토레그레시브 모델을 활용한 카메라 궤적 생성 시스템입니다. 비디오 생성에서 창의력과 표현력을 중시하여 카메라 움직임을 예술적이고 표현력 있는 궤적으로 생성할 수 있도록 설계되었습니다.
- ***Technical Details***: 새롭게 제안된 DataDoP 데이터셋은 현실 세계의 영상에서 자유롭게 움직이는 카메라 궤적과 심도 지도, 그리고 구체적인 캡션을 포함하여 29K 이상의 샷으로 구성되어 있습니다. 이 데이터셋을 기반으로 오토레그레시브 디코더 전용 Transformer 모델 GenDoP를 훈련하여 텍스트 안내와 RGBD 입력을 기반으로 카메라 움직임을 생성합니다. 이는 기존의 기하학적 최적화나 절차적 시스템의 한계를 극복하고 상세한 궤적 조정과 더 나은 안정성을 제공합니다.
- ***Performance Highlights***: GenDoP는 기존 CCD 및 E.T. 모델과 비교하여, 텍스트-궤적 정렬(Alignment)에서 36.179의 CLaTr-CLIP 점수를 기록하며, 더 높은 정렬도와 궤적 품질을 보여줍니다. 특히, 다양한 입력 조건을 처리하는 능력을 발휘하며 RGBD 및 텍스트 조건의 작업에서 탁월한 성능을 입증하였습니다.

### [FantasyTalking: Realistic Talking Portrait Generation via Coherent Motion Synthesis](https://arxiv.org/abs/2504.04842)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.04842.png)

Vote: 15

Authors: Mu Xu, Yunpeng Zhang, Yaqi Fan, Mengchao Wang, Kun Zhao, Fan Jiang, Yonggang Qi, Qiang Wang

- ***What's New***: FantasyTalking은 새로운 프레임워크를 제안하여 고해상도, 일관된 모션의 사실적인 말하는 초상화를 생성합니다. 이 방법은 특히 오디오와 시각적 요소의 정밀한 동기화를 통해 생동감 있는 애니메이션을 가능하게 합니다.
- ***Technical Details***: 이 프레임워크는 사전 학습된 비디오 확산 변환기 모델(Video Diffusion Transformer)을 활용합니다. 이중 단계 음성-시각 정렬 전략을 도입하여 클립 수준의 트레이닝을 통해 배경과 맥락 객체 전체의 동작을 조화롭게 만들고, 프레임 수준에서는 입술 움직임을 정밀하게 조정합니다. 얼굴 집중 교차 주의 모듈(Cross-Attention)을 통해 얼굴 특징의 일관성을 유지하고, 모션 강도 조절 모듈을 통해 표현과 신체 모션의 강도를 조절할 수 있습니다.
- ***Performance Highlights***: 실험 결과 FantasyTalking은 영상 품질, 시간적 일관성, 모션 다양성 면에서 새로운 SOTA(State-of-the-Art)를 달성했습니다. Sync-C와 Sync-D 측정치에서 높은 음성 싱크정확도를 보였으며, 동시에 캐릭터의 정체성 유지와 향상된 얼굴 표현을 보여주었습니다.

### [Self-Steering Language Models](https://arxiv.org/abs/2504.07081)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.07081.png)

Vote: 11

Authors: Joshua B. Tenenbaum, Gabriel Grand, Jacob Andreas, Alexander K. Lew, Vikash K. Mansinghka

- ***What's New***: 이 논문은 DISCIPL을 소개하며, 이는 언어 모델(Language Models; LMs)이 자연어에서의 검색이나 계획을 통해 느리거나 비싸고 오류가 발생할 수 있는 추론을 극복하는 방법입니다. DISCIPL은 계획자 모델(Planner Model)이 작업별로 추론 프로그램을 생성하고 팔로워 모델(Follower Models)의 집단에 의해 실행되는 '자기 주도' 방식의 LMs입니다.
- ***Technical Details***: DISCIPL은 LLMs가 재귀적인 검색 절차를 작성할 수 있게 하여 새로운 형태의 검증 가능하고 효율적인 추론을 가능하게 합니다. 이 접근 방식은 계획을 실행과 분리하여 고도로 병렬화된 몬테 카를로(Monte Carlo) 추론 전략의 설계 공간을 열어주며, 표준 best-of-N 샘플링을 능가하고 미세 조정이 필요 없습니다.
- ***Performance Highlights***: DISCIPL은 작은 팔로워 모델(e.g., Llama-3.2-1B)을 사용하여 GPT-4o와 같은 큰 모델에서도 어려운 제약 생성 작업에서 비슷하거나 더 뛰어난 성능을 보여주었습니다. DISCIPL은 Collie와 같은 도전적인 벤치마크와 다양한 퍼즐 과제에서 효율성을 증명하며, 제약된 생성 및 퍼즐 작업에서 강력한 추론 모델의 성능에 도달합니다.

### [A Sober Look at Progress in Language Model Reasoning: Pitfalls and Paths to Reproducibility](https://arxiv.org/abs/2504.07086)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.07086.png)

Vote: 10

Authors: Andreas Hochlehnert, Samuel Albanie, Vishaal Udandarao, Ameya Prabhu, Matthias Bethge, Hardik Bhatnagar

- ***What's New***: 이 논문은 언어 모델(LMs)의 추론 능력을 평가하는 기존의 수학적 추론 벤치마크가 재현 가능성과 통계적 타당성이 부족함을 밝힙니다. 이를 개선하기 위해 표준화된 평가 프레임워크와 명확하게 정의된 모범 사례 및 보고 기준을 제안합니다.
- ***Technical Details***: 본 연구는 디코딩 파라미터, 임의 시드, 프롬프트 형식, 하드웨어 및 소프트웨어 등 세부적인 구현 선택에 매우 민감한 수학적 추론 벤치마크의 민감성을 실증적으로 분석했습니다. 또한, 최근 강화 학습(RL) 방법의 개선이 미미하고 특히 AIME'24 같은 소규모 벤치마크에서 과적합됨을 발견했습니다. 반면에, 지도 학습(SFT)은 일관되고 강력한 일반화를 보여주며 모든 코드, 프롬프트 및 모델 출력을 공개하여 재현 가능성을 촉진합니다.
- ***Performance Highlights***: 표준화된 프레임워크로 재평가한 결과, 최근의 강화 학습 기반 접근 방식의 성능 향상이 기존 결과보다 낮으며, 베이스 모델을 큰 폭으로 능가하지 않는 것으로 나타났습니다. 반면, SFT 방법은 일관되게 더 나은 일반화 성능을 나타내며, 강화 학습 방법은 여전히 개선이 필요함을 시사합니다.

### [Caption Anything in Video: Fine-grained Object-centric Captioning via Spatiotemporal Multimodal Prompting](https://arxiv.org/abs/2504.05541)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.05541.png)

Vote: 9

Authors: Pinxin Liu, Daiki Shimada, Zeliang Zhang, Chenliang Xu, Liu He, Yizhi Song, Zhuo Liu, Chao Huang, Ali Vosoughi, Jiebo Luo, Luchuan Song, Yunlong Tang, Yunzhong Xiao, Susan Liang, Jinxi He, Junjia Guo, Jing Bi, Hang Hua, Mingqian Feng

- ***What's New***: CAT-V는 비디오 내 사용자가 선택한 객체에 대한 상세한 설명을 생성할 수 있는 객체 중심의 비디오 캡셔닝을 위해 스페이시오템포럴 멀티모달 프롬트(Spatiotemporal Multimodal Prompting)를 활용하여 추가적인 학습 데이터 없이 작동하는 트레이닝 프리(Training-Free) 프레임워크를 제안합니다.
- ***Technical Details***: CAT-V는 SAMURAI를 기반으로 한 세그멘터(Segmenter), TRACE-Uni를 활용한 시간 분석기(Temporal Analyzer), InternVL-2.5를 사용하는 캡셔너(Captioner) 세 가지 주요 구성 요소로 구현됩니다. SAMURAI는 사용자가 비디오의 특정 프레임에서 지정한 객체를 정확하게 분할하고, TRACE-Uni는 이벤트 경계를 감지하여 비디오 내 이벤트를 분석하며, 캡셔너는 체인 오브소드 추론(Chain-of-Thought Reasoning)을 활용해 객체 중심의 구체적인 설명을 생성합니다.
- ***Performance Highlights***: CAT-V는 객체 중심의 비디오 캡셔닝에서 세부적인 시간 인식과 공간적 정밀도를 확보합니다. TRACE-Uni와 SAMURAI의 결합을 통해 객체의 상태 변화와 상호작용을 명확하게 포착하며, 실험 결과 다양한 유저 인터랙션을 효과적으로 처리합니다.

### [OmniCaptioner: One Captioner to Rule Them All](https://arxiv.org/abs/2504.07089)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.07089.png)

Vote: 9

Authors: Xin Li, Tao Chen, Bo Zhang, Jiakang Yuan, Licheng Wen, Yiting Lu, Qi Qin, Shitian Zhao, Peng Gao, Xinyue Li, Xiangchao Yan, Lei Bai, Zhen Li, Dongyang Liu, Yuewen Cao, Botian Shi, Le Zhuo, Zhibo Chen

- ***What's New***: OMNICAPTIONER는 다양한 시각적 도메인에서 세밀한 텍스트 설명을 생성할 수 있는 범용 시각 캡셔닝 프레임워크로써, 자연 이미지, 시각적 텍스트(포스터, UI, 교과서 등), 구조화된 시각(문서, 표, 차트 등을 포함)을 모두 다룰 수 있는 통일된 솔루션을 제공합니다. 이 프레임워크는 시각적 모달 및 텍스트 모달 간의 격차를 해소합니다.
- ***Technical Details***: OMNICAPTIONER는 다채로운 시각 도메인 커버리지와 픽셀-텍스트 매핑의 두 가지 특징을 갖습니다. 다양한 이미지 타입에 자세한 캡션을 결합하여 저수준 픽셀 정보에서 의미가 풍부하고 세밀하게 텍스트 설명을 전환합니다. 이를 위해 GPT-4o와 같은 폐쇄형 멀티모달 모델을 활용해 자연 이미지 등의 모든 시각적 요소를 상세히 설명합니다.
- ***Performance Highlights***: OMNICAPTIONER를 통합한 LLM은 수학적 시각 문제 해결과 시각적 추론 태스크에서 탁월한 성능을 발휘해 기존의 모델 대비 우수한 성능을 보였습니다. 예를 들어, MathVerse 벤치마크에서 OMNICAPTIONER + DS-R1-Distill-Qwen-32B는 통합하기 전 모델 대비 더욱 뛰어난 성능을 보여 LLM의 시각 추론 능력을 향상시키는 데 기여했습니다.

### [DiTaiListener: Controllable High Fidelity Listener Video Generation with Diffusion](https://arxiv.org/abs/2504.04010)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.04010.png)

Vote: 7

Authors: Minh Tran, Ashutosh Chaubey, Mohammad Soleymani, Hongkun Gong, Di Chang, Maksim Siniukov

- ***What's New***: DiTaiListener는 스피커의 오디오와 안면 움직임 입력을 기반으로 고화질의 청취자 비디오를 생성하는 혁신적인 비디오 생성 모델이다. 이 모델은 텍스트 컨트롤을 통해 청취자 행동을 사용자 지정할 수 있는 기능을 제공하며, 긴 형태의 비디오 생성에서도 자연스러운 전환을 지원한다.
- ***Technical Details***: DiTaiListener는 Diffusion Transformer(DiT)를 기반으로 한 비디오 생성 모델로, Causal Temporal Multimodal Adapter(CTM-Adapter)를 사용하여 입력된 다중 모달 정보를 시간적 인과 관계에 맞게 처리한다. 이 모델은 DiTaiListener-Gen과 DiTaiListener-Edit로 구성되며, DiTaiListener-Gen은 짧은 영상 세그먼트를 생성하고, DiTaiListener-Edit는 세그먼트 간의 부드러운 전환을 위해 사용된다.
- ***Performance Highlights***: DiTaiListener는 photorealism 평가에서 FID +73.8% (RealTalk 데이터셋 내)와 motion representation 평가에서 FD metric +6.1% (VICO 데이터셋 내)에서 최첨단 성능을 기록했다. 이는 기존 기술의 한계를 뛰어넘어 사용자가 지향하는 텍스트 기반의 청취자 행동 생성에서 강력한 성능을 보여준다. 사용자 연구에서도 다른 경쟁 모델들보다 피드백, 다양성 및 부드러움 면에서 월등히 선호되었다.

### [VideoChat-R1: Enhancing Spatio-Temporal Perception via Reinforcement Fine-Tuning](https://arxiv.org/abs/2504.06958)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.06958.png)

Vote: 6

Authors: Yinan He, Yi Wang, Limin Wang, Xinhao Li, Yu Qiao, Lu Dong, Yali Wang, Desen Meng, Xiangyu Zeng, Ziang Yan

- ***What's New***: VideoChat-R1은 다중작업 강화 학습 미세 조정(Multi-task Reinforcement Fine-Tuning; RFT)을 통해 강화된 비디오 MLLM으로, 시공간 지각(Spatio-Temporal Perception) 능력을 크게 개선하면서 일반적인 챗 능력을 유지할 수 있습니다. 이 연구는 Qwen2.5-VL-7B와 비교하여, 시간적 그라운딩(Temporal Grounding) 및 객체 추적(Object Tracking)과 같은 작업에서 성능을 수 배 향상시켰습니다.
- ***Technical Details***: VideoChat-R1은 그룹 상대 정책 최적화(Group Relative Policy Optimization; GRPO)를 이용한 미세 조정 기술을 통해 비디오 언어 이해를 강화합니다. 이 과정에서는 시간적 그라운딩, 객체 추적, 비디오 질문 응답(Video QA) 같은 비디오 관련 작업에서 다양한 보상 함수들을 포함하여 포맷 보상, IoU 보상, 정확도 보상 등을 사용합니다. GRPO는 후보 응답 그룹을 사용하여 평가하며 KL 발산 제약을 통해 정책의 과도한 편차를 방지합니다.
- ***Performance Highlights***: 비디오 MLLM의 성능을 측정하는 여러 벤치마크에서 VideoChat-R1은 state-of-the-art 성능을 달성했습니다. 시간적 그라운딩 작업에서 VideoChat-R1은 이전 모델에 비해 mIoU 지표에서 31.8, 객체 추적에서 31.2의 성능 증가를 보였습니다. VideoMME, MVBench 등의 일반 QA 벤치마크에서도 최대 1.0의 향상을 보였습니다.

### [Are We Done with Object-Centric Learning?](https://arxiv.org/abs/2504.07092)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.07092.png)

Vote: 5

Authors: Alexander Rubinstein, Seong Joon Oh, Matthias Bethge, Ameya Prabhu

- ***What's New***: 이 논문은 객체 중심 학습(Object-Centric Learning; OCL)이 현재의 목표를 어느 정도 달성했음을 주장합니다. 특히, 샘플 효율적인 세분화 모델(Sample-Efficient Segmentation Models)을 통해 무감독 메서드와 비교해 뛰어난 성능을 보이며, 물체 분리를 위한 객체 중심 표현을 제공할 수 있음을 강조합니다. 새로운 프로브인 OCCAM(Object-Centric Classification with Applied Masks)을 제안하여, 객체 중심의 분할 방식이 기존의 슬롯 기반 OCL 방식을 크게 능가한다고 설명합니다.
- ***Technical Details***: OCCAM은 두 단계로 이루어져 있습니다. 먼저 객체별 마스크 생성 및 이미지를 통해 객체 중심 표현을 얻고, 그 후 부적절한 배경 신호를 제거한 상태에서 객체 중심 표현을 응용하여 강력한 이미지 분류를 수행합니다. 이 연구는 두 가지 주요 구성 요소를 제안합니다: (1) 각 객체에 대해 마스크를 생성하여 객체 중심 표현을 획득하고, (2) 스푸리어스한 배경을 무시하고 관련된 객체 특징에 집중함으로써 다운스트림 작업에 적용합니다.
- ***Performance Highlights***: 샘플 효율적인 세분화 모델인 HQES와 같은 모델들은 무감독 객체 탐색 벤치마크에서, 특히 Movi-E 데이터셋에서 기존 OCL 모델보다 뛰어난 성능을 보였습니다. 실험에서 HQES 기반의 마스크와 SigLip 모델은 ImageNet-D와 같은 난도 높은 벤치마크에서 78.5%라는 성능을 보이며, 최근 모델들보다도 더 우수한 성능을 입증했습니다.

### [Masked Scene Modeling: Narrowing the Gap Between Supervised and Self-Supervised Learning in 3D Scene Understanding](https://arxiv.org/abs/2504.06719)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.06719.png)

Vote: 5

Authors: Leon Sick, Christian Stippel, Pedro Hermosilla

- ***What's New***: Masked Scene Modeling은 3D 장면 이해를 위한 자가 지도 학습(Self-Supervised Learning; SSL)과 지도 학습(Supervised Learning) 간의 성능 격차를 줄이는 새로운 프레임워크를 제안합니다. 이 모델은 대규모 3D 장면에 대한 작업 및 태스크 무관한 특징 학습을 통해, 처음으로 자가 지도 모델이 지도 모델과 유사한 성능을 보일 수 있도록 합니다.
- ***Technical Details***: 이 연구는 하향식 마스킹 전략에 기반한 'Masked Scene Modeling' 프레임워크를 통해, 계층적 3D 모델을 활용한 강력한 자가 지도 학습 접근법을 제공합니다. 이는 UNet 구조와 조합한 하향식 계층 마스킹 접근으로, 마스킹된 패치의 심층 특징을 복원하는 방식으로 설계되었습니다. 이로 인해, 모델은 기하학적 단서의 정보 유출 없이 계층적 장면 복원을 수행할 수 있게 되며, 풍부한 의미론적 특징 학습이 가능합니다.
- ***Performance Highlights***: 제안된 모델은 기존의 3D 장면 자가 지도 학습 모델을 크게 상회하는 성과를 보였습니다. ScanNet, ScanNet200, S3DIS 데이터셋에서 자가 지도 학습 모델 대비 +30, +15, +10 이상의 개선을 이루었으며, 제한된 데이터 조건에서도 지도 학습 모델을 능가하는 경쟁력 있는 성능을 제공합니다. 또한, 2D 기초 모델과 비교하여도 우수한 성능을 나타냈습니다.

### [Pretraining Language Models for Diachronic Linguistic Change Discovery](https://arxiv.org/abs/2504.05523)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.05523.png)

Vote: 4

Authors: Leshem Choshen, Sabrina Li, Elisabeth Fittschen, Tom Lippincott, Craig Messner

- ***What's New***: 이 논문에서는 대형 언어 모델(LLMs)이 시간에 따른 언어 변화를 탐지하기 위한 도구로서 사용될 수 있음을 보여줍니다. 특히, 역사적 언어학 분야에서 코퍼스 기반 가설 발견 및 테스트에 새로운 접근법을 제공합니다.
- ***Technical Details***: 이 연구에서는 여러 10백만 단어로 구성된 시계열 분할 데이터를 사용하여 다수의 모델을 사전 학습(pretraining)합니다. BabyLlama-2 및 Llama3-8B 파라미터 효율성을 활용하여 모델을 훈련하였고, DoRA 어댑터를 사용한 모델 미세 조정(finetuning)과 비교하였습니다.
- ***Performance Highlights***: 사전 학습된 모델은 미세 조정된 모델보다 두 배 빠르게 학습되며 시간적으로 특정한 정보 경계를 더 잘 유지합니다. 그리고 비록 낮은 유창성을 가지고 있지만, 특정 시기의 언어적 특성을 포착하는 데 유리하여 언어적 변화와 가설 제시에 새로운 가능성을 제공합니다.

### [RuOpinionNE-2024: Extraction of Opinion Tuples from Russian News Texts](https://arxiv.org/abs/2504.06947)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.06947.png)

Vote: 3

Authors: Anna Lapanitsyna, Natalia Tkachenko, Natalia Loukachevitch, Nicolay Rusnachenko, Mikhail Tikhomirov

- ***What's New***: RuOpinionNE-2024는 러시아 뉴스 텍스트에서 의견 튜플(Opinion Tuples)을 추출하는 새로운 공유 과제를 도입하였습니다. 이 과제는 지정된 문장에서 감정 보유자, 대상, 감정 표현 및 그에 따른 감정을 구성 요소로 하는 의견 튜플을 추출하는 것을 목표로 합니다. 100개 이상의 제출물이 있는 가운데, 많은 참가자들이 대형 언어 모델(Large Language Models; LLMs)을 zero-shot, few-shot 및 fine-tuning 방식으로 실험했습니다. 최종 테스트 세트에서 최고 성능은 대형 언어 모델의 미세 조정으로 얻어졌습니다.
- ***Technical Details***: RuOpinionNE-2024 데이터셋은 이름이 명명된 엔티티와 긍정/부정 관계의 증거를 포함하며, 의견 튜플(cluster, holder, target, polarity, expression)이 각 텍스트 조각에 할당됩니다. 이 평가의 주된 메트릭은 F1 지표입니다. 참가자들은 다양한 모델 및 prompts를 통해 zero-shot, few-shot, fine-tuning 방식으로 성능을 평가하였습니다.
- ***Performance Highlights***: VatolinAlexey 참가자는 Qwen2.5-32B-Instruct 모델을 통해 F1=0.41의 최고 성능을 기록했습니다. 다른 참가자들은 다양한 접근법과 모델을 사용하여 최종 평가 단계에서 F1=0.35까지의 성과를 얻었습니다. Semeval 2022의 뉴스 데이터셋과 비교할 때 유사한 수준의 F-측정치를 기록했습니다.

### [WildGS-SLAM: Monocular Gaussian Splatting SLAM in Dynamic Environments](https://arxiv.org/abs/2504.03886)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.03886.png)

Vote: 3

Authors: Valentin Bieri, Marc Pollefeys, Iro Armeni, Zihan Zhu, Jianhao Zheng, Songyou Peng

- ***What's New***: WildGS-SLAM은 동적 환경에서 유연하게 작동하며, 단일 모노큘러 RGB 입력만을 사용하는 3D Gaussian Splatting 기반의 SLAM 시스템입니다. 이 시스템은 동적인 방해 요소를 효과적으로 제거하고, 고정밀도 카메라 트래킹 및 3D 재구성을 달성합니다. 이는 사전 깊이 정보나 명시적 의미론적 레이블 없이도 작동하여 보다 일반화된 성능을 보여줍니다.
- ***Technical Details***: WildGS-SLAM은 사전 학습된 DINOv2 특징을 사용하여 픽셀 단위 불확실성을 예측하는 얕은 MLP(Shallow MLP)를 통해 트래킹 및 매핑 단계에서 동적 요소를 제거합니다. 3D Gaussian Splatting(3DGS) 표현을 사용하며, 밀도 번들 조정(Dense Bundle Adjustment)에서 불확실성 정보를 활용하여 신뢰 가능한 영역에 우선순위를 둡니다. 새로운 데이터셋인 Wild-SLAM Dataset을 사용하여 다양한 리얼월드 시나리오에서의 성능을 평가합니다.
- ***Performance Highlights***: 새로운 비디오 신서시스 및 트래킹 벤치마크 실험에서, WildGS-SLAM은 특히 Bonn RGB-D Dynamic Dataset 및 TUM RGB-D Dataset과 같은 동적 환경에서 기존 방법보다 우월한 성능을 보여주었습니다. ATE RMSE 평가에서 최고 성능을 기록하며, 기존 시스템들에 비해 향상된 렌더링 품질을 입증했습니다.

### [SkillWeaver: Web Agents can Self-Improve by Discovering and Honing Skills](https://arxiv.org/abs/2504.07079)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.07079.png)

Vote: 1

Authors: Michael Y. Fatemi, Gaowen Liu, Graham Neubig, Apurva Gandhi, Jayanth Srinivasa, Zora Zhiruo Wang, Yu Su, Xiaolong Jin, Yueqi Song, Yu Gu, Boyuan Zheng

- ***What's New***: SkillWeaver는 웹 에이전트(Web Agents)가 새로운 기술을 발견하고 이를 API로 정제하여 스스로 개선할 수 있는 기술 중심 프레임워크입니다. 이 프레임워크는 웹사이트 탐색을 통해 에이전트가 기술을 자동으로 발견하고 실행하며, 경험을 AP로 정제합니다. 이러한 방법은 경량의 플러그 앤 플레이 API 라이브러리를 확장하여 에이전트의 능력을 크게 향상시킵니다. 실험 결과, SkillWeaver는 WebArena 및 실제 웹사이트에서 최대 54.3%의 성능 향상을 이뤘습니다.
- ***Technical Details***: SkillWeaver는 세 가지 주요 단계로 구성된 파이프라인으로 개발되었습니다. 첫 번째는 웹 환경을 체계적으로 탐색하여 새로운 기술을 제안하는 'Skill Proposal' 단계입니다. 두 번째는 제안된 기술을 연습하고 이를 안정적인 Python 함수로 변환하는 'Skill Synthesis' 단계입니다. 세 번째는 API의 신뢰성을 보장하기 위해 테스트와 디버깅을 수행하는 'Skill Honing' 단계입니다. 이를 통해 에이전트는 스스로 기술을 제안하고, 연습하며, 이를 재사용 가능한 API로 구현할 수 있습니다.
- ***Performance Highlights***: WebArena에서 Synthesized API를 이용한 에이전트는 성공률이 평균 39.8% 증가했으며, GPT-4o-mini 기반 에이전트에서는 40%에서 133%까지 향상되었습니다. 실제 웹사이트에서도 성공률이 평균 39.8% 상승했으며, 찾을 수 있는 모든 정보를 탐색하고 정리할 수 있는 능력이 검증되었습니다. 이러한 결과는 강력한 에이전트가 생성한 API가 더 약한 에이전트의 성능을 크게 개선할 수 있음을 보여줍니다. 이는 API의 플러그 앤 플레이 모듈로 효과적인 지식 전수가 가능하다는 것을 보여줍니다.

### [Unified World Models: Coupling Video and Action Diffusion for Pretraining on Large Robotic Datasets](https://arxiv.org/abs/2504.02792)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02792.png)

Vote: 1

Authors: Raymond Yu, Benjamin Burchfiel, Paarth Shah, Chuning Zhu, Abhishek Gupta, Siyuan Feng

- ***What's New***: 이 논문에서 제안된 Unified World Models(UWM)는 비디오와 액션 데이터를 결합하여 로봇 정책을 학습하는 새로운 프레임워크입니다. 일반적인 모방 학습(imitation learning)과 세계 모델링(world modeling)을 통일하는 접근으로, 다양한 로봇 데이터셋에서 유연한 사전 학습을 가능하게 합니다.
- ***Technical Details***: UWM은 통합된 transformer 아키텍처 내에서 영상 확산 과정(video diffusion process)과 액션 확산 과정(action diffusion process)을 결합합니다. 이 통합 모델은 독립적인 diffusion timestep을 활용하여, 정책(policy), 순방향 동역학(forward dynamics), 역방향 동역학(inverse dynamics), 비디오 예측(video prediction) 등 다양한 목적으로 유연하게 사용할 수 있습니다. 이는 각 변형에 대해 독립적인 시간 단계를 제어함으로써 상이한 데이터셋으로부터 학습할 수 있도록 하고 있습니다.
- ***Performance Highlights***: UWM은 대규모 로봇 데이터셋에서 효과적인 사전 학습을 가능하게 하였으며, 행동 없는 비디오 데이터의 학습을 통해 성능을 더욱 개선합니다. 실제 로봇 실험과 시뮬레이션 실험에서 UWM은 전반적으로 정책 성능을 크게 개선했으며, 특히 다양한 설정의 도전 과제에서도 높은 성공률을 기록하여 견고성과 일반화 능력을 보였습니다.

### [Fast Controlled Generation from Language Models with Adaptive Weighted Rejection Sampling](https://arxiv.org/abs/2504.05410)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.05410.png)

Vote: 0

Authors: Li Du, Vikash Mansinghka, Benjamin Lipkin, João Loula, David R. MacIver, Benjamin LeBrun, Jason Eisner, Tim Vieira, Alexander K. Lew, Jacob Hoover Vigly, Ryan Cotterell, Timothy J. O'Donnell

- ***What's New***: 이 논문은 기존의 지역적 제약 디코딩(Locally Constrained Decoding; LCD)이 가진 문제점을 극복하기 위해 적응 가중치 거절 샘플링(Adaptive Weighted Rejection Sampling; AWRS)이라는 새로운 알고리즘을 소개합니다. 이는 대규모 단어 집합에서의 제약 평가를 현저하게 줄이고, 샘플의 전역적 편향을 교정하는 비편향 저분산의 중요도 가중치 추정을 가능하게 합니다.
- ***Technical Details***: AWRS는 제약 조건을 충족하는 문자열을 생성하는 Las Vegas 알고리즘으로, 각 단계마다 필요한 계산량을 동적으로 조정하여 더 적은 시간 내에 정확한 샘플을 생성합니다. 또한, AWRS는 비편향적인 Z 추정치를 계산하여 순차적 몬테카를로(Sequential Monte Carlo; SMC) 방법에 통합할 수 있습니다.
- ***Performance Highlights***: AWRS는 성능 평가에서 최첨단 방법들과 비교하여 제약 조건에 대한 처리 능력에서 개선된 성능을 보여줍니다. 예를 들어, '텍스트-투-SQL', '분자 합성', '패턴 매칭' 등 다양한 도메인에서 기존 방법보다 향상된 정확성을 나타내며, 특히 더 없이 최첨단 방법에 비해 실행 시간도 단축시켰습니다. 또한, AWRS는 제약 조건과 기존 언어 모델의 분산 차이에 따라 실행 시간이 조절되며, 보다 정확한 언어 모델에서 더 빠르게 작동합니다.

### [RobustDexGrasp: Robust Dexterous Grasping of General Objects from Single-view Perception](https://arxiv.org/abs/2504.05287)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.05287.png)

Vote: 0

Authors: Linyi Huang, Zijian Wu, Sammy Christen, Hui Zhang, Jie Song

- ***What's New***: RobustDexGrasp는 한 번의 관찰(single-view perception)으로 다양한 새 물체를 정밀하게 잡을 수 있는 강화학습 기반 프레임워크를 제안합니다. 이 시스템은 외부 요인으로 인해 물체의 상태가 변화하더라도 인간처럼 실시간으로 적응할 수 있습니다.
- ***Technical Details***: 손 중심의 물체 형태 표현(hand-centric object representation)을 사용하여 물체와 손가락 간의 상호작용에 관련된 지역적인 형태를 강조하고, 불확실성에 강인한 형태 특징 추출을 수행합니다. 본 프레임워크는 초기에는 모방 학습(imitation learning)을 통해 특권적 시각-촉각 정보를 사용한 정책을 전이하고, 이후 강화 학습( reinforcement learning)을 통해 관측 노이즈 및 동적 무작위화(dynamic randomization)에 대응하는 적응적인 동작을 배우게 됩니다.
- ***Performance Highlights***: RobustDexGrasp는 강화학습과 모방 학습을 혼합하여 총 247,786개의 시뮬레이션된 물체와 512개의 실제 물체에 대해 성공률 97.0%와 94.6%를 달성하였습니다. 이는 새로운 물체에 대한 강력한 일반화 능력을 보여주며, 물체 이동 및 외부 힘으로 인한 방해 요소에 대해서도 뛰어난 강인성을 보증합니다.

