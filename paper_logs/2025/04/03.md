## Daily Papers (2025-04-03)

### [MergeVQ: A Unified Framework for Visual Generation and Representation with Disentangled Token Merging and Quantization](https://arxiv.org/abs/2504.00999)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.00999.png)

Vote: 56

Authors: Luyuan Zhang, Zicheng Liu, Haonan Lu, Siyuan Li, Haoqian Wang, Juanxi Tian, Zedong Wang, Zhen Lei, Cheng Tan, Chang Yu, Qingsong Xie

- ***What's New***: MergeVQ는 시각적 생성과 표현 학습을 통합하는 새로운 VQ 기반 프레임워크입니다. MergeVQ는 표현 학습과 생성 사이의 불일치를 줄이기 위해 디스엔탱글드 토큰 병합과 양자화를 도입하였습니다.
- ***Technical Details***: MergeVQ는 자기 주의 블록 이후 토큰 병합 모듈을 통해 최상위 K개 의미를 잠재 공간에서 분리하고, 디코더에서는 복원을 위한 교차 주의를 통해 세부 정보를 복구합니다. 2차 생성 단계에서는 KV 캐시 압축을 수행하는 MergeAR를 소개하여 효율적인 래스터 순서 예측을 수행합니다.
- ***Performance Highlights***: ImageNet 실험에서 MergeVQ는 경쟁력 있는 성능을 보여주었으며, 256개의 토큰을 사용하여 0.54의 rFID 점수를 기록했고, 효과적이고 효율적인 성능을 발휘했습니다.

### [Improved Visual-Spatial Reasoning via R1-Zero-Like Training](https://arxiv.org/abs/2504.00883)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.00883.png)

Vote: 42

Authors: Haonan Lu, Zhijie Deng, Zhenyu Yang, Zijian Kong, Zhenyi Liao, Qingsong Xie, Yanhao Zhang

- ***What's New***: R1-Zero-Like 트레이닝을 통해 개선된 시각-공간 추론 능력을 보였습니다. 이러한 접근법은 Qwen2-VL 모델을 사용하여 멀티모달 대형 언어 모델(MLLMs)의 시각적-공간적 추론 역량을 향상시키는 데 중점을 두고 있습니다.
- ***Technical Details***: 본 연구에서는 Qwen2-VL 모델군을 중심으로 VS-GRPO(Group Relative Policy Optimization) 훈련을 수행, VSI-100k라는 대규모 비디오 기반 질문-답변 데이터셋을 활용했습니다. 이는 세밀하게 주석된 3D 객체 정보를 기반으로 한 100k 이상의 샘플로 구성됩니다. 훈련 시, CoT(Chain of Thought) 프롬프트가 비슷한 규모의 모델에서는 비효율적임을 밝혀냈으며, VS-GRPO를 활용하여 모델의 추론 능력을 향상시켰습니다. 또한, GRPO 훈련 과정에서 KL penalty를 유지하는 것이 필요함을 확인했습니다.
- ***Performance Highlights***: 120 GPU 시간 내에, VS-GRPO-2B 모델은 기본 Qwen2-VL-2B 모델을 12.1% 개선하였으며 GPT-4o를 능가하는 성능을 달성했습니다. 또한, vsGRPO-7B 모델은 최고의 오픈 소스 모델인 LLaVA-NeXT-Video-72B에 필적하는 성능을 보였습니다. GRPO는 감독학습 및 DPO와 비교했을 때 시각적-공간적 추론 능력 향상에 대한 확실한 우위를 입증했습니다.

### [AnimeGamer: Infinite Anime Life Simulation with Next Game State Prediction](https://arxiv.org/abs/2504.01014)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.01014.png)

Vote: 29

Authors: Junhao Cheng, Jing Liao, Ying Shan, Yuying Ge, Yixiao Ge

- ***What's New***: AnimeGamer는 MLLM(Multimodal Large Language Models)을 사용하여 무한 애니메이션 라이프 시뮬레이션 게임 상태를 생성하는 새로운 모델입니다. 이 모델은 캐릭터 상태 업데이트와 동적인 애니메이션 샷을 포함하여 게임 진행을 보다 일관성 있고 몰입감 있게 만드는 데 중점을 두고 있습니다.
- ***Technical Details***: AnimeGamer는 MLLM을 사용하여 다음 게임 상태를 예측하는데, 액션 인식 멀티모달 표현(action-aware multimodal representations)을 도입하여 애니메이션 샷을 고품질 비디오 클립으로 디코딩합니다. 이 모델은 역사적인 애니메이션 샷 표현을 문맥으로 사용하여 일관된 게임플레이를 유지합니다. 또한, 효율적인 데이터 수집 파이프라인을 구축하여 사용자가 선호하는 캐릭터로 게임을 커스터마이즈할 수 있도록 지원합니다.
- ***Performance Highlights***: AnimeGamer는 기존 방법들보다 캐릭터 일관성, 문맥적 일관성 및 전반적인 게임 경험에서 뛰어난 성능을 보입니다. 자동화된 메트릭과 사용자 평가를 통해 이 모델의 활용 가능성과 우수성을 입증하였으며, 거의 모든 평가 지표에서 불선형 모델들보다 우수한 성능을 기록하였습니다.

### [DreamActor-M1: Holistic, Expressive and Robust Human Image Animation with Hybrid Guidance](https://arxiv.org/abs/2504.01724)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.01724.png)

Vote: 24

Authors: Zhengkun Rong, Yongming Zhu, Lizhen Wang, Yuxuan Luo, Tianshu Hu, Longhao Zhang

- ***What's New***: DreamActor-M1은 복합 가이던스(hybrid guidance)를 통해 정밀하고 전체적인 제어 가능성을 제공하는 DiT 기반 인간 이미지 애니메이션 프레임워크를 제안합니다. 이 프레임워크는 다중 스케일 적응력과 장기적인 시간 일관성을 달성하여 이전의 접근 방식들이 가지고 있던 한계를 극복합니다.
- ***Technical Details***: DreamActor-M1은 암묵적인 얼굴 표현(implicit facial representations), 3D 헤드 구체(head spheres), 3D 바디 스켈레톤(bone skeleton)을 사용하여 얼굴 표정과 신체 움직임을 견고하게 제어하는 혼합 제어 신호를 설계하였습니다. 다양한 포즈와 이미지 스케일을 처리하기 위해 트레이닝 데이터에 다양한 해상도와 스케일을 포함한 점진적인 트레이닝 전략을 사용합니다. 또한, 복합 시각 참조를 통해 보이지 않는 영역에서 장기적인 시간적 일관성을 보장합니다.
- ***Performance Highlights***: 실험 결과, DreamActor-M1은 최첨단 작품들과 비교할 때 탁월한 표현력을 보여주며, 초상화, 상반신, 전신 생성에서 견고한 장기 일관성을 유지합니다. 다양한 테스트에서 FID, SSIM, PSNR, LPIPS, FVD 등의 평가 지표에서 기존 방법을 능가하는 성능을 입증하였습니다.

### [Understanding R1-Zero-Like Training: A Critical Perspective](https://arxiv.org/abs/2503.20783)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.20783.png)

Vote: 24

Authors: Tianyu Pang, Wenjun Li, Changyu Chen, Wee Sun Lee, Min Lin, Zichen Liu, Chao Du, Penghui Qi

- ***What's New***: 이 논문은 DeepSeek-R1-Zero의 강화학습(Reinforcement Learning; RL)을 비판적으로 검토하고, 이를 통해 대규모 언어 모델(LLMs)의 추론 능력을 개선할 수 있는 방법을 탐구합니다. 특히, Dr. GRPO라는 새로운 최적화 방법을 제안하여 기존의 GRPO의 편향 문제를 해결함으로써 토큰 효율성을 높입니다.
- ***Technical Details***: R1-Zero-like 훈련은 Qwen2.5와 같은 다양한 기본 모델의 사전 학습 특성을 분석하여, 강화학습이 각 모델에 미치는 영향을 조사합니다. 강화학습 단계에서는 그룹 상대 정책 최적화(Group Relative Policy Optimization; GRPO)에서 발견된 최적화 편향을 제거하기 위해 Dr. GRPO가 제안되었습니다. Dr. GRPO는 응답 길이에 민감하지 않도록 편향 없는 최적화 기법을 채택하여 학습 중 점진적인 오류 응답 길이 증가를 억제합니다.
- ***Performance Highlights***: Dr. GRPO를 적용한 최소화된 R1-Zero 레시피는 AIME 2024에서 7B 기본 모델로 43.3%의 정확도를 달성하여 새로운 최고 성과를 달성했습니다. 실험 결과는 강화학습이 응답 길이와 관계 없이 정확도를 향상시킬 수 있음을 보여주었습니다.

### [VideoScene: Distilling Video Diffusion Model to Generate 3D Scenes in One Step](https://arxiv.org/abs/2504.01956)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.01956.png)

Vote: 22

Authors: Fangfu Liu, Hanyang Wang, Yueqi Duan, Jiawei Chi

- ***What's New***: VideoScene는 비디오 디퓨전 모델을 3D 장면을 한 번에 생성할 수 있도록 증류(distilling)하는 새로운 프레임워크입니다. 기존의 긴 지연 시간과 3D 제약 부족 문제를 극복하여 더욱 효율적이고 효과적입니다. 특히, 3D 우선순위(prior)를 활용해 빠른 한 스텝으로 3D 장면을 생성할 수 있습니다.
- ***Technical Details***: VideoScene은 3D 인식 leap flow 증류 전략을 통해 시간 소모적 감소 단계를 건너뛰며, 적응적 해석을 위해 동적 노이즈 제거 정책 네트워크(Dynamic Denoising Policy Network; DDPNet)를 설계했습니다. 이것은 입력 이미지 쌍과 대응하는 카메라 포즈를 기반으로 장면을 생성하며, 3D 일관성을 유지하면서 정보 유지 및 노이즈 추가의 균형을 맞춥니다.
- ***Performance Highlights***: VideoScene은 기존의 비디오 디퓨전 모델들을 뛰어넘어 다양한 실제 데이터 세트에서 충실도와 속도 면에서 우수한 성과를 거뒀습니다. 비디오가 나타내는 3D 구조의 일관성을 유지하면서 고속의 샘플링을 가능하게 하여 향후 비디오에서 3D 응용으로의 전환 시 유용한 도구가 될 잠재력을 보여줍니다.

### [Towards Physically Plausible Video Generation via VLM Planning](https://arxiv.org/abs/2503.23368)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.23368.png)

Vote: 21

Authors: Zhenfei Yin, Tien-Tsin Wong, Lei Bai, Xindi Yang, Huchuan Lu, Baolu Li, Yiming Zhang, Xu Jia, Jianfei Cai, Zhiyong Wang, Liqian Ma

- ***What's New***: 이 논문은 비전 언어 모델(Vision Language Model; VLM)을 활용하여 물리적으로 타당한 비디오 생성을 목표로 하는 새로운 프레임워크를 제안합니다. 두 단계로 구성된 이 프레임워크는 VLM을 사용하여 물리학을 인식하고, 비디오 확산 모델(Video Diffusion Model; VDM)과 결합하여 현실 세계의 역학을 반영하는 비디오를 생성합니다.
- ***Technical Details***: 이 연구에서는 두 단계의 이미지-비디오 생성 프레임워크를 제안합니다. 첫 번째 단계에서는 VLM이 물리학 인식 추론을 통하여 객체의 예비적인 움직임 경로(trajectory)를 예측합니다. 두 번째 단계에서는 VDM이 VLM의 예측 내용을 활용하여 세밀한 모션을 생성합니다. 특히, 움직임 경로에 노이즈를 추가하여 VDM의 디테일한 움직임 생성을 돕고자 합니다.
- ***Performance Highlights***: 제안된 프레임워크는 두 개의 주요 비디오 물리 벤치마크에서 기존 방법들을 능가하며 우수한 결과를 보여줍니다. 특히 두 가지 평가 지표에서 기존 경쟁 모델보다 평균적으로 각각 15.3%와 11.1% 성능 향상을 기록하였습니다.

### [PaperBench: Evaluating AI's Ability to Replicate AI Research](https://arxiv.org/abs/2504.01848)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.01848.png)

Vote: 18

Authors: Tejal Patwardhan, Oliver Jaffe, Benjamin Kinsella, Wyatt Thompson, Jun Shern Chan, Leon Maksin, Evan Mays, James Aung, Amelia Glaese, Johannes Heidecke, Dane Sherburn, Rachel Dias, Giulio Starace

- ***What's New***: 이 논문에서는 PaperBench라는 새로운 벤치마크를 소개하며, AI 에이전트가 첨단 AI 연구를 복제할 수 있는 능력을 평가합니다. PaperBench는 ICML 2024의 스포트라이트 및 구두 발표 논문 중 20개의 논문을 선택하여, 각 논문의 기여를 이해하고, 코드베이스를 개발하며, 실험을 성공적으로 실행할 수 있는 AI 에이전트를 평가합니다.
- ***Technical Details***: PaperBench 벤치마크는 8,316개의 개별 과제로 구성되어 있으며, 각 논문의 원작자들과 공동 개발한 루브릭을 사용하여 측정합니다. AI 에이전트는 원본 코드베이스를 사용하거나 참조하는 것이 금지되며, 모델의 능력을 코드 작성 및 복잡한 실험을 수행할 수 있는지 평가합니다. 이를 위해 LLM 기반의 자동 평가 시스템을 개발하고 JudgeEval이라는 별도의 벤치마크로 자동 평가 시스템의 성능을 평가합니다.
- ***Performance Highlights***: PaperBench에서 가장 성능이 우수한 Claude 3.5 Sonnet(New)은 평균 복제 점수 21.0%를 기록하였으며, 사람이 수행한 결과와 비교하여 아직 AI 모델의 성능이 인간 전문가를 능가하지 못함을 보여줍니다. 이러한 결과는 AI 에이전트가 복잡한 연구 과제를 수행하는 데 있어 아직 많은 과제가 남아 있음을 시사합니다.

### [ScholarCopilot: Training Large Language Models for Academic Writing with Accurate Citations](https://arxiv.org/abs/2504.00824)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.00824.png)

Vote: 18

Authors: Wenhu Chen, Xueguang Ma, Ping Nie, Yi Lu, Xiang Yue, Yubo Wang, Huaye Zeng, Zhiheng Lyu, Yuxuan Zhang, Benjamin Schneider

- ***What's New***: ScholarCopilot은 학술 논문 작성에 필요한 정확한 인용을 제공하기 위해 대형 언어 모델(Large Language Models; LLMs)을 강화하는 통합 프레임워크입니다. 기존의 정적 검색-생성 방식과 달리, ScholarCopilot은 진행 중인 생성 맥락에 맞춰 필요할 때 [RET] 토큰을 생성하여 인용 검색을 동적으로 관리하는 혁신적인 접근을 제안합니다.
- ***Technical Details***: ScholarCopilot은 학술 논문에서 동적인 참조 검색을 위해 대조 학습(Contrastive Learning)을 활용하여 검색 토큰의 밀집 표현(Dense Representations)을 최적화합니다. 검색된 참고 문헌은 생성 과정 중 통합되어 효율성을 높이며, 검색과 생성을 동시에 최적화하여 추론 오버헤드를 줄이고 인용 정확성을 높입니다. 500K 편의 arXiv 논문을 사용하여 학습되었으며, 학술적 작성 관행을 강화합니다.
- ***Performance Highlights***: ScholarCopilot은 평가 데이터셋에서 상위 1위(top-1) 검색 정확도 40.1%를 기록하여, E5-Mistral-7B-Instruct(15.0%)와 BM25(9.8%)를 크게 상회합니다. 또한, 1,000개의 학술 작성 샘플에서, 관련성, 응집성, 학문적 엄격함, 완전성, 혁신성에서 16.2/25의 점수를 기록하며, 더 크기가 큰 모델보다 우수한 성능을 나타냅니다. 사용성 연구에서도 참가자들은 ScholarCopilot의 인용 정확성과 작성 효율성에 높은 평가를 내렸습니다.

### [ILLUME+: Illuminating Unified MLLM with Dual Visual Tokenization and Diffusion Refinement](https://arxiv.org/abs/2504.01934)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.01934.png)

Vote: 16

Authors: Wei Zhang, Lu Hou, Chunwei Wang, Yunlong Yuan, Runhui Huang, Jianhua Han, Junwei Yang, Hengshuang Zhao, Lanqing Hong, Hang Xu, Guansong Lu

- ***What's New***: ILLUME+는 시각적 이해, 생성, 편집을 통합한 향상된 멀티모달 대형 언어 모델입니다. 새로운 듀얼 비주얼 토큰화(Dual Visual Tokenization; DualViTok)와 디퓨전 디코더(Diffusion Decoder)를 사용하여 이미지 생성 품질과 고해상도 처리 기능을 개선했습니다. 이를 통해 다양한 과제에서 유연하고 효율적인 시각적 편집과 생성이 가능합니다.
- ***Technical Details***: ILLUME+는 듀얼 비주얼 토크나이저인 DualViTok을 도입하여 심층 의미와 세밀한 텍스처를 모두 보존합니다. 시맨틱 브랜치는 미리 훈련된 텍스트 정렬 비전 인코더를 사용하고, 픽셀 브랜치는 CNN 기반의 픽셀 인코더를 포함합니다. 이미지 디토크나이징을 위해 디퓨전 모델을 사용하며, 연속 입력, 이산 출력 방식으로 비전을 처리합니다. 점진적 훈련 절차를 통해 동적 해상도 지원을 가능하게 합니다.
- ***Performance Highlights***: ILLUME+는 기존의 통합 모델들과 전문 모델들에 비해 경쟁력 있는 성능을 보이며, 특히 문서 관련 작업에서 높은 성과를 보입니다. MJHQ-30K 벤치마크에서 6.00 FID 점수를 기록하여 고화질 이미지 생성에서 뛰어난 품질과 다양성을 제공합니다. 다양한 멀티모달 이해, 생성, 편집 과제에서 우수한 성능을 보여줍니다.

### [Articulated Kinematics Distillation from Video Diffusion Models](https://arxiv.org/abs/2504.01204)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.01204.png)

Vote: 12

Authors: Chenfanfu Jiang, Donglai Xiang, Xuan Li, Qianli Ma, Yongxin Chen, Ming-Yu Liu, Tsung-Yi Lin

- ***What's New***: 이번 논문에서는 Articulated Kinematics Distillation (AKD)이라는 새로운 프레임워크를 소개합니다. 이 프레임워크는 스켈레톤 기반 애니메이션과 현대적 생성 모델의 장점을 결합하여 고품질 캐릭터 애니메이션을 생성합니다. 특히, 학습된 큰 비디오 확산 모델(Video Diffusion Models)로부터 복잡한 관절 동작을 추출하고 이를 물리 기반 시뮬레이션에 자연스럽게 결합할 수 있다는 점이 새롭습니다.
- ***Technical Details***: AKD는 스켈레톤 기반 표상을 사용하여 3D 자산의 조인트 레벨 제어에 초점을 맞춰 자유도(Degrees of Freedom; DoFs)를 줄입니다. 이는 Score Distillation Sampling (SDS)을 통해 사전에 학습된 비디오 확산 모델로부터 관절 동작 시퀀스를 추출하는 작업을 간소화합니다. 또한 이 방법은 물리적 기본에 기반한 시뮬레이션과 호환되어 물리적으로 타당한 인터랙션을 제공합니다.
- ***Performance Highlights***: 실험 결과 AKD는 기존 텍스트를 4D로 변환하는 방식보다 더 나은 3D 형태 일관성과 표현력 높은 동작을 보여주었습니다. VideoPhy에 의해 평가된 자동 점수에서 AKD는 TC4D 방법보다 우수한 점수를 기록하며 전반적으로 더 나은 품질의 모션을 생성한다는 것이 입증되었습니다.

### [Safeguarding Vision-Language Models: Mitigating Vulnerabilities to Gaussian Noise in Perturbation-based Attacks](https://arxiv.org/abs/2504.01308)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.01308.png)

Vote: 11

Authors: Yichun Feng, Yushen Zuo, Kin-man Lam, Yichen Fu, Yuanjun Chai, Jiawei Wang, Zhendong Liu

- ***What's New***: 이 연구는 Vision-Language Models(VLMs)의 Gaussian 노이즈에 대한 취약성을 분석하고, 이에 대응하기 위해 Robust-VLGuard라는 새로운 멀티모달 안전 데이터셋을 제안하였습니다. Gaussian 노이즈가 포함된 데이터로 미세 조정하는 기법을 통해 VLM의 안전 정렬을 개선하고 도움의 유용성을 유지합니다. 또한, DiffPure-VLM이라는 새로운 방어 체계를 통해 최적화 기반 공격 및 Gaussian 노이즈에 견딜 수 있는 강화된 보호를 제공합니다.
- ***Technical Details***: Robust-VLGuard 데이터셋은 이미지와 텍스트의 정렬 및 비정렬 안전 시나리오로 구성되며, 4,467개의 일반 질문 및 문답 지시사항을 포함합니다. 실험에서는 Gaussian 노이즈로 미세 조정된 VLM이 다양한 강도의 적대적 공격 및 Gaussian 노이즈에 대해 어떻게 대응하는지 확인하였습니다. DiffPure-VLM은 diffusion models을 사용하여 적대적 노이즈를 Gaussian 유사 노이즈로 변환하고, 노이즈 강화 정돈된 VLM의 저항력을 끌어올립니다.
- ***Performance Highlights***: 실험 결과, DiffPure-VLM은 기본 방어 방식에 비해 Gaussian 및 적대적 노이즈에 대해 더 우수한 성능을 보여줍니다. Gaussian 노이즈 조건에서도 VLM의 원래 성능을 상당 부분 유지시키면서 공격 성공률을 감소시켰습니다. 다만, 높은 강도의 공격에서는 더 많은 diffusion steps이 요구된다는 사실이 확인되었습니다.

### [Boost Your Own Human Image Generation Model via Direct Preference Optimization with AI Feedback](https://arxiv.org/abs/2405.20216)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2405.20216.png)

Vote: 10

Authors: Sanghyeon Na, Yonggyu Kim, Hyunjoon Lee

- ***What's New***: HG-DPO는 Direct Preference Optimization(DPO)을 사용하여 인간 이미지 생성의 현실감을 개선하는 새로운 접근 방법을 제안합니다. 기존 DPO 방식의 제한을 극복하기 위해 실제 고품질 이미지를 승리 이미지로 사용하여 생성된 이미지가 아닌 진짜 이미지를 목표로 하는 시스템을 설계했습니다.
- ***Technical Details***: HG-DPO는 커리큘럼 학습(Curriculum Learning) 프레임워크를 채택하여 모델이 점진적으로 인간 이미지의 현실감을 높이도록 훈련합니다. 이 접근은 3단계의 점진적 도전 과제를 포함하며, 각 단계는 Easy, Normal, Hard 스테이지로 구성됩니다. 각 단계에서 모델은 생성 이미지와 서로 다른 도메인의 승리 이미지를 사용하여 점진적으로 개선됩니다. 이 구조는 생성적 적대 신경망(GAN)의 훈련 메커니즘을 확산 모델에 통합하여 현실성을 높입니다.
- ***Performance Highlights***: HG-DPO는 기존 방법들에 비해 뛰어난 성능을 보입니다. 예를 들어, Pick-a-Pic v2와 비교하여, HG-DPO는 P-Score와 CLIP 점수에서 더 높은 성능을 보이며, FID 점수도 더 낮아 더 높은 이미지 품질과 현실성을 보여줍니다. 또한, 다양한 이미지 생성 및 개인화된 텍스트-이미지 작업에서 고품질의 식별성 높은 이미지를 추가적인 훈련 없이 생성할 수 있습니다.

### [DASH: Detection and Assessment of Systematic Hallucinations of VLMs](https://arxiv.org/abs/2503.23573)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.23573.png)

Vote: 9

Authors: Matthias Hein, Maximilian Augustin, Yannic Neuhaus

- ***What's New***: DASH는 시각 언어 모델(Vision Language Models; VLM)의 체계적인 환각(Systematic Hallucinations)을 자동으로 대규모로 검출하고 평가하는 새로운 파이프라인입니다. 이전의 벤치마크가 제한된 레이블 데이터 세트로 환각을 평가했던 것과 달리, DASH는 실제 이미지에서 VLM이 체계적으로 환각하는 상황을 식별합니다.
- ***Technical Details***: DASH는 크게 두 가지 구성 요소로 나뉩니다. DASH-OPT는 VLM을 오도하도록 최적화된 이미지를 생성하는 기술이며, 이 과정에서 '자연 이미지 매니폴드(Natural Image Manifold)'를 활용합니다. 반면, DASH-LLM은 대형 언어 모델(LLM)에 의해 생성된 텍스트 기반 쿼리를 사용해 유사한 이미지 군집을 탐색합니다. 이러한 방법을 통해 VLM이 특정 객체를 환각하는 이미지를 대규모로 식별하고, 이미지 중복이나 비슷한 이미지들을 클러스터링하여 최종 집합을 구성합니다.
- ***Performance Highlights***: DASH는 PaliGemma 및 LLaVA-NeXT 등의 모델에 걸쳐 380개 객체 클래스의 이미지 중 총 19,000개 이상의 환각 클러스터와 950,000개 이상의 이미지를 발견했습니다. 이 중 상당한 수의 체계적인 환각이 QwenV2-72B와 같은 다른 VLM들로도 전이되었습니다. DASH-B라는 새로운 벤치마크는 이러한 문제를 더 신뢰성 있게 평가할 수 있도록 개발되었습니다.

### [YourBench: Easy Custom Evaluation Sets for Everyone](https://arxiv.org/abs/2504.01833)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.01833.png)

Vote: 6

Authors: Clémentine Fourrier, Gokhan Tur, Sumuk Shashidhar, Alina Lozovskia, Dilek Hakkani-Tür, Thomas Wolf

- ***What's New***: YourBench는 대규모 언어 모델(LLMs)의 평가에 있어서 전통적인 정적 벤치마크의 한계를 극복하기 위한 새로운 오픈 소스 프레임워크입니다. 사용자 제공 문서로부터 동적으로 신뢰할 수 있는, 최신의, 도메인별 맞춤형 벤치마크를 자동으로 생성할 수 있어, 비용 효율적이고 수작업 주석 없이도 가능합니다.
- ***Technical Details***: YourBench는 문서-평가 생성(Document-to-Evaluation Generation; D2EG) 원칙을 기반으로 LLM을 활용하여 다양한 질문-답 사례를 생성하며, 커버리지, 다양성, 응답 가능성을 최적화합니다. TEMPORA-0325라는 새로운 데이터셋을 도입하여 데이터를 모델의 사후 파라미터 지식에 의존하지 않고 입력 내용을 기반으로 하도록 보장합니다. 또한, 문서 전처리, 의미 청크화, 질문 생성 및 품질 제어를 위한 강력한 파이프라인을 통합합니다.
- ***Performance Highlights***: YourBench는 7개의 MMLU 하위 집합을 복제하여 초기 벤치마크에서 관찰된 상대적 모델 성능 순위를 완벽하게 유지하면서도 더 어렵고 오염에 강한 평가를 생성하였습니다. 이는 LLM의 실제 능력을 더 정확하게 이해하고 추적할 수 있도록 합니다. 총 26개의 SoTA 모델을 검증했으며, Human 평가와 알고리즘 확인을 통해 생성된 평가의 높은 품질을 입증하였습니다.

### [LSNet: See Large, Focus Small](https://arxiv.org/abs/2503.23135)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.23135.png)

Vote: 4

Authors: Guiguang Ding, Ao Wang, Zijia Lin, Jungong Han, Hui Chen

- ***What's New***: LSNet은 인간의 비전 시스템에서 영감을 받아 'See Large, Focus Small' 전략을 도입한 경량 비전 네트워크입니다. 이 전략은 넓은 시야로 주변을 인지하고, 특징적인 부분을 집중적으로 집계하여 시각 정보를 효과적으로 처리합니다.
- ***Technical Details***: LSNet은 대형 커널에 의한 인지(Large-Kernel Perception; LKP)와 작은 커널에 의한 집계(Small-Kernel Aggregation; SKA)를 결합한 LS convolution을 사용합니다. 대형 커널은 넓은 주변 정보를 수집하고, 작은 커널은 관련성이 높은 시각 영역에서 세부 정보를 집계합니다.
- ***Performance Highlights***: LSNet은 ImageNet-1K 데이터셋에서 최신 경량 네트워크에 비해 우수한 성능과 효율성을 보여주었습니다. 예를 들어, LSNet-B는 다른 첨단 모델보다 탁월한 Top-1 정확도와 더욱 빠른 추론 속도를 기록하며, 다양한 비전 태스크들에서도 경쟁력을 입증했습니다.

### [VerifiAgent: a Unified Verification Agent in Language Model Reasoning](https://arxiv.org/abs/2504.00406)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.00406.png)

Vote: 2

Authors: Ehsan Shareghi, Wray Buntine, Jiuzhou Han

- ***What's New***: VerifiAgent는 대형 언어 모델(Large Language Models; LLMs)이 추론 작업에서 불완전하거나 오류가 있는 결과를 생성하는 문제를 해결하기 위해 개발된 통합 검증 에이전트입니다. 메타 검증(meta-verification) 및 도구 기반 적응 검증(tool-based adaptive verification)이라는 두 가지 수준의 검증을 통합하여 다양한 검증 시나리오에 대한 효율성과 견고성을 보장합니다.
- ***Technical Details***: VerifiAgent는 메타 검증을 통해 응답의 완전성과 일관성을 평가하고, Python 해석기(Python Interpreter), 기호 솔버(symbolic solvers), 검색 엔진(search engines) 등의 다양한 외부 도구를 통해 적절한 검증 도구를 자동으로 선택하여 수학적, 논리적, 상식적 추론 작업을 수행합니다. 이 접근법은 기존의 방법론보다 더 나은 검증 정확도를 달성하며 추론 정확성을 향상시킬 수 있습니다.
- ***Performance Highlights***: VerifiAgent는 수학, 논리, 상식 및 혼합 추론 작업에서 기본적인 검증 방법들보다 나은 성능을 보입니다. 특히, 수학적 추론 분야에서 추론 작업을 위한 기존 프로세스 보상 모델들보다 더 적은 비용으로 더 나은 결과를 제공함으로써 비용 효율성을 입증했습니다.

### [Quamba2: A Robust and Scalable Post-training Quantization Framework for Selective State Space Models](https://arxiv.org/abs/2503.22879)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.22879.png)

Vote: 2

Authors: Mohamed S. Abdelfattah, Natalia Frumkin, Chi-Chih Chang, Diana Marculescu, Hung-Yueh Chiang, Kai-Chiang Wu

- ***What's New***: Quamba2는 선택적 상태 공간 모델(Selective State Space Models; SSMs)에 맞춰 설계된 강력하고 확장 가능한 사후 트레이닝 양자화(Post-training Quantization; PTQ) 프레임워크입니다. 이 프레임워크는 Mamba1과 Mamba2 백본에서 W8A8, W4A8, W4A16을 지원하며, 다양한 플랫폼에서 SSM 배포의 증가하는 수요를 충족하도록 설계되었습니다.
- ***Technical Details***: Quamba2는 입력 데이터의 채널 순서를 유지하고 활성화의 지속성을 바탕으로 '정렬 및 클러스터링(sort-and-cluster)'과 '각 상태 그룹 양자화(per-state-group quantization)' 방법을 제안하여 8비트로 입력을 양자화합니다. 이 과정에서 특정 SSM 입력 종속 매개변수에 대해 상태 그룹별 양자화를 적용하여 양자화 정확도를 높이고 있습니다. 오프라인 클러스터를 고려한 가중치 재배열을 통해 채널의 다양한 값 범위를 공유하며 양자화 정확도를 개선합니다.
- ***Performance Highlights***: Quamba2는 Prefilling 단계에서 최대 1.3배, 생성 단계에서 최대 3배의 속도 향상을 이루며, 메모리 사용량을 4배 줄이면서도 평균 정확도가 1.6% 감소하는 수준에 그쳤습니다. MMLU 대형 멀티태스킹 데이터셋에서도 Quamba2의 일반화와 강건성이 입증되었습니다. Quamba2는 다양한 비트 폭 설정에서 뛰어난 성능과 실용적인 속도 향상을 제공하여 SSM 기반 애플리케이션의 배포를 지원합니다.

### [MegaTTS 3: Sparse Alignment Enhanced Latent Diffusion Transformer for Zero-Shot Speech Synthesis](https://arxiv.org/abs/2502.18924)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.18924.png)

Vote: 1

Authors: Xiaoda Yang, Ziyue Jiang, Jialong Zuo, Zhou Zhao, Rui Liu, Bai Jionghao, Yu Zhang, Xiang Yin, Yi Ren, Chen Zhang, Shengpeng Ji, Zhenhui Ye, Boyang Zhang, Ruiqi Li

- ***What's New***: MegaTTS 3는 음성텍스트 정렬의 어려움을 낮추고 자연스러움을 유지하기 위한 혁신적인 sparse alignment 알고리즘을 특징으로 하는 새로운 TTS 시스템입니다. 이를 통해 기존의 암시적 정렬과 사전 정의된 정렬의 강점을 결합할 수 있습니다.
- ***Technical Details***: MegaTTS 3는 latent diffusion transformer (DiT)에 sparse alignment를 적용하여 음성텍스트 정렬을 개선합니다. phoneme은 강제 정렬 영역 내에 sparse하게 배치되어 있으며, 이를 통해 대략적인 발음 정보를 제공하고, 이는 latent DiT 모델에 의해 정교화됩니다. 또한, multi-condition classifier-free guidance (CFG) 전략을 도입하여 악센트 강도 조절을 지원하며, piecewise rectified flow (PeRFlow) 기술을 사용하여 생성 과정을 가속화합니다.
- ***Performance Highlights***: MegaTTS 3는 LibriSpeech test-clean 세트에서 8단계 sampling으로 상위 수준의 음성 명료성과 화자 유사성을 달성하며, 높은 자연스러움도 보장합니다. 실험 결과, MegaTTS 3는 기존의 암시적 정렬 방법보다 우수한 성능을 보였으며, 특히 강제 정렬 방식을 사용하는 다른 모델과 비교하여 더 높은 탄력성을 나타냈습니다.

### [Enhanced OoD Detection through Cross-Modal Alignment of Multi-Modal Representations](https://arxiv.org/abs/2503.18817)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18817.png)

Vote: 1

Authors: Sangheum Hwang, Jeonghyeon Kim

- ***What's New***: 이 연구에서는 다중 모달 표현(Multi-Modal Representations)의 교차 모달 정렬(Cross-Modal Alignment)을 통해 비분포(out-of-distribution; OoDD) 검출 성능을 향상시키는 방법을 제안합니다. 이는 이미지 및 텍스트 임베딩의 거리를 정규화함으로써 사전 학습된 텍스트 정보 활용을 개선하는 새로운 다중 모달 파인 튜닝(Multi-Modal Fine-Tuning; MMFT) 방법론을 제안합니다.
- ***Technical Details***: 제안된 방법은 이미지와 텍스트의 임베딩을 정렬하여 서로 다른 모달리티의 유사한 의미 체계를 초구면(hypersphere) 상에서 밀접하게 배치하는 방식으로 이루어져 있습니다. 이 목표를 달성하기 위해 모든 모달리티에 대해 대비 손실(contrastive loss)과 추가적인 CMA 정규화 손실을 사용합니다. 이 방법론은 에너지 기반 모델(Energy-Based Model; EBM)의 최대 우도 추정에 해당하며, 이는 EBM 프레임워크를 멀티모달 영역으로 확장하는 데 효과적입니다.
- ***Performance Highlights***: ImageNet-1k OoD 벤치마크 데이터를 활용한 실험에서 제안된 방법은 사후적 OoD 접근법(Post-Hoc OoDD Approaches)과 결합하여 최첨단 OoD 성능 및 높은 ID 정확도를 달성하였습니다. 특히, MOS 벤치마크 및 OpenOOD v1.5 데이터셋에서 기존 방법을 능가하는 성과를 보였습니다.

### [Medical large language models are easily distracted](https://arxiv.org/abs/2504.01201)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.01201.png)

Vote: 1

Authors: Eric Karl Oermann, Jin Vivian Lee, Krithik Vishwanath, Anton Alyakin, Daniel Alexander Alber, Douglas Kondziolka

- ***What's New***: 이 논문에서는 Large Language Models (LLMs)의 의료 분야에서의 성능에 잡음(Noise)이 미치는 영향을 평가하기 위해 MedDistractQA라는 벤치마크를 개발했습니다. 이 벤치마크는 USMLE 스타일의 질문에 실제 임상 환경에서 자주 발생할 수 있는 잡음 요소를 시뮬레이션하여 삽입함으로써, LLM이 임상 환경에서 얼마나 잡음에 민감한지를 정량화합니다.
- ***Technical Details***: MedDistractQA는 MedQA를 기반으로 하여 제작되었으며, 이 벤치마크는 MedQA 질문에 임상적으로 무관한 요소를 추가하여 LLM이 임상 환경에서 혼란을 겪지 않고 정확하게 진단할 수 있는지를 평가합니다. 이 벤치마크는 MedDistractQA-Nonliteral과 MedDistractQA-Bystander라는 두 가지 유형의 잡음을 포함합니다. 또한, Retrieval Augmented Generation (RAG)이 임상 정보를 제공하면서 잡음을 혐오도록 얼마나 효과적인지 평가합니다.
- ***Performance Highlights***: 실험 결과, MedDistractQA를 적용했을 때 LLM의 정확도가 17.9%까지 감소할 수 있음을 확인했습니다. 특히, 오픈 소스 모델은 독점 모델에 비해 더 많은 영향을 받아서 성능 저하가 컸습니다. 방향성 연구는 모델의 기본 성능이 높을수록 잡음에 대한 저항력이 크다는 것을 보여주었습니다. RAG는 정보를 추가하면서 오히려 모델의 성능을 저하시킬 수 있는 것으로 밝혀졌습니다.

### [Adaptive Layer-skipping in Pre-trained LLMs](https://arxiv.org/abs/2503.23798)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.23798.png)

Vote: 0

Authors: Weizhi Wang, Xuan Luo, Xifeng Yan

- ***What's New***: 이번 연구는 대형 언어 모델(LLM)에서 토큰을 생성할 때 변동되는 계산 요구를 동적으로 조정하는 FlexiDepth를 소개합니다. FlexiDepth는 플러그인 라우터(router)와 어댑터(adapter)를 통해 트랜스포머 계층을 건너뛰는 적응형 계층 스킵핑을 가능하게 하여 LLM의 원본 파라미터를 변경하지 않고도 효율성을 개선합니다.
- ***Technical Details***: FlexiDepth는 각 트랜스포머 계층에서 플러그인 라우터와 어댑터를 통합하여 입력을 전진할지 또는 계층을 건너뛸지를 결정합니다. 라우터는 RMSNorm으로 정규화된 입력 숨겨진 상태를 처리하여 게이팅 점수를 계산하고, 어텐션 및 FFN 모듈을 건너뛴 숨겨진 상태를 변환하여 처리된 숨겨진 상태와 결합합니다. KV 캐시를 유지함으로써 모든 숨겨진 상태에 대한 전체 문맥을 보존하여 일관성을 유지합니다.
- ***Performance Highlights***: FlexiDepth는 LLMs에 대하여 32개의 계층 중 평균 8개를 건너뛰어도 성능을 100% 이상 유지하며, 특히 multi-token 생성 작업에서 기존 방법보다 우수한 결과를 보여줍니다. 예를 들어, GSM8K와 HumanEval 작업에서 FlexiDepth는 평균 성능 유지율 100.7%를 기록하여 기존의 계층 스킵핑 방법들을 능가했습니다.

### [Target-Aware Video Diffusion Models](https://arxiv.org/abs/2503.18950)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18950.png)

Vote: 0

Authors: Taeksoo Kim, Hanbyul Joo

- ***What's New***: 이 연구는 타겟 중심 비디오 디퓨전 모델(Target-Aware Video Diffusion Model)을 소개합니다. 이 모델은 특정 타겟을 포함한 입력 이미지로부터 배우가 명시된 행동을 수행하는 비디오를 생성합니다. 이는 세그멘테이션 마스크와 텍스트 프롬프트를 통해 모델이 타겟을 인식할 수 있도록 하여, 고급 수준의 액션 계획을 가능케 하고 로봇공학과 같은 응용 분야에서 사용될 수 있습니다.
- ***Technical Details***: 타겟 인식을 강화하기 위해, 텍스트 프롬프트에 타겟의 공간 정보를 인코딩하는 특수 토큰([TGT])을 도입했습니다. 모델은 학습된 데이터셋을 통해 크로스-어텐션 손실(cross-attention loss)을 최적화하여 토큰의 크로스-어텐션 맵과 입력 타겟 마스크를 정렬합니다. 이 손실은 가장 중요한 변환기 블록과 주의 영역에 선택적으로 적용됩니다.
- ***Performance Highlights***: 이 타겟 중심 비디오 디퓨전 모델은 배우가 명시된 타겟과 정확하게 상호작용하는 비디오를 합성하는 기존 솔루션을 능가합니다. 이는 비디오 콘텐츠 생성 및 제로-샷 3D 인체-객체 상호작용 운동 합성의 두 가지 응용 프로그램에서도 효과적임을 입증했습니다.

