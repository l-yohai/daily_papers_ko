## Daily Papers (2025-04-18)

### [CLIMB: CLustering-based Iterative Data Mixture Bootstrapping for Language Model Pre-training](https://arxiv.org/abs/2504.13161)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.13161.png)

Vote: 66

Authors: Yoshi Suhara, Xin Dong, Yingyan, Lin, Dan Su, Yu Yang, Zijia Chen, Yonggan Fu, Pavlo Molchanov, Shizhe Diao, Markus Kliegl, Jan Kautz, Mostofa Patwary, Hongxu Yin, Peter Belcak

- ***What's New***: 이 논문에서는 'CLIMB(Clustering-based Iterative Data Mixture Bootstrapping)'라는 새로운 프레임워크를 제안하여, 대규모 언어 모델(LLM) 학습을 위한 데이터 혼합 최적화를 자동화합니다. CLIMB는 대규모 데이터 세트를 임베딩하고 클러스터링한 후, 대리 모델(proxy model)과 예측기(predictor)를 사용하여 최적의 데이터 혼합을 반복적으로 탐색합니다.
- ***Technical Details***: CLIMB는 세 가지 주요 단계로 구성됩니다: (1) 대규모 데이터 세트를 의미론적 공간에 임베딩하고 클러스터링, (2) 데이터 혼합을 샘플링하고 가지치기함으로써 혼합 성능 쌍을 생성한 후, 대리 모델로 학습, (3) 예측기를 맞춤. CLIMB는 데이터를 기능으로, 성능 지표를 타겟 레이블로 삼아 회귀 모델을 학습하여 예측을 수행합니다. 이후, 데이터 혼합 구축을 검색 문제로 프레임하고 부트스트래핑 전략을 사용하여 해결합니다.
- ***Performance Highlights***: CLIMB는 MMLU, PIQA, ARC_E, HellaSwag 등의 벤치마크 작업에서 자신의 데이터 혼합을 이용해 최적의 성능을 보여주었습니다. 지능적인 데이터 혼합 최적화를 통해 제한된 컴퓨팅 비용 하에서 탁월한 성능을 발휘하며, 1B 모델은 현 상태의 Llama-3.2-1B를 2.0% 상회하는 성능을 보였습니다. 최적의 데이터 혼합을 통해 'ClimbMix'라는 400억 토큰의 데이터 셋도 제작하여 데이터 디스트리뷰션을 다양하게 활용한 고효율 사전 학습을 가능하게 했습니다.

### [Antidistillation Sampling](https://arxiv.org/abs/2504.13146)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.13146.png)

Vote: 52

Authors: Yash Savani, J. Zico Kolter, Alexander Robey, Marc Finzi, Avi Schwarzschild, Asher Trockman, Zhili Feng

- ***What's New***: 이 논문은 새로운 개념인 반증류 샘플링(Antidistillation Sampling)을 소개합니다. 이는 모델의 논리 추론 경로를 독성이 있는 방식으로 변화시켜, 모델 증류(Distillation)의 효과를 제한하면서 모델의 실용성을 유지합니다. 이를 통해 모형 소유자는 경쟁자가 쉽게 모형 능력을 복제하지 못하도록 보호할 수 있습니다.
- ***Technical Details***: 반증류 샘플링은 모델의 불순화되지 않은 다음 토큰 확률분포를 조정하여 증류 시도를 독성화합니다. 학생 모델의 성능 저하를 목표로 설정하며, 교사 모델의 유틸리티는 보존합니다. 이 과정은 주요 계산에서 유한차이(Finite-Difference) 근사를 사용하여 효율적으로 수행됩니다.
- ***Performance Highlights***: 실험 결과, 반증류 샘플링은 교사 모델에서의 동일한 성능을 유지하면서도 증류된 학생 모델의 성능을 크게 저하시켰습니다. GSM8K와 MATH 벤치마크에서 온도 샘플링에 비해 학생 모델의 성능이 상당히 저하되는 것을 확인했습니다. 이는 새로운 아키텍처에서도 반증류 샘플링이 유효함을 나타냅니다.

### [Generate, but Verify: Reducing Hallucination in Vision-Language Models with Retrospective Resampling](https://arxiv.org/abs/2504.13169)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.13169.png)

Vote: 26

Authors: Jiaxin Ge, Tsung-Han Wu, Joseph E. Gonzalez, Trevor Darrell, David M. Chan, Heekyung Lee

- ***What's New***: 이번 연구에서는 Vision-Language Models (VLMs)가 시각적 환각(Visual Hallucination) 문제를 해결하기 위해 새로운 프레임워크인 REVERSE(REtrospective VERification and SElf-correction)를 소개했습니다. 이 프레임워크는 생성 조정 방법과 사후 검증(post-hoc verification) 방법을 단일 VLM 아키텍처에 통합하여 환각을 줄입니다.
- ***Technical Details***: REVERSE는 특수한 훈련 데이터세트를 사용하여 환각 인식 모델을 학습시키며, 모델은 생성 과정에서 환각을 자동적으로 태그하게 됩니다. 또한, 새로운 추론 시점의 회고적 리샘플링(retrospective resampling) 기법을 도입하여 환각을 탐지하고 이를 동적으로 수정합니다. 이를 위해 모델의 어휘에 <SPAN>, </CN>, </UN> 등의 토큰을 추가하여 모델의 자신감 수준을 표시하고, 생성 중 </UN> 토큰의 확률이 특정 기준을 초과하면 회귀조정과 자체 수정 프로세스를 작동시킵니다.
- ***Performance Highlights***: REVERSE는 다양한 벤치마크에서 최고 성능을 기록했습니다. CHAIR-MSCOCO에서 최대 12%, HaloQuest에서 28%의 환각 감소를 달성했습니다. 더 나아가 MMHal 및 HaloQuest와 같은 개방형 질의 응답 작업에서도 SOTA 모델 대비 최대 10%의 정확도 향상과 환각률 감소를 보였습니다.

### [WORLDMEM: Long-term Consistent World Simulation with Memory](https://arxiv.org/abs/2504.12369)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.12369.png)

Vote: 23

Authors: Wenqi Ouyang, Zeqi Xiao, Yifan Zhou, Yanhong Zeng, Xingang Pan, Shuai Yang, Yushi Lan

- ***What's New***: WORLDMEM은 메모리를 활용한 장기적인 일관적인 세계 시뮬레이션을 가능하게 하는 새로운 프레임워크입니다. 이는 기존의 시뮬레이션 방법이 한정된 시간적 맥락 창의 크기 때문에 일관성 있는 3D 공간 유지를 실패하는 것을 극복합니다.
- ***Technical Details***: WORLDMEM은 메모리 유닛을 통해 중요한 시각 및 상태 정보를 저장하는 메모리 뱅크를 특징으로 합니다. 메모리 주의 메커니즘은 메모리 프레임으로부터 관련 정보를 추출하여 시간적 혹은 관점의 간격이 상당함에도 불구하고 이전에 관찰한 장면을 정확히 재구성할 수 있습니다. 또한 시간 정보를 상태에 포함시켜 정적 세상을 모델링할 뿐 아니라 시간에 따른 동적 변화를 포착합니다. 이를 위해 Plücker embeddings과 같은 전문적인 임베딩이 사용됩니다.
- ***Performance Highlights***: WORLDMEM은 커스터마이즈된 Minecraft 벤치마크와 RealEstate10K에서도 탁월한 성능을 보였습니다. 3D 공간 일관성이 크게 개선되었으며, 모델은 생성된 세상의 변화와 사건을 정확히 추적하는 능력을 확인했습니다. 이는 상호작용적 세계 시뮬레이터에 대한 중대한 발전을 나타내며, 향후 연구에 대한 영감을 제공합니다.

### [A Strategic Coordination Framework of Small LLMs Matches Large LLMs in Data Synthesis](https://arxiv.org/abs/2504.12322)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.12322.png)

Vote: 22

Authors: Jiang Wu, Lijun Wu, Xin Gao, Honglin Lin, Yu Li, Conghui He, Zinan Tang, Qizhi Pei

- ***What's New***: 이 논문에서는 여러 작은 LLM들(Small LLMs)을 전략적으로 조율하여 데이터를 합성하는 GRA 프레임워크를 제안합니다. 이는 대형 LLM들을 사용하는 데이터 증류(Data Synthesis) 방법에 비해 효율적이며, 높은 품질의 데이터를 생성할 수 있습니다. GRA는 생성자(Generator), 평가자(Reviewer), 판결자(Adjudicator) 역할을 설정하여 협력적으로 데이터 합성을 수행합니다.
- ***Technical Details***: GRA 프레임워크는 작은 LLM들을 세 가지 역할로 구분하여 작동합니다. 생성자는 초기 데이터 샘플을 제안하고, 평가자는 이 샘플의 품질과 다양성을 평가하며, 판결자는 리뷰어들 간의 충돌을 해결하여 최종 출력을 결정합니다. 이렇게 역할별로 작업을 나누어 협력적으로 데이터 합성을 수행합니다. 실험은 여러 벤치마크를 통해 검증되었으며, GRA가 생성한 데이터가 단일 대형 LLM 기반 출력을 뛰어넘거나 그에 필적한다는 것을 보여줍니다.
- ***Performance Highlights***: GRA 프레임워크는 대형 LLM(Qwen-2.5-72B-Instruct)의 데이터 증류와 비교하여 비슷하거나 더 나은 품질의 데이터를 생성하는데, 이는 훨씬 적은 계산 자원을 요구합니다. 다양한 도메인(일반 질문 응답, 추론, 수학적 문제 해결 등)에 대한 실험 결과, GRA가 생성한 데이터는 최신의 단일 대형 LLM을 기반으로 하는 데이터와 비교해 성능이 뛰어남이 입증되었습니다.

### [Packing Input Frame Context in Next-Frame Prediction Models for Video Generation](https://arxiv.org/abs/2504.12626)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.12626.png)

Vote: 22

Authors: Lvmin Zhang, Maneesh Agrawala

- ***What's New***: 이 논문은 비디오 생성의 차세대 프레임 예측 모델을 위해 FramePack이라는 새로운 신경 네트워크 구조를 소개합니다. FramePack은 입력 프레임을 압축하여 비디오 길이에 관계없이 콘텐츠 길이를 고정된 값으로 유지함으로써 '기억상실(forgetting)' 문제를 해결하고, 주요 종료점을 바탕으로 비순차적인 프레임을 생성하여 '드리프팅(drifting)' 문제를 감소시키는 반-드리프팅 샘플링 방식을 제안합니다.
- ***Technical Details***: FramePack 구조는 시간적 근접성에 따라 입력 프레임의 중요도를 평가하고, 덜 중요한 프레임에 점진적 압축을 적용합니다. 이를 통해 입력 프레임 수에 상관없이 전체 컨텍스트 길이가 일정한 상한에 수렴합니다. 제안된 반-드리프팅 샘플링 방법은 양방향 컨텍스트를 포함하여 오류 발생과 전파를 줄이기 위해 종단 프레임을 먼저 생성하고 중간 콘텐츠를 채우는 방향을 취합니다. 이는 이론적으로 비디오 길이 상관없이 적용 가능합니다.
- ***Performance Highlights***: 실험 결과, FramePack은 기존 프레임 예측 모델에 비해 더 큰 배치 크기와 개선된 시각적 품질을 가능하게 하여, 더 긴 비디오 생성 시에도 모델의 반응성을 높일 수 있는 능력을 증명하였습니다. 또한 이 방법은 HunyuanVideo 및 Wan과 같은 기존 비디오 확산 모델에 적용할 수 있으며, 특히 인간 평가(ELO 점수)에 의해 가장 높은 평가를 받았습니다.

### [VistaDPO: Video Hierarchical Spatial-Temporal Direct Preference Optimization for Large Video Models](https://arxiv.org/abs/2504.13122)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.13122.png)

Vote: 16

Authors: Haojian Huang, Hao Fei, Hanwang Zhang, Xinya Du, Meng Luo, Jinlan Fu, Shengqiong Wu, Haodong Chen

- ***What's New***: VistaDPO는 대형 비디오 모델 (Large Video Models; LVMs)의 비디오 이해 문제를 해결하기 위해 설계된 새로운 프레임워크입니다. 이 프레임워크는 텍스트와 비디오 간의 선호도 정렬을 세 가지 계층적 수준(Instance Level, Temporal Level, Perceptive Level)으로 강화하여 인간의 직관과의 일치성을 높입니다.
- ***Technical Details***: VistaDPO는 비디오와 텍스트 선호도 정렬을 더욱 세밀하게 하여 비디오-언어 오류 문제를 해결합니다. 세 가지 정렬 수준을 도입했으며, VistaDPO-7k라는 7.2K의 QA 쌍으로 이루어진 대규모 데이터셋을 제작하여 각 쌍에 대해 선택된 응답과 거부된 응답을 포함한 시공간적 정보(타임스탬프 등)를 제공합니다. 이를 통해 비디오에 대한 동적 시공간 의존성을 효율적으로 포착하며, 기존 LVMs의 성능을 크게 향상시킵니다.
- ***Performance Highlights***: VistaDPO는 PLLaVA 대비 26.42%, Video-LLaVA 대비 53.92%의 평균 성능 향상을 보이며, 비디오 보정 및 일반 비디오 QA와 캡션 성능에서의 능력을 확실하게 입증합니다. 이러한 성능 향상은 계통적 시공간 정렬 전략 덕분입니다.

### [Perception Encoder: The best visual embeddings are not at the output of the network](https://arxiv.org/abs/2504.13181)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.13181.png)

Vote: 14

Authors: Peize Sun, Andrea Madotto, Christoph Feichtenhofer, Chen Wei, Junke Wang, Hu Xu, Hanoona Rasheed, Daniel Li, Jathushan Rajasegaran, Shiyu Dong, Marco Monteiro, Nikhila Ravi, Po-Yao Huang, Jiale Zhi, Tengyu Ma, Jang Hyun Cho, Piotr Dollár, Daniel Bolya

- ***What's New***: 페르셉션 인코더(Perception Encoder; PE)는 최신 비전-언어 학습을 통해 이미지 및 동영상 이해를 위한 최첨단 인코더로 소개되었습니다. 전통적으로 비전 인코더는 분류, 캡션 생성, 지역화와 같은 특정 다운스트림 과제에 맞게 각기 다른 사전학습 목표에 의존해왔습니다. PE는 대조적 비전-언어 훈련을 통해 모든 다운스트림 작업에 강력하고 일반적인 임베딩을 생성할 수 있음을 발견했습니다. 이러한 임베딩은 네트워크의 중간 레이어에 숨겨져 있는데, 이 임베딩을 이끌어내기 위해 다중모달 언어 모델링을 위한 언어 정렬과 밀집 예측을 위한 공간 정렬 두 가지 정렬 방법을 도입했습니다.
- ***Technical Details***: PE 모델군은 강력한 대조적 사전학습 레시피와 합성된 비디오로 정렬된 비디오 파인튜닝을 사용하여 다양한 비전 작업에서 최첨단 성능을 달성합니다. PE는 대규모 대조적 사전학습을 활용하여 이러한 일반 기능을 활용할 수 있도록 정렬 조정을 통해 다운스트림 작업으로 전이할 수 있는 기능을 해제합니다. 모델과 관련 코드는 공개됩니다.
- ***Performance Highlights***: PE 모델은 이미지 및 비디오의 제로샷 분류 및 검색, 문서, 이미지, 비디오 질문 및 응답, 탐지, 깊이 추정, 추적과 같은 공간 작업에서 최첨단 성능을 달성하였으며, 다양한 성능 측정에서 최고 수준을 기록했습니다.

### [MLRC-Bench: Can Language Agents Solve Machine Learning Research Challenges?](https://arxiv.org/abs/2504.09702)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.09702.png)

Vote: 13

Authors: Lajanugen Logeswaran, Yunxiang Zhang, Moontae Lee, Lu Wang, Grant D Murphy, Muhammad Khalifa, Shitanshu Bhushan, Jaekyeom Kim, Honglak Lee

- ***What's New***: MLRC-Bench는 대형 언어 모델(LLM) 에이전트가 머신러닝 연구에서 새로운 방법론을 제안하고 구현하는 능력을 평가하는 새로운 벤치마크입니다. 이 벤치마크는 기존의 표준 연구 과제를 넘어서는 혁신적인 접근을 요구하는 7개의 연구 경쟁 과제를 포함합니다. 이러한 과제는 ML 연구에서의 애매한 도전 과제를 해결하고자 하는 방법론의 참신성과 효과성을 평가하도록 설계되었습니다.
- ***Technical Details***: MLRC-Bench는 회의에서 발표된 7개의 연구 경쟁 과제로 구성되어 있으며 각 과제는 LLM 안전성, 멀티모달 인식, 몇 샷 학습과 같은 다양한 분야를 포함합니다. 대표적인 과제로는 LLM 병합(LLM Merging)과 기계 익히기(Machine Unlearning)가 있습니다. 벤치마크는 LLM 에이전트가 제안하고 구현하는 새로운 연구 방법의 핵심 단계를 측정하기 위해 설계되었습니다. 각각의 과제에는 시간제한 내에서의 성능 달성과 같은 구체적인 평가 지표가 포함됩니다.
- ***Performance Highlights***: Gemini-exp-1206는 테스트된 에이전트 중 가장 좋은 성능을 보였으나 인간 참가자 점수와의 격차를 단 9.3%만 메웠습니다. 에이전트의 실제로 제안된 연구 아이디어의 평가와 성능 사이에는 불일치가 발견되었으며, 이는 LLM을 심판으로 사용하는 주관적 평가의 신뢰성에 의문을 제기합니다. 이러한 결과는 LLM 연구 에이전트가 ML 솔루션을 혁신적으로 생성하고 구현하는 데 현저한 한계가 있음을 보여줍니다.

### [DMM: Building a Versatile Image Generation Model via Distillation-Based Model Merging](https://arxiv.org/abs/2504.12364)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.12364.png)

Vote: 12

Authors: Shuai Wang, Tianhui Song, Weixin Feng, Bo Zheng, Limin Wang, Tiezheng Ge, Xubin Li

- ***What's New***: DMM라는 새로운 모델 병합(merging) 접근법이 소개되었습니다. 이 접근법은 스타일 벡터(style vectors)를 활용하여 다양한 스타일의 이미지를 정확하게 생성할 수 있는 스타일-프롬프트 가능한 이미지 생성 파이프라인을 도입하며, 다수의 강력한 모델 기능을 하나로 통합하는 데 성공했습니다.
- ***Technical Details***: DMM은 스코어 증류(score distillation)을 기반으로 한 모델 병합 패러다임을 제공하여 다수의 교사 모델로부터 하나의 다재다능한 T2I 모델로 통합합니다. 이 접근법은 여러 교사 모델로부터 지식을 압축적으로 재구성하고, 가이드 가능한 임의 스타일 생성을 지원합니다. 또한, 새로운 병합 목표 및 평가 프로토콜을 제시하여 모델 병합 작업을 재구성합니다.
- ***Performance Highlights***: DMM은 8개의 다른 모델을 병합하여 FIDt 77.51의 점수를 달성했으며, 이전의 병합 방식과 비교하여 동등한 스타일 혼합 성능을 유지했습니다. 이 벤치마크는 다목적 모델로서 다양한 스타일을 동시에 충실히 재현할 수 있음을 입증합니다.

### [NoisyRollout: Reinforcing Visual Reasoning with Data Augmentation](https://arxiv.org/abs/2504.13055)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.13055.png)

Vote: 12

Authors: Xiangyan Liu, Zijian Wu, Longxu Dou, Tianyu Pang, Haonan Wang, Michael Qizhe Shieh, Chao Du, Jinjie Ni

- ***What's New***: NoisyRollout은 비주얼리인포스먼트러닝(Visual Reinforcement Learning) 방법 중 하나로, 잡음이 추가된 이미지와 텍스트 쌍으로부터 발생한 롤아웃(Trajectory)을 활용하여 비주얼리인포스먼트러닝의 탐색 능력을 향상시키는 간단하면서도 효과적인 방법을 제안합니다. 기존 데이터에 대한 추가 훈련 비용 없이도, 불완전한 시각적 인식과 함께 비주얼리인포스먼트러닝 모델의 탐색 기능을 강화합니다.
- ***Technical Details***: NoisyRollout은 정제된 이미지와 적절히 왜곡된 이미지로부터 혼합된 롤아웃을 통해 시각 인식을 다양화하고, 결과적으로 논리적 패턴에 다양성을 더합니다. GRPO(Group Relative Policy Optimization) 방법론을 사용하여 기존 정책 모델(πθold)과 현재 정책 모델(πθ)을 결합하여 변형된 시각적 입력을 통해 새로운 탐색 기회를 제공합니다. 또 이 접근법은 잡음이 있는 신호를 안정적으로 유지하기 위해 잡음 완화 일정(Noise Annealing Schedule)을 활용하여 훈련 초반 강한 왜곡 효과를 완화하고 점차적으로 감소시킵니다.
- ***Performance Highlights***: NoisyRollout은 Geometry3K와 K12 같은 다양한 훈련 세트에 대해 이미 존재하는 오픈소스 비주얼리인포스먼트러닝 튜닝된 모델들 사이에서 최첨단 성능을 기록했습니다. 특히 MathVerse, MathVision, HallusionBench와 같은 오픈도메인 벤치마크에서 두드러진 성과를 보이며, 시각적 추론과 인식 과업에서 높은 일반화 성능을 기록했습니다. 이에 반해 동일한 베이스라인 (Vanilla GRPO)과 비교해 성능이 지속적으로 우수한 결과를 보였습니다.

### [ChartQAPro: A More Diverse and Challenging Benchmark for Chart Question Answering](https://arxiv.org/abs/2504.05506)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.05506.png)

Vote: 11

Authors: Mohammed Saidul Islam, Shafiq Joty, Megh Thakkar, Firoz Kabir, Shadikur Rahman, Md Tahmid Rahman Laskar, Mahir Ahmed, Enamul Hoque, Mehrad Shahmohammadi, Mizanur Rahman, Ahmed Masry, Aayush Bajaj, Aaryaman Kartha, Md Rizwan Parvez

- ***What's New***: CHARTQAPRO는 차트 질문 응답(Chart Question Answering; CQA)을 위한 새로운 벤치마크로, 157개의 다양한 소스에서 수집된 1,341개의 차트를 포함하여 현실 세계의 도전 과제를 보다 잘 반영합니다. 이 벤치마크는 다양한 차트 유형을 다루며, 1,948개의 질문을 통해 다중 선택, 대화형, 가설적인 질문 및 무응답 질문 등의 다양한 질문 유형을 제공합니다.
- ***Technical Details***: CHARTQAPRO 데이터셋은 157개의 온라인 플랫폼에서 실제 차트 이미지를 수집하였으며, 시드 질문 답변 쌍을 생성하고 다양한 VLMs(GPT-4o, Gemini 등)의 도움으로 질문 답변 쌍을 확장하는 방식으로 구성되었습니다. 이 데이터셋은 시각적 및 주제적 다양성을 포함하며, 질문 유형으로는 사실적, 다중 선택, 대화형, 가설적, 무응답, 사실 확인이 포함되어 있습니다.
- ***Performance Highlights***: Claude Sonnet 3.5는 CHARTQAPRO에서 55.81%의 정확도를 기록하여 ChartQA에서의 90.5%에 비해 성능이 대폭 하락하였습니다. 전체적으로 최신 LVLM들이 CHARTQAPRO에서 드러낸 성능 저하를 통해 차트 이해와 추론이 아직 해결되지 않은 도전 과제임을 강조하며, 향후 연구를 위한 중요한 통찰을 제공합니다.

### [InstantCharacter: Personalize Any Characters with a Scalable Diffusion Transformer Framework](https://arxiv.org/abs/2504.12395)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.12395.png)

Vote: 10

Authors: Qin Lin, Linqing Wang, Qinglin Lu, Yiji Cheng, Qixun Wang, Yanbing Zhang, Haofan Wang, Zhengguang Zhou, Xu Bai, Ruihuang Li, Jiale Tao, Chunyu Wang

- ***What's New***: InstantCharacter는 캐릭터 커스터마이제이션을 위한 확장 가능한 Diffusion Transformer 프레임워크를 제안하는 새로운 접근법입니다. 이 프레임워크는 다양한 캐릭터 외관, 자세, 스타일을 오픈 도메인에서 높은 충실도로 개인화할 수 있으며, 대규모 캐릭터 데이터셋을 사용해 아이덴티티 일관성과 텍스트 편집 가능성을 동시에 최적화합니다.
- ***Technical Details***: InstantCharacter는 기본 Diffusion Transformer의 잠재 공간과 원활하게 상호작용할 수 있는 확장 가능한 어댑터를 도입합니다. 이 어댑터는 스택된 Transformer 인코더로 구성되며, 대규모의 1천만 레벨 캐릭터 데이터셋을 통해 학습됩니다. 또한, 제안된 3단계 학습 전략을 통해 다양한 학습 경로를 제공함으로써 이미지의 텍스트 제어 가능성과 일관성을 유지합니다.
- ***Performance Highlights***: InstantCharacter는 기존 방법론들과의 정성적인 비교 결과, 이미지 충실도와 캐릭터 일관성, 텍스트 제어 가능성에서 탁월한 성능을 입증했습니다. 특히 복잡한 액션 프롬프트에서도 높은 충실도와 캐릭터 세부사항을 유지하며 우수한 결과를 보여줍니다.

### [PerceptionLM: Open-Access Data and Models for Detailed Visual Understanding](https://arxiv.org/abs/2504.13180)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.13180.png)

Vote: 10

Authors: Philipp Krähenbühl, Salman Khan, Miguel Martin, Peize Sun, Piotr Dollár, Huiyu Wang, Triantafyllos Afouras, Yale Song, Shashank Jain, Effrosyni Mavroudi, Hanoona Rasheed, Babak Damavandi, Tammy Stark, Po-Yao Huang, Tengyu Ma, Vivian Lee, Daniel Bolya, Lorenzo Torresani, Shane Moon, Tushar Nagarajan, Christoph Feichtenhofer, Suyog Jain, Nikhila Ravi, Jang Hyun Cho, Kristen Grauman, Shuming Hu, Andrea Madotto, Andrew Westbury, Muhammad Maaz

- ***What's New***: PerceptionLM은 시각 및 비디오 이해를 위한 완전 개방형 및 재현 가능한 프레임워크로 구축된 인식 언어 모델(Perception Language Model; PLM)입니다. 이는 기존 비공개 모델들로부터의 증류 없이 투명한 연구를 목표로 하며, 세세한 비디오 이해를 위한 데이터 격차를 정의하고 이를 해결하기 위해 2.8M의 세세한 비디오 질문-응답 쌍과 시공간적으로 기반이 있는 비디오 캡션을 제공합니다.
- ***Technical Details***: PerceptionLM은 8B 미만의 파라미터를 가진 비주얼 인코더 및 LLM 디코더로 구성됩니다. LLM과 비주얼 인코더는 2단계의 MLP 프로젝트로 연결됩니다. 시각적 데이터 입력에는 동적 타일링(dynamically tiling)을 사용하여 최대 해상도를 지원하고, 비디오 입력의 경우 32 프레임을 4482 해상도로 처리합니다. 모든 데이터는 비공개 모델 없이 생성된 대규모 합성 데이터와 사람 주석 데이터를 사용하여 투명하게 제공됩니다.
- ***Performance Highlights***: PLM의 최종 모델은 주요 이미지 및 비디오 벤치마크에서 Qwen2.5VL을 비롯한 여러 최신 공개 모델보다 우수한 성능을 보이며, 특히 영상 캡션 (+39.8 CIDEr), 세세한 비디오 QA (+3.8 포인트)에서 뛰어난 발전을 이루었습니다. 이는 현재 VLMs가 세세한 시각적 정보 이해 및 시공간적 추론에서의 놀라운 성능을 입증합니다.

### [Exploring Expert Failures Improves LLM Agent Tuning](https://arxiv.org/abs/2504.13145)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.13145.png)

Vote: 9

Authors: Li-Cheng Lan, Tianyi Zhou, Cho-Jui Hsieh, Andrew Bai, Ruochen Wang, Minhao Cheng

- ***What's New***: 이 논문에서는 전문가의 실패를 활용하여 대규모 언어 모델(Large Language Models; LLMs)의 에이전트 튜닝을 개선하는 새로운 방법인 전문가 실패 탐색(Exploring Expert Failures; EEF)을 제안합니다. 이는 기존의 수용거부 미세 조정(Rejection Sampling Fine-Tuning; RFT) 방법의 한계를 극복하기 위해 실패한 전문가 경로에서 유용한 행동을 식별하여 교육 데이터 세트에 통합합니다.
- ***Technical Details***: EEF는 기존의 RFT 방식과 유사하게 행동 클로닝(Behavior Cloning), 탐색(Exploration), 강화 미세조정(Reinforcement Fine-tuning)의 세 가지 주요 단계를 포함합니다. 이 방법은 전문가 상태 시뮬레이션을 통해 실패 경로에서 유익한 행동을 식별하며, 오류가 발생하기 전의 중요한 상태 및 회복이 필요한 상태를 선별하여 해당 행동들로 모델을 미세 조정합니다.
- ***Performance Highlights***: EEF 방법은 Webshop 환경에서 62%의 승률을 기록하며, 이는 RFT의 53.6% 및 GPT-4의 35.6%를 능가하였습니다. 또한, SciWorld 환경에서는 최초로 81점을 초과하는 성과를 거두어, 기존 방법들의 성능을 뛰어넘는다는 것을 입증했습니다.

### [70% Size, 100% Accuracy: Lossless LLM Compression for Efficient GPU Inference via Dynamic-Length Float](https://arxiv.org/abs/2504.11651)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11651.png)

Vote: 7

Authors: Tianyi Zhang, Shaochen Zhong, Vipin Chaudhary, Anshumali Shrivastava, Xia Hu, Yang Sui

- ***What's New***: 이 논문에서는 Dynamic-Length Float(DFloat11)라는 새로운 손실 없는 압축 프레임워크를 소개하여 대규모 언어 모델(LLM)의 크기를 30% 감소시키면서 원래 모델과의 비트-바이-비트 동일성을 유지합니다. 이는 LLM의 BFloat16 가중치 표현에서 낮은 엔트로피를 활용하여 새로운 저장 포맷 및 빠른 online decompression을 지원하는 GPU 커널을 개발한 것입니다.
- ***Technical Details***: DFloat11은 BFloat16 형식의 비효율성을 해결하기 위해 가중치를 frequency 기반의 동적 길이 인코딩으로 변환하여 효율적인 손실 없는 압축을 제공합니다. GPU에서 빠른 추론을 보장하기 위해, SRAM에 저장할 수 있는 compact lookup tables(LUTs)로 메모리를 균형 잡게 이용하며, 두 단계의 커널을 사용하여 스레드의 읽기 및 쓰기 위치를 조정합니다. 실험 결과는 다양한 최신 모델에서 약 30% 크기 감소를 입증하며, 동일한 비트-바이-비트 출력이 산출됨을 보여줍니다.
- ***Performance Highlights***: DFloat11은 Llama-3.1-405B 모델을 포함하여 여러 모델의 메모리 요구 사항을 두 개의 GPU 노드에서 단일 노드로 줄이고, CPU에 대한 오프로드 없이 최대 38.8배 높은 처리량을 보였습니다. 또한, 고정된 GPU 메모리 예산에서 최대 5.3에서 13.17배 더 긴 컨텍스트 길이를 허용하여 inference의 효율을 높였습니다.

### [Sleep-time Compute: Beyond Inference Scaling at Test-time](https://arxiv.org/abs/2504.13171)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.13171.png)

Vote: 7

Authors: Ion Stoica, Joseph E. Gonzalez, Charlie Snell, Sarah Wooders, Yu Wang, Kevin Lin, Charles Packer

- ***What's New***: 이 논문은 sleep-time compute라는 새로운 접근 방식을 제안하며, 이는 사용자 쿼리 전에 모델이 잠재적인 쿼리를 예측하여 오프라인에서 미리 계산을 수행함으로써 테스트 시간의 연산 요구사항을 대폭 줄이는 방법입니다. 이를 통해 모델은 테스트 시점에 정확성은 유지하되 지연 및 비용이 감소합니다.
- ***Technical Details***: sleep-time compute는 기존의 상태 비유지(stateless) 문제를 상태 유지(stateful) 문제로 변환하여, 주어진 문맥에 대한 유용한 계산을 사전에 수행합니다. Stateful GSM-Symbolic 및 Stateful AIME와 같은 수정된 추론 과제를 통해 이 방법의 효용성을 입증합니다. 여러 관련 쿼리의 문맥에서 sleep-time compute를 통해 비용을 절감하고, 사용자 쿼리에 대해 기존의 테스트 시간 계산보다 낮은 지연과 비용으로 응답할 수 있습니다.
- ***Performance Highlights***: Stateful GSM-Symbolic 및 Stateful AIME에서 동일한 정확도를 달성하기 위해 필요한 테스트 시간 연산을 약 5배까지 감소시킬 수 있었으며, 추가적인 정확도 개선이 Stateful GSM-Symbolic에서 최대 13%, Stateful AIME에서 최대 18%까지 가능했습니다. 또한, 여러 쿼리에 대해 평균 비용을 2.5배까지 줄일 수 있었습니다.

### [CCMNet: Leveraging Calibrated Color Correction Matrices for Cross-Camera Color Constancy](https://arxiv.org/abs/2504.07959)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.07959.png)

Vote: 7

Authors: Dongyoung Kim, Michael S. Brown, Seon Joo Kim, Mahmoud Afifi, Dongyun Kim

- ***What's New***: CCMNet는 크로스 카메라 색상 일관성(Cross-Camera Color Constancy)을 위해 ISP의 사전 보정된 색상 조정 행렬(CCMs)을 활용하는 새로운 학습 기반 방법을 소개합니다. 이 방법은 새로운 카메라 데이터에 대해 재훈련 없이 일반화될 수 있습니다.
- ***Technical Details***: CCMNet는 ISP에서 사용할 수 있는 사전 보정된 색상 조정 행렬(Calibrated Color Correction Matrices; CCMs)을 이용해 표준 색 공간(CIE XYZ)에서 테스트 카메라의 원시 공간으로 예비 정의된 조명 색을 변환합니다. 이러한 변환된 조명은 카메라 고유의 응답 기능을 인코딩하며, 8차원 임베딩된 카메라 지문(CFE)으로 압축되어 네트워크가 이전에 보지 못한 카메라에 적응할 수 있도록 도와줍니다. 훈련 중 사용되는 제한된 카메라와 CCM에 대한 오버피팅을 방지하기 위해 카메라와 그 CCM 간의 보간을 활용한 데이터 증강 기술을 도입합니다.
- ***Performance Highlights***: CCMNet는 다양한 데이터셋과 백본에서 실험 결과 최첨단의 크로스 카메라 색상 일관성 성능을 달성하는 동시에 경량성을 유지하며 카메라 ISP 내의 쉽게 구할 수 있는 데이터에만 의존합니다. CCMNet는 추가적인 테스트 카메라 이미지 없이도 일관되게 높은 정확도를 유지합니다.

### [FocusedAD: Character-centric Movie Audio Description](https://arxiv.org/abs/2504.12157)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.12157.png)

Vote: 6

Authors: Chun Wang, Sheng Zhou, Xiaojun Ye, Liangcheng Li, Jiajun Bu, Yiren Song

- ***What's New***: FocusedAD는 스토리 중심의 영화 오디오 설명(Movie Audio Description; AD) 생성을 위한 새로운 프레임워크를 제안합니다. 이 모델은 주인공 캐릭터에 초점을 맞춘 서술을 제공하며, 캐릭터 인식 및 이야기에 중요한 세부사항을 강조합니다. 이는 다중 벤치마크에서 최첨단 성능을 달성하며, 특히 MAD-eval-Named 및 Cinepile-AD 데이터셋에서 인상적인 제로샷 성능을 보여줍니다.
- ***Technical Details***: FocusedAD는 세 가지 주요 모듈을 포함합니다: 첫째, Character Perception Module(CPM)은 클러스터링 기반 얼굴 인식 알고리즘으로 주 캐릭터를 식별하고 이름을 연결합니다. 둘째, Dynamic Prior Module(DPM)은 이전 AD와 자막으로부터 맥락적 단서를 주입하여 장면에 따라 동적으로 적응하는 소프트 프롬프트를 사용합니다. 셋째, Focused Caption Module(FCM)은 장면, 캐릭터, 텍스트 토큰을 공동으로 추론하여 이야기 중심의 AD를 생성합니다. 최적의 캐릭터 쿼리 뱅크 생성을 위한 자동화 파이프라인도 소개됩니다.
- ***Performance Highlights***: FocusedAD는 MAD-eval-Named와 Cinepile-AD에서 최고 성능을 보입니다. 베이스라인 모델과 비교하여, BertScore(BertS)에서 MAD-eval-Named는 57.7, Cinepile-AD는 64.5를 기록하며, 이는 영화 오디오 설명의 서술 품질과 중요 인물 중심 서술의 중요성을 입증합니다.

### [Retrieval-Augmented Generation with Conflicting Evidence](https://arxiv.org/abs/2504.13079)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.13079.png)

Vote: 5

Authors: Archiki Prasad, Mohit Bansal, Elias Stengel-Eskin, Han Wang

- ***What's New***: 이번 연구에서는 정보 검색을 기반으로 한 생성 방식인 RAG 시스템에서 발생할 수 있는 정보 충돌의 문제를 해결하기 위해 RAMDocs라는 새로운 데이터셋을 도입했습니다. 이 데이터셋은 사용자 쿼리와 관련된 문서 내에서 발생할 수 있는 모호성, 잘못된 정보, 그리고 잡음을 현실적으로 시뮬레이션합니다. 또한, 다중 에이전트 시스템인 MADAM-RAG를 활용하여 다양한 정보 출처에서 발생하는 충돌을 효과적으로 처리하는 방법론을 제시하였습니다.
- ***Technical Details***: MADAM-RAG는 각 문서마다 개별 에이전트를 배정하고, 에이전트들 간의 다중 라운드 토론을 통해 정보를 요약하고 잡음을 걸러냅니다. 각각의 에이전트는 자신의 문서만을 기반으로 중간 응답을 생성하며, 토론 후에 통합 모듈이 모든 에이전트의 응답을 종합하여 최종 답변을 생성합니다. RAMDocs 데이터셋은 기존의 AmbigDocs와 FaithEval에 기반을 두고, 잘못된 정보와 잡음이 포함된 문서를 추가하여 모델들이 현실적인 정보 검색 시나리오에서 얼마나 잘 작동하는지를 평가합니다.
- ***Performance Highlights***: 극복해야 할 과제가 많음에도 불구하고, Llama3.3-70B-Instruct와 같은 모델에서 MADAM-RAG는 FaithEval 기준에서 15.80%, AmbigDocs에서 11.40%까지 성능 개선을 이끌어냈습니다. RAMDocs와 같은 복잡한 데이터 셋에서도 성능을 향상시켰지만, 여전히 다양한 정보 처리에서의 과제를 덜어줄 여지가 많음을 확인했습니다.

### [Set You Straight: Auto-Steering Denoising Trajectories to Sidestep Unwanted Concepts](https://arxiv.org/abs/2504.12782)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.12782.png)

Vote: 3

Authors: Yan Ren, Adams Wai-Kin Kong, Leyang Li, Shilin Lu

- ***What's New***: 새로운 ANT(Finetuning Framework) 기법을 소개하며, 이는 중후반 비조정(denoising) 단계에서 분류사(free classifier)의 가이드 조건 방향을 역전시켜 불필요한 개념을 피하고자 하는 자동 항로 조정 기법입니다. 이 접근 방식은 기존의 제한점을 극복합니다.
- ***Technical Details***: ANT는 중-후기의 비조정 단계에서 분류사(free classifier) 가이드의 조건 방향을 역전시켜 초기에 이미지 구조적 정합성을 유지하면서 정확한 콘텐츠 수정을 가능하게 합니다. 이를 통해 초기 단계의 스코어 함수 필드의 완전성을 잃지 않고, 앵커(Anchor) 개념 선택에 의존하지 않습니다. 싱글 개념 제거를 위해서는 증강 기반의 가중치 중요도 맵을 통해 불필요한 개념에 가장 큰 기여를 하는 중요한 파라미터를 식별하며, 멀티 개념 제거의 경우 플러그앤플레이 방식으로 성능을 크게 향상시킵니다.
- ***Performance Highlights***: ANT는 싱글 및 멀티 개념 제거 모두에서 최첨단(State-Of-The-Art) 결과를 달성하며, 생성 마당(Meadow)의 충실도를 희생하지 않고 양질의 안전한 출력을 제공합니다. 이를 통해 개념 제거와 비관련 개념 보존 간의 균형을 성공적으로 유지합니다.

### [Complex-Edit: CoT-Like Instruction Generation for Complexity-Controllable Image Editing Benchmark](https://arxiv.org/abs/2504.13143)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.13143.png)

Vote: 2

Authors: Siwei Yang, Yuyin Zhou, Nataniel Ruiz, Mude Hui, Bingchen Zhao, Cihang Xie

- ***What's New***: Complex-Edit는 다양한 복잡성 수준을 가진 명령을 기반으로 이미지 편집 모델을 평가하기 위한 벤치마크입니다. 이는 Chain-of-Edit 파이프라인을 도입하여, 개별 원자적 편집 작업을 생성하고 이를 결합하여 복잡한 명령으로 통합하는 구조를 가지고 있습니다.
- ***Technical Details***: 이 데이터셋은 GPT-4o를 활용하여 대규모로 다양하고 복잡성을 조절할 수 있는 평가 데이터셋을 생성합니다. 24개의 원자적 작업을 정의하고, 이를 9개의 주요 범주로 나누어 시퀀스를 생성합니다. 복잡한 명령은 단순화 및 결합 단계를 거쳐 만들어지며, 최종적으로 VLM 기반 자동 평가 시스템을 통해 대규모로 평가됩니다.
- ***Performance Highlights***: 공공 모델은 독점적 모델에 비해 상당한 성능 차이를 보이며, 복잡성이 증가할수록 그 차이가 커집니다. 명령이 복잡해짐에 따라 편집 모델의 전체적인 미적 품질은 떨어집니다. 연속 편집은 직접 실행보다 더 부정적인 결과를 나타내며, 현재 모델들이 상당한 도전에 직면하고 있음을 시사합니다.

### [FreshStack: Building Realistic Benchmarks for Evaluating Retrieval on Technical Documents](https://arxiv.org/abs/2504.13128)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.13128.png)

Vote: 2

Authors: Michael Carbin, Nandan Thakur, Sam Havens, Andrew Drozdov, Omar Khattab, Jimmy Lin

- ***What's New***: FreshStack는 기술 문서 검색 평가를 위한 실용적이고 최신의 벤치마크를 자동으로 구축할 수 있는 프레임워크입니다. 이 프레임워크는 커뮤니티에서 질문과 답변으로 유래한 데이터를 바탕으로 정보 검색(IR) 평가 벤치마크를 구축할 수 있게 해주며, 특별히 최신 주제와 틈새 분야에 특히 집중합니다. 이 연구는 평가지표로서의 현실성과 복잡성을 갖춘 새로운 데이터셋을 제안합니다.
- ***Technical Details***: FreshStack는 다음 세 가지 메인 단계로 구성됩니다: (1) GitHub 저장소로부터 코딩 및 기술 문서의 자동 수집, (2) 커뮤니티 질문과 답변으로부터 GPT-4o를 사용한 Nugget 생성, (3) 여러 검색 기법과 혼합된 아키텍처를 기반으로 한 문서 검색. 각 단계에서 합성 기법과 모델을 사용하여 문서를 검색하고 평가합니다. 이는 진화하는 기술 문서에서의 현실적인 검색과 RAG 평가 벤치마크를 구축하는 데 유용합니다.
- ***Performance Highlights***: 실험 결과, 기존 검색 모델들은 Oracle 접근 방식에 비해 크게 성능이 저조해, 개선의 여지가 많음을 보여주었습니다. 또한 모델 간의 다양성이 검색 성능을 향상시키며, 특정 주제에서는 재정렬 기법이 명확하게 성능을 향상시키지만, 모든 주제에 적용되지는 않는다는 사실을 발견하였습니다. FreshStack의 다양한 평가 수치는 이러한 검색 및 재정렬 시스템의 개선 방향을 제시합니다.

### [MetaSynth: Meta-Prompting-Driven Agentic Scaffolds for Diverse Synthetic Data Generation](https://arxiv.org/abs/2504.12563)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.12563.png)

Vote: 1

Authors: Vinayak Arannil, Graham Horwood, Haris Riaz, Miguel Ballesteros, Sourav Bhabesh

- ***What's New***: 메타싱스(MetaSynth)는 다양한 합성 데이터를 생성하기 위해 메타-프롬팅(meta-prompting)을 활용한 새로운 방법을 제안합니다. 이 방법은 메타 언어 모델(meta-Language Model)과 여러 전문가 LLM 에이전트를 활용하여 다양성을 높이는 합성 데이터를 생성하도록 설계되었습니다.
- ***Technical Details***: 메타싱스는 초기 생성 시드(seed) 설정에 따라 데이터 다양성이 영향을 받을 수 있다는 가설을 바탕으로 개발되었습니다. 합성 데이터 생성 방법에는 조건부 인스턴스 생성(Conditional Instance Generation)을 사용하여 이전에 생성된 인스턴스를 기반으로 새로운 인스턴스를 생성합니다. 각 인스턴스는 시드 키워드 확장 전문가(Seed Keyword Expansion Expert), 콘텐츠 분석 전문가(Content Analyst Expert), 요약 전문가(Summarizer Expert) 등 다양한 전문가를 반복적으로 호출하여 다양한 문서를 생성합니다.
- ***Performance Highlights***: 메타싱스에서 생성한 2500만 토큰의 합성 데이터로 파이낸스와 생물의학 두 개의 특수 도메인에 잘 훈련된 LLM(Mistral-7B-v0.3)을 성공적으로 적응시켰으며, 기초 모델 대비 파이낸스 도메인에서 최대 4.08% 및 생물의학 도메인에서 13.75%의 성능 향상을 보였습니다. 이로 인해 메타싱스 기반의 합성 데이터가 템플릿-프롬팅 방식보다 높은 성능을 제공함이 확인되었습니다.

