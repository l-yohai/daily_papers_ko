## Daily Papers (2025-04-07)

### [Multi-SWE-bench: A Multilingual Benchmark for Issue Resolving](https://arxiv.org/abs/2504.02605)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02605.png)

Vote: 30

Authors: Yuyu Zhang, Qi Liu, Yongsheng Xiao, Aoyan Li, Liang Xiang, Linhao Zhang, Kai Shen, Lu Chen, Rui Long, Daoguang Zan, Tianyu Liu, Jing Su, Zhirong Huang, Shulin Xin, Siyao Liu, Xiaojian Zhong, Wei Liu, Hanwu Chen, Liangqiang Chen

- ***What's New***: Multi-SWE-bench는 Java, TypeScript, JavaScript, Go, Rust, C, C++의 7개 주요 프로그래밍 언어를 다루는 다국어(issue-resolving benchmark)로 소개되었습니다. 이는 다양한 언어로 구성된 소프트웨어 생태계에서 대형 언어 모델(LLMs)의 문제 해결 능력을 평가하기 위해 고안되었습니다. 또한, (Multi-SWE-RL)이라는 오픈 소스 커뮤니티를 시작하여 issue-resolving 작업에 대한 대규모 (reinforcement learning; RL) 학습 데이터를 구축하고 있습니다.
- ***Technical Details***: Multi-SWE-bench는 총 1,632개의 검증된 GitHub issue로 구성되어 있으며, 5단계 파이프라인을 통해 구축되었습니다: (1) 높은 품질의 GitHub 저장소 선택, (2) (Pull Request; PR) 수집, (3) 실행 가능한 환경 설정, (4) PR 필터링, 및 (5) 수작업 검증. 각 PR은 (Docker)에 기반한 실행 환경이 구축되며, 각 언어에 따라 해당 문제의 해결 가능성을 평가합니다. Multi-SWE-RL 커뮤니티는 4,723개의 컨테이너화된 데이터셋을 제공하면서, 오픈 소스 공동 작업을 장려합니다.
- ***Performance Highlights***: Python에서는 많은 모델이 높은 해결율을 보였으나, 다른 프로그래밍 언어에서는 성능이 떨어졌습니다. 예를 들어, OpenAI-o1과 Claude-3.7-Sonnet은 Python에서 높은 해결율을 기록했으나 다른 언어에서의 효과는 상대적으로 저조했습니다. LLM 기반 에이전트는 일반적으로 문제 난이도가 증가함에 따라 성능이 저하되었습니다. 고난도의 문제에 대해서는 대부분의 모델들이 해결 능력이 제한적이었습니다.

### [MegaMath: Pushing the Limits of Open Math Corpora](https://arxiv.org/abs/2504.02807)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02807.png)

Vote: 19

Authors: Zhengzhong Liu, Guowei He, Zengzhi Wang, Nikhil Ranjan, Eric P. Xing, Liping Tang, Zhoujun Cheng, Fan Zhou

- **What's New**: MegaMath는 인간 지능의 근본적인 요소인 수학적 추론을 대규모 언어 모델(LLMs)의 고급 기능 평가 기준으로 제시하며, 기존에 부족했던 개방형 대규모 수학 데이터세트를 제공하는 최초의 시도입니다. 이를 통해 대규모 수학적 전이 학습 데이터를 공개적으로 접근 가능하게 합니다.
- **Technical Details**: MegaMath는 웹 데이터, 코드 데이터, 합성 데이터 세 가지 주요 컴포넌트로 구성되며, 총 371B 토큰을 포함합니다. 웹 데이터는 Common Crawl에서 수집하여 LaTeX를 활용한 수학적 표현을 보존하는 방식으로 처리하였으며, 수학과 관련된 코드는 Stack-V2에서 선별되었습니다. 합성 데이터는 QA 스타일의 텍스트와 코드 블록을 생성하여 데이터 다양성을 극대화했습니다.
- **Performance Highlights**: MegaMath는 기존의 인피MM 웹 수학 및 FineMath 등과 비교하여 더 많은 토큰 수를 제공하며, 특히 MegaMath-Web-Pro는 현재까지 공개된 최고의 품질을 자랑합니다. 이는 다양한 컴퓨팅 예산을 충족하며, 최신 Llama-3 모델과 함께 사용될 때 수학적 추론 성능을 최대 20% 이상 개선합니다.

### [Agentic Knowledgeable Self-awareness](https://arxiv.org/abs/2504.03553)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.03553.png)

Vote: 19

Authors: Shuofei Qiao, Baochang Ren, Zhisong Qiu, Xiaobin Wang, Pengjun Xie, Huajun Chen, Fei Huang, Yong Jiang, Ningyu Zhang, Xiangyuan Ru, Xiang Chen

- ***What's New***: 이 연구는 에이전트의 시나리오별 자기 인식을 촉진하는 새로운 접근 방식인 KnowSelf를 제안합니다. KnowSelf는 크게 데이터 중심으로, 대형 언어 모델(LLM)에 기반한 에이전트가 상황에 따라 지식 활용을 선택적으로 조절할 수 있게 합니다.
- ***Technical Details***: KnowSelf는 에이전트에게 상황 판단 기준을 통해 에이전트가 자율적으로 탐색한 데이터에서 특별한 토큰을 마킹하여 학습 데이터를 수집하도록 합니다. 그리고 이러한 데이터를 바탕으로, 감독된 미세 조정(supervised fine-tuning)과 RPO loss를 활용한 2단계 학습과정을 통해 에이전트 모델에 자기 인식 능력을 부여합니다. 이를 통해 에이전트는 특정 상황을 인식하고 필요한 경우에만 반영(reflection)을 하거나 외부 지식을 도입하여 계획 능력을 최적화합니다.
- ***Performance Highlights***: KnowSelf는 두 가지 시뮬레이션된 에이전트 플래닝 데이터셋에서 다양한 강력한 베이스라인을 능가하는 성능을 보여주며, 외부 지식의 사용을 최소화하면서도 뛰어난 성능을 발휘합니다. 특히, Llama-8B와 같은 오픈 소스 모델에서도 뛰어난 능력을 발휘함을 입증하였습니다.

### [SynWorld: Virtual Scenario Synthesis for Agentic Action Knowledge Refinement](https://arxiv.org/abs/2504.03561)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.03561.png)

Vote: 14

Authors: Shuofei Qiao, Xiaobin Wang, Pengjun Xie, Huajun Chen, Runnan Fang, Yong Jiang, Fei Huang, Ningyu Zhang, Jialong Wu, Zekun Xi, Yuan Liang

- ***What's New***: SynWorld는 에이전트가 새로운 환경에서 미숙한 액션을 학습할 수 있도록 돕는 새로운 프레임워크로, 다단계 액션 수행을 포함한 가상 시나리오를 생성합니다. 이를 통해 에이전트는 현실 환경과의 피드백을 통해 자신의 액션 지식을 강화할 수 있습니다.
- ***Technical Details***: SynWorld 프레임워크는 다단계 도구 조합(task generation)을 통해 시나리오를 합성하여 시작하며, 그런 다음 몬테카를로 트리 탐색(Monte Carlo Tree Search; MCTS) 최적화를 통해 가상 시나리오에서 액션 지식을 탐색하고 발견합니다. 초기 정의된 액션 지식을 가진 루트 노드에서 시작하여, MCTS 과정을 통해 상위 신뢰 한계(uCB)가 가장 높은 노드를 선택하면서 탐색을 진행하게 됩니다.
- ***Performance Highlights***: SynWorld가 ToolBench와 HotpotQA 데이터 세트에서 각각 PASS 점수 59.33, WIN 점수 73.00 및 59.93을 달성하며 여러 Baseline보다 우수한 성과를 보여주었습니다. 특히 ToolBench의 다중 도구를 사용하는 작업에서, 경쟁 Baseline과 비교해 상당한 성능 향상을 이루었습니다.

### [MME-Unify: A Comprehensive Benchmark for Unified Multimodal Understanding and Generation Models](https://arxiv.org/abs/2504.03641)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.03641.png)

Vote: 10

Authors: Yang Shi, Chaoyou Fu, Tieniu Tan, Hongkai Chen, Liang Wang, Wulin Xie, Yi-Fan Zhang, Zhang Zhang, Bingyan Nie

- ***What's New***: MME-Unify는 통합 멀티모달 이해 및 생성 모델(U-MLLMs)의 이해, 생성, 및 혼합 모달 능력 평가를 위한 최초의 종합적인 벤치마크입니다. 이는 12개의 데이터셋으로부터 다양한 작업을 구성하여 표준화된 평가 프레임워크를 제안하며, 모델의 상호 보완적인 이해와 생성 능력을 심층 평가합니다.
- ***Technical Details***: MME-Unify는 10개의 이해 작업과 30개의 하위 작업을 포함하여, 표현적 질문-답변(Visual Q&A), 이미지/비디오 생성 및 편집을 포함한 다양한 멀티모달 과제를 다룹니다. 통합 작업(Unify Tasks)에서는 이미지 편집 및 설명, 상식 질문 대답, 보조 선 그리기, SpotDiff, Visual CoT 등의 다섯 가지 하위 작업을 설계하여, 각 단계에서 생성된 다중 모달 출력의 순차적 추론 능력을 평가합니다.
- ***Performance Highlights***: Gemini2.0-flash-exp는 전반적인 이해, 생성 및 통합 작업의 평형 잡힌 성능을 보여주며 MME-U 점수 45.57로 최고 성과를 기록했습니다. 각 모델들은 각기 다른 차원에서 상당한 점수 차이를 나타내며, 특히 Visual CoT 같은 다단계 작업에서는 오류 누적 문제로 인한 낮은 성과를 보였습니다.

### [APIGen-MT: Agentic Pipeline for Multi-Turn Data Generation via Simulated Agent-Human Interplay](https://arxiv.org/abs/2504.03601)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.03601.png)

Vote: 9

Authors: Weiran Yao, Tulika Awalgaonkar, Zuxin Liu, Thai Hoang, Juan Carlos Niebles, Shelby Heinecke, Huan Wang, Shiyu Wang, Haolin Chen, Caiming Xiong, Jianguo Zhang, Zhiwei Liu, Silvio Savarese, Akshara Prabhakar, Ming Zhu

- ***What's New***: APIGen-MT는 시뮬레이션된 에이전트와 인간의 상호작용을 통해 다회 대화 데이터를 생성하는 두 단계 프레임워크로, 현실적인 다회 대화 에이전트 데이터를 효율적으로 생성하는 혁신적인 방법입니다.
- ***Technical Details***: APIGen-MT는 두 단계로 구성됩니다. 첫 번째 단계에서는 'blueprint' 생성하기 위해 LLM 리뷰어 위원회와 피드백 루프를 활용합니다. 그런 다음, 이 설계도를 활용하여 시뮬레이션된 인간-에이전트 상호작용을 통해 전체 대화 궤적을 생성합니다. 이 과정에서 생성된 블루프린트는 형식/실행 검증과 반영 기반 메커니즘을 통해 검증됩니다.
- ***Performance Highlights***: xLAM-2-fc-r 모델군은 τ-bench 및 BFCL 벤치마크에서 GPT-4o와 Claude 3.5와 같은 프런티어 모델을 능가했습니다. 특히 다회 대화 설정에서 작은 규모의 모델들이 더 큰 모델에 비해 뛰어난 성능을 보였습니다.

### [VARGPT-v1.1: Improve Visual Autoregressive Large Unified Model via Iterative Instruction Tuning and Reinforcement Learning](https://arxiv.org/abs/2504.02949)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02949.png)

Vote: 9

Authors: Yuguo Yin, Dongchao Yang, Xianwei Zhuang, Liming Liang, Yufan Deng, Yuxin Xie, Jinghan Ru, Yuexian Zou

- ***What's New***: VARGPT-v1.1은 기존의 VARGPT 모델에 비주얼 지시 조율(Visual Instruction Tuning)과 강화 학습(Reinforcement Learning)을 결합하여 개선된 비주얼 자동회귀 모델(Visual Autoregressive Model)로 발전하였습니다. 최신의 발전된 학습 전략들과 이미지 생성 해상도의 개선을 통해 다중 모달(Multimodal) 이해와 텍스트-이미지 지시 수행에서 최첨단 성능을 달성합니다.
- ***Technical Details***: VARGPT-v1.1은 네 가지 주요 기술 혁신을 통해 개선되었습니다: (1) 비주얼 지시 조율과 DPO(Direct Preference Optimization)를 활용한 강화 학습의 결합, (2) 830만 개의 비주얼 지시 쌍으로 확장된 학습 코퍼스, (3) Qwen2-7B 백본으로 업그레이드된 언어 모델, (4) 아키텍처 수정 없이 얻은 이미지 편집 기능. 이후 비주얼 인코더와 비주얼 디코더를 통합하여 비주얼 이해와 생성 모두에서 뛰어난 능력을 발휘합니다.
- ***Performance Highlights***: 리팩터링된 VARGPT-v1.1은 SEED-Bench, POPE, MMMU 등 다수의 비주얼 이해 벤치마크에서 최첨단 성능을 기록했습니다. 특히, GQA, VQAv2, TextVQA 등 다양한 데이터셋에서 성능 향상을 보이며, 텍스트-이미지 생성에서도 일관성을 나타냅니다. 새로운 모형은 기존 VARGPT보다 높은 실행 성능을 유지하면서 더욱 효율적인 데이터 처리 및 이미지 편집 능력을 갖추게 되었습니다.

### [TransMamba: Flexibly Switching between Transformer and Mamba](https://arxiv.org/abs/2503.24067)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.24067.png)

Vote: 8

Authors: Jie Jiang, Shuaipeng Li, Di Wang, Weidong Han, Zhen Yang, Yu Cheng, Xingwu Sun, Ruobing Xie, Chengzhong Xu, Yixing Li, Zhanhui Kang

- ***What's New***: TransMamba는 Transformer와 Mamba를 통합하여 서로 다른 토큰 길이와 계층에 따라 주의 메커니즘과 상태공간모델(SSM; State Space Model) 메커니즘을 유동적으로 전환할 수 있는 새로운 프레임워크입니다. 이를 통해 두 구조의 장점을 동시에 활용하여 일정한 토큰 형식과 계층에서 주의 메커니즘 또는 상태공간모델 메커니즘 간의 매끄러운 정보 흐름을 보장합니다.
- ***Technical Details***: TransMamba는 QKV와 CBx 같은 공유 매개변수를 사용하여 Transformer와 Mamba를 결합합니다. Memory Converter를 설계하여 주의 출력 결과를 SSM 호환 상태로 변환하며, TransPoint 스케줄링을 통해 Transformer에서 Mamba로의 변환을 최적화합니다. 이를 통해 상당한 운영 효율과 성능이 확보됩니다.
- ***Performance Highlights***: TransMamba는 다양한 모델 크기와 태스크에서 전반적으로 뛰어난 성능을 보여주며, 같은 조건 하의 Transformer, Mamba2, Hybrid 모델에 비해 효율성과 성능이 우수합니다. 특히, 최적의 TransPoint 스케줄링을 적용하면 훈련 효율성이 약 25% 개선되는 것으로 나타났습니다.

### [Comprehensive Relighting: Generalizable and Consistent Monocular Human Relighting and Harmonization](https://arxiv.org/abs/2504.03011)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.03011.png)

Vote: 7

Authors: Jimei Yang, Zhixin Shu, Nanxuan Zhao, Krishna Kumar Singh, Tuanfeng Y. Wang, Simon S. Chen, Jingyuan Liu, He Zhang, Jae Shin Yoon, Junying Wang, Ulrich Neumann, Xin Sun

- ***What's New***: Comprehensive Relighting은 인물의 사진이나 동영상에서 임의의 신체 부위에 대한 조명을 조절하고 조화를 이루는 최초의 모델입니다. 데이터셋의 부족으로 기존의 이미지 기반 조명 모델은 특정 시나리오에 한정되는 문제를 해결하기 위해 사전 훈련된 확산 모델을 사용하여 일반적인 이미지 프라이어로 활용한 점이 새로운 접근입니다.
- ***Technical Details***: Comprehensive Relighting은 거칢-세분화된(coarse-to-fine) 프레임워크에서 인간 조명과 배경 조화를 공동으로 모델링하여 확장 가능성을 높였습니다. 조명 일관성을 강화하기 위해 감독되지 않은 시간 기반 조명 모델을 도입하였으며, 이는 많은 실세계 동영상에서 학습되었습니다. 이를 통해 시간적 일관성을 보장하기 위한 Spatio-temporal feature blending algorithm이 사용됩니다.
- ***Performance Highlights***: Comprehensive Relighting은 조명의 시간적 일관성 및 높은 일반성을 보여주며, 기존의 이미지 기반 인간 조명 및 조화 방법을 능가했습니다. 다양한 불연속 장면에서 강력한 성능을 입증했으며 실험 결과에 따르면 높은 품질의 조명 및 배경 조화 결과를 제공합니다.

### [ShieldAgent: Shielding Agents via Verifiable Safety Policy Reasoning](https://arxiv.org/abs/2503.22738)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.22738.png)

Vote: 7

Authors: Bo Li, Zhaorun Chen, Mintong Kang

- ***What's New***: SHIELDAGENT는 자율 에이전트의 행동 궤적에서 명시적인 안전 정책 준수를 보장하는 첫 번째 LLM 기반 가드레일 에이전트로, 논리적 추론을 통해 에이전트를 보호합니다. SHIELDAGENT-BENCH라는 데이터셋을 도입하여 SOTA 공격을 통해 6개의 웹 환경과 7개의 위험 범주에서 수집한 3,000개의 안전 관련 에이전트 명령 및 행동 궤적 쌍을 제공합니다.
- ***Technical Details***: SHIELDAGENT는 정책 문서에서 검증 가능한 규칙을 추출하고 이를 액션 기반 확률적 규칙 회로(action-based probabilistic rule circuits)로 구조화하여 안전 정책 모델을 구축합니다. 행동 궤적이 주어지면 SHIELDAGENT는 관련 규칙 회로를 불러오고 포괄적인 도구 라이브러리를 활용하여 형식 검증을 수행하여 방어 계획을 생성합니다.
- ***Performance Highlights***: SHIELDAGENT는 SHIELDAGENT-BENCH 및 기존 벤치마크에서 평균 11.3% 향상된 성능으로 SOTA를 달성했습니다. 높은 리콜(90.1%)을 갖추면서 API 요청을 64.7%, 추론 시간을 58.2% 줄임으로써 에이전트의 안전을 효과적으로 보호함과 동시에 높은 효율성을 입증했습니다.

### [HumanDreamer-X: Photorealistic Single-image Human Avatars Reconstruction via Gaussian Restoration](https://arxiv.org/abs/2504.03536)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.03536.png)

Vote: 6

Authors: Chaojun Ni, Lihong Liu, Guan Huang, Zheng Zhu, Runqi Ouyang, Guosheng Zhao, Xingang Wang, Boyuan Wang, Xiaofeng Wang

- ***What's New***: HumanDreamer-X는 단일 이미지로부터 포토리얼리스틱한 인간 아바타를 재구성하는 새로운 프레임워크를 제안합니다. 이 프레임워크는 3D Gaussian Splatting을 기반으로 초기 기하학 및 외형을 파악하고, HumanFixer를 통해 3DGS 렌더링을 복원하여 포토리얼리스틱한 결과를 보장합니다. 또한, 다중 뷰 인간 생성에서 발생하는 주의 메커니즘의 문제를 해결하기 위한 주의 조정 전략을 도입하여 기하학적 세부사항과 정체성 일관성을 효과적으로 강화합니다.
- ***Technical Details***: HumanDreamer-X는 3D Gaussian Splatting을 통해 단일 이미지에서 인간의 기본 기하학적 구조를 구축하고, 생성된 다중 보기 이미지를 통해 강력한 기하학적 및 외형 우위를 제공합니다. HumanFixer는 미세한 영상 복원을 수행하며, 영상 복원 과정에서 주의 조정 모듈을 사용하여 다중 보기의 영상 일관성을 개선합니다. 주의 마스크는 시간적 자기 주의 메커니즘 단계에서 프레임 간 관계를 조정하여 지속된 3DGS와 함께 고품질의 인간 모델을 생성합니다.
- ***Performance Highlights***: HumanDreamer-X는 PSNR에서 생성 지표를 16.45%, 복원 지표를 12.65% 개선하여 최대 25.62 dB를 달성했습니다. 이러한 성과는 다양한 데이터셋에서의 실험을 통해 확인되었으며, 야생 데이터에 대한 일반화 능력과 다양한 인간 복원 백본 모델에 대한 적용성을 보여주었습니다. 특히, CustomHumans 데이터셋에서 PSHuman 대비 PSNR, SSIM, LPIPS 및 FID 지표가 각각 12.65%, 12.07%, 58.81% 및 18.86% 향상되었습니다.

### [EvMic: Event-based Non-contact sound recovery from effective spatial-temporal modeling](https://arxiv.org/abs/2504.02402)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02402.png)

Vote: 5

Authors: Shi Guo, Xudong XU, Huchuan Lu, Lu Zhang, Dong Wang, Si Liu, Tianfan Xue, Xu Jia, Hao Yin

- ***What's New***: EvMic는 사건 기반 카메라(Event Camera)를 활용하여 비접촉 방식으로 사운드를 복구하는 새로운 접근 방식을 제시합니다. 시뮬레이션 처리 파이프라인을 통해 대규모 학습 데이터셋을 생성하고, Mamba 모델을 사용해 이벤트 스트림의 긴 시퀀스를 분석하여 더욱 우수한 공간-시간적 정보를 제공하는 시스템을 설계했습니다.
- ***Technical Details***: EvMic는 스페클 패턴(Speckle Patterns) 기반의 공간적 집적 블록(Spatial Aggregation Block)을 활용한 네트워크 구조를 갖추고 있습니다. 초고성능 시간 해상도와 이벤트데이터의 희소성(Sparsity)을 함께 이용하여 최소화된 연산으로 시각적 특징을 추출합니다. 또한 학습 기반 방식을 통해 광학적 단순함, 광시야각(Field of View), 높은 주파수 데이터를 효과적으로 얻는 시스템을 구축하였습니다.
- ***Performance Highlights***: 실험 결과, 제안된 방법은 기존의 EvPhase, RGBPhase보다 전반적으로 주파수 대역폭과 신호 일관성 측면에서 우수한 성능을 보였습니다. 특히 이벤트 카메라의 높은 시간적 해상도를 바탕으로 4kHz 및 8kHz 샘플링 속도에서 우수한 음질 복구가 가능하며, 대규모 필드 뷰에서 높은 신호 대 잡음비(SNR)를 기록하였습니다.

### [MedSAM2: Segment Anything in 3D Medical Images and Videos](https://arxiv.org/abs/2504.03600)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.03600.png)

Vote: 4

Authors: Bihui Chen, Hongwei Lyu, Adibvafa Fallahpour, Bo Wang, Sumin Kim, Reza Asakereh, Zongxin Yang, Mohammed Baharoon, Jun Ma

- ***What's New***: MedSAM2는 3D 의료 이미지 및 비디오 분할을 위한 새로운 프롬트 가능한 세그먼트이션 모델로, 다양한 조직, 병변 및 이미지 모달리티를 대상으로 우수한 성능을 보입니다. 455,000개 이상의 3D 이미지-마스크 쌍과 76,000개 이상의 프레임을 포함하는 대규모 의료 데이터셋을 사용하여 Segment Anything Model 2(SAM2)를 미세 조정(fine-tuning)하여 개발되었습니다.
- ***Technical Details***: MedSAM2는 이미지 인코더(image encoder), 메모리 주의 모듈(memory attention module), 프롬트 인코더(prompt encoder), 및 마스크 디코더(mask decoder)를 포함한 SAM2 네트워크 아키텍처에 기반합니다. 이 모델은 Hiera라는 계층적 비전 트랜스포머(hierarchical vision transformer)를 사용하여 2D 슬라이스 및 비디오 프레임에서 다중 규모의 특징을 추출합니다. 이 프레임의 특징을 기억 은행(memory bank)의 정보를 바탕으로 조건을 지정하여 처리하며, 다양한 사용자 인터랙션(예: 포인트, 바운딩 박스, 마스크)을 임베딩으로 변환하여 정확한 세그먼트이션 마스크를 생성합니다.
- ***Performance Highlights***: MedSAM2는 CT와 MRI 기관의 3D 분할에서 최고의 Dice Similarity Coefficient (DSC) 점수를 기록합니다. CT 기관에서 88.84%, CT 병변에서 86.68%, MRI 기관에서 87.06%, MRI 병변에서 88.37%, PET 병변에서 87.22%를 달성했습니다. MedSAM2는 다양한 케이스에서 SAM2.1 모델들을 능가하며, 특히 도전적인 케이스에서 낮은 변동성과 높은 일관성을 보여줍니다.

### [BEATS: Bias Evaluation and Assessment Test Suite for Large Language Models](https://arxiv.org/abs/2503.24310)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.24310.png)

Vote: 3

Authors: Alok Abhishek, Lisa Erickson, Tushar Bandopadhyay

- ***What's New***: 이 논문에서는 대형 언어 모델(Large Language Models; LLMs)의 편향성, 윤리성, 공정성 및 사실성을 평가하기 위한 새로운 프레임워크인 BEATS를 도입합니다. 이 프레임워크는 29개의 구체적인 지표를 통해 LLM 성능을 측정하며, 이러한 지표들은 인구통계학적, 인지적, 사회적 편향 및 윤리적 추론, 그룹 공정성, 사실성 관련 misinformation 위험성을 포함합니다.
- ***Technical Details***: BEATS 프레임워크는 다양한 편향 관련 행동을 식별하고 분석하기 위한 체계적이고 규모 확장이 가능한 절차를 확립합니다. 프레임워크는 대규모의 테스트 질문 데이터셋을 사용하여 LLM 출력을 평가하고, 구조화된 SQLite 데이터베이스로 응답을 저장해 벤치마크 평가 및 통계 분석을 수행합니다.
- ***Performance Highlights***: 실험 결과에 따르면, 주요 모델의 37.65%의 출력에서 일부 형태의 편향성이 발견되었습니다. 이는 중요한 의사결정 시스템에서 이러한 모델들을 사용할 때 상당한 위험을 강조합니다. 연구 결과는 LLM이 현재의 편향성과 윤리 문제를 안고 있으며, 이러한 문제를 진단하고 완화 전략을 개발하기 위한 기본 자료를 제공합니다.

### [Delineate Anything: Resolution-Agnostic Field Boundary Delineation on Satellite Imagery](https://arxiv.org/abs/2504.02534)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02534.png)

Vote: 3

Authors: Yevhenii Salii, Volodymyr Kuzin, Andrii Shelestov, Zoltan Szantoi, Bohdan Yailymov, Mykola Lavreniuk, Nataliia Kussul

- ***What's New***: 이 논문은 위성 이미지를 통해 농경지 경계를 해상도에 관계 없이 구별하는 새로운 기법 'Delineate Anything'과 이를 위한 대규모 다중 해상도 데이터셋 FBIS-22M을 소개합니다. 이 연구는 인스턴스 세분화(instance segmentation) 관점을 도입하여 기존의 한계를 극복하고, 농업 분야의 이미지 분석을 한 단계 끌어올렸습니다.
- ***Technical Details***: FBIS-22M은 Sentinel-2, Planet, Maxar, Pleiades 등의 다양한 위성 플랫폼에서 수집된 673,000여 장의 고해상도 위성 이미지와 약 22,900,000개의 농작물 인스턴스 마스크로 구성되어 있습니다. Delineate Anything 모델은 YOLOv11 기반으로 구축되었으며, 다양한 해상도를 처리할 수 있는 해상도 비의존적(design-agnostic) 설계를 채택하고 있습니다.
- ***Performance Highlights***: Delineate Anything 모델은 mAP@0.5에서 0.720, mAP@0.5:0.95에서 0.477의 성능을 기록하며, 이전 기술 대비 각각 88.5%와 103%의 성능 향상을 이루었습니다. 또한, 독보적으로 빠른 추론 속도를 자랑하며 실시간 적용에 적합한 성능을 보였습니다. 특정 지역에 대한 제로-샷(Zero-Shot) 일반화에서도 강력한 성능을 보여주었습니다.

### [Real-is-Sim: Bridging the Sim-to-Real Gap with a Dynamic Digital Twin for Real-World Robot Policy Evaluation](https://arxiv.org/abs/2504.03597)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.03597.png)

Vote: 3

Authors: Brandon May, Krishan Rana, Lingfeng Sun, Karl Schmeckpeper, Maria Vittoria Minniti, Jad Abou-Chakra, Laura Herlant

- ***What's New***: 본 연구에서는 'real-is-sim'이라는 새로운 행동 클로닝 프레임워크를 제안합니다. 이는 실세계 로봇 정책 평가를 위한 동적 디지털 트윈(Dynamic Digital Twin)을 사용하여 시뮬레이션과 실제 세계 간의 격차를 줄이고자 합니다. 이 방법을 통해 시뮬레이터와 실제 세계의 상태를 지속적으로 동기화하여, 정책 개발 전체 과정에서 데이터 수집, 훈련, 배치에까지 활용할 수 있습니다.
- ***Technical Details***: real-is-sim 프레임워크는 Embodied Gaussians를 기반으로 한 실시간 수정 가능한 물리 시뮬레이터로 구성되어 있습니다. 시뮬레이터는 현실 세계 관측값을 사용해 물리적 세계와의 동기화를 유지하고, 가상 카메라를 통해 이미지를 렌더링하거나 객체의 자세 정보를 얻어내는 유연한 상태 표현을 지원합니다. 또한 Conditional Flow Matching(CFM)이라는 생성적 모델링 방법을 사용하여 행동 시퀀스를 부드럽고 일관되게 제어합니다.
- ***Performance Highlights***: PushT 조작 작업에서 real-is-sim을 검증한 결과, 시뮬레이터에서 얻은 성공률과 실제 세계에서의 평가 간에 강력한 상관관계가 있음을 확인했습니다. 오프라인 평가 모드에서의 성공률이 실제 테스트에 대한 신뢰할 수 있는 대리 지표로서 기능할 수 있음을 보여주었습니다. 또한, 오프라인 데이터 수집 및 증강을 통해 단번에 성능을 크게 향상시킬 수 있었으며, 특히 그리퍼에 장착된 가상 카메라를 사용하는 정책이 가장 높은 성공률을 기록했습니다.

### [Slow-Fast Architecture for Video Multi-Modal Large Language Models](https://arxiv.org/abs/2504.01328)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.01328.png)

Vote: 1

Authors: Humphrey Shi, Jitesh Jain, Kai Wang, Zhiding Yu, Chieh-Yun Chen, Shihao Wang, Min Shi, Guilin Liu, Junjun Xiong

- ***What's New***: 이 논문에서는 비디오 기반 멀티모달 대형 언어 모델(Video Multi-Modal Large Language Models; MLLMs)을 위한 혁신적인 슬로우-패스트 아키텍처(Slow-Fast Architecture)를 제안합니다. 이는 비디오 표현을 고정된 수의 '빠른(fast)' 비주얼 토큰으로 압축하여 LLM에 미리 보여주고, '느린(slow)' 비주얼 토큰과 텍스트 임베딩을 크로스 어텐션(cross-attention)을 통해 상호작용시킴으로써 공간 세부 사항을 보존하면서 더 많은 입력 프레임을 사용할 수 있게 합니다.
- ***Technical Details***: 슬로우-패스트 아키텍처는 듀얼 토큰 전략을 사용합니다: '빠른' 비주얼 토큰은 압축된 비디오 특징을 LLM에 제공하여 전체 상황을 미리 보여줍니다. 반면, '느린' 비주얼 토큰은 하이브리드 디코더 레이어(Hybrid Decoder Layers)에서 텍스트 임베딩과 크로스 어텐션을 통해 상호작용합니다. 이로 인해 역방향 손실이 없는 시각 정보를 LLM에 효율적으로 통합할 수 있습니다. 실험에서는 16에서 128프레임으로 입력 용량을 확장하면서 3%의 계산 증가만으로 수행할 수 있음을 보여줍니다.
- ***Performance Highlights***: 제안된 슬로우-패스트 아키텍처는 5개의 비디오 이해 벤치마크에서 평균 성능이 16% 향상되었으며, 7B 모델이 유사한 크기의 모델들 중에서 최첨단 성능을 달성했습니다. 다른 비디오 MLLM과 결합 시 효율성과 확장성을 향상시킬 수 있는 플러그 앤 플레이 디자인을 제공합니다.

### [SPF-Portrait: Towards Pure Portrait Customization with Semantic Pollution-Free Fine-tuning](https://arxiv.org/abs/2504.00396)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.00396.png)

Vote: 1

Authors: Qingyu Li, Long Zeng, Wenyu Qin, Zhichao Liao, Pingfa Feng, Pengfei Wan, Weicheng Xie, Xiaole Xian, Linlin Shen

- ***What's New***: SPF-Portrait는 텍스트 기반 초상화 커스터마이징에서 의미 오염(Semantic Pollution)을 제거하는 혁신적인 방법을 제안합니다. 이 연구는 대조 학습(Contrastive Learning)을 활용하여, 사용자 정의 특성을 순수하게 캡처하고 증분 학습(Incremental Learning)을 가능하게 하여 기존 모델의 성능을 보존하면서 목표 의미에 효과적으로 반응하도록 합니다.
- ***Technical Details***: SPF-Portrait는 이중 경로 대조 학습 경로(Dual-Path Contrastive Learning Pipeline)를 통해 동작합니다. 여기에는 얼어붙은 기존 모델을 기준으로 한 레퍼런스 경로(Reference Path)와 반응 경로(Response Path)가 포함됩니다. 레퍼런스 경로는 기본 텍스트(Base Text)만을 입력으로 받아 안정적인 기준을 제공합니다. 반면 반응 경로는 기본 및 목표 텍스트(Target Text)를 입력으로 받아 훈련됩니다. 기존 모델의 성능을 보존하기 위해 주어진 경로의 크로스-어텐션 레이어에서 주의 기능(Attention Features)을 추출합니다. 또한, 의미 인식 세밀 제어 지도(Semantic-Aware Fine Control Map)를 설계하여 대조 경로의 중간 특징의 정렬을 공간적으로 안내합니다.
- ***Performance Highlights***: SPF-Portrait는 기존 방법보다 텍스트 기반 초상화 커스터마이징 작업에서 의미 오염을 방지하는 데 훨씬 뛰어난 성능을 보여줍니다. 실험 결과, SPF-Portrait는 성능 보존과 목표 의미의 반응성에서 모두 최고 수준의 성능을 달성합니다. 또한, 세그먼트 일관성(Seg-Cons) 측면에서 뛰어난 픽셀 수준의 정렬 정확성을 입증하여 다른 방법을 크게 능가합니다.

