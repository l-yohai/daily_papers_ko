## Daily Papers (2025-04-23)

### [Kuwain 1.5B: An Arabic SLM via Language Injection](https://arxiv.org/abs/2504.15120)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15120.png)

Vote: 94

Authors: Mohamed Motaism Hamed, Khalil Hennara, Safwan AlModhayan, Sara Chrouf, Zeina Aldallal, Omar Hadid

- ***What's New***: Kuwain 1.5B는 기존 소스 모델에 아랍어를 주입(Language Injection)하여 아랍어 성능을 획기적으로 개선하는 새로운 방법을 제안합니다. 이 모델은 기존의 영어 중심 언어 모델(English-centric LLM)을 아랍어 지원으로 확장하면서, 기존 지식은 그대로 보존합니다.
- ***Technical Details***: Kuwain 1.5B는 TinyLlama 1.1B 모델을 바탕으로 8개의 새로운 레이어를 추가하고, 26,000개의 아랍어 토큰을 확장하여 기존 레이어는 고정한 상태로 학습합니다. 이 방법은 tokenizer의 어휘 확장을 통해 아랍어의 정교함과 효율성을 높입니다.
- ***Performance Highlights***: 아랍어 벤치마크에서 Kuwain 모델은 기존 TinyLlama를 능가하는 성능을 보였으며, 8% 향상된 성능을 기록했습니다. 또한, 20%의 영어 데이터를 사용하여도 기존 영어 성능을 유지하거나 약간 향상시킬 수 있었습니다.

### [TTRL: Test-Time Reinforcement Learning](https://arxiv.org/abs/2504.16084)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.16084.png)

Vote: 57

Authors: Xuekai Zhu, Li Sheng, Bowen Zhou, Kaiyan Zhang, Ganqu Cui, Ning Ding, Youbang Sun, Yuxin Zuo, Biqing Qi, Shang Qu

- ***What's New***: TTRL(Test-Time Reinforcement Learning)는 라벨이 없는 데이터를 사용하여 대형 언어 모델(Large Language Models; LLMs)을 강화학습(Reinforcement Learning)으로 학습시키는 혁신적인 방법을 제안합니다. 이는 사전 학습된 모델의 프라이어를 활용하여 모델의 자기 진화(Self-evolution)를 가능하게 하며, 주어진 데이터의 암묵적인 패턴을 파악하고, 테스트 데이터 기반에서 보상 추정에 기반하여 성능을 향상시킵니다.
- ***Technical Details***: TTRL은 다수결 투표(Majority Voting) 보상 함수를 사용하여 라벨이 없는 데이터에서 규칙 기반의 보상을 생성합니다. 모델은 상태로 주어진 프롬프트에서 다수의 출력 후보를 생성하고, 다수결 투표를 통해 최적 행동의 대리로 사용되는 합의 출력을 생성합니다. 이는 라벨이 없는 데이터에서 강화학습 목표를 극대화하여 모델 성능을 향상시킵니다.
- ***Performance Highlights***: 실험 결과, TTRL을 적용한 Qwen-2.5-Math-7B 모델은 AIME 2024에서 159% 성능 향상을 달성하며, AMC와 MATH-500에서도 평균 84%의 성능 향상을 보였습니다. 이는 라벨이 없는 테스트 데이터에서 자기 진화를 통해 대규모 라벨 데이터셋을 학습한 기존 모델에 필적하는 성능을 달성했다는 점에서 의미가 있습니다.

### [The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks](https://arxiv.org/abs/2504.15521)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15521.png)

Vote: 50

Authors: Kaifu Zhang, Chenyang Lyu, Minghao Wu, Sinuo Liu, Longyue Wang, Yu Zhao, Weixuan Wang, Weihua Luo, Huifeng Yin, Xintong Wang

- ***What's New***: 이 연구는 2021년부터 2024년까지 발표된 2000개 이상의 다국어 벤치마크를 분석하여, 다국어 평가의 현재 상태와 앞으로의 방향을 정리합니다. 영어가 여전히 과도하게 대표되는 현황과 번역 대신 원어 콘텐츠를 사용하는 경향성을 지적하며, 문화적 및 언어적으로 맞춤화된 벤치마크의 중요성을 강조합니다.
- ***Technical Details***: 데이터는 arXiv cs.CL 카테고리에서 수집하고 필터링을 통해 148개국에서 온 2024개의 연구를 포함시켰습니다. 다국어 벤치마크는 번역 대신 원어 콘텐츠를 주로 사용하며, 번역은 인간 번역이 13.2%에 불과합니다. STEM 관련 작업은 인간 평가와의 강한 상관관계를 보이지만, 전통적인 자연어 처리(NLP) 작업은 상관관계가 약합니다.
- ***Performance Highlights***: STEM 관련 과제에서 ARC와 같은 벤치마크는 인간 평가와 매우 높은 상관관계를 보였으나, XQuAD 같은 질문응답 작업은 낮은 상관관계를 보였습니다. CMMLU와 같은 지역화된 벤치마크가 인간 판단과의 상관관계에서 더 높은 점수를 기록해, 번역된 벤치마크보다 우수함을 입증했습니다.

### [Describe Anything: Detailed Localized Image and Video Captioning](https://arxiv.org/abs/2504.16072)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.16072.png)

Vote: 39

Authors: Long Lian, Marco Pavone, Yifan Ding, Boyi Li, Ming-Yu Liu, Trevor Darrell, Hanzi Mao, Sifei Liu, Adam Yala, Yin Cui, Yunhao Ge

- ***What's New***: 이 연구는 사물을 상세하고 위치적으로 지역화된 캡션으로 설명할 수 있는 새로운 모델인 Describe Anything Model(DAM)을 소개합니다. 특히, 이미지와 동영상의 특정 지역에 초점을 맞추어 설명할 수 있는 능력을 새로운 방식으로 접근합니다.
- ***Technical Details***: DAM은 고해상도로 타겟 지역을 인코딩하는 포컬 프롬프트(focal prompt)와 넓은 맥락 내에서 정확한 위치화를 달성하는 지역화된 비전 백본(localized vision backbone)을 중심으로 설계되었습니다. 추가로, 반지도 학습(semi-supervised learning) 기반 데이터 파이프라인(DLC-SDP)을 개발하여 다양한 웹 이미지로부터 고품질의 로컬라이즈된 캡션을 생성합니다. 또한, 고품질의 DLC 데이터 세트가 부족한 문제를 해결하기 위해 DLC-Bench라는 벤치마크를 도입했습니다.
- ***Performance Highlights***:  DAM은 7개의 벤치마크에서 새로운 state-of-the-art를 설정하였습니다. 특히, 객체와 부품을 구별하는 것이 요구되는 어려운 PACO 벤치마크에서 탁월한 성능을 보이며, 이전 최고 성능 대비 23.2% 향상된 73.2%의 Semantic IoU와 8.5% 향상된 84.2%의 Semantic Similarity를 달성했습니다. 또한, Ref-L4 벤치마크에서 33.4% 이상의 상대적 개선을 보여줍니다. 종합적으로, DAM은 현재 사용 가능한 일반 모델 및 지역별 특화 모델을 뛰어넘는 성능을 보여주었습니다.

### [Learning Adaptive Parallel Reasoning with Language Models](https://arxiv.org/abs/2504.15466)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15466.png)

Vote: 34

Authors: Long Lian, Alane Suhr, Trevor Darrell, Kurt Keutzer, Jiayi Pan, Xiuyu Li, Charlie Snell, Yifei Zhou, Adam Yala

- ***What's New***: Adaptive Parallel Reasoning(APR)은 언어 모델(Language Models)이 동시에 직렬 및 병렬 계산을 조정할 수 있도록 하는 새로운 추론 프레임워크입니다. 이를 통해 모델은 스폰(spawn) 및 조인(join) 작업을 사용하여 다중 스레드 추론을 적용할 수 있으며, 사전 정의된 추론 구조 없이 강화 학습을 통해 성공률을 최적화합니다.
- ***Technical Details***: APR은 언어 모델이 주어진 문맥에서 자식을 생성하고, 자식 스레드가 각각 독립적으로 추론 경로를 탐색하는 스레드 구조를 사용합니다. 각 스레드는 자신에게 할당된 한정된 문맥에서 실행되고, 실행 결과를 부모 스레드로 반환합니다. SGLang 프레임워크를 기반으로 하여 실시간 지연을 줄이는 병렬 실행이 가능합니다. 모델은 자식과 부모 스레드 모두에 대해 종단간 강화 학습을 통해 미세 조정됩니다.
- ***Performance Highlights***: Countdown 작업 실험에서 APR은 동일한 컨텍스트 윈도우 내에서 83.4%의 성공률을 기록하였고, 이는 직렬 방법의 60.0%에 비해 높은 성능을 보여줍니다. 또한 APR은 20,000 토큰에서 80.1%의 성능을 달성하여, 동일 토큰 수 대비 이전 방법의 66.6%보다 우수한 확장성을 보여줍니다. 5,000ms의 동일한 지연에서 75.2%의 정확도를 기록하여 직렬 추론 방법의 57.3%보다 우수한 성과를 달성하였습니다.

### [BookWorld: From Novels to Interactive Agent Societies for Creative Story Generation](https://arxiv.org/abs/2504.14538)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.14538.png)

Vote: 17

Authors: Xintao Wang, Tian Qiu, Deqing Yang, Jiaqing Liang, Yanghua Xiao, Yiting Ran

- ***What's New***: BookWorld는 책을 기반으로 한 멀티 에이전트 시스템(Multi-Agent Systems)을 통해 소설 기반의 상호작용 에이전트 사회를 만드는 최초의 시스템을 소개합니다. 이 시스템은 기존의 소설 세계와 캐릭터를 시뮬레이션하여 창의적 이야기 생성과 상호작용 게임 및 사회 시뮬레이션을 가능하게 합니다.
- ***Technical Details***: BookWorld는 원본 소설로부터 캐릭터 데이터와 배경 지식을 추출하여 롤 에이전트(Role Agents)와 월드 에이전트(World Agent)를 통합한 멀티 에이전트 시스템을 구축합니다. 에이전트는 각 장면에서 소통, 거래, 작업 등의 다양한 상호작용을 수행하며, 이 과정에서 기억과 상태, 목표를 업데이트합니다. 월드 에이전트는 시뮬레이션의 흐름을 조정하여 환경 피드백을 제공합니다. 또한, 소설에서 월드뷰(Worldview) 데이터를 체계적으로 수집하여 에이전트가 적절하게 행동할 수 있도록 지원합니다.
- ***Performance Highlights***: BookWorld는 75.36%의 승률로 이전 방법들보다 높은 질의 창의적인 이야기를 생성합니다. Immersion과 Character Fidelity 측면에서 우수한 성과를 보였으나, Storyline Quality와 Writing Quality에서는 도전 과제를 보이고 있습니다.

### [Efficient Pretraining Length Scaling](https://arxiv.org/abs/2504.14992)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.14992.png)

Vote: 15

Authors: Sijun Zhang, Ya Wang, Jianqiao Lu, Shen Yan, Xun Zhou, Yutao Zeng, Bohong Wu

- ***What's New***: 본 연구는 사전 학습 단계에서 길이 확장(length scaling)을 효율적으로 가능하게하는 새로운 프레임워크인 Parallel Hidden Decoding Transformer (PHD-Transformer)을 제안합니다. 이는 독창적인 KV 캐시 관리 전략을 통해 전통적인 트랜스포머와 동일한 KV 캐시 크기를 유지하면서도 효율적으로 길이 확장을 지원하는 최초의 시스템입니다.
- ***Technical Details***: PHD-Transformer는 원본 토큰과 숨겨진 디코딩 토큰을 구분하여 각기 다른 KV 캐시 전략을 채택합니다. 슬라이딩 윈도우 주의(SWA)와 청크 방식(sliding window attention)을 도입하여 지역적 종속성을 유지하고 선형 확장 시간을 제거합니다. 실험을 통해 PHD-Transformer 시리즈가 여러 벤치마크에서 상당한 정확도 향상을 보이며, 전처리 및 디코딩 단계에서 허용 가능한 컴퓨팅 오버헤드를 유지함을 입증했습니다.
- ***Performance Highlights***: 실험 결과, PHD-CSWA 모델은 여러 벤치마크에서 평균 2.0%의 정확도 향상과 훈련 손실에서 0.034 감소를 기록했습니다. 이는 길이 확장이 효과적이고 효율적임을 증명합니다. 또한 PHD 시리즈는 추론 단계에서 거의 추가적인 지연을 초래하지 않으며, 이는 메모리 제한된 상황에서도 효율적인 작동을 보장합니다.

### [IV-Bench: A Benchmark for Image-Grounded Video Perception and Reasoning in Multimodal LLMs](https://arxiv.org/abs/2504.15415)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15415.png)

Vote: 15

Authors: Yancheng He, Zhoufutu Wen, Jun Ma, Zhenzhu Yang, David Ma, Meng Cao, Yuanxing Zhang, Ge Zhang, Zhongyuan Peng, Shiwen Ni, Boyu Feng, Wenhao Huang, Jincheng Ren, Jarvis Guo, King Zhu, Jiaheng Liu, Xiaojie Jin, Xiao Gu, Zhenlin Wei, Yifan Yao

- ***What's New***: IV-Bench는 이미지 기반 비디오 인식 및 추론을 평가하기 위한 최초의 종합적 벤치마크입니다. 기존의 멀티모달 대형 언어 모델(Multimodal Large Language Models; MLLMs)이 이미지 맥락을 활용하여 비디오 콘텐츠를 이해하고 추론하는 데에 제한적이라는 점을 밝혀내어, 이를 개선하기 위해 설계되었습니다.
- ***Technical Details***: IV-Bench는 967개의 비디오와 2,585개의 이미지-텍스트 쿼리로 구성되어 있으며, 이는 13개의 다양한 작업(7개의 인식 작업과 6개의 추론 작업)으로 나누어져 있습니다. 데이터셋은 외부에서 수집된 이미지를 포함하여 5개의 대표적인 카테고리(지식, 영화 및 TV, 스포츠 경기, 예술 공연, 생활 기록)를 다루며, 각 작업은 비디오 이해에서 이미지의 중요성을 평가합니다.
- ***Performance Highlights***: IV-Bench에서 테스트된 최신 MLLM 모델들은 최상의 성능에서조차 28.9%의 정확도만을 달성하여, 이미지 기반 비디오 인식 및 추론이 여전히 큰 도전 과제임을 보여줍니다. 특히 복잡한 추론 작업에서는 모델들이 크게 어려움을 겪고 있으며, 이는 MLLMs의 더 나은 이미지-비디오 인식 및 추론 능력이 필요함을 시사합니다.

### [CheXWorld: Exploring Image World Modeling for Radiograph Representation Learning](https://arxiv.org/abs/2504.13820)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.13820.png)

Vote: 14

Authors: Yulin Wang, Chenxin Tao, Shiji Song, Yang Yue, Gao Huang, Pan Liu

- ***What's New***: CheXWorld는 최초로 방사선 사진을 위한 자기 지도 기반(Self-supervised) 세계 모델(World Model)을 도입했습니다. 이 framework는 방사선 영상에서 의학적 지식을 세 가지 핵심 요소로 모델링하며, 방사선과 전문성이 요구되는 세 가지 차원의 의학적 지식을 통합합니다.
- ***Technical Details***: CheXWorld는 세 가지 세계 모델링 과제를 제안합니다: 1) 국소 해부 구조 모델링(Local Anatomical Structure Modeling)은 뼈나 폐 세그먼트 등의 국소 해부학적 구조를 예측합니다. 2) 전반적인 해부 배열 모델링(Global Anatomical Layout Modeling)은 심장, 폐, 횡격막의 상대적 위치를 이해하며 인체의 전반적인 해부학적 구성을 학습합니다. 3) 도메인 변동 모델링(Domain Variation Modeling)은 방사선 사진의 다양한 도메인 간 전환을 학습합니다.
- ***Performance Highlights***: CheXWorld는 8개의 의학 영상 분류 및 세분화 벤치마크에서 기존의 자기 지도 학습(SSL) 방법과 대규모 의학 기반 모델을 능가하며 뛰어난 성능을 보여주었습니다. 특히 대규모 데이터가 아닌 상황에서도 학습 효율과 전이 학습 능력을 증명하였습니다.

### [Personalized Text-to-Image Generation with Auto-Regressive Models](https://arxiv.org/abs/2504.13162)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.13162.png)

Vote: 13

Authors: Xian Liu, Yao Teng, Xihui Liu, Kaiyue Sun

- ***What's New***: 이 논문은 개인화된 이미지 합성을 위한 방법으로 오토레그레시브 모델 (Auto-Regressive Models)을 최적화한 결과를 제시합니다. 텍스트와 이미지 모델링을 위해 통합된 아키텍처를 사용하는 오토레그레시브 모델은 개인화된 이미지 생성 도메인에서 적극적으로 탐구되지 않았던 방향으로 새로운 연구를 개척했습니다.
- ***Technical Details***: 제안된 두 단계의 훈련 전략은 텍스트 임베딩 최적화와 트랜스포머 레이어 미세 조정을 결합합니다. Lumina-mGPT 7B 모델을 실험에 사용하여, 이 접근 방식이 Textual Inversion 및 Dreambooth와 같은 기존 최적화 기반 방법과 성능이 유사함을 보였습니다. 이 과정에서 각 주제의 고유 식별자를 찾고 트랜스포머 레이어를 참조 이미지 집합으로 미세 조정합니다.
- ***Performance Highlights***: 제안된 방법을 통해 오토레그레시브 모델은 Dreambooth와 유사한 주제 충실도와 프롬프트 준수를 달성했으며, 이는 오토레그레시브 모델이 새로운 개념을 포함하는 데 있어 영향을 미치지 않으면서도 기존의 생성 능력을 유지할 수 있음을 보여줍니다. Dreambench 데이터셋에서 Textual Inversion, Re-Imagen 및 BLIP-Diffusion과 같은 모델들보다 우수한 성능을 나타냈습니다.

### [Vidi: Large Multimodal Models for Video Understanding and Editing](https://arxiv.org/abs/2504.15681)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15681.png)

Vote: 11

Authors: Celong Liu, Qingyu Chen, Xing Mei, Jiamin Yuan, Xueqiong Qu, Xiaohui Shen, Lu Guo, Lusha Li, Wen Zhong, Vidi Team, Chia-Wen Kuo, Guang Chen, Tong Jin, Rachel Deng, Longyin Wen, Sijie Zhu, Wei Lu, Fan Chen, Stuart Siew, Lingxi Zhang, Xin Gu, Dawei Du

- ***What's New***: Vidi는 다양한 비디오 이해 및 편집 시나리오를 위한 대형 멀티모달 모델(Large Multimodal Models; LMMs)입니다. 이 첫 번째 릴리스는 주로 시각적, 청각적 이해를 통해 자연어 쿼리에 따라 적절한 시간 범위를 식별하는 'Temporal Retrieval' 작업에 중점을 둡니다. 또한 Vidi는 VUE-TR 벤치마크를 소개하여 기존 벤치마크보다 훨씬 긴 비디오 길이를 처리할 수 있도록 설계되었습니다.
- ***Technical Details***: Vidi 모델은 텍스트, 비전, 오디오라는 세 가지 입력 양식을 사용하며, Decomposed Attention 메커니즘을 채택해 매우 긴 비디오 시퀀스를 효과적으로 처리합니다. 모델은 세 가지 주요 교육 단계를 거치며, 각 단계에서 다양한 모달리티 데이터를 학습하고 알맞은 타임스탬프와 쿼리의 동기화를 강화합니다. Vidi의 아키텍처는 GPT-4o 및 Gemini 같은 기존 모델보다 더 긴 비디오에서의 동시대 처리가 가능하고, 모달리티 간 균형 잡힌 정보를 처리합니다.
- ***Performance Highlights***: Vidi는 가장 긴 비디오 구간에서도 상당한 성능을 보여주며, 다양한 쿼리 형식과 모달리티에서 우수한 결과를 냅니다. 특히 Ultra-long 카테고리(>60분)에서의 IoU는 업계의 다른 최첨단 모델들과 비교해도 더 높은 수치를 기록합니다. 이는 비디오 편집 및 검색 작업 시 Vidi의 정확하고 총체적인 능력을 입증합니다.

### [LiveCC: Learning Video LLM with Streaming Speech Transcription at Scale](https://arxiv.org/abs/2504.16030)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.16030.png)

Vote: 11

Authors: Zejun Ma, Ziyun Zeng, Mike Zheng Shou, Wei Li, Yiqi Lin, Joya Chen

- ***What's New***: LiveCC는 대규모의 스트리밍 음성 인식(ASR) 전사 데이터를 활용하여 비디오 LLM(Video LLM)을 훈련하는 혁신적인 방식을 제안합니다. 이 방법은 비디오 프레임과 ASR 단어를 타임스탬프에 따라 밀접하게 교차 배치하여 모델이 정교한 시간적 상관관계를 학습할 수 있도록 합니다.
- ***Technical Details***: LiveCC의 데이터 프로덕션 파이프라인은 새롭고 정확한 ASR 전사를 생성하기 위해 WhisperX를 사용하며, YouTube 비디오 및 캡션를 처리하여 샘플을 수집합니다. 수집된 데이터셋은 사전 훈련을 위한 Live-CC-5M 및 고품질 지도 세분화 미세 조정을 위한 Live-WhisperX-526K로 나뉩니다. 모델은 시계열적으로 정렬된 ASR 단어와 프레임 피처 수집을 통해 학습하며, 이는 모델이 일종의 스트리밍 해석을 가능하게 합니다.
- ***Performance Highlights***: LiveCC-7B-Instruct 모델은 고급 72B 모델들을 뛰어넘는 실시간 주석 품질을 자랑하며, VideoMME 및 OVOBench와 같은 인기 비디오 질문 응답(QA) 벤치마크에서도 7B/8B 스케일에서 최첨단 결과를 달성했습니다. 이러한 실험 결과는 우리의 방법론이 실시간 비디오 주석과 포괄적인 비디오 이해 모두에서 강력한 성능을 발휘할 수 있음을 보여줍니다.

### [LLMs are Greedy Agents: Effects of RL Fine-tuning on Decision-Making Abilities](https://arxiv.org/abs/2504.16078)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.16078.png)

Vote: 10

Authors: Thomas Schmied, Jordi Grau-Moya, Markus Wulfmeier, Razvan Pascanu, Jörg Bornschein

- ***What's New***: 이 논문은 대규모 언어 모델(LLMs; Large Language Models)이 의사결정 시나리오에서 서브 최적의 탐사와 '알고 행하는 간극'(knowing-doing gap) 문제로 인해 성능이 저하되는 주요 원인을 분석합니다. LLM의 탐색 능력을 강화시키고자 자가 생성된 Chain-of-Thought(CoT) 논리를 통해 강화 학습(RL; Reinforcement Learning) 파인튜닝을 적용하며, 이를 통해 LLM의 탐색 능력을 향상시키고 이러한 단점을 극복할 수 있음을 보입니다.
- ***Technical Details***: 이 연구는 다중 무장 밴딧(multi-armed bandits)와 문맥적 밴딧(contextual bandits) 시나리오, 그리고 Tic-tac-toe 환경에서 시험을 진행하였습니다. 감마(Gemma2) 모델을 2B, 9B, 27B 스케일로 실험하여 RL 파인튜닝(RLFT)의 효과를 시험하였으며, RLFT는 자가 생성된 CoT 논리로 환경 보상을 통해 학습합니다. 전통적 탐사 메커니즘(e.g., 𝜖-greedy)과 LLM 특유의 접근법(e.g., self-correction, self-consistency)을 평가하여 의사결정 능력을 강화할 수 있도록 하였습니다.
- ***Performance Highlights***: Gemma2 모델의 RLFT는 탐색을 증가시키고 알고 행하는 간극을 좁힘으로써 LLM의 의사결정 능력을 크게 개선시켰습니다. 2B 모델의 경우 RLFT는 행동 커버리지를 12% 증가시켰으며, 탐사 보너스를 추가해 UCB(EXPERT) 에 근접한 수준으로 성능이 향상되었습니다. Tic-tac-toe 환경에서는 RL 파인튜닝이 랜덤 상대에 대한 승률을 평균 0.15(15%의 승률)에서 0.75까지 높였습니다.

### [WALL-E 2.0: World Alignment by NeuroSymbolic Learning improves World Model-based LLM Agents](https://arxiv.org/abs/2504.15785)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15785.png)

Vote: 8

Authors: Guodong Long, Deheng Ye, Siyu Zhou, Chengqi Zhang, Jing Jiang, Tianyi Zhou, Yijun Yang

- ***What's New***: WALL-E 2.0은 대형 언어 모델 에이전트(LLM agents)의 성능을 향상시키기 위해 훈련 없이 환경의 상징적 지식을 학습하는 '월드 얼라인먼트(World Alignment)' 방법을 제안합니다. 이는 뉴로심볼릭 학습(NeuroSymbolic Learning)을 통해 행동 규칙(action rules), 지식 그래프(knowledge graphs), 장면 그래프(scene graphs)를 학습하여 LLM의 선행 지식을 보완합니다.
- ***Technical Details***: WALL-E 2.0은 모델 예측 제어(Model-Predictive Control; MPC) 프레임워크를 활용하여 훈련이 필요 없는 모델 기반 에이전트를 제안합니다. 뉴로심볼릭 월드 모델과 상호작용하여 LLM 에이전트는 향후 행동 단계를 효율적으로 최적화할 수 있습니다. 이는 LLM의 추론 능력을 활용해 간결한 상징적 지식을 추출하고 이를 실행 가능한 코드 규칙으로 변환하여 의사결정을 안내합니다.
- ***Performance Highlights***: WALL-E 2.0은 Mars와 ALFWorld와 같은 개방형 환경에서 기존 방법들을 현저히 능가하며, Mars에서 기준치보다 16.1%에서 51.6% 높은 성공률을 기록하고, ALFWorld에서는 4회 반복만에 98%의 성공률을 달성하였습니다. 이 결과는 현재 LLM의 예측이 환경 동적성에 맞춰질 경우, 높은 성능의 모델 기반 에이전트가 가능함을 시사합니다.

### [From Reflection to Perfection: Scaling Inference-Time Optimization for Text-to-Image Diffusion Models via Reflection Tuning](https://arxiv.org/abs/2504.16080)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.16080.png)

Vote: 6

Authors: Peng Gao, Le Zhuo, Yi Xin, Hongsheng Li, Renrui Zhang, Liangbing Zhao, Yue Liao, Mohamed Elhoseiny, Sayak Paul

- ***What's New***: 이 논문에서는 Text-to-Image 확산 모델(Text-to-Image Diffusion Models)의 추론 시간을 최적화하기 위한 ReflectionFlow라는 새로운 프레임워크를 소개합니다. 이 프레임워크는 기존 확산 모델이 가지는 한계를 극복하고자, 모델이 스스로 생성물을 반성하고 수정할 수 있도록 합니다.
- ***Technical Details***: ReflectionFlow는 세 가지 추론 시간 스케일링 축을 도입합니다: 노이즈 레벨 스케일링(Noise-Level Scaling)은 잠재 초기화를 최적화하고, 프롬프트 레벨 스케일링(Prompt-Level Scaling)은 정밀한 의미론적 안내를 제공합니다. 특히, 리플렉션 레벨 스케일링(Reflection-Level Scaling)은 명시적인 피드백을 통해 이전 세대의 결과물을 평가 및 수정합니다. 이를 위해 100만 개의 트리플릿을 포함하는 GenRef 데이터셋을 구축하였으며, 이를 이용해 최신 확산 Transformer 모델인 FLUX.1-dev을 튜닝하였습니다.
- ***Performance Highlights***: ReflectionFlow는 기존의 단순 노이즈 레벨 스케일링 방법보다 성능이 크게 향상되었음을 보여줍니다. 다양한 추론 실행 시간 예산 상에서도 지속적으로 이미지 생성 품질을 개선하며, 특히 복잡한 프롬프트에서 상당한 성능 향상을 이룰 수 있습니다.

### [RealisDance-DiT: Simple yet Strong Baseline towards Controllable Character Animation in the Wild](https://arxiv.org/abs/2504.14977)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.14977.png)

Vote: 5

Authors: Weihua Chen, Jingkai Zhou, Chao Fan, Min Wei, Yifan Wu, Shikai Li, Fan Wang, Wei Jiang

- ***What's New***: RealisDance-DiT는 야생 환경에서 컨트롤 가능한 캐릭터 애니메이션을 위한 강력한 베이스라인을 제공하는 새로운 접근 방법을 제시합니다. 이 연구에서는 기존의 무거운 Reference Net 설계의 한계를 지적하며 간단한 모델 수정과 유연한 fine-tuning 전략만으로도 충분한 성능을 발휘할 수 있음을 증명하였습니다.
- ***Technical Details***: RealisDance-DiT는 동영상 기반 모델 Wan-2.1에 기반하여 설계되었으며, 조건 입력 레이어 추가와 RoPE (RoPE position encoding) 위치 인코딩을 수정하였습니다. 이는 Reference Net 사용 대신 모델의 고유 능력을 최대한 활용하는 방식입니다. 또한, 모델의 사전 학습 데이터 속성을 최대한 보존하면서 fine-tuning을 가속화하기 위해 low-noise warmup과 큰 배치 크기 및 적은 반복(iteration) 전략을 제안합니다.
- ***Performance Highlights***: TikTok 및 UBC 패션 비디오 데이터셋과 RealisDance-Val 데이터셋에서 RealisDance-DiT는 기존 방법들보다 월등히 나은 성능을 보여주었습니다. 특히, FVD 및 FID 평가 지표에서 큰 격차로 선두를 기록하며, 얇은 메모리 기반과 빠른 수렴 속도로 경쟁력을 보여주었습니다.

### [Progent: Programmable Privilege Control for LLM Agents](https://arxiv.org/abs/2504.11703)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11703.png)

Vote: 4

Authors: Wenbo Guo, Dawn Song, Linyu Wu, Jingxuan He, Tianneng Shi, Hongwei Li, Zhun Wang

- ***What's New***: Progent는 LLM 에이전트의 권한 제어를 프로그래밍 방식으로 구현한 최초의 시스템 보안 메커니즘입니다. 도메인-특정 언어를 통해 세분화된 권한 제어 정책을 표현함으로써, 불필요하고 잠재적으로 위험한 도구 호출을 차단하면서 업무 수행에 필수적인 도구 호출만을 허용함으로써 보안을 강화합니다.
- ***Technical Details***: Progent의 핵심은 도메인-특정 언어로 구현된 정책을 통해 도구 호출에 대한 미세한 제약을 설정하는 것입니다. 이는 다양한 시나리오에 적용될 수 있도록 설계되었으며, JSON 생태계를 사용하여 LLM의 정책을 자동으로 생성하고 업데이트하여 보안과 유틸리티를 동시에 보장합니다.
- ***Performance Highlights***: 세 가지 주요 벤치마크인 AgentDojo, ASB, AgentPoison에서 Progent는 공격 성공률(ASR)을 크게 줄였습니다. 특히 AgentDojo 벤치마크에서는 공격 성공률을 41.2%에서 2.2%로 낮추는 데 성공했으며, ASB 벤치마크에서는 수동으로 지정된 정책들이 공격 성공률을 0%로 감소시켰습니다. 이는 Progent가 다양한 에이전트 및 공격 시나리오에서 강력한 보안을 제공하는 것을 보여줍니다.

### [MR. Video: "MapReduce" is the Principle for Long Video Understanding](https://arxiv.org/abs/2504.16082)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.16082.png)

Vote: 3

Authors: Yu-Xiong Wang, Ziqi Pang

- ***What's New***: MR. Video는 긴 비디오 이해를 위해 'MapReduce' 원리를 적용한 새로운 프레임워크입니다. 이 원리는 MR. Video의 'Map' 단계와 'Reduce' 단계를 통해 기존의 시퀀스-투-시퀀스 비전-언어 모델(Vision-Language Models; VLMs)과 동영상 에이전트(Video Agents)의 한계를 극복하는 데 중점을 둡니다.
- ***Technical Details***: MR. Video는 두 개의 'MapReduce' 단계를 사용합니다: (A) 캡셔닝(Captioning) - 짧은 비디오 캡션을 생성하고 반복되는 캐릭터와 객체를 통합하는 'map' 단계와 'reduce' 단계로 구성됩니다. (B) 분석(Analysis) - 사용자 질문에 대한 관련 정보를 분석하는 'map' 단계와 최종 답변으로 통합하는 'reduce' 단계로 이루어집니다.
- ***Performance Highlights***: MR. Video는 LVBench라는 도전적인 벤치마크에서 최첨단 VLMs 및 비디오 에이전트보다 10% 이상 정확도가 향상되었습니다. 이는 긴 영상 이해에서 'MapReduce' 원리의 효과성을 제시합니다.

### [CAPTURe: Evaluating Spatial Reasoning in Vision Language Models via Occluded Object Counting](https://arxiv.org/abs/2504.15485)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15485.png)

Vote: 3

Authors: Jaemin Cho, Elias Stengel-Eskin, Atin Pothiraj, Mohit Bansal

- ***What's New***: CAPTURe는 가려진 물체의 개수를 세는 과제를 통해 비전 언어 모델(Vision Language Models; VLMs)의 공간적 추론 능력을 평가하는 새로운 벤치마크를 제시합니다. 이 과제는 패턴 인식과 추론을 요구하며, VLMs가 가려진 물체를 포함한 세계 모델을 형성할 수 있는지 평가합니다.
- ***Technical Details***: CAPTURe는 실제 물체의 이미지를 사용하는 CAPTUREreal과 통제된 변수를 사용하는 CAPTUREsynthetic의 두 부분으로 구성되어 있습니다. 가려진 부분 뒤의 패턴을 완성하여 물체의 총 개수를 세야 하는 과제입니다. 네 가지 VLMs(GPT-4o, Intern-VL2, Molmo, 및 Qwen2-VL)을 이 벤치마크로 평가했습니다.
- ***Performance Highlights***: 모든 모델은 가려진 물체를 포함한 패턴에서 개수를 세는 데 어려움을 겪었으며, 특히 가려진 상황에서 성능이 저하되었습니다. 예를 들어, GPT-4o는 가려진 상태에서 오차율이 14.75%로 나타났으며, 이는 VLMs가 가려진 공간 관계를 추론하는 데 부족함이 있음을 시사합니다. 인간의 경우 CAPTURe 과제를 거의 무오류로 해결할 수 있었습니다.

### [IPBench: Benchmarking the Knowledge of Large Language Models in Intellectual Property](https://arxiv.org/abs/2504.15524)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15524.png)

Vote: 3

Authors: Run Luo, Linwei Li, Hamid Alinejad-Rokny, Yilin Yue, Kan Xu, Lei Zhang, Jiaming Li, Hongfei Lin, Yuan Lin, Ziqiang Liu, Jiayan Li, Hongbo Wang, Shiwen Ni, Huaren Liu, Shiqiang Wang, Longze Chen, Qiyao Wang, Min Yang, Guhong Chen, Zhifei Qin, Minghui Zhu, Yihang Wu, Liyang Fan

- **What's New**: IPBench는 대형 언어 모델(LLM)들이 지식 집약적인 지적 재산(Intellectual Property; IP) 작업을 처리하는 능력을 평가하기 위해 설계된 최초의 포괄적인 벤치마크입니다. 이 벤치마크는 8개의 IP 메커니즘과 20개의 작업을 포함하며, IP 관련 실제 응용을 다룸으로써 LLM의 이해 및 생성 능력을 평가합니다.

### [DiffVox: A Differentiable Model for Capturing and Analysing Professional Effects Distributions](https://arxiv.org/abs/2504.14735)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.14735.png)

Vote: 1

Authors: Marco A. Martínez-Ramírez, Ben Hayes, György Fazekas, Junghyun Koo, Wei-Hsiang Liao, Chin-Yun Yu, Yuki Mitsufuji

- ***What's New***: DiffVox는 새로운 해석 가능한 모델로, 음악 제작에서 보컬 효과를 매칭하기 위한 목적으로 개발되었습니다. 이는 파라메트릭 이퀄라이저, 동적 범위 제어, 딜레이, 리버브 등을 통합하여, 효율적인 미분 가능한 구현으로 파라미터 추정을 위한 기울기 기반 최적화를 가능하게 합니다.
- ***Technical Details***: DiffVox는 미분이 가능한 보컬 이펙트 모델로, 주어진 보컬 오디오 트랙에 대해 파라미터 최적화를 통해 리버브, 딜레이 등의 공간 효과가 얼마나 중요하게 작용하는지를 분석할 수 있습니다. 각 효과에는 미분 가능한 알고리즘을 병렬적으로 사용하여 GPU에서 훈련 속도를 향상시키고, 여러 해상도의 신호 미세동작을 일치시키는 손실 함수를 제안합니다.
- ***Performance Highlights***: DiffVox는 실험에서 팽팽한 마이크로다이내믹스와 스펙트럼 매칭 성능을 통해 보컬 효과의 높은 매칭 성능을 보여주었습니다. 공간 효과를 추가할 경우 미세동작 손실을 크게 줄일 수 있었으며, MedleyDB와 Internal 데이터셋의 분석 결과, 딜레이 시간과 피드백 게인 사이에 강한 상관관계가 있음이 드러났습니다.

### [Roll the dice & look before you leap: Going beyond the creative limits of next-token prediction](https://arxiv.org/abs/2504.15266)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15266.png)

Vote: 1

Authors: Aditi Raghunathan, Charles Ding, Vaishnavh Nagarajan, Chen Henry Wu

- **What's New**: 이 연구는 현재 언어 모델의 창의적 한계를 양적 평가하는 알고리즘적 과제를 설계하여 분석하는 것을 목표로 합니다. 이 연구는 일반적인 다음-토큰 예측의 한계를 넘어서 다양한 결과를 낼 수 있는 멀티-토큰 접근법과 '확산 모델(diffusion models)'의 이점을 제안합니다.
- **Technical Details**: 연구진은 다양한 알고리즘적 과제를 통해 언어 모델의 창의성을 평가했습니다. 이 과제들은 새로운 지식 그래프 연결을 발견하거나 새로운 패턴을 구축하는 것을 포함합니다. 연구진은 '해시 조건(hash-conditioning)'이라고 불리는 기법을 사용하여 Transformer의 입력 레이어에 직접적으로 노이즈를 주입하여 랜덤성을 유도했습니다. 이는 전통적인 출력 레이어의 온도 샘플링에 의존하는 것보다 더 나은 결과를 보였습니다.
- **Performance Highlights**: 연구 결과에 따르면, 멀티-토큰 예측 접근법은 기존의 다음-토큰 예측에 비해 높은 알고리즘적 창의성을 달성했습니다. 또한 해시 조건을 통한 입력 레벨에서의 랜덤화가 가능한 한 높은 다양성을 가진 출력을 생성하는 데 기여했습니다. 이러한 방법들을 통해서 언어 모델의 창의적 응답이 더 다양하고 독창적인 결과를 보여주었습니다.

