## Daily Papers (2025-04-16)

### [xVerify: Efficient Answer Verifier for Reasoning Model Evaluations](https://arxiv.org/abs/2504.10481)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10481.png)

Vote: 62

Authors: Xinchi Li, Wentao Zhang, Pengyuan Wang, Qingchen Yu, Minchuan Yang, Bo Tang, Zhiyu Li, Feiyu Xiong, Ding Chen

- ***What's New***: xVerify는 복잡한 추론 과정을 갖춘 LLM의 평가를 위해 설계된 효율적인 답변 검증 모델입니다. 기존 평가 방법들이 복잡한 중간 단계와 자기 반영을 포함한 LLM의 긴 답변에서 최종 답변을 명확히 추출하지 못하는 문제를 해결하기 위해 개발되었습니다.
- ***Technical Details***: xVerify는 다양한 객관식 질문 유형에서 LLM이 생성한 결과가 기준 답변과 동등한지를 판단합니다. VAR 데이터셋을 통해 다양한 LLM과 평가 벤치마크에서 수집한 질문-답변 쌍을 사용하여 여러 규모의 xVerify 모델을 훈련시켰습니다. 이 과정은 다중 라운드의 주석을 통해 레이블 정확도를 보장합니다. 특히 수학적 표현 매칭, 자연 언어의 의미 정렬 및 서식 오류 관용성이 특징입니다.
- ***Performance Highlights***: 모든 xVerify 모델은 테스트 세트와 일반화 세트에서 F1 점수 및 정확도가 95%를 넘는 성과를 기록했습니다. 특히 가장 작은 모델인 xVerify-0.5B-I는 GPT-4o를 제외한 모든 평가 방법을 능가했으며, xVerify-3B-Ib는 GPT-4o를 전체 성능 측면에서 뛰어넘었습니다. 이는 결과적으로 xVerify의 효과성과 일반화 가능성을 뒷받침합니다.

### [Genius: A Generalizable and Purely Unsupervised Self-Training Framework For Advanced Reasoning](https://arxiv.org/abs/2504.08672)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.08672.png)

Vote: 41

Authors: Zhiyong Wu, Fangzhi Xu, Junxian He, Chang Ma, Jun Liu, Kanzhi Cheng, Hang Yan, Haiteng Zhao, Qiushi Sun

- ***What's New***: Genius는 대형 언어 모델(LLM)의 추론 능력을 외부 감독 없이 향상시키는 새로운 범용 및 순전히 비지도 학습(self-training) 프레임워크입니다. 이 프레임워크는 일반 질의를 사용하여 LLM의 자기 개선을 가능하게 하여 추론 확장 법칙을 혁신적으로 접근하고자 합니다.
- ***Technical Details***: Genius는 단계별 샘플링 및 미래 결과 시뮬레이션을 통해 응답 시퀀스를 최적화하며, 부가적인 이점을 평가하여 단계별 예측 재샘플링(stepwise foresight re-sampling) 방법을 적용합니다. 학습 최적화를 위해 장점 보정 최적화(ACO) 손실 함수를 도입하여 살펴본 결과 일치 불일치를 완화합니다.
- ***Performance Highlights***: Genius는 2만 5천 개의 비지도 학습 일반 질의로 LLama3.1-8B 모델의 평균 성능을 7.43% 향상시켰습니다. 이 방법은 다양한 성능 검증 실험에서 무감독 학습 조건에서도 LLM 추론 능력을 큰 개선 없이 끌어올릴 수 있음을 보여줍니다.

### [Heimdall: test-time scaling on the generative verification](https://arxiv.org/abs/2504.10337)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10337.png)

Vote: 28

Authors: Wenlei Shi, Xing Jin

- ***What's New***: Heimdall는 강화학습(Reinforcement Learning; RL)을 통해 검증 능력을 단련한 긴 사고 연결(Chain-of-Thought; CoT) 검증 모델입니다. 경기 수학 문제에서의 검증 정확도를 62.5%에서 94.5%로 향상시켰으며, 반복 샘플링을 통한 확장으로 최대 97.5%까지 도달했습니다. 새로운 '비관적 검증(Pessimistic Verification)' 알고리즘을 도입하여 문제 해결 시 불확실성을 최소화하는 방식으로 솔루션 선택을 개선했습니다.
- ***Technical Details***: Heimdall는 PPO 알고리즘을 활용하여 훈련되었으며, 문제의 난이도를 파악하지 않고 직접 오류를 찾아내기 위해 데이터 처리 방식을 중요하게 여깁니다. 특히, 지나치게 쉬운 문제나 어려운 문제를 훈련 데이터에서 제외하여 모델이 검증 능력을 효과적으로 학습하게 됩니다. 비관적 검증 알고리즘은 문제 해결 방식에서의 다중 무장강도(multi-arm bandit) 문제로 해결 방법을 통일하여 불확실성을 최소화합니다.
- ***Performance Highlights***: Heimdall은 AIME2025에서 기본 솔버(Gemini 2.5 Pro)와 협력하여 최종적으로 93%의 정확도를 달성했습니다. 또한 자동 지식 발견 환경에서 Heimdall은 합성된 데이터의 약 절반이 결함이 있음을 성공적으로 밝혀냈습니다. 이는 최근 다른 연구에서 보고된 바와 일치하여 인상적인 일반화 능력을 보여줍니다.

### [Seedream 3.0 Technical Report](https://arxiv.org/abs/2504.11346)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11346.png)

Vote: 25

Authors: Zhichao Lai, Yuwei Zhang, Rui Wang, Yichun Shi, Lixue Gong, Shijia Zhao, Xiaoxia Hou, Wei Liu, Ye Wang, Liang Li, Weilin Huang, Xiaochen Lian, Fanshi Li, Xuanda Wang, Zhi Tian, Guofeng Wu, Chao Liao, Jie Wu, Xinyu Zhang, Xun Wang, Liyang Liu, Qi Zhang, Peng Wang, Xuefeng Xiao, Zhonghua Zhai, Qiushan Guo, Jianchao Yang, Yu Gao, Yu Tian, Xin Xia, Shiqi Sun

- ***What's New***: Seedream 3.0는 고성능 중영-영어 이중 언어 이미지 생성 모델로, 이전 Seedream 2.0의 문제점을 개선하였습니다. 특히 복잡한 프롬프트와 정밀한 타이포그래피 생성, 시각적 미학 및 충실도의 부족, 제한된 이미지 해상도를 극복하기 위한 기술적 개선 사항들이 도입되었습니다.
- ***Technical Details***: Seedream 3.0는 결함 인식 훈련 패러다임과 이중 축 협력 데이터 샘플링 프레임워크를 통해 데이터 셋을 두 배로 늘렸습니다. 혼합 해상도 훈련, 크로스 모달 RoPE, 표현 정렬 손실 및 해상도 인식 시간 샘플링과 같은 효과적인 기법들을 사전 훈련 단계에 채택하였으며, SFT에서 다양한 미학 캡션과 스케일링된 VLM 기반의 리워드 모델 적용이 성능을 큰 폭으로 향상시켰습니다.
- ***Performance Highlights***: Seedream 3.0은 이전 버전보다 여러 면에서 현저한 개선을 보여줍니다. 특히 중국어와 영어로 작은 텍스트 문자를 생성하는 성능이 뛰어나며, 시네마틱 시나리오와 인물 사진 생성에서의 미적 품질이 탁월하게 개선되었습니다. 인공 분석 분야에서는 최고 순위를 기록하며 주요 산업모델인 GPT-4o 보다 우수한 성능을 보였습니다.

### [How Instruction and Reasoning Data shape Post-Training: Data Quality through the Lens of Layer-wise Gradients](https://arxiv.org/abs/2504.10766)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10766.png)

Vote: 25

Authors: Ziyue Li, Yanhong Li, Tianyi Zhou, Ming Li

- ***What's New***: 이 연구는 대형 언어 모델(Large Language Models; LLMs)의 후처리 훈련(post-training)에서 지침 및 추론 데이터의 품질이 모델의 층별 경사도(layer-wise gradients)에 미치는 영향을 분석하며, 이는 데이터 품질을 평가하는 여러 지표들을 층별 경사도의 스펙트럼 속성을 통해 통합할 수 있음을 보여줍니다. 추론 데이터가 지침 데이터보다 더 높은 유효 순위(effective ranks)를 가져보다 복잡한 작업에서 더 풍부한 경사도를 나타냅니다.
- ***Technical Details***: 이 연구는 Qwen2.5, Llama3, Gemma2와 같은 다양한 LLM 계열에서 다양한 크기의 모델을 실험하였으며, SVD 기반의 경사도 측정 기준(SVD-based Metrics), 유사성 기반의 경사도 측정 기준(Similarity-based Metrics) 등을 사용하여 데이터 품질을 분석합니다. 경사도의 크기를 나타내는 핵(norm) 규범의 감소 및 유효 순위의 증가는 높은 품질의 데이터와 연관되어 있음을 발견했습니다.
- ***Performance Highlights***: 데이터 품질에 따른 분할 실험 결과, 모든 데이터 품질 지표는 고품질 데이터에서 낮은 핵 규범과 높은 유효 순위를 나타내는 일관된 스펙트럼 속성을 보여주었습니다. 또한, 저품질과 고품질 데이터 간의 유효 순위의 차이가 추론 데이터에서 더 뚜렷하게 드러났으며, 이는 데이터의 복잡성 증가와 관련하여 추론 성능을 크게 개선할 수 있음을 시사합니다.

### [Pixel-SAIL: Single Transformer For Pixel-Grounded Understanding](https://arxiv.org/abs/2504.10465)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10465.png)

Vote: 22

Authors: Shunping Ji, Tao Zhang, Xueqing Deng, Jiashi Feng, Shihao Chen, Yanwei Li, Xiangtai Li, Weixian Lei, Zilong Huang

- ***What's New***: Pixel-SAIL은 시각-언어 모델(SAIL) 구조를 확장하여 픽셀 수준의 이해를 수행하는 가장 단순한 다중 모달 대형 언어 모델(MLLM)을 제안합니다. Pixel-SAIL은 초과된 비전 인코더나 세분화 전문가 없이 단일 Transformer로 픽셀 단위 이해를 가능하게 하며, Segmentation과 시각적 프롬프트(Visual Prompt) 이해를 위한 새로운 벤치마크 PerBench도 포함합니다.
- ***Technical Details***: Pixel-SAIL은 세 가지 기술적 개선으로 구성됩니다. 첫째, 학습 가능한 업샘플링 모듈을 설계하여 고해상도에서 시각적 토큰을 정제합니다. 둘째, 비주얼 프롬프트 주입 방식을 고안하여 Transformer가 비주얼 프롬프트 입력을 이해할 수 있도록 합니다. 마지막으로, 비전 전문가 증류 기법을 활용하여 단일 Transformer의 세부 특징 추출 기능을 향상시킵니다. PerBench는 세 가지 작업으로 구성된 포괄적 벤치마크입니다: 세부 객체 설명, 시각적 프롬프트 기반 질문 응답, 시각-텍스트 참조 세분화.
- ***Performance Highlights***: Pixel-SAIL은 RefCOCO 시리즈 데이터셋에서 SOTA 성과를 기록하며, 3B 모델에서는 GLaMM(7B)과 OMG-LLaVA(7B) 같은 기존 MLLM보다 1.5-3.0% 더 나은 성과를 냅니다. PerBench에서 Pixel-SAIL은 METEOR 24.2, 정확도 74%, cIoU 33.4, 전체 점수 42.2로 기존 최고 성능 MLLM들을 뛰어넘었습니다. 이 성과는 보다 간단한 구조로도 경쟁력 있는 결과를 달성할 수 있음을 시사합니다.

### [TextArena](https://arxiv.org/abs/2504.11442)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11442.png)

Vote: 20

Authors: Cheston Tan, Leon Guertler, Bobby Cheng, Leshem Choshen, Simon Yu, Bo Liu

- ***What's New***: TextArena는 대형 언어 모델(LLMs)의 에이전트 행동을 평가하기 위한 오픈 소스 텍스트 기반 게임 모음입니다. 기존 벤치마크가 드문 사회적 기술, 예를 들면 협상, 마음 이론(Theory of Mind), 기만 등의 동적 사회적 기술을 평가하는 데 중점을 두고 있으며, 이는 TextArena가 해결하려고 하는 격차입니다.
- ***Technical Details***: TextArena는 57개 이상의 고유 환경을 포함하며(단일 플레이어, 두 명 및 여러 명의 플레이어 설정 포함), OpenAI Gym과 유사한 인터페이스를 제공하여 RL (Reinforcement Learning) 교육에 적합하게 설계되었습니다. 게임은 텍스트 기반 환경에서 다양한 능력(마음의 이론, 설득, 기만, 공간적 추론 등)을 테스트합니다.
- ***Performance Highlights***: TextArena는 실시간 TrueSkill™ 프로파일링으로 모델의 성능을 동적으로 평가하며, 모델 간, 모델 대 인간의 게임플레이를 통해 상대적인 능력을 측정합니다. 이는 기존의 정적인 벤치마크와 비교하여 통찰력을 제공합니다.

### [The Scalability of Simplicity: Empirical Analysis of Vision-Language Learning with a Single Transformer](https://arxiv.org/abs/2504.10462)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10462.png)

Vote: 12

Authors: Jun Hao Liew, Haochen Wang, Jiacong Wang, Jiashi Feng, Xiangtai Li, Weixian Lei, Zilong Huang

- ***What's New***: 이 논문은 SAIL이라는 단일 Transformer 기반 멀티모달 대형 언어 모델 (Multimodal Large Language Model; MLLM)을 제안하여, 기존의 미리 학습된 Vision Transformer (ViT)를 필요로 하지 않으며, 시각적 인코더를 제거하여 더욱 단순한 아키텍처를 제공합니다. SAIL은 비전 및 언어 모달리티의 교차 정보를 더 잘 결합하기 위해 혼합-어텐션 메커니즘과 멀티모달 위치 인코딩을 채택합니다.
- ***Technical Details***: SAIL은 시각 및 텍스트 모달리티 데이터를 단일 Transformer 아키텍처 내에서 처리하며, 이미지 패치에 대해 양방향 어텐션 (Bidirectional Attention) 메커니즘과 원형 위치 임베딩 (Rotary Position Embdding)을 사용합니다. 이 모델은 ViT 구성요소를 제거하였으며, 데이터 및 모델 크기를 확장함으로써 모듈형 MLLM과 유사한 성능을 보입니다.
- ***Performance Highlights***: 실험 결과, SAIL은 비전-언어 데이터 확장 시 탁월한 성능 향상을 보이며, 512M 샘플에서 모듈형 MLLM과 성능이 비슷해졌습니다. 또한 SAIL은 비전 과제에서 강한 표현 능력을 발휘하여 이미지 분류 및 의미 분할과 같은 비전 과제에서 ViT-22B와 유사한 결과를 기록했습니다.

### [Efficient Process Reward Model Training via Active Learning](https://arxiv.org/abs/2504.10559)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10559.png)

Vote: 10

Authors: Qiguang Chen, Keyu Duan, Michael Qizhe Shieh, Tianyu Pang, Longxu Dou, Zichen Liu, Changyu Chen, Xin Mao

- ***What's New***: ACTPRM은 능동학습(Active Learning) 기법을 활용한 프로세스 보상 모델(Process Reward Model; PRM) 훈련을 제안합니다. 주어진 데이터 중에서 불확실성이 높은 샘플을 선택하여 라벨링하여, 라벨링 비용을 크게 줄이면서도 유사하거나 더 나은 성능을 달성합니다.
- ***Technical Details***: ACTPRM은 앙상블 PRM을 활용하여 데이터의 불확실성을 추정하고 불확실성이 높은 데이터에 대해서만 고성능 추론 모델을 사용하여 라벨링을 수행합니다. 각 추론 단계별로 예측 신뢰도와 변동성을 계산하여 불확실한 단계들을 식별합니다. 이렇게 선택된 데이터에 대해서만 라벨링하고 훈련을 진행함으로써, 라벨링 비용을 절감할 수 있습니다.
- ***Performance Highlights***: ACTPRM은 ProcessBench에서 새로운 최첨단 성능(SOTA)을 달성했으며, 이전 최첨단 모델 UniversalPRM 대비 0.7% 더 나은 성능을 단 20%의 라벨링 비용으로 달성했습니다. 또한, Qwen2.5-Math-PRM-7B 대비 더 적은 비용으로 1.5% 높은 성능을 기록했습니다.

### [Efficient Reasoning Models: A Survey](https://arxiv.org/abs/2504.10903)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10903.png)

Vote: 10

Authors: Sicheng Feng, Gongfan Fang, Xinyin Ma, Xinchao Wang

- ***What's New***: 이 논문은 최근의 효율적 추론(Efficient Reasoning) 기술을 종합적으로 개관하여, 세 가지 주요 연구 방향으로 분류합니다: (1) 짧게 – 긴 이유 체인(Chain-of-Thoughts; CoTs)을 압축하는 방법, (2) 작게 – 지식 증류(Knowledge Distillation), 강화 학습(Reinforcement Learning) 등으로 강력한 추론 능력을 지닌 컴팩트한 언어 모델을 개발하는 방법, (3) 빠르게 – 추론 속도를 가속화하기 위한 효율적인 디코딩 전략을 설계하는 방법.
- ***Technical Details***: 이 서베이는 효율적 추론을 위해 다양한 방법론을 소개합니다. 긴 CoT를 짧게 만드는 방법에서는 강화학습(RL)과 감독 세밀 조정(Supervised Fine-Tuning; SFT)을 사용하는 여러 접근법이 논의되었습니다. 작은 모델을 구축하는 부분에서는 모델 압축 기술이 집중적으로 다뤄졌고, 특히 지식 증류와 양자화(Quantization), 가지치기(Pruning) 기술이 포함되었습니다. 빠른 디코딩을 구현하는 방법으로는 테스트 시간 확장(Test-Time Scaling; TTS) 전략에 기반한 효율적 샘플링과 자체 일관성(Self-consistency)을 최적화하는 방법이 제안되었습니다.
- ***Performance Highlights***: AIME 데이터셋에 대한 실험에서, RL 기반 DAST 방법은 53.30%의 정확도와 6337개의 토큰을 사용한 결과를 보였고, CoT-Valve는 43.30%의 정확도와 4630개의 토큰을 사용했습니다. Mix와 같은 지식 증류 방법은 10.00%의 정확도를 기록하며 상당한 성능 격차를 드러냈습니다. 효율적 추론 전략들은 특히 긴 CoT를 단축하고 작고 강력한 모델을 제작하며 추론 속도를 가속화하는 데 집중하고 있습니다.

### [NormalCrafter: Learning Temporally Consistent Normals from Video Diffusion Priors](https://arxiv.org/abs/2504.11427)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11427.png)

Vote: 8

Authors: Wenbo Hu, Yanrui Bin, Xinya Chen, Bing Wang, Haoyuan Wang

- ***What's New***: NormalCrafter는 새로운 비디오 정상 예측 모델로, 개방형 비디오의 임의 길이에서 세심한 디테일과 시간적으로 일관된 정상 시퀀스를 생성할 수 있습니다. 기존의 이미지 기반 정상 예측기와 비교해 공간적 충실도와 시간적 일관성을 크게 향상시켰습니다.
- ***Technical Details***: NormalCrafter는 비디오 확산 모델(Video Diffusion Models)의 내재된 시간적 선행 조건을 활용하며, 높은 충실도의 정상 예측을 확보하기 위해 Semantic Feature Regularization (SFR)을 제안합니다. SFR은 확산 특징을 의미론적 단서와 정렬하여 장면의 내재된 의미론적 내용을 모델이 집중하도록 유도합니다. 또한, 우리는 잠재 공간 학습과 픽셀 공간 학습을 활용한 이단계 학습 프로토콜을 도입하여 공간 정확도를 유지하면서도 긴 시간적 맥락을 보존합니다.
- ***Performance Highlights***: NormalCrafter는 널리 인정된 벤치마크에서 뛰어난 성능을 발휘하여, 기존 방법들을 상당히 능가합니다. 특히, 카메라 움직임과 빠르게 움직이는 물체가 많은 Sintel 데이터셋에서 평균 각도 오류 1.6°, 중간 각도 오류 1.6° 및 일정 각도 이하의 픽셀 비율에서 큰 개선을 보였습니다.

### [ReZero: Enhancing LLM search ability by trying one-more-time](https://arxiv.org/abs/2504.11001)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11001.png)

Vote: 7

Authors: Thinh Le, Alan Dao

- ***What's New***: ReZero 프레임워크는 LLM의 검색 능력을 향상시키기 위해 새로운 시도인 '한번 더 시도하기'를 권장합니다. 이는 실패한 검색 후 다시 시도하는 행동을 직접적으로 보상하여, 초기 검색 쿼리가 불충분할 때 대안을 탐색하도록 장려합니다.
- ***Technical Details***: ReZero는 그룹 상대적 정책 최적화(Group Relative Policy Optimization; GRPO)를 활용하여 검색 쿼리를 재시도할 경우 모델에 긍정적인 보상 신호를 제공하는 강화 학습(RL) 프레임워크입니다. 이 프레임워크는 검색 도구와의 상호작용에서 LLM이 지속적으로 새로운 쿼리 전략을 탐색하고 인내심을 발휘할 수 있도록 설계되었습니다.
- ***Performance Highlights***: ReZero는 46.88%의 정확도를 달성하였으며 이는 25%의 기준 대비 상당한 개선입니다. 이는 초기 쿼리가 실패한 경우에도 검색 도구를 효과적으로 사용하는 LLM의 능력을 향상시킵니다. 그러나 학습 중 모델의 정확도가 감소하는 문제도 발견되어 향후 연구가 필요합니다.

### [RealHarm: A Collection of Real-World Language Model Application Failures](https://arxiv.org/abs/2504.10277)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10277.png)

Vote: 7

Authors: Jiaen Liu, Matteo Dora, Luca Rossi, Pierre Le Jeune

- ***What's New***: RealHarm은 AI 언어 모델의 실제 응용 분야에서 발생하는 문제를 체계적으로 검토하여 수집한 데이터세트로, 700건이 넘는 사건을 검토하여 136개의 주석이 달린 문제 사례를 포함합니다. 이 데이터세트는 실증적 증거를 기반으로 단순 규제 체계나 이론 분석을 넘어 실제로 발생한 실패 모드를 조사하는 것이 주요 목적입니다.
- ***Technical Details***: RealHarm 데이터세트는 AI 대화형 시스템과 관련된 문제적 상호작용을 문서화한 텍스트 대화로 구성되어 있습니다. 각 대화는 인간과 AI 에이전트 간의 상호작용을 나타내며, '안전하지 않음'과 '안전함' 두 부분으로 나뉩니다. 안전하지 않은 세트는 AI의 오작동이 포함된 원본 대화를 포함하며, 안전한 세트는 해당 문제를 수정한 대화를 포함합니다. 이러한 데이터는 공개적으로 보고된 AI 사건 데이터베이스와 기타 출처를 통해 수집되었습니다.
- ***Performance Highlights***: 현재 상용 콘텐츠 모더레이션 API와 전문적 안전장치 시스템을 시험한 결과, AI 모델의 실패를 충분히 감지하지 못하는 상당한 한계가 있으므로 단독의 보호 수단으로서는 부족한 것으로 드러났습니다. 특히, 현재의 거버넌스 시스템으로는 기록된 사건 중 대략 20%만이 탐지될 수 있었습니다.

### [Efficient Generative Model Training via Embedded Representation Warmup](https://arxiv.org/abs/2504.10188)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10188.png)

Vote: 7

Authors: Deyuan Liu, Peng Sun, Xufeng Li, Tao Lin

- ***What's New***: 이 논문에서는 임베디드 표현 사전준비 단계(Embedded Representation Warmup; ERW)를 도입하여, 고품질의 사전 학습된 자체 지도(대표적으론 Dinov2) 표현을 확산 모델의 초기 레이어에 통합함으로써 훈련 효율성과 표현 품질을 크게 개선했습니다. 이를 통해 ERW는 기존 REPA 방법론 대비 최대 40배 빠른 학습 속도를 보이며 새로운 최첨단 성능을 달성했습니다.
- ***Technical Details***: ERW는 두 가지 구체적인 단계로 구성된 플러그 앤 플레이(plung-and-play) 프레임워크입니다. 첫 번째 단계에서는 사전 학습된 자체 지도 백본을 사용하여 초기 레이어의 가중치를 초기화하고, 두 번째 단계에서는 작은 표현 정렬 손실을 통해 생성 훈련으로 전환합니다. 이는 주로 모델이 기능 표현을 다음 생성 단계로 변환하는 표현 처리 영역에서 정확하게 통합되어 학습 효과를 극대화합니다.
- ***Performance Highlights***: ERW는 SiT-XL 모델이 ImageNet-1k에서 8개의 H800 GPU로 4시간 이하로 FID 6.4 도달 가능함을 보여주며, 최신 방법론 대비 짧은 훈련 단계를 통한 높은 FID 개선을 달성했습니다. 특히 훈련 속도를 최대 40배 향상시키며, 더욱 뛰어난 학습 효율성과 세밀한 데이터 생성 품질을 입증했습니다.

### [VisualPuzzles: Decoupling Multimodal Reasoning Evaluation from Domain Knowledge](https://arxiv.org/abs/2504.10342)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10342.png)

Vote: 7

Authors: Graham Neubig, Yibo Kong, Tianyue Ou, Xiang Yue, Zecheng Li, Yueqi Song

- ***What's New***: 이 연구에서는 VISUALPUZZLES라는 새로운 벤치마크를 소개합니다. 이 벤치마크는 도메인 지식에 대한 의존성을 최소화하면서 다중 모달 추론 능력을 평가하도록 설계되었습니다. VISUALPUZZLES는 알고리즘적, 유추, 연역, 귀납, 공간적 추론 등 다섯 가지 추론 카테고리를 포함하는 다양한 문제로 구성되어 있습니다. 이러한 구성은 도메인별 지식보다는 일반 추론 능력을 독립적으로 평가하고자 하는 목적을 가지고 있습니다.
- ***Technical Details***: VISUALPUZZLES는 1,168개의 퍼즐 형태의 문제로 구성되어 있으며, 각 문제는 알고리즘적, 유추, 연역, 귀납, 공간적 추론의 다섯 가지 카테고리로 구분됩니다. 데이터 작성 과정에서는 논리 추론 문제를 중국 공무원 시험에서 번역하여 사용하였으며, 다양성 있는 추론 도전을 위해 기존 멀티모달 벤치마크에서 문제를 재구성하였습니다. 또, 각 문제는 기초적인 상식과 제공된 이미지, 텍스트만을 사용하여 해결이 가능하도록 설계되었습니다.
- ***Performance Highlights***: VISUALPUZZLES에서 최첨단 멀티모달 대형 언어 모델들이 인간의 5번째 백분위수에도 미치지 못하는 성능을 보여 도전적임을 확인했습니다. 특히, 도메인 지식에 의존하는 기존 벤치마크에서 높은 평가를 받는 모델들도 도메인 독립적인 추론 작업에서는 성능이 현저히 떨어지고 있어, 대형 모델들의 추론 능력을 제고할 필요성을 시사하고 있습니다.

### [DataDecide: How to Predict Best Pretraining Data with Small Experiments](https://arxiv.org/abs/2504.11393)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11393.png)

Vote: 6

Authors: Dirk Groeneveld, Jena D. Hwang, Ian Magnusson, Luca Soldaini, Pang Wei Koh, Akshita Bhagia, Oyvind Tafjord, David Heineman, Noah A. Smith, Jesse Dodge, Jiacheng Liu, Ben Bogin, Nguyen Tai

- ***What's New***: DataDecide는 다양한 예비 실험을 통해 최적의 전이학습 데이터(pretraining data) 선택을 실험적으로 검증하기 위한 가장 광범위한 공개 모형 모음을 제공합니다. 이 연구는 25개의 다른 코퍼스에서 100B 토큰까지의 제어된 전이학습 실험을 포함하며, 각각의 데이터셋에서 더 나은 성능을 보여주는 코퍼스를 예측하는 모형의 정확도를 평가합니다.
- ***Technical Details***: DataDecide는 다섯 개의 공통 코퍼스(Dolma, DCLM, RefinedWeb, C4, FineWeb)를 포함하여 소스 혼합, 중복 제거, 필터링 등의 중재를 적용한 25가지 데이터 레시피로 구성되어 있습니다. 1B 파라미터의 모델을 포함한 14가지 모형 크기로 모든 데이터를 전이 학습하며, 3개의 랜덤 시드(randon seeds)를 사용하여 각 데이터셋을 체계적으로 분석할 수 있도록 설계되었습니다.
- ***Performance Highlights***: DataDecide의 실험 결과, 작은 모델 크기(예: 150M 파라미터)의 모델 순위가 큰 목표 크기(1B)에서 최상의 모델을 예측하기 위한 기준으로 강력한 기반이 될 수 있음을 밝혔습니다. 이 방법은 약 80%의 비교에서 올바른 결과를 보였습니다. 8개의 기본 스케일링 법칙(scaling law) 중 어떤 것도 단일 크기 예측의 분석 정확도를 능가하지 않았으나, DataDecide는 미래의 스케일링 법칙에서의 개선을 측정할 수 있는 기능을 제공합니다.

### [DeepMath-103K: A Large-Scale, Challenging, Decontaminated, and Verifiable Mathematical Dataset for Advancing Reasoning](https://arxiv.org/abs/2504.11456)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11456.png)

Vote: 6

Authors: Jiahao Xu, Wenxuan Wang, Zhuosheng Zhang, Zhaopeng Tu, Haitao Mi, Zhenwen Liang, Yue Wang, Dong Yu, Rui Wang, Qiuzhi Liu, Zhiwei He, Dian Yu, Tian Liang, Xingyu Chen, Linfeng Song

- **What's New**: DeepMath-103K은 심화된 수리적 추론을 위한 대규모 수학 데이터셋으로, 문제의 도전적 수준과 검증 가능한 답안을 갖춘 것이 특징입니다. 기존 데이터셋과 달리 학습 평가에 오염되지 않은 데이터를 제공하여 강화학습(reinforcement learning) 모델의 자가 학습을 촉진합니다.
- **Technical Details**: DeepMath-103K 데이터셋은 약 103,000개의 수학 문제로 구성되어 있으며, 문제의 난이도는 주로 레벨 5-9로 설정되어 있습니다. 각 문제는 검증 가능한 최종 답안을 포함하며, 세 개의 서로 다른 R1 모델 솔루션이 제공되어 다양한 학습 패러다임에 활용될 수 있습니다. 데이터는 철저히 '데이터 오염(decontamination)'을 방지하여 평가 데이터셋과의 중복을 제거하였습니다.
- **Performance Highlights**: DeepMath-103K를 기반으로 학습된 모델은 MATH500, AMC23, Olympiad 등 여러 수학적 추론 벤치마크에서 탁월한 성능 향상을 나타냈습니다. 특히, 강화학습 기법 중 하나인 RL-Zero를 적용한 경우, 다른 데이터셋으로 학습된 모델보다 모든 벤치마크에서 높은 정확도를 보여주었습니다. 이러한 결과는 데이터셋의 고난이도 문제와 검증된 답안이 수학적 추론 능력 향상에 효과적임을 입증합니다.

### [A Minimalist Approach to LLM Reasoning: from Rejection Sampling to Reinforce](https://arxiv.org/abs/2504.11343)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11343.png)

Vote: 6

Authors: Lei Wang, Hanze Dong, Wei Xiong, Doyen Sahoo, Nan Jiang, Jiarui Yao, Caiming Xiong, Tong Zhang, Yuhui Xu, Bo Pang, Junnan Li

- ***What's New***: 이 논문에서는 대형 언어 모델(LLMs)의 수학적 추론 작업을 위한 보상 기반 미세 조정에 있어, 단순한 리젝션 샘플링(Rejection Sampling) 기법을 통해 경쟁력 있는 성능을 달성할 수 있음을 발견했습니다. RAFT와 새로운 Reinforce-Rej 알고리즘이 GRPO나 PPO 같은 복잡한 기법에 비해 간단하면서도 효과적인 대안이 될 수 있음을 보여줍니다.
- ***Technical Details***: 본 연구는 RAFT 알고리즘을 RAFT++로 확장하여 중요도 샘플링(importance sampling)과 클리핑(clipping) 기술을 적용, GRPO와 비슷한 성능을 달성하였습니다. Reinforce-Rej 알고리즘은 완전히 올바르거나 완전히 잘못된 샘플을 필터링하여 KL 효율성과 안정성을 개선했습니다. 다양한 강화학습(Policy Gradient) 알고리즘 변형을 통해 샘플 선택의 중요성을 강조했습니다.
- ***Performance Highlights***: RAFT와 RAFT++ 알고리즘은 복잡한 강화학습 기반 방법들과 비교하여 경쟁력 있는 성능을 보였으며, 특히 초기 학습 단계에서 더 빠르게 수렴했습니다. GRPO의 성능 우위는 보상(normalization)보다는 완전히 잘못된 응답을 버리는 프로세스에서 비롯되었습니다. Reinforce-Rej 알고리즘은 KL 효율성과 엔트로피 안정성을 유지하면서 강력한 성능을 보여주었습니다.

### [AI-University: An LLM-based platform for instructional alignment to scientific classrooms](https://arxiv.org/abs/2504.08846)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.08846.png)

Vote: 4

Authors: Mostafa Faghih Shojaei, Dangli Cao, Krishna Garikipati, Benjamin A. Jasperson, Simone Cimolato, Shangshang Wang, Willie Neiswanger, Rahul Gulati

- ***What's New***: AI-University(AI-U)는 대형 언어 모델(LLM)과 정보 검색 강화 생성(Retrieval-Augmented Generation; RAG)을 활용하여 교수의 강의 스타일에 맞춘 내맞춤형 과학 강좌 콘텐츠를 제공하는 혁신적인 플랫폼입니다. AI-U는 강좌 강의 동영상, 노트, 교재를 활용하여 맞춤형 AI 비서를 생성하며, 이는 대학 수준의 학습 환경에서 유연하게 작동할 수 있도록 설계되었습니다.
- ***Technical Details***: AI-U는 로우랭크 적응(Low-Rank Adaptation; LoRA)을 이용하여 오픈소스 LLM을 미세 조정하고 RAG 기반 합성을 통해 강좌 자료와 강력하게 일치하도록 최적화합니다. 데이터 생성 파이프라인은 강좌 자료에서 질의응답 쌍을 생성하여 도메인 전문가 수준의 LLM를 훈련하며, 이를 LLaMA-TOMMI-1.0이라 명명하였습니다. 이 모델은 실시간 자료 검색을 통해 단편적인 정보와 통합하여 지속적이고 개인화된 학습 경험을 제공합니다.
- ***Performance Highlights***: AI-U의 성능 평가에서는 코사인 유사성(cosine similarity)과 LLM 심판을 사용하였으며, AI-U의 모델은 약 86%의 테스트 케이스에서 기준 모델보다 우세한 코사인 유사성을 나타냈습니다. 또한, AI-U는 5번 중 거의 4번정도 기준 모델보다 뛰어난 성능을 보여 고급 강좌 자료와의 강력한 정렬을 입증했습니다.

### [SimpleAR: Pushing the Frontier of Autoregressive Visual Generation through Pretraining, SFT, and RL](https://arxiv.org/abs/2504.11455)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11455.png)

Vote: 4

Authors: Weilin Huang, Xun Wang, Junke Wang, Zhi Tian, Zuxuan Wu, Xinyu Zhang, Yu-Gang Jiang

- ***What's New***: SimpleAR는 복잡한 구조적 변경 없이 순수한 autoregressive visual generation framework로, 0.5B 매개변수로도 1024×1024 해상도의 이미지를 고품질로 생성할 수 있는 성과를 보이며, 향상된 생성 미학과 prompt alignment를 위해 통합된 supervised fine-tuning (SFT) 및 Group Relative Policy Optimization (GRPO) 기반의 강화 학습을 도입하여 경쟁력 있는 결과를 이끌어 냈습니다.
- ***Technical Details***: SimpleAR는 이미지와 텍스트 토큰을 통합하여 autoregressively joint distribution을 모델링하는 Transformer 구조를 사용하며, 텍스트 인코더 없이 unified transformer architecture를 통해 텍스트 인코딩과 시각적 생성을 통합합니다. 고품질 데이터로 SFT 및 GRPO를 통해 멀티모달 align을 추가적으로 개선하고, inference 단계에서는 vLLM과 같은 inference acceleration 기술로 모델의 추론 시간을 14초까지 줄입니다.
- ***Performance Highlights***: GenEval에서 0.59, DPG에서 79.66의 점수로 1B 미만의 모델들 중 최고 성능을 기록했습니다. SimpleAR 1.5B 모델은 GenEval에서 0.63, DPG에서 81.97로 개선된 성과를 보이며 파라미터 증가에 따른 예측한 성능 향상을 나타냈습니다. KV Cache와 vLLM 기술을 통해 향상된 속도를 보여 최대 1024×1024 이미지 생성을 14초만에 달성합니다.

### [Diffusion Distillation With Direct Preference Optimization For Efficient 3D LiDAR Scene Completion](https://arxiv.org/abs/2504.11447)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11447.png)

Vote: 4

Authors: Ling Yang, Haoran Xu, Jiale Wu, Perry Pengyun GU Lingyun Sun, An Zhaol, Zejian Li, AnYang Wei, Shengyuan Zhang

- ***What's New***: 이 논문에서는 LiDAR 장면 완성을 위한 새로운 확산 증류 프레임워크인 Distillation-DPO를 제안합니다. 이 방법은 기존의 확산 모델을 선호 데이터 쌍을 사용하여 증류하는 첫 번째 시도입니다. Distillation-DPO는 장면 완성의 품질을 향상시키면서도 속도를 기존 모델보다 5배 이상 빠르게 합니다.
- ***Technical Details***: Distillation-DPO는 학생 모델이 서로 다른 초기 잡음 레벨을 가진 장면을 생성하여 완성 장면 쌍을 만듭니다. 그런 다음 LiDAR 장면 평가 메트릭을 사용하여 우승 및 패배 샘플 쌍을 구성하며, 이는 모델의 최적화를 위한 비분화 가능하지만 정보가 많은 데이터를 제공합니다. 이후 교육 보조 모델을 통해 학생 모델의 점수를 최적화합니다.
- ***Performance Highlights***: 실험 결과, Distillation-DPO는 기존의 최고 성능 모델 LiDiff보다 샘플링 속도가 5배 이상 빠르면서도 장면 완성 품질에서 6% (Chamfer Distance)와 7% (JSD)의 향상을 이루었습니다. 이는 LiDAR 장면 완성 및 시각적 생성 분야에서 선호 정렬 확산 증류의 통찰력을 제공합니다.

### [PVUW 2025 Challenge Report: Advances in Pixel-level Understanding of Complex Videos in the Wild](https://arxiv.org/abs/2504.11326)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11326.png)

Vote: 4

Authors: Nikhila Ravi, Shuting He, Xu Liu, Lingling Li, Xiangtai Li, Ming-Hsuan Yang, Shuyuan Yang, Mengru Ma, Haobo Yuan, Xuqiang Cao, Linnan Zhao, Chang Liu, Wei-Shi Zheng, Lu Qi, Kexin Zhang, Fang Liu, Xiankai Lu, Yunchao Wei, Yuting Yang, Licheng Jiao, Hao Fang, Zhiyang Che, Jian-Fang Hu, Philip Torr, Runmin Cong, Song Bai, Junpei Zhang, Haichao Jiang, Tao Zhang, Mengjiao Wang, Kehuan Song, Wei Zhan, Xinglin Xie, Tianming Liang, Jiaxuan Zhao, Henghui Ding

- ***What's New***: PVUW 2025 챌린지는 복잡한 비디오 오브젝트 세분화(VOS)와 모션 가이드 언어 기반 비디오 세분화에 초점을 맞춘 MOSE 및 MeViS 두 트랙을 통해 실세계 시나리오에 더 밀접하게 반영된 새로운 데이터셋을 소개합니다. 이번 챌린지를 통해 현재 최첨단 기술 및 복잡한 비디오 세분화 연구의 추세를 파악할 수 있습니다.
- ***Technical Details***: MOSE 트랙은 복잡한 환경에서 오브젝트를 추적하고 세분화하기 위한 MOSE 데이터셋을 기반으로 하며, 2,149개의 비디오 클립과 36개의 카테고리에서 5,200개의 오브젝트를 포함한 43만 개 이상의 고품질 세분화 마스크가 포함되어 있습니다. MeViS 트랙에서는 모션 중심의 언어 표현을 활용하여 비디오에서 객체를 세분화합니다. MeViS 데이터셋은 모션 중심의 언어 표현과 복잡한 비디오 장면의 군중 및 동적 환경을 특징으로 합니다.
- ***Performance Highlights***: MOSE 트랙에서 BrainyBots 팀은 테스트 세트에서 J &F 점수 87.26%를 기록하며 1위를 차지했습니다. MeViS 트랙에서는 MVP-Lab 팀이 J &F 점수 61.98%로 1위를 기록했습니다. 두 트랙 모두에서 강력한 성능을 보였으며, 특히 언어 가이드 비디오 작업에서 멀티모달 LLMs의 잠재력이 강조되었습니다.

### [Adaptive Computation Pruning for the Forgetting Transformer](https://arxiv.org/abs/2504.06949)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.06949.png)

Vote: 3

Authors: Johan Obando-Ceron, Aaron Courville, Zhixuan Lin, Xu Owen He

- ***What's New***: Adaptive Computation Pruning(ACP)은 최신 발생 방식 대형 모델인 Forgetting Transformer(FoX)를 위한 방법으로, 입력-출력 의존성이 게이트로 인해 크게 감소된 계산을 동적으로 가지치기(pruning)하는 방식을 제안합니다. 이는 Forgetting Transformer와 Softmax Attention을 결합한 혁신적인 방법입니다.
- ***Technical Details***: ACP는 동적으로 설정된 임계값을 사용하여 무시할 수 있을 만큼 작아진 주의 가중치의 계산을 가지치기합니다. 게이트의 효과로 인해 계산을 줄일 수 있으며, 이는 FlashAttention 알고리즘과 호환됩니다. 특히 QK-norm을 사용하여 손쉽게 주의 가중치의 상한을 계산할 수 있습니다.
- ***Performance Highlights***: ACP를 사용하면 Softmax Attention의 FLOP 수를 모델 크기와 문맥 길이에 상관없이 약 70%감소시킬 수 있으며, 훈련 처리량을 약 10%에서 35%까지 향상시킵니다. 이를 통해 성능 저하 없이 속도를 개선합니다.

### [D^2iT: Dynamic Diffusion Transformer for Accurate Image Generation](https://arxiv.org/abs/2504.09454)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.09454.png)

Vote: 3

Authors: Nan Chen, Zhendong Mao, Weinan Jia, Mengqi Huang, Lei Zhang

- ***What's New***: 이 논문에서 제안한 D^2iT(Dynamic Diffusion Transformer)은 기존의 고정적 압축 방식의 한계를 극복하기 위해 지역별로 다양한 압축 수준을 적용하여 글로벌 일관성과 지역적 현실성을 동시에 확보했습니다. 이 두 단계 프레임워크는 이미지를 보다 정확하고 자연스럽게 생성할 수 있도록 설계되었습니다.
- ***Technical Details***: 두 단계 프레임워크는 첫째, DVAE(Dynamic VAE)를 사용하여 이미지 지역의 공간 밀도에 따라 서로 다른 다운샘플링을 진행하고, 둘째, D^2iT에서 Dynamic Grain Transformer와 Dynamic Content Transformer를 활용하여 다양한 수준의 노이즈를 예측하고 수정합니다. 이 과정은 지역적 정보를 고려하여 노이즈 예측을 세분화하여 전반적인 일관성과 디테일을 구현합니다.
- ***Performance Highlights***: 제안된 D2iT 모델은 DiT 대비 57.1%의 학습 리소스를 사용하여 학습 시 23.8%의 품질 향상을 달성하였습니다. ImageNet 데이터셋에서 1.73의 FID 값을 기록하며, FFHQ와 ImageNet 데이터셋에서 탁월한 성능을 보였습니다.

### [Efficient Hybrid Language Model Compression through Group-Aware SSM Pruning](https://arxiv.org/abs/2504.11409)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11409.png)

Vote: 3

Authors: Saurav Muralidharan, Raviraj Joshi, Marcin Chochowski, Oluwatobi Olabiyi, Daniel Korzekwa, Ameya Sunil Mahabaleshwarkar, Pavlo Molchanov, Jan Kautz, Yoshi Suhara, Mohammad Shoeybi, Zijia Chen, Sharath Turuvekere Sreenivas, Ali Taghibakhshi, Bryan Catanzaro, Ashwath Aithal, Yashaswi Karnati, Nima Tajbakhsh, Mostofa Patwary

- ***What's New***: 본 연구에서는 하이브리드 언어 모델(Hybrid Language Model; HLM)의 압축을 위해 그룹 인식 SSM 가지치기(Group-Aware SSM Pruning) 방법을 제안합니다. 이 방법은 Mamba 레이어의 구조적 무결성을 유지하면서 SSM 블록의 시퀀스 모델링 기능을 보존합니다. 이를 통해 모델 크기를 줄이면서도 기존보다 더 나은 정확도와 추론 속도를 달성할 수 있습니다.
- ***Technical Details***: 제안된 방법은 하이브리드 아키텍처의 여러 차원을 압축합니다. Mamba 헤드와 헤드 채널, FFN 뉴런, 임베딩 채널 및 레이어를 가지치기하여 모델을 줄입니다. 중요도 추정에 기반한 가지치기 후, 지식 증류(Knowledge Distillation)를 통해 추가 학습을 수행하여 최종 압축 모델을 얻습니다. Nemotron-H 8B 하이브리드 모델을 4B 파라미터로 압축하면서 최대 40배 적은 학습 토큰으로도 정확도를 유지합니다.
- ***Performance Highlights***: 압축된 Nemotron-H 4B 모델은 유사한 크기의 모델들보다 최대 2배 빠른 추론 속도를 제공하며, 약 2.6% 더 높은 정확도를 달성했습니다. 이는 효율성과 정확성을 동시에 개선하며 Pareto 프론티어를 크게 확장합니다.

### [Summarization of Multimodal Presentations with Vision-Language Models: Study of the Effect of Modalities and Structure](https://arxiv.org/abs/2504.10049)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10049.png)

Vote: 2

Authors: Camille Guinaudeau, Frédéric Dufaux, Théo Gigant

- ***What's New***: 이 논문은 비전-언어 모델(Vision-Language Models; VLMs)을 사용하여 멀티모달 프레젠테이션의 자동 요약을 수행하는 방법을 제안합니다. 특히, 슬라이드와 텍스트를 교차하여 사용하는 구조화된 표현(interleaved slides-transcript representation)이 요약 성능을 가장 잘 향상시킨다는 점을 발견했습니다.
- ***Technical Details***: 이 연구는 TIB 데이터셋에서 추출한 822개의 프레젠테이션을 사용하여 진행되었습니다. 모델은 슬라이드나 전체 비디오와 같은 단일 모달리티(input modality)를 사용하거나, 슬라이드와 텍스트를 시간 맞춤을 통해 결합한 다중 모달리티(multimodal input representations)를 사용합니다. 주요 분석 대상으로는 Qwen2-VL 모델이 선택되었으며, 2B 및 7B 파라미터 크기의 모델을 연구에 활용했습니다.
- ***Performance Highlights***: 테스트 결과, Qwen2-VL 모델이 상위 성능을 기록하며, 특히 구조화된 다중 모달 입력이 비구조화된 입력보다 더 나은 요약 성능을 보여줬습니다. 512개 비주얼 토큰 버젯(visual token budget) 하에 성능이 수렴하며, 슬라이드 정보만을 사용하는 것보다 구조화된 다중 모달 접근이 더 효과적이라는 사실을 확인했습니다.

### [Multimodal Long Video Modeling Based on Temporal Dynamic Context](https://arxiv.org/abs/2504.10443)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10443.png)

Vote: 2

Authors: Xiangyu Yue, Jiaming Han, Yiyuan Zhang, Haoran Hao

- ***What's New***: 이 논문에서는 시간 동적 컨텍스트(Temporal Dynamic Context; TDC)라는 새로운 비디오 인코딩 방법을 제안하여 서로 다른 모달리티를 통합하여 장시간 비디오 이해를 효율적으로 개선하고자 합니다.
- ***Technical Details***: 먼저, 비디오를 프레임 간 유사성을 기반으로 의미적으로 일관된 장면으로 분할하여 각 프레임을 시각 및 오디오 인코더를 사용해 토큰으로 인코딩합니다. 그 후 쿼리 기반 Transformer를 활용한 새로운 시간 컨텍스트 압축기를 제안하여 각 세그먼트 내의 토큰 수를 줄입니다. 이 방법은 주어진 질문에 필요한 특정 모달리티에 대한 주의 집중을 가능하게 합니다. 또한, 초장기간 비디오 처리를 위해, 비디오 세그먼트에서 점진적으로 답변을 추출하는 교육 없는 사고의 연쇄 체인-오브-생각(Long Video Chain-of-Thought; LVCoT) 전략을 제안하였습니다.
- ***Performance Highlights***: 우리의 모델은 전반적인 비디오 이해 및 오디오-비디오 공동 이해 벤치마크에서 뛰어난 성능을 보여주었습니다. 특히 VideoLLaMA2 등과 비교하여 장시간 비디오 이해에서 15.6% 및 9.9%의 성능 향상을 기록하였고, 다양한 비디오 벤치마크에서 일관되게 최고의 성능을 보였습니다.

### [LazyReview A Dataset for Uncovering Lazy Thinking in NLP Peer Reviews](https://arxiv.org/abs/2504.11042)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11042.png)

Vote: 2

Authors: Iryna Gurevych, Anne Lauscher, Lizhen Qu, Zhuang Li, Sukannya Purkayastha

- ***What's New***: 피어 리뷰(peer review)는 과학 출판의 질 확보에 필수적인 과정입니다. 이번 연구는 NLP 피어 리뷰에 나타나는 'lazy thinking'이라는 휴리스틱 사용을 탐지하기 위한 데이터셋인 LAZYREVIEW를 소개합니다. 이는 자동화된 탐지 도구 개발을 지원합니다.
- ***Technical Details***: LAZYREVIEW 데이터셋은 500개의 전문가 주석 리뷰 세그먼트와 1276개의 은색 주석 리뷰 세그먼트로 구성됩니다. 각 세그먼트는 lazy thinking 클래스에 맞게 태그가 달렸습니다. LLMs는 zero-shot 방식으로 탐지하는 데 어려움을 겪지만, 데이터셋으로 fine-tuning하면 성능이 10-20포인트 향상됩니다.
- ***Performance Highlights***: 기존의 혼합 모델과 새로운 fine-tuning 방식 모두에서 fine-tuning은 명확하게 성능을 향상시킵니다. 예를 들어, Qwen 모델은 fine-tuning 후 성능이 최대 31pp 향상되었으며, 코스 그레인드(classification) 분류를 수행한 SciTülu 모델 역시 동일 분류에서 뛰어난 성능을 보였습니다.

### [MCP Safety Audit: LLMs with the Model Context Protocol Allow Major Security Exploits](https://arxiv.org/abs/2504.03767)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.03767.png)

Vote: 1

Authors: Brandon Radosevich, John Halloran

- ***What's New***: 이 논문은 Model Context Protocol (MCP)을 적용한 대형 언어 모델(LLM)이 보안 취약점을 악용하는 상황을 설명하고 있습니다. MCP는 API 표준화를 통해 LLM과 다양한 시스템 및 서비스의 원활한 통합을 목표로 하지만, 보안상 심각한 위험을 내포할 수 있음을 보여줍니다. 이러한 보안 문제를 사전에 탐지하기 위한 Auditing Tool인 McpSafetyScanner를 도입하여 MCP 서버의 보안을 평가하고 개선할 수 있도록 합니다.
- ***Technical Details***: MCP는 API 호출을 표준화하여 LLM, 데이터 소스, 에이전틱 툴 간의 통합을 돕습니다. 그러나 현재 MCP 서버는 LLM이 악의적인 코드 실행, 원격 접근 제어, 그리고 자격증명 탈취와 같은 여러 공격을 수행하도록 조작될 수 있는 위험이 있습니다. 이를 해결하기 위해 McpSafetyScanner가 도입되었으며, 이 도구는 다양한 에이전트를 사용하여 시스템의 취약점을 탐지하고 적절한 보안 보고서를 생성합니다.
- ***Performance Highlights***: McpSafetyScanner는 다양한 MCP 서버의 취약점을 정확하게 식별하고, 관련된 공격의 예시와 함께 보안 대응 방안을 제시합니다. 이 도구는 MCP 서버 환경을 빠르게 분석하여 보안 문제를 파악하고, 개발자에게 신속하고 실질적인 대응책을 제안하여 MCP 서버의 보안 방어를 강화할 수 있습니다.

### [Change State Space Models for Remote Sensing Change Detection](https://arxiv.org/abs/2504.11080)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11080.png)

Vote: 0

Authors: Erchan Aptoula, Elman Ghazaei

- ***What's New***: 이 논문에서는 원격 감지 변화 탐지를 위해 특별히 설계된 새로운 아키텍처인 Change State Space Model (CSSM)이 소개되었습니다. 이 모델은 이중시점 이미지 간의 관련 변화를 효과적으로 포착하면서 불필요한 정보를 필터링하도록 설계되어, 비골절 정보는 제거하면서도 높은 검출 정확도를 유지합니다.
- ***Technical Details***: CSSM은 Mamba 구조를 개선하여 변경된 영역과 관련된 특징만 추출하며, 이 불필요한 정보를 제거합니다. 이를 위해 L1 거리 기반 접근을 사용하였으며, 두 가지 다른 선택 매개변수를 통합하여 변동이 발생한 지역에 해당하는 특징을 효과적으로 추출합니다. 이러한 선택 메커니즘을 통해 CSSM은 정확도와 효율성을 개선합니다. MambaBCD 대비 최대 21.25배 적은 네트워크 파라미터로 성능을 크게 향상시킵니다.
- ***Performance Highlights***: 세 개의 기준 벤치마크 데이터세트에서 CSSM은 ConvNets, ViTs, Mamba 기반 방법들을 초과하며 보다 적은 계산 복잡성으로 높은 변동 검출 성능을 달성했습니다. 특히, CSSM은 높은 검출 성능과 입력 열화에 대한 강건성을 유지하면서 기존 방법들보다 더 나은 성능을 보여줍니다.

### [Aligning Generative Denoising with Discriminative Objectives Unleashes Diffusion for Visual Perception](https://arxiv.org/abs/2504.11457)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11457.png)

Vote: 0

Authors: Ziqi Pang, Yu-Xiong Wang, Xin Xu

- **What's New**: 이 논문은 생성적 확산 모델(Generative Diffusion Models)을 사용하여 시각적 지각(Visual Perception) 작업을 수행할 때 발생하는 문제를 해결하는 새로운 방법인 ADDP(Aligning Diffusion Denoising with Perception)를 제안합니다. 이 방법은 정교한 손실 함수와 데이터 증강 기법을 도입하여 지각 작업의 성능을 향상시킵니다.
- **Technical Details**: ADDP는 시간 단계(Timesteps)마다 다른 기여도를 반영하는 새로운 학습 목표를 제안합니다. 자세히 살펴보면, 초기 디노이징(denoising) 단계가 훨씬 중요한 기여를 하며, 이는 기존의 균일한 시간 단계와는 대조적입니다. 또한, 예측 품질 저하가 발생하는 시점을 데이터 증강(Diffusion-Tailored Data Augmentation)으로 보정하여 연습 데이터에 훈련 디노이징 분포 이동이 발생하는 것을 시뮬레이션합니다.
- **Performance Highlights**: ADDP는 다양한 확산 기반 지각 모델에서 성능을 향상시키며, 특히 Marigold와 같은 깊이 추정 모델에서 0.6-3%의 정확도 향상을 달성하였습니다. RIS(Referring Image Segmentation)에서도 이전에 존재하던 생성적 모델과 비생성적 방법 간의 성능 격차를 줄이는 데 성공하였습니다.

