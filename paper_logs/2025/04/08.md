## Daily Papers (2025-04-08)

### [SmolVLM: Redefining small and efficient multimodal models](https://arxiv.org/abs/2504.05299)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.05299.png)

Vote: 85

Authors: Mathieu Morlon, Elie Bakouch, Hugo Larcher, Miquel Farré, Leandro von Werra, Anton Lozhkov, Nouamane Tazi, Merve Noyan, Cyril Zakka, Thomas Wolf, Pedro Cuenca, Lewis Tunstall, Orr Zohar, Andrés Marafioti, Loubna Ben Allal, Joshua Lochner, Vaibhav Srivastav

- ***What's New***: SmolVLM은 휴대용 및 엣지 디바이스 상에서 실행할 수 있도록 자원 효율적인 소형 멀티모달 모델 시리즈를 도입했습니다. 이 모델들은 메모리 사용량을 크게 줄이면서 이미지와 비디오 작업에서 예외적인 성능을 보여줍니다. 특히 SmolVLM-256M 모델은 불과 1GB 이하의 GPU 메모리로도 18개월 전의 대형 모델을 능가하는 성능을 제공합니다.
- ***Technical Details***: SmolVLM은 다양한 아키텍처 구성(exploration), 토큰화 전략(tokenization strategies), 데이터 큐레이션(data curation)을 체계적으로 탐구하여 자원 효율적인 추론을 최적화했습니다. 컴팩트한 모델 설계와 공격적이지만 효율적인 토큰화가 결합되어 메모리 사용량을 최소화하면서도 이미지 및 비디오 이해에서 높은 성능을 발휘하도록 했습니다. 또한, SmolVLM의 아키텍처는 영상 프레임을 서브이미지로 나눈 후 인코딩하여 시각적 정보를 손실 없이 LLM 입력 공간으로 매핑합니다.
- ***Performance Highlights***: SmolVLM-256M은 단 1GB 미만의 GPU 메모리를 사용하여 추론을 실행하며, 거의 모든 벤치마크에서 80B 파라미터를 가진 Idefics 모델을 능가합니다. 가장 큰 SmolVLM-2.2B 모델은 4.9GB의 메모리 사용량으로 최대 59.8%의 벤치마크 평균 점수를 기록하며, 더 큰 모델들과 비교해도 메모리 효율성이 뛰어납니다.

### [One-Minute Video Generation with Test-Time Training](https://arxiv.org/abs/2504.05298)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.05298.png)

Vote: 55

Authors: Sanmi Koyejo, Tatsunori Hashimoto, Ka Chun Cheung, Jan Kautz, Carlos Guestrin, Jiarui Xu, Yu Sun, Daniel Koceja, Shihao Han, Xiaolong Wang, Gashon Hussein, Yejin Choi, Youjin Song, Yue Zhao, Karan Dalal

- ***What's New***: 이 논문은 Test-Time Training(TTT) 레이어를 사용하여 텍스트 스토리보드로부터 1분 영상 생성이 가능한 방식을 소개합니다. 이는 Transformer 기반 모델에서 3초의 클립만 생성 가능하던 것을 TTT 레이어를 추가하여 1분 길이의 복잡한 멀티씬 스토리를 보다 일관성 있게 생성할 수 있음을 보여줍니다.
- ***Technical Details***: 기존의 Diffusion Transformer에 TTT 레이어를 추가하여 사전 학습된 모델을 세그먼트별로 1분 비디오 생성이 가능하도록 합니다. TTT 레이어는 RNN 레이어의 숨겨진 상태를 대체할 수 있는 표현력이 풍부한 다층 신경망으로, 시험 시에도 학습된 상태를 업데이트할 수 있도록 설계되어 있습니다. TTT-MLP 구조는 Transformer에서 사용된 MLP와 유사하게 두 개의 레이어로 구성되고, 각 레이어는 입력 크기의 4배의 숨겨진 차원을 가집니다.
- ***Performance Highlights***: TTT 레이어를 사용한 모델은 기존의 Mamba 2, Gated DeltaNet 등과 비교하여 100개의 비디오에 대한 인간 평가에서 34 Elo 포인트의 개선을 보여주는 등, 보다 일관성 있는 비디오 생성을 가능하게 합니다. 기존의 20초 이내의 제한을 뛰어넘어 1분 길이의 비디오 생성이 가능하다는 점도 중요한 성과입니다.

### [Rethinking Reflection in Pre-Training](https://arxiv.org/abs/2504.04022)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.04022.png)

Vote: 48

Authors: Peter Rushton, Michael Callahan, Yash Vanjani, Mrinal Iyer, Tim Romanski, Divya S Mansingka, Somanshu Singla, Saurabh Srivastava, Andrew Hojel, Anthony Polloreno, Andrew Ma, Burhan Drak Sibai, Divya Shivaprasad, Karl Stratos, Ritvik Kapila, Khoi Nguyen, Ashish Tanwer, Mohit Parmar, Platon Mazarakis, Ashish Vaswani, Adarsh Chaluvaraju, Anil Thomas, Essential AI, Darsh J Shah, Philip Monk, Kurt Smith, Michael Pust, Ishaan Shah

- ***What's New***: 이 논문에서는 모델의 사전 훈련 단계에서도 자기 반성 능력이 발전할 수 있음을 처음으로 밝히고, 이러한 반성 능력을 측정하기 위한 새로운 프레임워크를 제안합니다.
- ***Technical Details***: 논문에서는 자신이 만든 오류를 확인하고 수정하는 능력을 측정하기 위해 여러 분야의 Adversarial Datasets를 생성하여 다양한 데이터셋에서 반성 능력을 연구했습니다. 두 가지 설정 즉, 외부 소스의 정보에 대한 반성(Situational-reflection)과 자신의 출력에 대한 반성(Self-reflection)을 통해 이를 수행합니다. OLMo-2 모델 패밀리를 사용하여 각각 7B, 13B, 32B의 파라미터에 대해 연구를 수행했습니다.
- ***Performance Highlights***: 실험 결과, OLMo-2 모델은 다양한 Adversarial Datasets에서 반성 능력을 보여주었으며, 사전 훈련에 더 많은 컴퓨팅을 사용할수록 반성 능력이 점진적으로 향상되었습니다. 240개의 데이터셋-체크포인트 조합 중 231개에서 최소 하나 이상의 situational-reflection 사례가 관찰되었으며, 154개 조합에서는 self-reflection 사례가 관찰되었습니다.

### [URECA: Unique Region Caption Anything](https://arxiv.org/abs/2504.05305)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.05305.png)

Vote: 26

Authors: Jaewoo Jung, Junwan Kim, Seungryong Kim, Sangbeom Lim, Heeji Yoon

- ***What's New***: URECA는 다중 세분화된 (multi-granularity) 이미지 영역 설명을 위한 새롭고 대규모의 데이터셋을 소개합니다. 이 데이터셋은 각 영역과 고유하게 매핑되는 설명을 통해, 이전 데이터셋과 달리 다양한 객체, 부품 및 배경 요소를 포함하여 다양한 세분화 수준에서 고유한 설명을 보장합니다.
- ***Technical Details***: URECA 데이터셋은 SA-1B 데이터셋을 기반으로 하여, 이미지의 각 영역을 상호 의존성을 기반으로 한 마스크 트리(structure)를 통해 계층적으로 구성합니다. 다단계 데이터 큐레이션 파이프라인을 통해 MLLM(Multimodal Large Language Model)을 활용하여, 각 노드의 캡션은 부모 노드와 자식 노드의 정보를 통합하여 고유하면서도 문맥적으로 일관된 설명을 생성합니다.
- ***Performance Highlights***: URECA 모델은 URECA 데이터셋의 테스트 세트에서 최첨단 성능을 달성했으며, Visual Genome 및 RefCOCOg와 같은 기존 데이터셋에서도 강력한 일반화 성능을 보여주었습니다. 또한, 많은 기존 캡션 모델들이 URECA 데이터셋으로 세분화된 캡션화에서 성능을 향상 시킬 수 있음을 입증했습니다.

### [T1: Tool-integrated Self-verification for Test-time Compute Scaling in Small Language Models](https://arxiv.org/abs/2504.04718)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.04718.png)

Vote: 26

Authors: Jongwon Jeong, Jaewoong Cho, Minki Kang

- ***What's New***: T1이라는 기법은 소형 언어 모델(small Language Models; sLMs)이 테스트 시점에서 자기 검증을 외부 툴과 통합하여 수행할 수 있도록 하는 접근법입니다. 기존에는 대형 모델을 검증자로 활용해야 했던 문제를 해결하기 위해 외부 툴(예: 코드 인터프리터)을 통합하여 모델 자체의 검증 능력 향상을 도모합니다.
- ***Technical Details***: T1은 Parallel Test-time Scaling 환경에서 sLM의 자기 검증 작업을 두 단계로 나누어 수행합니다. 먼저 Tool-based Verifier (ToolV)를 통해 계산 및 사실 확인과 같은 복잡한 검증을 외부 툴에 맡기고, 이후 RM 기반의 검증기로 잔여 응답을 평가합니다. ToolV 단계에서는 코드 생성을 통해 툴을 호출하고 이를 통해 정확성을 확인합니다.
- ***Performance Highlights***: T1은 MATH benchmark에서 1B 파라미터 모델이 8B 모델을 능가하는 성과를 거두었으며, 다양한 도메인에 걸쳐 툴 통합이 sLM의 자기 검증 능력을 크게 향상시킬 수 있음을 보여줍니다. 또한, ToolV는 기존의 검증 방법에 비해 계산 오류를 더 효과적으로 필터링하는 것으로 나타났습니다.

### [Concept Lancet: Image Editing with Compositional Representation Transplant](https://arxiv.org/abs/2504.02828)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02828.png)

Vote: 15

Authors: René Vidal, Jinqi Luo, Chris Callison-Burch, Kwan Ho Ryan Chan, Tianjiao Ding, Hancheng Min

- ***What's New***: Concept Lancet(CoLan)은 디퓨전 기반 이미지 편집에서 개념 표현 조작을 위한 무작위 플러그 앤 플레이 프레임워크를 도입합니다. CoLan은 개념 사전(Concept Dictionary)를 통해 소스 이미지를 분석하여 적절한 편집 강도를 추정할 수 있으며, CoLan-150K라는 대규모 개념 표현 데이터셋을 활용하여 다양한 시각적 개념과 시나리오를 제공합니다.
- ***Technical Details***: CoLan은 디퓨전 모델의 잠재 공간(latent space)에서 입력 이미지를 텍스트 임베딩이나 점수 공간(score space)의 희소 선형 결합으로 분해합니다. 이를 통해 각 이미지에서 개념의 존재를 정확하게 파악하고 이에 따른 편집 방향을 설정할 수 있습니다. 또한, 특정 편집 작업(교체, 추가, 제거)에 맞춰 맞춤형 개념 이식 프로세스를 수행합니다. CoLan-150K 데이터셋은 다양한 시각적 용어와 문구의 설명과 시나리오로 구성된 개념 사전을 제공합니다.
- ***Performance Highlights***: CoLan을 장착한 방법들은 편집 효과와 일관성 유지에서 최첨단 성능을 달성했습니다. 예를 들어, P2P-Zero 백본에서는 StruDist와 LPIPS가 거의 50% 낮아지고 PSNR과 SSIM은 약 10% 향상되었습니다. 또한, InfEdit와 CoLan을 함께 사용했을 때, 다양한 지표에서 가장 뛰어난 성능을 나타냈습니다. CoLan은 편집 강도를 효율적으로 추정하여, 다양한 소스 이미지에 맞춘 적절한 강도의 편집을 가능하게 합니다.

### [Quantization Hurts Reasoning? An Empirical Study on Quantized Reasoning Models](https://arxiv.org/abs/2504.04823)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.04823.png)

Vote: 14

Authors: Tiezheng Yu, Chun Yuan, Haoli Bai, Xianzhi Yu, Lu Hou, Yuxuan Sun, Ruikang Liu, Manyi Zhang

- ***What's New***: 이 논문은 대규모 언어 모델(LLMs)의 추론 모델(Reasoning Models)에 대한 양자화(Quantization)가 어떻게 성능에 영향을 미치는지에 대한 첫 번째 체계적인 연구를 수행하였습니다. 특히, 수학(AIME, MATH-500), 과학(GPQA), 프로그래밍(LiveCodeBench) 등의 다양한 추론 벤치마크에서 양자화된 추론 모델의 성능을 평가하였습니다.
- ***Technical Details***: 이 연구는 다양한 비트 폭에서 최신 알고리즘을 사용하여 무게(Weight), KV 캐시(KV Cache), 활성화(Activation) 양자화를 통해 추론 모델을 평가합니다. 특히, 양자화 알고리즘 간의 성능 비교와 각 모델의 크기, 모델의 출처, 그리고 과제의 난이도가 성능에 미치는 영향을 분석하였습니다. 또한, 모든 양자화된 모델 및 코드는 오픈 소스로 제공될 예정입니다(https://github.com/ruikangliu/Quantized-Reasoning-Models).
- ***Performance Highlights***: 비손실 양자화는 W8A8이나 W4A16 양자화로 달성할 수 있으며, 각 모델 크기 및 태스크 간의 성능을 거의 보존합니다. 그러나 더 낮은 비트 폭에서는 상당한 정확도 손실이 발생할 수 있습니다. 또한, 더 어려운 과제(AIME-120)일수록 더 큰 성능 저하가 발생하며, 양자화된 모델이 출력 길이를 늘리지 않는다는 점도 확인되었습니다. 전략적인 모델 크기 또는 추론 단계를 확장하면 성능을 효과적으로 향상시킬 수 있습니다.

### [VAPO: Efficient and Reliable Reinforcement Learning for Advanced Reasoning Tasks](https://arxiv.org/abs/2504.05118)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.05118.png)

Vote: 12

Authors: Mofan Zhang, Wang Zhang, YuYue, Xiaochen Zuo, Jiaze Chen, Wenyuan Xu, Qiying Yu, Ruofei Zhu, Lingjun Liu, Haibin Lin, Ru Zhang, Yufeng Yuan, Zhiqi Lin, Mingxuan Wang, Zhengyin Du, TianTian Fan, Bole Ma, Gaohong Liu, Lin Yan, Xiangpeng Wei, Juncai Liu, Yonghui Wu, Chengyi Wang, Hang Zhu, Chi Zhang, Xin Liu

- ***What's New***: VAPO는 실제로 고급 추론 과제를 위한 가치 기반 강화 학습 프레임워크(Value-based Augmented Proximal Policy Optimization framework)로, 긴 체인-오브-생각 (Long-Chain-of-Thought; Long-CoT) 추론 작업에서 탁월한 성능을 보여줍니다. VAPO는 기존의 가치 없는 방법들보다 성능이 뛰어나며, 향후 강화 학습의 새로운 벤치마크로 자리 잡을 가능성이 높습니다.
- ***Technical Details***: VAPO는 각각의 응답 길이에 따라 GAE (Generalized Advantage Estimation)의 λ 파라미터를 조절하는 Length-adaptive GAE를 제안함으로써, 다양한 응답 길이에 대한 고유한 편향-분산 트레이드오프 요구사항을 효과적으로 해결합니다. 또한, DAPO와 VC-PPO에서의 기술을 체계적으로 통합하였으며, Reward 신호의 희소성 문제를 해결하기 위해 Clip-Higher, Positive Example LM Loss, Group-Sampling 기법들을 사용합니다.
- ***Performance Highlights***: Qwen-32B 모델 바탕의 VAPO는 AIME24 벤치마크에서 60.4 점을 기록하여 기존 SOTA 방법인 DAPO 보다 10 포인트 이상 높은 성과를 달성했습니다. 이 실험 결과는 VAPO의 높은 안정성과 신뢰성을 강조하며, 5,000 스텝 내에 SOTA 성능을 보여준 점에서 매우 효율적인 학습 과정을 입증했습니다.

### [LiveVQA: Live Visual Knowledge Seeking](https://arxiv.org/abs/2504.05288)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.05288.png)

Vote: 12

Authors: Yao Wan, Mingyang Fu, Dongping Chen, Yuyang Peng, Benlin Liu

- ***What's New***: LIVEVQA는 자동으로 수집된 데이터셋으로 인터넷에서 최신 시각적 지식을 포함한 VQA(Visual Question Answering) 문제를 제공합니다. 이 데이터셋은 3,602개의 단일 및 다단계 시각적 질문을 포함하며, 14가지 뉴스 카테고리로 분류된 1,233개의 뉴스를 포함하고 있습니다.
- ***Technical Details***: LIVEVQA는 6개의 글로벌 뉴스 플랫폼에서 수집된 최신 VQA 문제로 구성되어 있으며, 자동화된 진실 데이터와 인간 검수를 포함하는 절차를 통해 데이터의 질을 높였습니다. 각 인스턴스는 기본적인 이미지 이해를 위한 질문과 심화적인 추론이 필요한 두 개의 멀티모달 다단계 질문을 포함합니다. 실험은 15개의 최첨단 MLLM(Multimodal Large Language Models)을 대상으로 수행되었습니다.
- ***Performance Highlights***: QvQ-72B-Preview 모델은 교차 모달리티 다단계 질문에서 뛰어난 성능을 보이며, 3단계 질문에서 7.41%의 정확도를 달성했습니다. Gemini-2.0-Flash 모델은 자체 내장된 이미지 검색 기능을 통해 평균 정확도가 29.00%로 향상되었으며, 더 어려운 2단계와 3단계 질문에서도 각각 22.75%와 13.66%의 정확도를 기록했습니다.

### [Are You Getting What You Pay For? Auditing Model Substitution in LLM APIs](https://arxiv.org/abs/2504.04715)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.04715.png)

Vote: 6

Authors: Xuandong Zhao, Dawn Song, Tianneng Shi, Will Cai

- ***What's New***: 이 논문에서는 사용자들이 광고된 모델의 기능에 따라 서비스를 선택하고 비용을 지불하지만, 제공자가 운영 비용을 줄이기 위해 더 저렴하고 낮은 품질의 대안을 은밀하게 대체할 수 있는 문제를 다룹니다. 이 투명성 부족은 사용자 신뢰를 약화시키고, 공정성을 저해하며, 안정적인 비교 기준을 복잡하게 만듭니다. 이 논문은 LLM API에서 모델 대체를 탐지하는 문제를 공식화하고 하드웨어 기반 솔루션인 신뢰 실행 환경(Trusted Execution Environments; TEEs)이 신뢰할 수 있는 모델 무결성을 제공할 수 있는 경로를 제공함을 강조합니다.
- ***Technical Details***: 문서에서 논의된 탐지 방법은 출력 기반 통계 테스트, 벤치마크 평가, 로그 확률 분석을 포함하며, 이러한 방법들이 다양한 현실적 공격 시나리오(모델 양자화, 랜덤 대체, 벤치마크 회피)에 대해 체계적으로 평가되었습니다. 로그 확률 분석(log probability analysis)은 더 강력한 보장을 제공하지만, 접근성이 제한적입니다. TEEs는 하드웨어 기반의 실행 무결성을 보장하며, 인프라 의존성과 제공자의 채택이라는 트레이드오프가 존재합니다.
- ***Performance Highlights***: 텍스트 출력 기반의 통계 테스트는 특정 공격에 대해 회복력이 있는 반면, 다른 공격에는 취약하다는 것이 드러났습니다. 로그 확률 분석을 사용할 수 있는 경우에는 더 강력하지만, 대부분의 API 제공자는 이 기능을 제한하고 있으며, 하드웨어 기반의 TEEs는 최소한의 성능 비용으로 모델 무결성을 보장할 수 있습니다. TEEs의 적용은 제한적이지만, 고빈도 요청 시 처리량이 상대적으로 낮은 오버헤드를 나타냅니다.

### [Gaussian Mixture Flow Matching Models](https://arxiv.org/abs/2504.05304)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.05304.png)

Vote: 6

Authors: Sai Bi, Hao Tan, Fujun Luan, Gordon Wetzstein, Hansheng Chen, Kai Zhang, Zexiang Xu, Leonidas Guibas

- ***What's New***: 이 논문에서는 새로운 Gaussian Mixture Flow Matching(GM-Flow) 모델이 소개됩니다. 기존의 Gaussian을 예측하는 대신, GM-Flow는 동적 Gaussian Mixture 파라미터를 예측하여 다중 모드 플로우 속도 분포를 캡처합니다. 이는 KL divergence loss로 학습됩니다.
- ***Technical Details***: GM-Flow 모델은 전 유도와 플로우 매칭 모델을 일반화합니다. Gaussian Mixture 분포의 파라미터를 예측하여, 유도와 플로우 매칭 모델에서의 단일 Gaussian 학습을 확장합니다. 이러한 예측을 통해 유도 분포와 속도 필드를 활용하여 정밀한 몇 단계 샘플링을 가능하게 합니다. 새로운 확률적 지침 기법을 소개하여 CFG에서의 과포화 문제를 줄이고 이미지 생성 품질을 향상시킵니다.
- ***Performance Highlights***: GM-Flow는 샘플링 품질에서 플로우 매칭 기반을 일관되게 능가하며, ImageNet 256×256에서 6 샘플링 스텝만으로 0.942의 Precision을 달성합니다. 이는 플로우 매칭 모델들이 높은 품질의 생성을 위해 많은 샘플링 스텝을 요구하는 것에 비해 GM-Flow의 우수성을 보여줍니다.

### [Why Reasoning Matters? A Survey of Advancements in Multimodal Reasoning (v1)](https://arxiv.org/abs/2504.03151)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.03151.png)

Vote: 5

Authors: Zhang Liu, Lianggong Bruce Wen, Susan Liang, Pinxin Liu, Guangyu Sun, Daoan Zhang, Jiarui Wu, Shu Yang, Chen Chen, Jinxi He, Jing Bi, Chao Huang, Jiebo Luo, Xiaofei Zhou, Yunlong Tang, Junjia Guo, Chenliang Xu, Luchuan Song

- ***What's New***: 이 논문은 멀티모달 reasoning의 최신 발전을 조사하며, 특별히 대형 언어 모델(LLM)과 멀티모달 대형 언어 모델(MLLM)의 reasoning 기술을 종합적으로 분석하였습니다. 이 연구는 구조적 문제 해결과 다양한 작업 전반에 걸친 reasoning 기법의 의미 있는 발전을 다루며, 기존의 LLMs의 reasoning 능력을 멀티모달 영역으로 효과적으로 확장하기 위한 도전과제와 기회를 클리어하게 정리하였습니다.
- ***Technical Details***: 논문은 Chain-of-Thought(CoT)과 같은 prompting 전략이 인간의 단계적 문제 해결 과정을 모방하여 LLM의 성능을 향상시키는 방법을 설명합니다. 연구는 또한 Gang전 수단인 post-training 최적화 및 test-time inference 과정을 포함한 다양한 방법론을 설명합니다. 시각적 단서와 텍스트적 단서를 결합하여 모델의 reasoning을 강화하고 시각 정보와 텍스트 정보가 불일치하거나 차이가 있을 때 이를 해결하는 방법을 논의합니다.
- ***Performance Highlights***: 모델의 reasoning 경로를 optimal하게 생성하고 선택하는 전략이 개선된 점을 강조합니다. 연구는 다양한 post-training 개선 방법(강화 학습, 정책 최적화)을 통해 MLLM의 reasoning 품질을 높이려는 최근 접근 방식을 살펴보고, 더 나아가 평가 과정에서의 모델 추론 및 정확성을 더욱 공고히 하는 test-time 전략을 제시합니다.

### [DiaTool-DPO: Multi-Turn Direct Preference Optimization for Tool-Augmented Large Language Models](https://arxiv.org/abs/2504.02882)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02882.png)

Vote: 4

Authors: Myeongcheol Shin, Gaeun Seo, Byeongil Ko, Daniel Lee, Junrae Cho, Eunggyun Kim, Shinbok Lee, Kihyun Kim, Sunghee Jung, Donghun Lee

- ***What's New***: DiaTool-DPO는 대화 도구 최적화를 통해 툴 증강 대형 언어 모델(Tool-Augmented Large Language Models; TA-LLMs)의 대화 능력을 강화하는 새로운 방법입니다. 이 방법은 직접적인 선호도 최적화(Direct Preference Optimization)를 통해, 불완전한 질의와 범위 외 요청을 효과적으로 처리할 수 있는 대화 모델을 개발합니다.
- ***Technical Details***: DiaTool-DPO는 TA-LLM의 대화 상호작용을 5개의 고유 대화 상태로 모델링하고, 사용자 질의를 세 가지 유형으로 분류하여 상태 전이 궤적 기준으로 대화제어 목적 손실을 도입합니다. TA-LLM은 마르코프 결정 과정(Markov Decision Process; MDP)으로 모델링되며, 선택된 궤적과 거부된 궤적을 자동으로 생성하여 전문화된 데이터셋을 구축합니다. 이 데이터셋은 다중 턴 상황도 처리하도록 설계되어 있습니다.
- ***Performance Highlights***: DiaTool-DPO는 대화 흐름을 판단할 수 있는 모델 능력을 94.8%까지 개선하여 GPT-4o(슬롯 채우기 94.8%, 도구 호출 거부 91%)에 가까운 성능을 보여주었습니다. 이는 기본 모델에 비해 도구 호출에 대한 거부 능력을 크게 향상시켰으며, 현재의 TA-LLM들이 실제 환경에서 다양성을 처리할 수 있도록 기회를 열어줍니다.

### [BOP Challenge 2024 on Model-Based and Model-Free 6D Object Pose Estimation](https://arxiv.org/abs/2504.02812)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02812.png)

Vote: 3

Authors: Hyeontae Son, Yann Labbe, Eric Brachmann, Carsten Rother, Bertram Drost, Stephen Tyree, Taeyeop Lee, Mederic Fourmy, Van Nguyen Nguyen, Jiri Matas, Sungphill Moon, Tomas Hodan, Andrew Guo, Anas Gouda, Jonathan Tremblay, Stan Birchfield, Martin Sundermeyer, Lukas Ranftl, Vincent Lepetit

- ***What's New***: BOP Challenge 2024는 6D 객체 포즈 추정에 대해 모델 기반(model-based) 및 모델 프리(model-free) 방법을 모두 평가하는 최초의 경연 대회입니다. 이번 대회의 목표는 기존의 연구실 환경에서 벗어나 실제 시나리오로의 전환을 통해 보다 실용적인 6D 객체 탐색 및 인식 과제를 소개하는 것입니다.
- ***Technical Details***: BOP-H3 데이터세트는 고해상도 센서와 AR/VR 헤드셋으로 기록되었으며, HOT3D, HOPEv2, HANDAL과 같은 데이터세트로 구성되어 있습니다. 참가자들은 모델 기반 및 모델 프리 설정에서 새로운 객체를 온보드(onboard)하는 능력을 테스트합니다. 3D 모델이 없는 모델 프리 과제에서는 참조 동영상으로부터 객체를 학습해야 합니다.
- ***Performance Highlights***: 모델 기반 6D 탐색에서 FreeZeV2.1은 BOP-Classic-Core에서 이전 챌린지의 최상 방법보다 22% 더 높은 정확도를 달성했으며, 너비 범위 판별 기반의 전차와 비교해 4% 낮습니다. 모델 프리 2D 탐색에서 MUSE는 이전 방법보다 21% 성능을 개선했으나, 보이는 객체 탐색과 비교해 여전히 낮은 정확도를 보여줍니다 (53% 미만).

### [Clinical ModernBERT: An efficient and long context encoder for biomedical text](https://arxiv.org/abs/2504.03964)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.03964.png)

Vote: 3

Authors: Anthony Wu, Jeffrey N. Chiang, Simon A. Lee

- ***What's New***: Clinical ModernBERT는 최신 BERT 기반 아키텍처를 활용하여 생의학 및 임상 문서에 최적화된 도메인 특화 트랜스포머 인코더입니다. 이 모델은 PubMed 초록, MIMIC-IV 임상 데이터, 의료 온톨로지를 포함한 대규모 생의학적 자료를 기반으로 사전 훈련되었습니다.
- ***Technical Details***: 전통적인 BERT 구조에서 발전된 ModernBERT 아키텍처를 적용하여, 회전 위치 임베딩(Rotary Positional Embeeddings; RoPE), Flash Attention, GeGLU 활성화 함수 등을 활용하였습니다. Clinical ModernBERT는 8,192 토큰까지의 긴 문맥을 처리할 수 있으며, BioClinicalBERT와 같은 기존 모델의 한계인 512 토큰의 제약을 극복하였습니다.
- ***Performance Highlights***: Clinical ModernBERT는 다양한 임상 NLP 벤치마크에서 BioBERT, BioClinicalBERT, Clinical Longformer 등을 초월하는 성능을 입증하였습니다. 특히 i2b2 데이터셋에서 긴 문맥 작업에서 뛰어난 성능을 발휘하였고, EHR(Electronic Health Record)와 PMC 환자 데이터의 수집에서도 우수한 결과를 보였습니다.

### [Mamba as a Bridge: Where Vision Foundation Models Meet Vision Language Models for Domain-Generalized Semantic Segmentation](https://arxiv.org/abs/2504.03193)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.03193.png)

Vote: 3

Authors: Robby T. Tan, Xin Zhang

- ***What's New***: 이 논문에서는 새로운 융합 프레임워크인 MFuser를 제안합니다. MFuser는 Vision Foundation Models (VFMs)와 Vision Language Models (VLMs)의 강점을 효율적으로 결합하여 일반적인 도메인에서의 시맨틱 세분화(Domain-Generalized Semantic Segmentation; DGSS)를 달성하도록 설계되었습니다. 이 프레임워크는 두 가지 핵심 구성 요소인 MVFuser와 MTEnhancer를 포함하며, 각각 시각 모델의 공동 파인 튜닝과 텍스트 임베딩 정교화를 통해 강력한 텍스트 정렬과 정확한 특징 위치를 제공합니다.
- ***Technical Details***: MFuser는 Mamba 기반의 FFusion 프레임워크로, VFMs와 VLMs의 시각적 특징을 통합하여 시퀀스 길이에 대한 선형 스케일링을 달성합니다. MVFuser는 두 모델의 시퀀셜 및 공간적 동력을 포착하여 공동으로 파인 튜닝하고, MTEnhancer는 이미지 우선순위를 포함하여 텍스트 임베딩을 정밀화합니다. 이를 통해 강력한 텍스트 정렬을 유지하면서도 계산량의 오버헤드를 최소화합니다.
- ***Performance Highlights***: 광범위한 실험에서, 제안된 MFuser는 최첨단 DGSS 방법들을 크게 능가하였으며, synthetic-to-real 및 real-to-real 벤치마크에서 각각 68.20 mIoU와 71.87 mIoU를 달성했습니다. 이는 다양한 비출력 도메인에 대한 일반화 성능을 대폭 개선한 결과로서, 실질적인 응용 가능성을 보여 줍니다.

### [JailDAM: Jailbreak Detection with Adaptive Memory for Vision-Language Model](https://arxiv.org/abs/2504.03770)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.03770.png)

Vote: 2

Authors: Yuehan Qin, Chaowei Xiao, Li Li, Ziyi Wang, Yi Nian, Yue Zhao, Shenzhe Zhu

- ***What's New***: JailDAM은 비전-언어 모델(Vision-Language Model; VLM)에서 발생하는 제일(Jailbreak) 공격을 탐지하기 위한 적응형 메모리 프레임워크인 JAILDAM을 소개합니다. 이 방법은 유해 데이터 없이 정책 주도 위험 지식을 통해 메모리 기반 접근 방식을 활용합니다.
- ***Technical Details***: JAILDAM은 멀티모달 안전 입력과 구조화된 메모리 뱅크에 저장된 위험한 메모리 간의 관계를 모델링하는 새로운 오토인코더 기반 탐지 파이프라인을 도입합니다. 이를 통해 VLM에서 잠재적인 제일 공격을 조기에 탐지할 수 있으며, 테스트 시간에 메모리를 동적으로 업데이트하여 미지의 제일 전략을 처리할 수 있습니다.
- ***Performance Highlights***: JAILDAM은 다양한 VLM 제일 벤치마크에서 최첨단 성능을 입증하며, 정확성과 속도를 모두 개선한 유해 콘텐츠 탐지 능력을 보여줍니다. 이 접근법은 이전 방법들에 비해 정확도와 효율성 면에서 우수하며, 완전히 블랙박스 모델에도 적용할 수 있습니다.

### [Distillation and Refinement of Reasoning in Small Language Models for Document Re-ranking](https://arxiv.org/abs/2504.03947)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.03947.png)

Vote: 1

Authors: Hamed Zamani, Chris Samarinas

- ***What's New***: 이 논문에서는 소형 언어 모델(Small Language Models)을 문서 재정렬(Document Re-ranking)에서의 추론 능력을 위해 훈련하는 새로운 접근 방식을 제안합니다. 이 방법은 대형 교사 LLM으로부터 지식 증류(Knowledge Distillation)와 강화 학습(Reinforcement Learning)을 결합하여 고품질 학습 예제와 관련 설명을 자동으로 생성합니다. 이 논문에서는 BRIGHT 벤치마크에서 기존 대형 모델보다 적은 파라미터를 사용하면서도 최첨단 성능을 달성하는 3B 파라미터 모델을 소개합니다.
- ***Technical Details***: 제안된 방법론은 웹에서 수집한 추론 집약적 질문을 통해 교육 시그널을 자동 생성하며, 70B 파라미터의 교사 LLM을 사용해 데이터셋을 20K 예제로 구성합니다. 그런 다음, 작은 학생 LLM(3B 파라미터)이 교사의 추론 능력을 모방하도록 지식 증류를 수행하고, 고품질 설명과 정확한 관련성 예측을 통해 강화 학습으로 이를 개선합니다. 매 단계별 추론을 통해 작은 언어 모델에서도 추론 성능을 향상시킵니다.
- ***Performance Highlights***: 3B 파라미터 모델은 BRIGHT 벤치마크에서 70B+ 파라미터 모델과 비교하여 성능을 발휘하며, leaderboard에서 세 번째로 순위가 높습니다. 이 방법은 파라미터 효율성을 유지하면서도 교사 모델의 성능을 줄여보여줍니다. 8B, 70B, 405B 파라미터의 모델과 비교할 때, InteRank 모델은 비슷하거나 더 나은 결과를 내며, 추론 기반의 랭킹에서 UMN에 있는 Reason-to-Rank와 같은 방법보다 우수한 성과를 보였습니다.

### [Sample, Don't Search: Rethinking Test-Time Alignment for Language Models](https://arxiv.org/abs/2504.03790)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.03790.png)

Vote: 0

Authors: Gonçalo Faria, Noah A. Smith

- **What's New**: QALIGN는 새로운 테스트 시 정렬 방법으로, 테스트 시 컴퓨팅을 확장할 때 각 프롬프트에 대한 최적의 정렬 분포에서 샘플링하게 합니다. 이 방법은 기본 모델의 수정 없이 더 나은 정렬된 출력을 생성하게 하여, 언어 모델(Language Models; LMs)의 테스트 시 성능을 향상시킵니다.
- **Technical Details**: QALIGN는 최근 마르코프 체인 몬테카를로(Markov chain Monte Carlo; MCMC) 기법을 채택하여 텍스트 생성을 수행하며, 각 프롬프트에 대응하는 보상 모델(Reward Model; RM)을 이용하여 효과적으로 정렬된 분포를 샘플링할 수 있습니다. 또한 GSM8K와 GSM-Symbolic과 같은 수학적 추론 벤치마크에서 시험되었으며, 기존의 테스트 시 방법들보다 안정적인 성능 향상을 보여주었습니다.
- **Performance Highlights**: QALIGN는 보상을 최적화한 기존의 방법 대비 각 데이터셋에서 35%에서 98%까지 정확도 증가를 나타냈습니다. TÜLU3 데이터셋을 사용한 일반적인 RM을 적용하더라도 Direct Preference Optimization(DPO)보다 57% 정확도 증가를 보였으며, 다양한 벤치마크에서 일관된 성능 향상을 달성했습니다.

### [Rethinking Multilingual Continual Pretraining: Data Mixing for Adapting LLMs Across Languages and Resources](https://arxiv.org/abs/2504.04152)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.04152.png)

Vote: 0

Authors: Hengyu Luo, Zihao Li, Jörg Tiedemann, Shaoxiong Ji

- ***What's New***: 이 연구는 다중 언어 지속 사전 학습(Multilingual Continual Pretraining; CPT) 전략을 체계적으로 평가하여 다국어 적응을 위한 데이터 믹싱(Data Mixing)의 영향을 분석하였습니다. 기존의 단일언어, 이중언어, 코드 통합 데이터 방식의 효과를 비교한 최초의 대규모 실험입니다.
- ***Technical Details***: 36개 CPT 구성을 평가하였고, 사용한 다국어 기반 모델들은 Llama-3.1-8B, Llama-2-7B 및 Viking-7B입니다. 데이터는 Lego-MT, NLLB, MADLAD-400 및 코드 데이터셋을 포함하며, 언어는 고, 중, 저 자원 카테고리로 나누어 사용되었습니다. 또한, 희생적(altruistic), 이기적(selfish), 정체(stagnant) 언어로 분류하였고, 각 언어는 다국어 훈련과 평가에서의 행동을 기반으로 선택되었습니다.
- ***Performance Highlights***: 이중언어 CPT는 중, 저 자원 언어의 분류 성능을 향상시키지만, 번역 시 언어 혼합 문제를 발생시켜 번역 과제의 활용도를 제한합니다. 코드 통합은 저자원 언어의 이해를 강화하지만, 세대 품질을 다소 저하시키는 거래오프(trade-off)를 제공합니다. 언어 분류 방식은 다양한 CPT 전략 하에서 일반화되지 않으며, 기존 연구에서 정의한 '희생적' 언어가 항상 도움이 되지 않음을 보여줍니다.

### [GlotEval: A Test Suite for Massively Multilingual Evaluation of Large Language Models](https://arxiv.org/abs/2504.04155)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.04155.png)

Vote: 0

Authors: Raúl Vázquez, Zihao Li, Ananda Sreenidhi, Jörg Tiedemann, Ona de Gibert, Shaoxiong Ji, Mengjie Wang, Bhavani Sai Praneeth Varma Mantina, Hengyu Luo, Peiqin Lin, Sawal Devkota, Samea Yusofi, Joseph Attieh

- ***What's New***: GlotEval은 대형 언어 모델(LLM)의 다국어 평가를 위해 설계된 경량의 프레임워크로, 기존의 영어 중심 평가에서 벗어나 수백 개의 언어를 아우르는 체계적인 평가를 지원합니다. 특히 저자원 언어에 대한 평가를 강화하여 다양한 언어적 맥락에서 모델의 강점과 약점을 정확히 진단할 수 있습니다.
- ***Technical Details***: GlotEval은 20개 이상의 다국어 벤치마크를 통합하여 일관된 다국어 벤치마킹을 수행합니다. ISO 639-3 언어 코드를 표준화하여 거의 모든 알려진 언어에 대한 평가를 가능하게 하며, 사용자 정의 가능한 언어별 프롬프트 템플릿 및 비영어 중심의 기계 번역 평가를 지원합니다. Microsoft Translator와의 통합을 통해 사용자 정의 프롬프트 템플릿을 130개 이상의 언어로 자동 변환할 수 있습니다.
- ***Performance Highlights***: GlotEval은 Nvidia A100과 AMD MI250X GPU 환경에서의 추론 속도를 비교하며, Nvidia A100이 일반적으로 더 높은 처리량을 달성하는 것으로 나타났습니다. 다양한 언어적 설정과 다국어 번역 시나리오에서의 성능 차이를 조사한 사례 연구에서 EMMA-500 모델이 Llama-2-7B 모델 대비 더 뛰어난 지시 따르기 및 번역 성능을 보였습니다. 이러한 실험은 GlotEval의 다국어 평가 기능을 잘 보여줍니다.

