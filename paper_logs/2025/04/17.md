## Daily Papers (2025-04-17)

### [ColorBench: Can VLMs See and Understand the Colorful World? A Comprehensive Benchmark for Color Perception, Reasoning, and Robustness](https://arxiv.org/abs/2504.10514)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10514.png)

Vote: 31

Authors: Yijun Liang, Ming Li, Jiuhai Chen, Dang Nguyen, Chenrui Fan, Fuxiao Liu, Kwesi Cobbina, Shweta Bhardwaj, Ziyue Li, Tianyi Zhou

- ***What's New***: ColorBench는 대형 비전-언어 모델들(Vision-Language Models; VLMs)의 색상 이해 능력을 종합적으로 평가하는 최초의 벤치마크입니다. 이는 색상 인식(Color Perception), 추론(Reasoning), 그리고 견고성(Robustness)에 대한 능력을 검증하며, 다양한 실제 응용 시나리오에 기반해 색상을 어떻게 감지하고 해석하는지를 시험합니다.
- ***Technical Details***: ColorBench는 VLMs의 색상 이해 능력을 드러내기 위해 3개의 핵심 능력으로 나뉜 11개의 세부 과제를 포함하는 1,448개의 인스턴스와 5,814개의 이미지-텍스트 질문들로 구성됩니다. 모델은 색상 변형이 있는 이미지들에서의 일관된 성능을 통해 견고성을 평가받습니다. 이는 세 가지 리컬러링 전략을 사용하여 달성됩니다: 전역 이미지 변화, 목표 세그먼트 변경, 그리고 가장 큰 세그먼트 변경입니다.
- ***Performance Highlights***: 32개의 다양한 VLMs에 대한 평가 결과, 모델의 크기가 증가할수록 성능도 향상되는 경향이 있지만, 언어 모델(Language Model)의 크기가 더욱 중요한 역할을 반면, 비전 인코더(Vision Encoder)의 크기와 성능 사이의 상관관계는 미미했습니다. 또한 기존 VLM들은 색상 이해에 부족함을 보였으며, 색상 단서들은 일부 작업에서 모델을 오도하기도 했습니다. 그렇지만 추가적인 추론 단계가 색상 이해의 정확성과 견고성을 향상시킬 수 있음을 발견했습니다.

### [BitNet b1.58 2B4T Technical Report](https://arxiv.org/abs/2504.12285)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.12285.png)

Vote: 28

Authors: Furu Wei, Hongyu Wang, Shaohan Huang, Yan Xia, Shuming Ma, Ying Hu, Xingxing Zhang, Ting Song

- ***What's New***: BitNet b1.58 2B4T는 최초의 오픈 소스 네이티브 1비트 대형 언어 모델(1-bit Large Language Model)로서, 20억 개의 매개변수를 포함하고 있습니다. 4조 개 토큰의 데이터로 학습되어, 이 모델은 비슷한 크기의 최고 성능 풀 프리시전(Full Precision) 모델과 유사한 성능을 유지하면서도 메모리 사용량, 에너지 소비, 디코딩 지연 시간을 대폭 줄일 수 있는 이점을 제공합니다.
- ***Technical Details***: BitNet b1.58 2B4T는 BitNet 프레임워크를 통해 표준 Transformer 모델을 기반으로 하고 있으며, 전체 네이티브 학습을 통해 만들어진 1비트 모델입니다. 주요 기술 세부사항으로는 BitLinear 레이어에서의 가중치 양자화, 8비트 활성화 양자화, 수블린 서브레벨 정규화(SubLN Normalization) 등이 있습니다. 학습은 대규모 전처리, 지도형 세부 튜닝(Supervised Fine-tuning), 직접 선호 최적화(Direct Preference Optimization)을 포함합니다.
- ***Performance Highlights***: BitNet b1.58 2B4T는 다양한 벤치마크에서 뛰어난 성능을 보입니다. 메모리 사용량과 에너지 소비 면에서 주목할 만한 감소를 보이며, 특정 작업에서는 최상의 결과를, 다른 작업에서는 최고 성능 풀 프리시전 모델과 대등한 성능을 나타냅니다. 이는 BitNet b1.58 2B4T가 기존의 INT4 양자화 모델들보다 더 높은 효율-성능 곡선을 제공한다는 것을 시사합니다.

### [ReTool: Reinforcement Learning for Strategic Tool Use in LLMs](https://arxiv.org/abs/2504.11536)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11536.png)

Vote: 25

Authors: Yujia Qin, Ge Zhang, Wanjun Zhong, Jiazhan Feng, Shijue Huang, Xingwei Qu, Jinxin Chi, Baoquan Zhong, Chengquan Jiang

- ***What's New***: ReTool은 LLMs의 도구 사용 전략을 강화하기 위해 제안된 혁신적인 강화 학습(Reinforcement Learning) 프레임워크로, 코드 인터프리터(Code Interpreter)를 활용하여 긴 형태의 추론을 개선합니다. 이 방법은 실시간 코드 실행과 자연어 추론 과정을 동적으로 조합하며, 자동화된 RL 패러다임을 통해 사용한 도구의 결과 피드백을 기반으로 모델이 도구를 언제, 어떻게 사용해야 할지를 학습하게 합니다.
- ***Technical Details***: ReTool은 두 가지 주요 구성 요소로 이루어져 있습니다: 고품질의 초기 데이터셋을 생성하여 코드 인터프리터의 사용 시점과 방법을 명시적으로 보여주는 파이프라인과 이를 통한 초기 감각을 모델에게 가르치는 방식, 그리고 도구 향상 강화 학습 파이프라인을 적용해 최적의 도구 조작 전략을 발견하고 행동을 수정하는 체계입니다. RL 트레이닝 중에 정책 모델이 샌드박스 스타일의 코드 인터프리터와 협력하여 실시간 코드 실행 결과를 작성하고 이를 활용해 이어지는 사고를 지원합니다.
- ***Performance Highlights***: ReTool은 AIME2024 및 AIME2025에서 32B 모델 기준 67% 정확성을 400번의 트레이닝 스텝으로 달성했으며, 텍스트 기반 RL 베이스라인을 뛰어넘는 성과를 보여주었습니다. 또한, 72.5%의 정확도를 확장 설정에서 달성하여 OpenAI의 o1-preview를 무려 27.9% 더 우수하게 상회했습니다. 이러한 실험 결과는 복잡한 수학적 추론을 발전시키기 위한 결과 중심의 도구 통합의 가능성을 보여줍니다.

### [AlayaDB: The Data Foundation for Efficient and Effective Long-context LLM Inference](https://arxiv.org/abs/2504.10326)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10326.png)

Vote: 17

Authors: Yitao Zheng, Haotian Liu, Yangshen Deng, Man Lung Yiu, Long Xiang, Zhengxin You, Huan Li, Qilong Li, Zhaoyang Hong, Rui Mao, Peiqi Yuan, Bo Tang, Runzhong Li, Wanting Li, Kyriakos Mouratidis, Qiaomu Shen

- ***What's New***: AlayaDB는 대용량 언어 모델(LLM)의 긴 문맥 추론을 효율적이고 효과적으로 지원하는 첨단 벡터 데이터베이스 시스템입니다. 이 시스템은 LLM 추론 시스템에서 KV 캐시(KV Cache)와 주의 계산(Attention Computation)을 분리하고 이를 혁신적인 벡터 데이터베이스 시스템으로 캡슐화함으로써 기존 솔루션과 비교할 때 더 높은 생성 품질과 하드웨어 자원 사용의 감소를 제공합니다.
- ***Technical Details***: AlayaDB는 효율적인 희소 주의 계산을 위해, 전통적인 Top-𝑘 쿼리의 한계를 극복하고 다양한 과제와 헤드에서 동적으로 중요한 토큰을 검색할 수 있는 Dynamic Inner-product Range Query(DIPR)를 소개했습니다. 또한, AlayaDB의 쿼리 처리 엔진은 네이티브 주의 엔진과 쿼리 최적화기(Query Optimizer)로 구성되어 있으며, GPU 메모리와 효율성을 동시에 고려하여 설계되었습니다.
- ***Performance Highlights***: AlayaDB는 다양한 LLM 추론 벤치마크에서 인상적인 성능을 보였습니다. 제안된 DIPR 쿼리는 SLO(SLO; Service Level Objective)를 보장하면서도 높은 생성 품질을 유지하였으며, GPU 메모리 사용량을 현저히 줄였습니다. 특히, 재사용 가능한 문맥에서 초단위의 빠른 해결 시간을 제공하며, 기존의 풀 어텐션과 비교해 품질이 유사하거나 더 나은 것으로 확인되었습니다.

### [Cobra: Efficient Line Art COlorization with BRoAder References](https://arxiv.org/abs/2504.12240)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.12240.png)

Vote: 17

Authors: Zhaoyang Zhang, Junhao Zhuang, Ying Shan, Xuan Ju, Lingen Li, Chun Yuan

- ***What's New***: Cobra는 만화 제작 산업의 요구를 충족시키기 위해 설계된 새로운 장기 컨텍스트 라인 아트 채색 프레임워크입니다. 이 프레임워크는 200개 이상의 참조 이미지와 컬러 힌트를 지원하며, 효율성과 유연성을 강조하여 고품질의 채색 결과를 제공합니다.
- ***Technical Details***: Cobra의 중심에는 Causal Sparse DiT 아키텍처가 있으며, 이는 특별히 설계된 위치 인코딩(positional encodings), 인과 희소 주의(causal sparse attention), 그리고 KV-Cache를 활용해 장기 컨텍스트 참조를 효과적으로 관리하고 색상 ID의 일관성을 보장합니다. 또한, Regional Localized Reusable Positional Encoding을 사용하여 사전 훈련된 2D 인코딩을 수정하지 않고도 무제한의 참조 이미지를 처리할 수 있습니다.
- ***Performance Highlights***: Cobra는 다양한 기준에 걸쳐 기존 방법론을 능가하며, 특히 맥락적 정보가 풍부할수록 만화 채색의 이미지 품질과 색상 ID 정확도가 크게 향상되는 것을 보여줍니다. 성능 면에서 Cobra는 컬렉션의 색상 ID를 정확하게 보존하면서 고품질의 결과를 제공하여 산업적 응용에 적합합니다.

### [SIFT-50M: A Large-Scale Multilingual Dataset for Speech Instruction Fine-Tuning](https://arxiv.org/abs/2504.09081)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.09081.png)

Vote: 12

Authors: Andreas Schwarz, Jian Xie, Grant P. Strimel, Arunasish Sen, Rupak Vignesh Swaminathan, K V Vijay Girish, Prabhat Pandey

- ***What's New***: SIFT(Speech Instruction Fine-Tuning)-50M는 50M 예제를 포함하는 대규모 멀티링구얼 데이터셋으로, 음성-텍스트 LLMs(large language models)의 인스트럭션 후속 학습과 사전 학습을 위한 목적으로 개발되었습니다. 이는 음성 이해 및 조절 가능한 음성 생성 인스트럭션을 포함하며, 다섯 가지 언어를 지원합니다. 또한, EvalSIFT라는 새로운 벤치마크 데이터셋을 제공하여 음성-텍스트 LLM의 인스트럭션-팔로잉 능력을 체계적으로 평가할 수 있습니다.
- ***Technical Details***: SIFT-50M은 Multi-lingual Librispeech, Common Voice, VCTK 등의 공개된 음성 데이터를 바탕으로 만들어졌습니다. 메타데이터에서 음성의 음조, 발음속도, 강도, 반향률 및 소음 수준 등의 정보를 추출하여 강화되었습니다. 그런 다음, LLM을 사용하여 자연어 인스트럭션을 생성하고 그 결과를 검증과 반복적인 프롬프트 개선 과정을 통해 개선했습니다. 인스트럭션 데이터는 폐쇄형, 개방형 및 조절 가능한 생성으로 구성되며, 각 음성 발화에 대해 최대 10개의 QA 쌍을 생성하도록 설계되었습니다.
- ***Performance Highlights***: SIFT-LLM은 기존의 공공 음성-텍스트 LLM을 능가하며, EvalSIFT 벤치마크에 대한 인스트럭션 팔로우 성능에서 뛰어난 결과를 보였습니다. 또한, Dynamic-Superb 벤치마크의 오디오 부문에서 두 번째로 좋은 성과를 내며, 다양한 음성 이해 과제를 처리할 수 있음을 보여줍니다.

### [REPA-E: Unlocking VAE for End-to-End Tuning with Latent Diffusion Transformers](https://arxiv.org/abs/2504.10483)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10483.png)

Vote: 11

Authors: Yunzhong Hou, Xingjian Leng, Jaskirat Singh, Saining Xie, Zhenchang Xing, Liang Zheng

- ***What's New***: REPA-E는 초기부터 종단간(VAE; Variational Auto-Encoder) 및 잠재 확산 변환기(Latent Diffusion Transformer)의 공동 학습을 가능하게 하는 혁신적인 학습 방법을 제안합니다. 기존의 확산 손실은 VAE와의 종단간 학습에서 비효율적이었으나, REPA-E는 표현 정렬 손실(Representation-Alignment Loss)을 사용하여 두 모델의 공동 학습을 가능하게 합니다.
- ***Technical Details***: REPA-E는 VAE와 잠재 확산 모델을 동시에 조율하기 위해 표현 정렬 손실을 활용합니다. 이 과정에서 배치 정규화(Batch-Normalization)를 사용하여 각 종단간 학습 과정에서 잠재 통계량을 동적으로 조정합니다. REPA-E는 또한 확산 손실을 VAE에 직접 역전파하는 것을 방지하는 간단한 'Stop-Grad' 연산으로 잠재 공간 구조의 손상을 피합니다.
- ***Performance Highlights***: REPA-E는 REPA와 비교해 학습 속도를 17배, 기존 전통적 학습법 대비 45배 이상 가속화합니다. 최종 성능 측면에서는 Imagenet 256x256 기준으로 분류자 프리 가이던스를 적용하지 않은 상태에서 FID(Fréchet Inception Distance) 1.83을 기록하였으며, 가이던스를 사용할 경우 FID 1.26로 새로운 최고 성능을 달성했습니다.

### [SFT or RL? An Early Investigation into Training R1-Like Reasoning Large Vision-Language Models](https://arxiv.org/abs/2504.11468)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11468.png)

Vote: 8

Authors: Hardy Chen, Yuyin Zhou, Xianfeng Tang, Fali Wang, Hui Liu, Haoqin Tu, Xinya Du, Cihang Xie

- ***What's New***: 이 연구는 대형 비전-언어 모델(Large Vision-Language Models; LVLMs)의 훈련에 있어서, 기존의 감독 학습(Supervised Fine-Tuning; SFT)과 강화 학습(Reinforcement Learning; RL) 방법론을 재검토하며, SFT가 종종 LVLMs의 본래 추론 능력을 저해할 수 있음을 발견했습니다. VLAA-Thinking이라는 새로운 멀티모달 데이터셋을 도입하여 이 현상을 체계적으로 연구하였습니다.
- ***Technical Details***: VLAA-Thinking은 이미지-텍스트 연쇄추론(chain-of-thought; CoT) 예시를 포함하며, 시각적 지시 튜닝에 적합한 SFT 스플릿과 더 깊은 적응형 추론을 장려하기 위한 RL 스플릿으로 구성되어 있습니다. 데이터셋은 메타데이터 수집, 이미지 캡션 생성, R1 기반 증류, 답변 재작성 및 검증 등의 여섯 단계 파이프라인을 통해 구성되었습니다. 대상별로 혼합 보상 모듈이 적용된 Group Relative Policy Optimization (GRPO)를 기반으로 한 RL 접근 방식을 사용하여 보다 진정한 추론 행동을 촉진합니다.
- ***Performance Highlights***: 우리의 모델인 VLAA-Thinker Qwen2.5VL-3B는 Open LMM Reasoning Leaderboard의 4B 규모 LVLMs 중에서 평균 1.8%의 성능 향상을 이루며, 이전의 최신 연구를 능가하였습니다. 이 연구는 SFT가 모델들을 모방적이고 경직된 추론 방식에 고정시키는 반면, RL은 보다 적응력 있는 추론을 촉진함을 보였습니다.

### [Towards Learning to Complete Anything in Lidar](https://arxiv.org/abs/2504.12264)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.12264.png)

Vote: 5

Authors: Ayca Takmaz, Neehar Peri, Tim Meinhardt, Riccardo de Lutio, Laura Leal-Taixé, Cristiano Saltori, Aljoša Ošep

- ***What's New***: 이 논문에서는 라이다 데이터에서 시맨틱 장면 완성을 수행하는 새로운 방법인 CAL(Complete Anything in Lidar)을 제안합니다. CAL은 기존 라이다 데이터세트에서 레이블이 지정된 폐쇄된 어휘와 달리, 관찰된 객체의 형상 및 시맨틱 특징을 다중 센서 시퀀스의 시간적 맥락에서 학습하여 제로 샷 인스턴스 수준 완료 및 인식 모델로 증류합니다.
- ***Technical Details***: CAL은 시맨틱/파놉틱 장면 완료(Semantic/Panoptic Scene Completion; SSC/PSC)와 관련이 있습니다. 이를 위해, CAL은 시각적 피처 및 시점으로부터 얻은 CLIP 피처를 결합하여, 라이다 스캔만을 사용하여 제로 샷(instance-level completion 및 recognition) 완료를 수행합니다. 마스크 및 시맨틱 정보를 스파스(sparse) 생성 기능 인코더-디코더 네트워크를 통해 훈련합니다.
- ***Performance Highlights***: CAL은 기존 라이다 데이터세트에서의 제한된 클래스 어휘를 초과하는 다양한 객체를 인식하고 완료할 수 있습니다. 실험 결과, CAL은 제로 샷 설정에서 우수한 성과를 보이며, 기존의 감독 학습 기반 방법에 비해 상당한 경쟁력을 가지고 있음을 보여줍니다.

### [Vivid4D: Improving 4D Reconstruction from Monocular Video by Video Inpainting](https://arxiv.org/abs/2504.11092)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11092.png)

Vote: 5

Authors: Jiaxin Huang, Yiyi Liao, Sheng Miao, Yuewen Ma, BangBnag Yang

- ***What's New***: Vivid4D는 단안 비디오(monocular video)로부터의 4D 재구성을 개선하기 위해 비디오 인페인팅(video inpainting)을 활용하는 새로운 접근 방법을 제안합니다. 기존 기하학적 우선순위와 생성적 우선순위를 결합하여 단안 입력으로부터 다중 뷰 비디오를 합성하는 방법을 도입하였습니다.
- ***Technical Details***: Vivid4D는 비디오 인페인팅 모델을 학습하기 위해 기하학적 우선순위를 사용하여 관측된 뷰를 새로운 시점으로 왜곡하고, 2D 트래킹을 통해 생성된 가림 마스크를 통해 웝핑 폐색을 모방하여 공간적, 시간적 일관성을 갖춘 비디오 인페인팅 모델을 훈련합니다. 이렇게 훈련된 모델은 알 수 없는 뷰 포인트에 대한 신뢰할 수 있는 RGB 이미지를 생성하여 재구성 품질을 향상시킵니다.
- ***Performance Highlights***: Vivid4D는 다양한 동적 장면에 대해 기존의 방법보다 뛰어난 성능을 보이며, 4D 재구성과 새로운 뷰 생성의 품질을 향상시킵니다. 실험 결과는 가려져 보이지 않는 영역을 효과적으로 채우고, 구조적 일관성과 시간적 일관성을 유지하면서 더욱 세밀한 세부사항을 잘 재현함을 보여줍니다.

### [Robust and Fine-Grained Detection of AI Generated Texts](https://arxiv.org/abs/2504.11952)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11952.png)

Vote: 4

Authors: Modabbir Adeeb, Kanwal Mehreen, Ashay Srivastava, Subhasya TippaReddy, Srinadh Vura, Jebish Purbey, Suraj Telugara Chandrashekhar, Ram Mohan Rao Kadiyala, Siddartha Pullakhandam, Hamza Farooq, Drishti Sharma, Siddhant Gupta, Arvind Reddy Bobbili

- ***What's New***: 이 논문은 AI가 생성한 텍스트를 정밀하게 감지하는 새로운 모델을 소개합니다. 특히, 인간과 LLMs(Large Language Models)가 공동으로 작성한 텍스트에 초점을 맞추어 짧은 텍스트에서도 성능을 발휘할 수 있도록 설계되었습니다. 이를 통해 다양한 언어와 영역에서 AI 생성 콘텐츠에 대한 감지를 가능하게 합니다.
- ***Technical Details***: 새로 도입된 모델은 토큰 분류(token classification) 접근법을 사용하여 텍스트의 작성 스타일을 구별합니다. 이 모델은 약 2.4M 개의 인간-기계 공동 작성 텍스트 데이터셋에 기반하여 학습됩니다. 또한, CRF(Conditional Random Field) 레이어를 추가한 멀티링구얼 트랜스포머(multilingual transformer) 모델을 사용하며, xlm-longformer가 긴 문맥을 처리하기에 적합하여 최종적으로 채택되었습니다.
- ***Performance Highlights***: 훈련 데이터와 다른 영역과 생성기를 대상으로 한 벤치마크 테스트에서 F1 스코어가 0.79에 달하는 상당한 성능을 보였습니다. 모델은 입력 텍스트의 3분의 1 이상이 기계 생성으로 분류될 경우 텍스트를 '기계 생성'으로 간주합니다. 비록 짧은 입력에서 기존의 상용 시스템들이 고전을 면치 못했으나, 이 모델은 뛰어난 성능을 유지합니다.

### [Syzygy of Thoughts: Improving LLM CoT with the Minimal Free Resolution](https://arxiv.org/abs/2504.09566)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.09566.png)

Vote: 2

Authors: Yang Yang, Yi Lu, Xudong Wang, Chaoning Zhang, Chenghao Li, Guoqing Wang, Qigan Sun, Heng Tao Shen, Jiaquan Zhang, Jiwei Wei

- ***What's New***: 이 연구에서는 대규모 언어 모델(LLM)의 체인 오브 쏘트(Chain-of-Thought, CoT)의 한계를 극복하기 위해 새로운 프레임워크인 Syzygy of Thoughts(SoT)를 제안합니다. Minimal Free Resolution(MFR)에서 영감을 받아 추가적이고 상호 연관된 추론 경로를 도입하여 논리적 의존성을 깊게 파악하고 구조를 더 체계화하여 문제 해결에 있어 더욱 강력하게 개선합니다.
- ***Technical Details***: SoT는 모듈(Module), Betti numbers, Freeness, Mapping, Exactness, Minimality 등의 대수학적 개념을 활용하여 주어진 복잡한 문제를 좀 더 구조화되고 키 문제 특징을 잃지 않으며 논리적으로 완벽한 최소 부분 문제로 체계적으로 분해합니다. 각 문제는 추론 경로 내에서 생성적 구조와 잠재적 의존성을 지닌 대수적 객체로 재해석됩니다.
- ***Performance Highlights***: SoT는 GSM8K, MATH와 같은 복잡한 데이터셋에서 GPT-4o-mini와 같은 모델을 통해 테스트되었고, 일반적인 CoT 방법을 초과하는 추론 정확도를 보여주었습니다. 특히 MATH 데이터셋에서 79.1%의 정확도로 최고의 7B 모델을 능가하는 성능을 보였으며, 이는 고차원 수학적 추론 과제에서 SoT의 우수성을 나타냅니다.

### [BlockGaussian: Efficient Large-Scale Scene Novel View Synthesis via Adaptive Block-Based Gaussian Splatting](https://arxiv.org/abs/2504.09048)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.09048.png)

Vote: 1

Authors: Zhengxia Zou, Yongchang Wu, Zhenwei Shi, Zipeng Qi

- ***What's New***: BlockGaussian는 대규모 장면의 새로운 뷰 합성을 위한 새로운 프레임워크로, 장면 콘텐츠의 복잡성과 블록 간 재구성 계산 부하의 분포를 동시에 고려하여 전략적으로 장면을 분할합니다.
- ***Technical Details***: Content-Aware Scene Partition는 장면의 복잡성에 기반하여 블록을 동적으로 분할하며, Visibility-Aware Block Optimization은 개별 블록의 재구성 시 보이지 않는 영역의 문제를 효과적으로 해결합니다. 또한, Pseudo-view Geometry Constraint는 대기의 '플로터' 생성을 억제하여 상호작용 렌더링을 지원합니다.
- ***Performance Highlights***: BlockGaussian은 여러 대규모 장면 데이터셋에서 최첨단 성능을 달성하였습니다. 특히, 재구성 품질 측면에서 PSNR이 평균 1.21 dB 개선되었습니다. 또한, 단일 24GB VRAM의 장치에서 대규모 장면의 재구성을 수행할 수 있는 최적화 속도 향상을 나타내며, 대기 포인트 수를 대폭 감소시켜 고품질의 엔드뷰 합성을 가능하게 합니다.

### ["It's not a representation of me": Examining Accent Bias and Digital Exclusion in Synthetic AI Voice Services](https://arxiv.org/abs/2504.09346)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.09346.png)

Vote: 1

Authors: Christo Wilson, Jeffrey Gleason, Sufi Kaur, Sarah Elizabeth Gillespie, Shira Michel, Avijit Ghosh

- ***What's New***: 이 연구는 합성 AI 음성 서비스(Synthetic AI Voice Services)에서 발견되는 악센트 편향(Accent Bias)과 디지털 배제(Digital Exclusion)를 조사하고 있습니다. 특히, 이 연구는 Speechify와 ElevenLabs 두 AI 음성 서비스를 평가하여 다양한 악센트와 음조에서의 기술적 성능 격차를 밝혀내고, 이러한 음성 기술이 언어적 특권 및 악센트 기반 차별을 강화할 가능성을 제시합니다.
- ***Technical Details***: 이 연구는 혼합 방법론을 사용하여 두 음성 생성 서비스(Speechify와 ElevenLabs)의 합성 음성 품질을 다양한 악센트 및 음조 조합에서 평가하고, 사용자 선호도를 조사하기 위해 구조화된 설문조사와 심층적인 인터뷰를 실시하였습니다. 250명의 참여자가 각자의 악센트와 일치하는 AI 생성 음성을 평가했으며, 모사의 정확도와 자연스러움을 측정했습니다.
- ***Performance Highlights***: 개량된 의미의견점수(MOS; Mean Opinion Score)를 사용하여 평가한 결과, Speechify는 상대적으로 모든 악센트에서 일관된 점수를 보였으나, ElevenLabs의 음성이 더 높은 악센트 정확도를 나타냈습니다. 그러나 AI 음성 복제(Voice Clones)에서의 자연스러움 평가에서는 두 서비스 모두 뚜렷한 선호도를 보이지 않았습니다. 다만, 참가자들은 ElevenLabs의 음성이 더 인간적인 특성을 포함하여 보다 현실감 있게 들린다고 평가했습니다.

