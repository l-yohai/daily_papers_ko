## Daily Papers (2025-04-28)

### [Towards Understanding Camera Motions in Any Video](https://arxiv.org/abs/2504.15376)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15376.png)

Vote: 120

Authors: Mingyu Chen, Zhiqiu Lin, Xue Bai, Chuang Gan, Daniel Jiang, Rushikesh Zawar, Siyuan Cen, Jay Karhade, Tiffany Ling, Hewei Wang, Deva Ramanan, Sifan Liu, Chancharik Mitra, Yuhan Huang, Yilun Du

- ***What's New***: CameraBench는 카메라 모션을 이해하고 개선하기 위한 대규모 데이터셋과 벤치마크로 소개되었습니다. 이 데이터셋은 다양한 인터넷 비디오 약 3,000개로 구성되어 있으며, 전문가들이 엄격한 품질 관리 프로세스를 통해 주석을 달았습니다. 카메라 모션 원시(Primitives)에 대한 분류 체계(Taxonomy)를 도입하여 장면 내용에 의존하는 동작과 정밀한 경로 추정이 필요한 기하학적 원시를 모두 포괄합니다.
- ***Technical Details***: 카메라 모션 원시의 분류 체계는 전문 촬영 감독과의 협업을 통해 고안되었습니다. 이 분류 체계는 물체 중심, 지면 중심, 카메라 중심의 참조 프레임을 사용하여 카메라 안정성, 번역, 회전, 내재적 변화 등을 포함합니다. 이러한 분류 체계는 장면 다이나믹스를 평가하는 데 사용되며, SfM(Structure-from-Motion)과 VLM(Video-Language Models)의 성능을 각각 기하학적 및 의미적 원시에서 평가하는 데 사용되었습니다.
- ***Performance Highlights***: VLM은 카메라 모션을 이해함에 있어서 뚜렷한 잠재력을 보이며, 특히 구조 및 의미적 추론이 필요한 작업에서 고성능을 보여줍니다. 실제 실험에서는 구조-모션 기반 모델들이 높은 성능을 보였으나 동적 장면에서는 어려움을 겪는 모습을 관찰할 수 있었습니다. 반면, VLM은 의미적 추론에 뛰어난 성능을 보였으며, 카메라Bench를 사용한 후학습 후에는 보다 향상된 성능을 보여주었습니다.

### [Skywork R1V2: Multimodal Hybrid Reinforcement Learning for Reasoning](https://arxiv.org/abs/2504.16656)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.16656.png)

Vote: 43

Authors: Chris, Weijie Qiu, Yichen Wei, Xiaokun Wang, Jianhao Zhang, Yang Liu, Yunzhuo Hao, Yahui Zhou, Yi Peng, Xuchen Song, Wei Shen, Jiangbo Pei, Tianyidan Xie

- ***What's New***: Skywork R1V2는 다음 세대 멀티모달 추론 모델로, 이전 모델인 Skywork R1V에서 크게 발전한 모델입니다. 이 모델은 혼합 선호 최적화(Mixed Preference Optimization; MPO)와 그룹 상대 정책 최적화(Group Relative Policy Optimization; GRPO)를 결합하여 보상 모델(Reward Model)의 가이드 및 규칙 기반 전략과 조화를 이루어 고급 추론 능력과 일반화 간의 균형을 해결합니다. 또한 선택적 샘플 버퍼(Selective Sample Buffer; SSB) 메커니즘을 도입하여 GRPO의 '미소실 이점(Vanishing Advantages)' 문제를 해결합니다. 이러한 혁신적인 접근 방식은 시각적 환각 현상을 낮추고 훈련 효율성을 극대화합니다.
- ***Technical Details***: Skywork R1V2는 대형 멀티모달 모델(Vision-Language Models; VLMs)의 추론 능력을 직접 강화 학습(Reinforcement Learning; RL)을 통해 습득하도록 설계되었습니다. 이 모델은 VisVIT-6B를 비전 인코더로, QwQ-32B를 언어 모델로 사용하여 시각적 입력을 효과적으로 처리하는 경량의 다층 퍼셉트론(MLP) 어댑터를 도입합니다. MPO는 상대적 선호도, 응답의 절대적 품질, 선호 응답 생성의 3가지 학습 목표를 따르며 Skywork-VL 보상 모델이 제공하는 높은 품질의 신호를 활용해 시각적 환각을 줄입니다. GRPO 알고리즘은 응답 그룹 내 후보 응답 간의 상대적 장점을 비교하여 계산합니다. GRPO의 '미소실 이점' 문제를 해결하기 위해 SSB 메커니즘을 사용하여 정책 업데이트 중 가치가 높은 샘플을 우선시합니다.
- ***Performance Highlights***: Skywork R1V2는 OlympiadBench에서 62.6%, AIME2024에서 78.9%, LiveCodeBench에서 63.6%, MMMU에서 73.6%의 점수를 기록하며, 기존의 오픈 소스 모델들을 능가하는 성능을 보였습니다. 이 모델은 더 큰 독점 모델들과의 성능 격차를 크게 줄이며, 경쟁력 있는 이점을 제공합니다. 특히, OlympiadBench에서의 성능은 Qwen2.5-VL-72B보다 상당히 높아 복잡한 수학적 추론 능력을 입증합니다.

### [BitNet v2: Native 4-bit Activations with Hadamard Transformation for 1-bit LLMs](https://arxiv.org/abs/2504.18415)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.18415.png)

Vote: 24

Authors: Furu Wei, Shuming Ma, Hongyu Wang

- ***What's New***: BitNet v2는 1-bit 대형 언어 모델(LLMs)을 위한 원본 4-bit 활성화(activation) 양자화를 가능하게 하는 새로운 프레임워크입니다. 이 혁신을 통해 하드웨어의 4-bit 연산 능력을 최대한 활용할 수 있습니다. 특히, BitNet v2는 활성화(outliers) 문제를 해결하기 위해 온라인 하다마드 변환(Hadamard Transformation)을 도입하여 활성화의 분포를 보다 가우스적인 형태로 만드는 것이 특징입니다.
- ***Technical Details***: BitNet v2의 핵심은 H-BitLinear 모듈로, 주의(attention)와 피드포워드 네트워크(FFN)의 출력 및 다운 프로젝션을 대체합니다. 이 모듈은 양자화 전 활성화에 하다마드 변환을 적용하며, 이는 분포를 매끄럽게 하고 활성화 중 극단 값을 줄입니다. BitNet v2는 1.58-bit 가중치(weights)로 학습되고, 구체적으로는 원본 모델 훈련 후 원본 4-bit 활성화를 위한 미세 조정을 통해 모델 효율성을 높입니다.
- ***Performance Highlights***: BitNet v2는 BitNet b1.58와 비교해 초점(square free) 성능에서 최소한의 손실을 보여주는 동시에 모든 모델이 원본 4-bit 활성화를 달성하여 일괄 추론 시 효율성을 극대화합니다. 1.3B, 3B, 7B 사이즈 모델에서 BitNet v2(a4)가 BitNet a4.8에 비해 다운스트림(task) 성능에서 우수한 성능을 보였습니다. 표준 후기 훈련(Post-Training) 양자화 방법인 SpinQuant와 QuaRot를 능가하는 정확도를 보였으며, 이는 특히 C4 데이터셋의 유효 세트에서 권장사항으로 부각됩니다.

### [VideoVista-CulturalLingo: 360^circ Horizons-Bridging Cultures, Languages, and Domains in Video Comprehension](https://arxiv.org/abs/2504.17821)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.17821.png)

Vote: 19

Authors: Wenhan Luo, Baotian Hu, Yaowei Wang, Min Zhang, Haoyuan Shi, Xinyu Chen, Yunxin Li

- ***What's New***: VideoVista-CulturalLingo는 영상 이해에서 문화, 언어, 도메인의 차이를 매끄럽게 연결하는 최초의 비디오 평가 벤치마크입니다. 이 연구는 1) 중국, 북미, 유럽 문화의 다양성을 통합하고, 2) 중국어와 영어라는 다국어 정보를 제공하며, 3) 수백 가지 인간 창작 도메인 출처 비디오를 포함하여 광범위한 도메인을 다루고 있습니다.
- ***Technical Details***: VideoVista-CulturalLingo는 1,389개의 비디오와 3,134개의 QA 쌍으로 구성되어 있으며, 총 24개의 최신 오픈소스 및 독점 비디오 대 모델을 평가하였습니다. 이 벤치마크는 Xiaohongshu(RedNote), BiliBili, YouTube 등 다양한 플랫폼에서 수집된 다양한 문화와 배경을 가진 비디오를 포함하고 있습니다. 데이터 주석은 Qwen2-VL, DeepSeek-R1과 같은 대규모 모델과 인간 주석자의 혼합 주석 체계를 사용하여 처리되었습니다.
- ***Performance Highlights***: Gemini-2.0-Flash는 76.3%의 최고 정확도 점수를 기록하며 모든 모델 중에서 가장 뛰어난 성능을 보였습니다. 오픈소스 비디오 모델 중에서는 Qwen2.5-VL-72B가 61.3%로 가장 높은 점수를 기록했으며, 중국 문화 이해에서는 여전히 65.8%로 성능 차이가 남아 있습니다. 기존의 오픈소스 모델들은 시간적 이해에서 여전히 제한점을 보이고 있으며, 특히 이벤트 위치확인 작업에서 최대 45.2%에 불과했습니다.

### [Can Large Language Models Help Multimodal Language Analysis? MMLA: A Comprehensive Benchmark](https://arxiv.org/abs/2504.16427)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.16427.png)

Vote: 11

Authors: Hanlei Zhang, Yeshuang Zhu, Hua Xu, Peiwu Wang, Zhuohang Li, Jie Zhou, Haige Zhu, Jinchao Zhang

- ***What's New***: MMLA는 대형 멀티모달 언어 모델(Multimodal Large Language Models; MLLMs)의 인지 수준(seantics)을 평가하기 위해 설계된 최초의 종합 벤치마크입니다. 이 벤치마크는 의도(intent), 감정(emotion), 대화 행동(dialogue act), 감정 태도(sentiment), 화법(speaking style), 그리고 의사소통 행동(communication behavior)의 6가지 핵심 멀티모달 의미를 다루고 있습니다.
- ***Technical Details***: MMLA는 다양한 출처의 61,000여 개의 멀티모달 발화 데이터를 포함하고 있으며, 주어진 영상, 텍스트, 오디오 정보를 활용하여 고난이도 대화 맥락에서 모델을 평가합니다. 평가 방법으로는 제로샷 추론(zero-shot inference), 지도 학습의 파인튜닝(supervised fine-tuning), 그리고 명령어 조정(instruction tuning)이 포함됩니다.
- ***Performance Highlights***: MLLMs의 성능은 상당한 도전에 직면해 있으며, 지도 학습을 통해 대비적인 성능 향상을 보였지만 최고 모델의 평균 정확도는 69%에 그쳤습니다. 소형 모델과 대형 모델 간 성능 차이는 미미하며, 소형 MLLMs도 효과적인 성능을 보여줍니다.

### [Subject-driven Video Generation via Disentangled Identity and Motion](https://arxiv.org/abs/2504.17816)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.17816.png)

Vote: 8

Authors: Qi Dai, Jaesik Park, Wonjoon Jin, Jingxu Zhang, Chong Luo, Daneul Kim, Sunghyun Cho

- ***What's New***: 이 논문에서는 주제 기반의 영상 생성(subject-driven video generation) 모델을 제안하여, 기존의 비용이 많이 드는 대규모 주석 비디오 데이터셋에 의존하는 방식 대신 이미지 커스터마이제이션 데이터셋(S2I dataset)을 사용하여 주제 고유의 학습과 시간적 역학을 독립시켜 훈련합니다.
- ***Technical Details***: 제안된 방법은 주제 기반 이미지 커스터마이제이션 데이터셋을 사용하여 주체 주입(identity injection)을 수행하고 미주석 비디오 데이터셋을 이용한 이미지-비디오(I2V) 훈련을 통해 시간 인식 보존(temporal awareness preservation)을 달성합니다. 이 과정에서 무작위 프레임 선택과 이미지 토큰 드롭아웃(dropout)을 통해 첫 프레임에 과도하게 의존하는 문제를 해결합니다. 또한 스토캐스틱 전환(stochastic switching) 기법을 사용하여 주제 특징과 시간 특징의 학습을 균형 있게 최적화합니다.
- ***Performance Highlights***: 이 방법은 든 든 포맷으로 평가를 받을 시 강한 주제 일관성과 확장성을 보여주며, 기존의 제로샷 영상 커스터마이제이션 모델보다 뛰어난 성과를 보입니다. 주제를 적절히 보존하면서도 자연스러운 모션을 생성하고, 인위적 조정 없이 정확한 주제와 동작을 결합한 고품질의 영상을 생성할 수 있음을 입증합니다.

### [The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs](https://arxiv.org/abs/2504.17768)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.17768.png)

Vote: 8

Authors: Edoardo M. Ponti, Renjie Huang, Kelly Marchisio, Robert Li, Sebastian Ruder, Piotr Nawrot

- ***What's New***: 본 논문은 Transformer 기반 대형 언어 모델(LLMs)의 긴 문맥 처리 능력을 확장하기 위한 전략으로 Sparse Attention을 심층 분석하고, 효율성과 정확성 간의 트레이드오프를 종합적으로 평가한 연구입니다. Sparse Attention 기법을 효율적이고 정확하게 확장시키는 법칙을 제시하고, 이는 다양한 모델 크기와 시퀀스 길이에 적용될 수 있음을 보였습니다.
- ***Technical Details***: Sparse Attention 기법은 쿼리와 키 간의 일부 상호 작용만 계산하는 방법으로, 개별 처리 없이 다양한 설계 원칙을 수립한 후, 블록 단위와 수직 단위 및 슬래시 슬래시 패턴의 중요도 평가 방법을 포함하여 다양한 패턴을 엄선하여 구현하였습니다. 실험 대상 모델은 7B부터 72B 파라미터를 포함한 Qwen 2.5 모델군이며, 벤치마크된 9개의 긴 문맥 작업을 통해 이를 평가하였습니다.
- ***Performance Highlights***: 이 연구의 결과, 구체적인 작업과 단계에 따라 다소 차이가 있지만, 긴 문맥 시퀀스에서 큰 모델은 Sparse Attention을 적용할 경우 높은 효율성을 보였습니다. 특히 32K 이상의 시퀀스 길이에서 고도의 Strip한 큰 모델이 더 작은 밀집 모델을 초과하는 성능을 보였습니다. 벤치마크 실험에서는 다양한 스파시티 제약 한계에도 불구하고, 평균적으로 모델 성능을 유지하거나 향상시킬 수 있는 풍성한 스파시티 수준이 관찰되었습니다. 이 연구는 Sparse Attention의 활용성을 입증하며, 향후 연구의 기초를 제공합니다.

### [Kimi-Audio Technical Report](https://arxiv.org/abs/2504.18425)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.18425.png)

Vote: 7

Authors: Yanru Chen, Weidong Sun, Yangyang Liu, Yutong Zhang, Yifei Xin, Yulun Du, Yuzhi Wang, Ding Ding, KimiTeam, Qingcheng Li, Chu Wei, Yichong Leng, Weiran He, Yutao Zhang, Ying Yang, Songxiang Liu, Zhilin Yang, Xinyu Zhou, Y. Charles, Ruibin Yuan, Zhengtao Wang, Guokun Lai, Wei Song, Jianwei Yu, Zeyu Shang, Jun Chen, Zaida Zhou, Heyi Tang, Zeqian Ju, Yuxin Wu, Kai Shen, Yuefeng Wu, Xu Tan, Jianzhou Wang, Xinran Xu, Tong Liu, Zhenxing Hu, Hao Yang, Dongchao Yang, Aoxiong Yin

- ***What's New***: Kimi-Audio는 오디오 이해, 생성, 대화를 통해 뛰어난 오픈소스 오디오 기반 모델입니다. 이 모델은 독특한 LLM 기반 아키텍처를 도입하여 연속적인 특징을 입력으로 받아 이산적인 토큰을 출력하며 오디오 토크나이저를 포함하여 높은 품질의 출력과 다양한 오디오 작업을 지원하는 최초의 모델 중 하나입니다.
- ***Technical Details***: Kimi-Audio는 12.5Hz 오디오 토크나이저(Tokenizer), 끊김 없는 흐름 매칭(flow matching)에 기반한 덱토크나이저(Detokenizer)와 LLM을 사용하여 다양한 오디오 처리 작업을 통합된 아키텍처 내에서 수행하는 모델입니다. 1,300만 시간 이상의 오디오 데이터를 포함한 대량의 data를 사전 학습이 완료된 LLM에서 초기화하여 성능을 사전에 학습한 후 다양한 오디오 관련 작업을 지원하기 위해 미세 조정하였습니다.
- ***Performance Highlights***: Kimi-Audio는 음성 인식(ASR), 오디오 이해, 오디오 질문 응답 및 음성 대화 등의 다양한 오디오 벤치마크에서 최첨단(SOTA) 성능을 달성했습니다. 테스트 결과에서 Kimi-Audio는 여러 기존 모델보다 우수한 성과를 내서 오디오 이해와 생성에서 강력한 성능을 보여주고 있습니다.

### [DianJin-R1: Evaluating and Enhancing Financial Reasoning in Large Language Models](https://arxiv.org/abs/2504.15716)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15716.png)

Vote: 7

Authors: Huaixia Dou, Jie Zhu, Lifan Guo, Chi Zhang, Qian Chen, Junhui Li, Feng Chen

- ***What's New***: DianJin-R1은 대형 언어 모델(Large Language Models; LLMs)의 금융 추론 능력을 향상시키기 위한 프레임워크입니다. 이는 추론 증대 감독(reasoning-augmented supervision)과 강화 학습(reinforcement learning)을 통해 도메인 특화 지식, 정밀한 수치 계산 및 엄격한 규제 준수의 복잡성을 다루기 위한 것입니다.
- ***Technical Details***: DianJin-R1은 CFLUE, FinQA, 그리고 독점 준수 코퍼스(Chinese Compliance Check, CCC)로 구성된 데이터셋을 기반으로 만들어졌습니다. 모델은 Qwen2.5-7B-Instruct와 Qwen2.5-32B-Instruct에서 파인 튜닝하였으며, Group Relative Policy Optimization (GRPO)라는 강화 학습 알고리즘을 적용하여 형식 보상(format reward)과 정확성 보상(accuracy reward)을 결합해 추론의 질을 더욱 향상시켰습니다.
- ***Performance Highlights***: 실험 결과, DianJin-R1 모델은 비추론 모델에 비해 금융 과제에서 뛰어난 성능을 보였습니다. 특히, 단일 호출(single-call)의 추론 모델은 다중 에이전트 시스템(multi-agent systems)을 능가하였으며, 금전적 비용이 훨씬 적게 들었습니다. CFLUE에서의 정확도는 77.95에서 86.74로, CCC에서는 56.50에서 96.00으로 향상되었습니다.

### [DC-SAM: In-Context Segment Anything in Images and Videos via Dual Consistency](https://arxiv.org/abs/2504.12080)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.12080.png)

Vote: 6

Authors: Xiangtai Li, Xiaoyang Bi, Pengfei Zhu, Mengshi Qi, Huadong Ma, Lu Qi, Ming-Hsuan Yang

- ***What's New***: DC-SAM은 이미지와 비디오의 인컨텍스트 세그멘테이션(In-context Segmentation)을 위한 Dual Consistency 방법론을 도입하며, Segment Anything Model(SAM) 및 SAM2의 프롬프트 튜닝(prompt-tuning)을 통해 일반화 능력을 향상시킨 새로운 접근 방식을 제안합니다. 특히, 비디오 도메인을 위한 최초의 인컨텍스트 세그멘테이션 벤치마크인 IC-VOS를 구축하였습니다.
- ***Technical Details***: DC-SAM은 두 가지 일관성 있는 프롬프트 생성 방식을 도입하여, 긍정 및 부정 프롬프트를 생성하는 이중 브랜치 디자인을 활용합니다. SAM의 프롬프트 인코더의 기능을 강화하며, 사이클 일관적인 크로스 어텐션을 새롭게 설계하여 특징과 시각적 프롬프트 간의 일관성을 보장합니다. 또한, 화상 튜브(supervision tube) 학습 방식을 적용하여 프롬프트 일관성을 적용합니다.
- ***Performance Highlights***: COCO-20i에서 55.5 (+1.4) mIoU, PASCAL-5i에서 73.0 (+1.1) mIoU를 달성하였으며, 새롭게 제안한 IC-VOS 벤치마크에서 J &F 점수 71.52를 기록했습니다. 이는 DC-SAM의 이미지와 비디오 도메인 전반에서 뛰어난 일반화 성능을 입증합니다.

### [Optimizing LLMs for Italian: Reducing Token Fertility and Enhancing Efficiency Through Vocabulary Adaptation](https://arxiv.org/abs/2504.17025)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.17025.png)

Vote: 3

Authors: Andrea Esuli, Giovanni Puccetti, Felice Dell'Orletta, Roberto Navigli, Edoardo Barba, Alessio Miaschi, Pere-Lluis Huguet Cabot, Luca Moroni, Andrei Stefan Bejgu

- ***What's New***: 이 논문은 이탈리아어에 최적화된 대형 언어 모델(Large Language Models; LLMs)의 토큰 수를 줄이고 효율성을 높이기 위한 어휘 적응 기술을 소개합니다. 특히 Semantic Alignment Vocabulary Adaptation (SAVA)라는 새로운 방법을 제안하여 여러 다운스트림 작업에서 경쟁력 있는 성능을 보여줍니다.
- ***Technical Details***: 이 연구는 영어 중심의 LLMs를 이탈리아어에 맞추기 위해 여러 어휘 적응(Adaptation) 기술을 비교 분석하였고, Mistral-7B-v0.1과 Llama-3.1-8B 모델을 토큰 수를 각각 25%와 16% 감소시켰습니다. SAVA는 헬퍼 모델의 임베딩 공간을 활용해 소스와 타겟 사이의 어휘를 효과적으로 대체하는 방식으로, 이는 인베딩 재구성을 통해 모델 성능을 개선합니다.
- ***Performance Highlights***: Mistral-7B-v0.1과 Llama-3.1-8B 모델을 활용한 실험에서, SAVA를 통한 적응 후 이탈리아어 텍스트에서의 토큰 수 감소 및 모델 성능 회복이 안정적으로 나타났습니다. 특히 Mistral-7B-v0.1 모델은 2억 개의 토큰을 처리한 후 기본 성능을 회복하였습니다. 또한, SAVA는 학습 초기에 Training Loss가 빠르게 감소하고, 헬퍼 모델과의 임베딩 구조 정렬이 더욱 개선되었습니다.

### [Even Small Reasoners Should Quote Their Sources: Introducing the Pleias-RAG Model Family](https://arxiv.org/abs/2504.18225)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.18225.png)

Vote: 2

Authors: Pavel Chizhov, Ivan P. Yamshchikov, Carlos Rosas Hinostroza, Irène Girard, Pierre-Carl Langlais, Matthieu Delsart, Othman Hicheur, Mattia Nee, Anastasia Stasenko

- ***What's New***: Pleias-RAG 모델 패밀리는 소규모 추론 모델의 새로운 세대를 소개합니다. Pleias-RAG-350m 및 Pleias-RAG-1B 모델은 RAG(Retrieval-Augmented Generation), 검색, 출처 요약을 위해 설계되었으며, Common Corpus에서 수집한 대규모 합성 데이터를 바탕으로 멀티링구얼 오픈 소스를 검색하는 방식으로 중간 학습을 거쳤습니다. 이 모델들은 인용과 근거 제시에 있어 새로운 기능과 함께 RAG 워크플로우와 연계된 기능을 선보입니다.
- ***Technical Details***: Pleias-RAG 모델들은 Pleias 1.0 시리즈의 베이스 모델을 기반으로 한 중간 학습 변형입니다. 완전히 감사 가능한 교육 데이터를 바탕으로 다국어 지원 강화 및 PDF와 같은 RAG 생성에 자주 사용되는 소스 포맷에 대한 더 깊은 이해를 제공합니다. 인용 생성을 모델 추론 과정에서 직접 시행함으로써 기존 모델의 실증성과 사실성을 강화하는 방식을 구축하였습니다.
- ***Performance Highlights***: Pleias-RAG 모델들은 4-8B 파라미터의 LLM에 비해 유사한 정확도를 보이며, 특히 유럽 주요 언어에서 일관된 성능을 유지하고 있습니다. 350M 모델이 같은 크기의 다른 모델보다 우수한 성능만이 아니라 보완적인 기능으로 존재한다는 것이 확인되었습니다. 또한, 다국어 지원을 적용한 HotpotQA 변환 버전에서 성능 손실이 거의 없는 유일한 모델이라는 평가를 받았습니다.

