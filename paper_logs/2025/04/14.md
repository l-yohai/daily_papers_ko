## Daily Papers (2025-04-14)

### [Seaweed-7B: Cost-Effective Training of Video Generation Foundation Model](https://arxiv.org/abs/2504.08685)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.08685.png)

Vote: 78

Authors: Xuefeng Xiao, Meng Wei, Feilong Zuo Xuejiao Zeng, Huixia Li, Shu Liu, Yuxuan Wang, Manlin Zhang, Liangke Gui, Team Seawead, Lu Qi, Rui Wang, Feng Ling, Jiashi Li, Xuyan Chi, Zhibei Ma, Houmin Wei, Ceyuan Yang, Peihao Zhu, Siyu Zhang, Yanghua Peng, Heng Zhang, Jerry Duncan, Xin Xia, Hao Chen, Fei Xiao, Xiaojie Li, Yuping Wang, Ziyan Yang, Jianchao Yang, Zhiwu Qing, Feng Cheng, Yang Zhao, Sheng Bi, Huafeng Kuang, Renfei Sun, Sen Wang, Zhongkai Zhao, Lu Jiang, Haoyuan Guo, Yuxi Ren, Tuyen Hoang, Jiangqiao Yan, Jiashi Feng, Zhijie Lin, Fangyuan Kong, Junru Zheng, Xiaobin Zhuang, Zhuo Chen, Li Sun, Zuquan Song, Shanchuan Lin, Zhenheng Yang, Junda Zhang, Qi Zhao

- ***What's New***: Seaweed-7B는 7억 개의 파라미터를 가진 중간 크기의 비디오 생성 모델로, 상대적으로 적은 연산 자원으로도 대형 모델과 경쟁할 수 있는 성능을 달성하는 경제적인 훈련 전략을 제시합니다.
- ***Technical Details***: Seaweed-7B는 665,000 H100 GPU 시간을 사용하여 완전히 새로 훈련된 7B 파라미터의 DiT(Diffusion Transformer)을 기반으로 합니다. 설계 선택이 매우 중요하며, 데이터 큐레이션과 모델 설계 그리고 훈련 전략에 초점을 맞추고 있습니다. VAE(Variational Autoencoder)의 디자인과 Diffusion Transformer의 하이브리드 스트림 구조를 채택하여 훈련 안정성을 높이고 있습니다.
- ***Performance Highlights***: Seaweed-7B는 텍스트-투-비디오 및 이미지-투-비디오 생성 작업에서 동급 모델과 비교하여 경쟁력을 보였으며, 몇몇 큰 모델을 능가합니다. 특히 Kling 1.6과의 대조에서 전반적으로 우수한 성능을 보여주었으며, 높은 해상도 이미지 재구성에서도 뛰어난 결과를 나타냈습니다. 또한, 12 NFEs(Neural Function Evaluations)로 증류된 모델은 Wan 2.1 모델 대비 62배의 빠른 처리 속도를 보였습니다.

### [GigaTok: Scaling Visual Tokenizers to 3 Billion Parameters for Autoregressive Image Generation](https://arxiv.org/abs/2504.08736)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.08736.png)

Vote: 29

Authors: Zilong Huang, Jun Hao Liew, Xihui Liu, Tianwei Xiong, Jiashi Feng

- ***What's New***: GigaTok는 이미지 생성에 있어 3억 개의 파라미터로 스케일링 가능한 최초의 시각 토크나이저(Visual Tokenizer)입니다. 이 연구는 시맨틱 정규화(Semantic Regularization)를 통해 이미지를 개선할 수 있도록 하였으며, 이를 통해 이미지 재구성, 생성, 표현 학습 측면에서 새로운 지표를 제시합니다.
- ***Technical Details***: GigaTok는 이미지 토크나이저를 확장할 때 발생하는 복잡한 잠재 공간 문제를 해결하기 위해, 미리 훈련된 시각 인코더(Pre-trained Visual Encoder)로부터 시맨틱 정규화 손실을 도입합니다. GigaTok는 1D 토크나이저, 비대칭 인코더-디코더 스케일링, 그리고 안정적 훈련을 위한 엔트로피 손실(Entropy Loss)을 채택하여 3억 파라미터로 스케일링된 시각 토크나이저 개발에 성공하였습니다.
- ***Performance Highlights***: GigaTok는 3억 파라미터로의 스케일링을 통해 ImageNet 256×256 해상도에서 현존 최고 수준의 이미지 재구성, 하류 AR 생성, 그리고 표현 품질을 달성합니다. 특히, 1.4B AR 모델과 결합하여 후속적 시각 생성에서 뛰어난 성능을 보여주며, 이후 AR 모델의 재현성과 표현 능력에서도 개선된 결과를 나타냅니다.

### [MineWorld: a Real-Time and Open-Source Interactive World Model on Minecraft](https://arxiv.org/abs/2504.08388)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.08388.png)

Vote: 22

Authors: Yang Ye, Jiang Bian, Junliang Guo, Tianyu He, Yushu Jiang, Tim Pearce, Haoyu Wu

- ***What's New***: MineWorld는 실시간으로 상호 작용할 수 있는 오픈 소스 월드 모델로서, Minecraft 게임을 기반으로 개발되었습니다. 시각적 상태와 행동의 상호 작용을 통해 게임이 어떻게 진행되는지를 모델링하여, 인간 플레이어와의 실시간 상호작용을 가능하게 합니다.
- ***Technical Details***: MineWorld는 시각적 게임 장면과 행동을 개별 토큰(ID)으로 변환하여 Transformer 모델의 입력으로 사용합니다. 이를 통해 모델은 상태와 행동 간의 관계를 학습합니다. 새로운 알고리즘인 병렬 디코딩(parallel decoding)을 사용해 공간 중복이 있는 토큰을 동시에 예측함으로써 초당 4~7프레임을 생성하며, 실시간 상호작용을 지원합니다.
- ***Performance Highlights***: MineWorld 모델은 최첨단(SOTA) 오픈 소스 확산 기반 월드 모델보다 성능이 우수하며, 제안된 병렬 디코딩 알고리즘 덕분에 3배 이상의 속도 향상을 달성했습니다. 1.2B 파라미터 모델은 초당 3프레임을 생성하며, 이는 초보 및 일반 게임 플레이어와의 실시간 상호작용에 충분합니다.

### [SQL-R1: Training Natural Language to SQL Reasoning Model By Reinforcement Learning](https://arxiv.org/abs/2504.08600)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.08600.png)

Vote: 12

Authors: Peixian Ma, Chengjin Xu, Ran Chen, Xialie Zhuang, Jian Guo, Xuhui Jiang

- ***What's New***: SQL-R1은 강화 학습(Reinforcement Learning; RL) 알고리즘을 통해 자연어를 SQL로 변환하는 모델(NL2SQL)을 훈련한 새로운 접근법을 제시합니다. 이 모델은 복잡한 상황에서의 추론 성능을 향상시키기 위해 설계되었으며, RL 기반의 보상 함수를 NL2SQL 작업에 맞춰 개발했습니다.
- ***Technical Details***: SQL-R1은 SynSQL-2.5M과 같은 합성 데이터를 사용하여 NL2SQL 모델을 훈련하며, Supervised Fine-Tuning(SFT)와 RL을 조합한 훈련 전략을 개발했습니다. 또한, Group Relative Policy Optimization(GRPO) 알고리즘을 사용하여 SQL 생성 과정의 보상 피드백을 최적화했습니다. 보상 함수는 형식, 실행, 결과, 길이 등의 요소를 포함하여 다양한 단계에서 자세한 피드백을 제공합니다.
- ***Performance Highlights***: SQL-R1은 Spider 벤치마크에서 88.6%, BIRD 벤치마크에서 66.6%의 실행 정확도를 기록하였습니다. 이는 Qwen2.5-Coder-7B 모델과 비교하여 비슷하거나 더 나은 성능을 보여줍니다. 특히, 닫힌 소스 모델 (e.g., GPT-4, GPT-4o)에 비해 뛰어난 성능을 보였습니다.

### [PixelFlow: Pixel-Space Generative Models with Flow](https://arxiv.org/abs/2504.07963)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.07963.png)

Vote: 9

Authors: Chongjian Ge, Shilong Zhang, Peize Sun, Shoufa Chen, Ping Luo

- ***What's New***: PixelFlow는 이미지 생성 모델로, 기존의 잠재 공간(Latent Space) 모델 대신 원시 픽셀 공간(Raw Pixel Space)에서 작동합니다. 이를 통해 사전 훈련된 변분 오토인코더(VAE)가 필요 없으며, 모델이 전체적으로 종단 간(end-to-end) 훈련될 수 있습니다.
- ***Technical Details***: PixelFlow는 효과적인 cascade flow modeling을 통해 픽셀 공간에서의 연산 비용을 줄입니다. 모델은 다중 해상도에서의 샘플을 Flow Matching을 사용해 처리하며, 초기 노이즈 수준이 높은 단계에서는 낮은 해상도로 작동하고, 점차 해상도를 높이며 최종 목표 해상도에 도달합니다.
- ***Performance Highlights***: PixelFlow는 256x256 ImageNet 클래스 조건부 이미지 생성 벤치마크에서 FID 1.98을 기록하며, 이는 기존 잠재 공간 기반 모델과 비교해 경쟁력 있는 성능을 보여줍니다. 또한, 텍스트 조건부 이미지 생성에서는 다양한 벤치마크에서 강력한 결과를 보였습니다.

### [FlexIP: Dynamic Control of Preservation and Personality for Customized Image Generation](https://arxiv.org/abs/2504.07405)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.07405.png)

Vote: 7

Authors: Kaiwen Xiao, Haonan Lin, Linyan Huang, Yanning Zhou

- ***What's New***: 이 논문에서는 이미지 생성에서 주체 속성 편집을 유연하게 조절할 수 있는 새로운 프레임워크인 FlexIP를 소개합니다. FlexIP는 기존 방법의 한계를 극복하여 고유성 보존(Identity Preservation)과 맞춤형 편집(Personalized Editing)을 독립적으로 제어할 수 있는 차원으로 분리합니다.
- ***Technical Details***: FlexIP는 데이터셋 종류에 따라 보존 어댑터(Learnable Queries, CLIP [CLS] Embeddings)와 개인화 어댑터(Text Embeddings)를 통해 주체 속성과 편집 기능을 효과적으로 관리합니다. 또한, 동적 가중치 게이팅(Dynamic Weight Gating) 메커니즘을 활용하여 이미지와 비디오 데이터셋에 대한 학습 및 추론 시 보존과 개인화의 균형을 조정할 수 있습니다.
- ***Performance Highlights***: 실험 결과, FlexIP는 모든 평가 지표에서 다른 최첨단 방법들보다 우수하였습니다. 특히, 개인화와 보존 점수인 CLIP-T와 CLIP-I, DINO-I에서 높은 점수를 기록하였고, 이미지 품질에서도 CLIP-IQA와 Aesthetic 점수가 높았습니다. 사용자 연구에서도 FleX와 ID-Pres의 측면에서 61.4%와 76.8%의 높은 만족도를 보였습니다.

### [Do PhD-level LLMs Truly Grasp Elementary Addition? Probing Rule Learning vs. Memorization in Large Language Models](https://arxiv.org/abs/2504.05262)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.05262.png)

Vote: 5

Authors: Yu Lu, Renjun Xu, Zhenzhong Lan, Yang Yan

- ***What's New***: 이 연구는 대형 언어 모델(Large Language Models; LLMs)이 기본적인 덧셈 원칙을 실제로 학습했는지 아니면 단순히 패턴을 기억하는지를 조사합니다. 이를 위해 기본적인 두 정수 덧셈을 활용하여 LLMs의 규칙 학습 대 기억 시험을 진행했습니다.
- ***Technical Details***: 조사에서는 두 가지 핵심 성질인 교환법칙(A + B = B + A)과 구성적 일반화(구조적 표현 변환을 통한)를 탐구했습니다. 다양한 모델 구조와 크기에서 LLMs는 화려한 성과 지표를 보였지만, 기호적(isomorphic) 표현에서 성능이 급하락하여 일반화된 규칙을 제대로 학습하지 못했음을 보여주었습니다.
- ***Performance Highlights***: 숫자 추가에서 73.8%에서 99.8%의 정확도를 보였으나, 기호적 매핑에서는 성능이 7.5% 이하로 급락했습니다. 또, 자릿수 스케일링에서 비단조적 성능 패턴을 보이며, 교환법칙 위반이 1,700건 이상 관찰되었습니다. 이는 모델이 주로 기억된 패턴에 의존하며, 실제 규칙 학습에는 한계가 있음을 시사합니다.

### [Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs](https://arxiv.org/abs/2504.07866)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.07866.png)

Vote: 5

Authors: Liqun Deng, Naifu Zhang, Lin Li, Peng Guo, Jun Zhao, Zhe Liu, Wei Li, Yue Li, Bo Wang, Zhicheng Liu, Yujun Li, Fei Mi, Xiaojun Meng, Weiwen Liu, Duyu Tang, Yaoyuan Wang, Bin Wang, Yichun Yin, Xueyu Wu, Yonghan Dong, Yehui Tang, Zhenhe Zhang, Changzheng Zhang, Yin Li, Yu Pan, Yunsheng Ni, Fisher Yu, Qiang Li, Baojun Wang, Nianzu Zheng, Kaikai Song, Ruiming Tang, Shengjun Cheng, Boxiao Liu, Yunhe Wang, Tianyu Guo, Hui Jin, Yasheng Wang, Dong Li, Rongju Ruan, Xinyi Dai, Jiansheng Wei, Can Chen, Wenyong Huang, Yufei Wang, Wei Guo, Minghui Xu, Jinpeng Li, Jiarui Qin, Wei He, Wulong Liu, Dandan Tu

- ***What's New***: Pangu Ultra는 1350억 개의 파라미터를 가진 대형 언어 모델(LLM)로, 화웨이의 Ascend NPU에서 훈련된 최초의 밀집 Transformer 모델입니다. 이 모델은 이전의 길이별 손실 스파이크를 제거함으로써 훈련의 안정성을 높이는 Depth-Scaled Sandwich Normalization이라는 혁신적인 방법을 도입하였습니다.
- ***Technical Details***: Pangu Ultra는 1350억 개 파라미터와 94개의 레이어로 구성된 밀집 Transformer 구조를 사용합니다. 모델의 손실 스파이크 문제를 해결하기 위해 깊이-스케일된 샌드위치 정규화(Depth-Scaled Sandwich Norm)와 미세 초기화(TinyInit) 기법을 채택했습니다. 이 모델은 8,192개의 Ascend NPU를 사용하여 대규모 모델 훈련을 성공적으로 구현했으며, 데이터 병렬 처리(Data Parallelism; DP), 텐서 병렬 처리(Tensor Parallelism; TP), 시퀀스 병렬 처리(Sequence Parallelism; SP), 파이프라인 병렬 처리(Pipeline Parallelism; PP)를 결합하여 시스템 효율성을 극대화하였습니다.
- ***Performance Highlights***: Pangu Ultra는 대부분의 주요 벤치마크에서 기존의 Llama 405B와 Mistral Large 2와 같은 밀집 LLM을 뛰어넘는 우수한 성능을 보였으며, 심지어 더 큰 파라미터 수를 가진 DeepSeek-R1와 같은 모델과도 경쟁력 있는 결과를 달성했습니다. 또한, 차세대 Ascend NPU가 수백억 개의 파라미터를 가진 큰 규모의 밀집 모델을 훈련할 수 있는 잠재력을 확인했습니다.

### [ModernBERT or DeBERTaV3? Examining Architecture and Data Influence on Transformer Encoder Models Performance](https://arxiv.org/abs/2504.08716)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.08716.png)

Vote: 5

Authors: Wissam Antoun, Djamé Seddah, Benoît Sagot

- ***What's New***: 이 논문에서는 ModernBERT와 DeBERTaV3 모델의 아키텍처와 데이터의 영향을 분석하여 Transformer Encoder 모델의 성능을 평가하고 비교했습니다. 특히 동일한 데이터셋(CamemBERTaV2)을 사용하여 ModernBERT를 재훈련함으로써 아키텍처 디자인의 효과를 명확히 구분하고자 합니다.
- ***Technical Details***: ModernBERT는 BERT와 RoBERTa와 같은 기존 모델보다 향상된 속도와 효율성을 제공하는 설계를 특징으로 합니다. FlashAttention, 글로벌 및 로컬 Attention Layer 교차, Sequence Packing, RoPE(Rotary Positional Embeddings)와 같은 기법들을 통합하여 훈련 속도와 추론 속도를 개선하였습니다. 실험은 CamemBERTaV2와 같은 데이터셋에서 ModernBERT-CV2를 훈련하고, 고품질 필터링 데이터셋을 적용하여 ModernBERT-HQ를 추가로 분석하였습니다.
- ***Performance Highlights***: DeBERTaV3는 ModernBERT보다 전반적인 성능 및 훈련 샘플 효율성에서 우수하였으며, 이는 DeBERTaV3의 아키텍처와 훈련 목표 최적화의 우수성을 보여줍니다. ModernBERT는 훈련 및 추론 속도에서의 실용성에 중점을 두며, 고품질 필터 데이터 사용 시 수렴을 가속화하지만 최종 성능 개선은 제한적이었습니다. 이는 현재 NLP 벤치마크의 포화 가능성을 시사합니다.

### [CoRAG: Collaborative Retrieval-Augmented Generation](https://arxiv.org/abs/2504.01883)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.01883.png)

Vote: 4

Authors: Virginia Smith, Aashiq Muhamed, Mona Diab

- ***What's New***: CoRAG는 기존 RAG(Retrieval-Augmented Generation) 모델을 확장하여 여러 클라이언트가 공동으로 모델을 훈련하는 협업 환경을 지원하는 프레임워크입니다. 이를 평가하기 위해 CRAB라는 협업 오픈 도메인 질문 응답 벤치마크를 소개했습니다.
- ***Technical Details***: CoRAG는 협력적으로 구축된 'Collaborative Passage Store'를 사용하여 클라이언트들이 공통으로 모델을 훈련할 수 있게 하며, 클라이언트들이 추론 시에도 자신만의 'Local Passage Store'를 사용할 수 있도록 합니다. 이 과정에서 각 클라이언트는 자신의 로컬 데이터에 기반하여 모델을 업데이트하며 'FedAvg'와 같은 방법을 통해 글로벌 모델을 생성합니다.
- ***Performance Highlights***: CRAB 벤치마크를 통해 CoRAG가 적은 리소스 환경에서 파라메트릭 협업 학습 방법 및 지역 RAG 모델보다 일관되게 높은 성능을 보였으며, 특히 협력적으로 구축된 데이터스토어 내 관련 패시지의 중요성을 강조했습니다. CoRAG는 적은 양의 레이블된 QA 데이터로도 향상된 성능을 보이며, 특히 16샷 환경에서 33.8% 향상된 결과를 나타냈습니다.

### [Training-free Guidance in Text-to-Video Generation via Multimodal Planning and Structured Noise Initialization](https://arxiv.org/abs/2504.08641)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.08641.png)

Vote: 4

Authors: Han Lin, Jaemin Cho, Jaehong Yoon, Mohit Bansal, Shoubin Yu, Jialu Li

- ***What's New***: VIDEO-MSG는 텍스트-비디오 생성(T2V)에서 텍스트와 비디오 모델을 통합하여 다중모달 계획과 구조적 노이즈 초기화(Structured Noise Initialization)를 활용한 훈련 없는 가이던스 방법입니다. 이를 통해 대형 T2V 모델의 구현을 용이하게 하며, 텍스트와의 정렬을 개선합니다.
- ***Technical Details***: VIDEO-MSG는 세 단계를 포함하며, 먼저 다중모달 모델을 사용해 배경 및 전경 객체의 레이아웃과 경로를 계획합니다. 그런 다음, 생성된 비디오 초안(VIDEO SKETCH)과 노이즈 역전환 기법을 사용하여 T2V 모델을 안내합니다. 이러한 절차는 배경과 전경 객체의 위치 조화를 개선합니다.
- ***Performance Highlights***: VIDEO-MSG는 대표적인 T2V 백본 모델인 VideoCrafter2와 CogVideoX-5B에서 텍스트와의 정렬 및 움직임 일치에 있어서 상당한 성능 향상을 보여주었습니다. 모션 바인딩에서는 VideoCrafter2에서 0.1499, CogVideoX-5B에서 0.1544 증가를 기록하며 우수한 결과를 나타냈습니다.

### [VLM-R1: A Stable and Generalizable R1-style Large Vision-Language Model](https://arxiv.org/abs/2504.07615)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.07615.png)

Vote: 4

Authors: Ruochen Xu, Peng Liu, Qianqian Zhang, Chunxin Fang, Haozhan Shen, Qiaoli Shen, Zilun Zhang, Tiancheng Zhao, Jingcheng Li, Yibo Ma, Jiajia Liao, Kangjia Zhao

- ***What's New***: VLM-R1은 비전-언어 모델(Vision-Language Models; VLMs)의 성능을 강화하기 위해 R1 스타일의 강화 학습을 도입한 프레임워크입니다. 이를 통해 특히 시각적 이해 작업에서의 성능과 일반화 능력을 개선하려고 합니다.
- ***Technical Details***: VLM-R1 프레임워크는 다양한 VLM 아키텍처를 지원하며, 유연한 데이터 정의와 모델 모듈성을 제공합니다. 강화 학습에 사용된 주요 알고리즘은 GRPO(Group Relative Policy Optimization)입니다. VLM 모듈은 다양한 VLM을 지원하고 사용자 정의 데이터 세트를 쉽게 통합하여 실험을 수행할 수 있습니다.
- ***Performance Highlights***: 강화 학습은 슈퍼바이즈드 파인튜닝(Supervised Fine-Tuning; SFT)보다 일반화 능력에서 현저한 개선을 제공합니다. 예를 들어, 외부 도메인 평가에서 RL 모델은 기존 SFT 모델보다 더 우수한 성능을 보였습니다. 특히 복잡한 작업에서는 RL의 사용으로 인해 더욱 큰 성능 향상을 보여줍니다. 또한, 보상 엔지니어링을 통해 잘못된 보상 부여 문제(Reward Hacking)를 방지하여 모델의 정확한 예측 능력을 향상시켰습니다.

### [Visual Chronicles: Using Multimodal LLMs to Analyze Massive Collections of Images](https://arxiv.org/abs/2504.08727)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.08727.png)

Vote: 4

Authors: Thomas Funkhouser, Songyou Peng, Leonidas Guibas, Noah Snavely, Boyang Deng, Kyle Genova, Gordon Wetzstein

- ***What's New***: 이 논문은 다중모달 대형 언어 모델(Multimodal Large Language Models; MLLMs)을 활용하여 거대한 이미지 컬렉션의 시각적 변화를 분석하는 새로운 접근법을 제안합니다. 이 접근법은 레이블이 없는 대규모 데이터셋에서 도시 전체의 시각적 변화를 발견하는 데 중점을 두고 있으며, 이전의 학습 기반 또는 비지도 시각적 분석 도구의 한계를 넘어서는 방법론입니다.
- ***Technical Details***: 본 연구는 구글 스트리트 뷰(Google Street View)에서 수집된 수천만 장의 이미지를 사용하여, MLLMs 기반의 하향식 시스템을 설계하였습니다. 이 시스템은 먼저 지각된 시각적 변화를 탐지하고 이를 바탕으로 전체적인 변화를 추적합니다. 즉, 지역적 시각 변화를 감지하고 이를 요약한 후, 이들 지역적 변화에서 전역적 트렌드를 식별합니다. 제안된 알고리즘은 MLLM의 한계를 극복하기 위해 효율적인 클러스터링과 검증 메커니즘을 포함하고 있습니다.
- ***Performance Highlights***: 실험 결과에 따르면, 제안된 시스템은 이전의 솔루션보다 시각적 변화를 더 정확하게 발견하며, 대규모 도시 이미지에서 흥미로운 트렌드를 자동으로 발견할 수 있습니다. 예를 들어, 뉴욕과 샌프란시스코의 도시에서 야외 식사 공간의 추가, 일부 고가도로의 색상 변화 등 다양한 트렌드를 발견할 수 있었습니다. 이 시스템은 수억 장의 이미지에서도 트렌드를 검증하는 데 몇 시간 만에 분석을 완료할 수 있도록 설계되었습니다.

### [ZipIR: Latent Pyramid Diffusion Transformer for High-Resolution Image Restoration](https://arxiv.org/abs/2504.08591)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.08591.png)

Vote: 4

Authors: Haitian Zheng, Yongsheng Yu, Zhe Lin, Jianming Zhang, Connelly Barnes, Wei Xiong, Zhifei Zhang, Yuqian Zhou, Yuchen Liu, Jiebo Luo

- ***What's New***: ZipIR는 초고해상도 이미지 복원(Efficient High-Resolution Image Restoration)을 위해 설계된 새로운 프레임워크로, 고압축 잠재 표현(Highly Compressed Latent Representation)과 고성능 모델인 확산 변환기(Diffusion Transformer; DiT)를 활용하여 효율성을 높였습니다. Latent Pyramid VAE (LP-VAE)를 도입하여 잠재 공간을 서브밴드로 구조화하고, 전체 이미지를 최대 2K 해상도로 학습하여 기존 확산 기반 방법을 능가하는 성능을 보입니다.
- ***Technical Details***: ZipIR는 32배의 공간 다운샘플링 요인을 갖춘 고압축 잠재 표현을 활용하며, 따라서 고성능 모델 사용을 가능하게 합니다. LP-VAE는 저차원 정보(저해상도 정보)를 초기에 코딩하고, 고차원 정보(고해상도 정보)를 순차적으로 캡처하면서 훈련되는 방식을 취합니다. 이는 저해상도 변질이 주로 보다 미세한 수준의 잠재적 특징에 영향을 주도록 하여 학습 과정을 단순화합니다.
- ***Performance Highlights***: ZipIR는 2K 해상도 이미지에서 SeeSR보다 10배 빠른 추론 속도를 달성하며, 심하게 열화된 입력에서도 뛰어난 복원 품질을 제공합니다. 16배 및 8배 슈퍼 해상도 시나리오에서 ZipIR는 LPIPS 및 FID 지표에서 뛰어난 성능을 보입니다. 혼합 열화 조건에서도 많은 양의 변질을 줄이며, ZipIR는 전반적으로 우수한 지각 품질과 분포 일관성을 나타냅니다.

### [In-2-4D: Inbetweening from Two Single-View Images to 4D Generation](https://arxiv.org/abs/2504.08366)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.08366.png)

Vote: 4

Authors: Daniel Cohen-Or, Ali Mahdavi-Amiri, Hao Zhang, Sauradip Nag

- ***What's New***: In-2-4D는 두 개의 단일 뷰 이미지(Two Single-View Images)로부터 4차원(3D + 모션) 생성을 수행하는 새로운 문제를 제안합니다. 이는 다양한 객체와 모션 유형에 대해 사전 가정 없이 원활하고 자연스러운 4D 모션을 생성할 수 있는 최초의 방법입니다.
- ***Technical Details***: In-2-4D는 영상 보간(Video Interpolation) 모델을 사용하여 모션을 예측하지만, 큰 프레임 간 모션은 해석의 모호성을 초래할 수 있습니다. 이를 해결하기 위해, 계층적 접근을 통해 입력 상태에 시각적으로 가까운 키프레임(Keyframes)을 식별하고, 그 사이를 부드럽게 연결하는 단편(Fragment)을 생성합니다. 각 단편에 대해, 3D 표현은 가우시안 스플래팅(Gaussian Splatting)을 통해 구성되며, 다중 뷰 확산(Multi-View Diffusion)을 확장하여 시간적 일관성을 개선하고, 3D 모션을 정제하기 위해 강체 변환 규제(Rigid Transformation Regularization)를 적용합니다. 최종적으로는 경계 변형 필드를 보간하고 최적화하여 각 동영상 조각을 독립적으로 생성된 3D 모션 세그먼트로 병합합니다.
- ***Performance Highlights***: I4D-15 데이터셋을 사용한 정량적 실험에서, In-2-4D는 기하학적 및 외관적 측면에서 기존 방법들보다 뛰어난 성능을 보였습니다. 예를 들어, CLIP 점수는 0.91, LPIPS는 0.103, FVD는 679.23을 기록하며, 모션의 복잡성을 세분화하여 처리함으로써 시각적 품질과 다양성을 높이는 데 성공했습니다.

### [SAEs Can Improve Unlearning: Dynamic Sparse Autoencoder Guardrails for Precision Unlearning in LLMs](https://arxiv.org/abs/2504.08192)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.08192.png)

Vote: 2

Authors: Virginia Smith, Aashiq Muhamed, Mona Diab, Jacopo Bonato

- ***What's New***: 이 논문에서는 대형 언어 모델(LLM)의 안전성을 개선하기 위해 머신 언러닝(Machine Unlearning) 기법을 제안합니다. 특히, 동적인 희소 오토인코더 가드레일즈(DSG; Dynamic Sparse Autoencoder Guardrails)를 도입하여 기존의 그래디언트 기반 언러닝 방식보다 효율적이고 해석 가능한 언러닝을 제공합니다.
- ***Technical Details***: DSG는 Fisher Information 기반의 피처 선택과 동적인 분류기를 사용하여 목표로 하는 데이터의 피처를 선택적으로 활성화 합니다. 이 피처들은 잊어야 할 데이터와 원인적으로 연결된 피처들로, 필요할 때만 선택적으로 개입을 하도록 설계되었습니다. 이는 특정 입력에 대한 특정 지식 경로를 차단하여 일반적인 모델 기능을 유지합니다.
- ***Performance Highlights***: DSG는 WMDP 벤치마크 테스트에서 탁월한 성능을 보여줬습니다. 일반적인 활용도를 99% 이상 유지하면서 WMDP-Bio에서 성공적인 잊기와 유틸리티 보존의 최상의 균형을 이루었습니다. 이는 그래디언트 기반 방법들과 비교하여 상당한 향상을 나타내며, 효율성과 안정성을 높입니다.

### [BlenderGym: Benchmarking Foundational Model Systems for Graphics Editing](https://arxiv.org/abs/2504.01786)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.01786.png)

Vote: 2

Authors: Yunqi Gu, Guandao Yang, Leonidas Guibas, Jihyeon Je, Ian Huang

- ***What's New***: BlenderGym은 3D 그래픽 편집을 위한 VLM 시스템의 성능을 코드 기반 3D 장면 재구성을 통해 평가하는 첫 번째 포괄적인 벤치마크입니다. 그래픽스 편집의 핵심 태스크인 객체 배치, 조명 조정, 절차적 소재 편집, 블렌드 쉐이프 조작, 및 절차적 기하학 편집을 포함하여 245개의 수작업 시작-목표 장면 쌍을 제공합니다.
- ***Technical Details***: BlenderGym벤치마크는 Blender라는 3D 소프트웨어를 사용하여 시작 장면에서 목표 장면으로 재구성하는 작업을 목표로 합니다. 각 장면 쌍에는 Python으로 작성된 시작 및 목표 장면을 생성하는 스크립트가 포함되어 있습니다. VLM 시스템은 화면에 표시된 시각적 차이점을 분석하고 Python 코드(BPY 및 Infinigen)를 수정하여 목표 장면을 재구성해야 합니다. 평가 메트릭으로는 Photometric Loss(PL), Chamfer Distance(CD), CLIP Score(N-CLIP)이 사용됩니다.
- ***Performance Highlights***: 인간 Blender 사용자가 모든 태스크에서 VLM 시스템보다 월등한 성능을 보여주며, 이는 VLM 시스템이 여전히 그래픽 편집에 어려움을 겪고 있음을 뜻합니다. 특히, BlenderAlchemy와 같은 현재의 SOTA 파이프라인은 다양한 VLM 시스템과의 원활한 통합을 지원하며 제너레이터-베리파이어 구조를 사용하여 최적의 편집 솔루션을 찾습니다.

### [Latent Diffusion Autoencoders: Toward Efficient and Meaningful Unsupervised Representation Learning in Medical Imaging](https://arxiv.org/abs/2504.08635)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.08635.png)

Vote: 1

Authors: Francesco Fontanella, Frederick J. A. Meijer, Henkjan Huisman, Claudio De Stefano, Gabriele Lozupone, Alessandro Bria

- ***What's New***: 이 연구는 의료 이미징에서 효율적이고 의미 있는 비지도 학습을 위한 새로운 프레임워크인 Latent Diffusion Autoencoder (LDAE)를 소개합니다. 이 모델은 압축된 잠재 공간에서 확산 프로세스를 적용하여 계산 효율을 개선하고 3D 의료 이미징 표현 학습을 보다 다루기 쉽게 만듭니다. 이를 통해 알츠하이머병(Alzheimer’s Disease; AD)에 관한 중요한 의미를 가진 표현을 학습할 수 있으며, 이는 주요 연구 방향으로 제시됩니다.
- ***Technical Details***: LDAE는 세 가지 주요 단계로 구성됩니다. 첫째, 퍼셉추얼 오토인코더(Autoencoder; AE)가 3D 뇌 MRI 데이터를 저차원의 잠재 공간으로 압축합니다. 둘째, 압축된 잠재표현에서 확산 모델(Diffusion Model; DM)을 사전훈련합니다. 마지막으로 Posterior Mean Gap을 메우기 위한 인코더-디코더 과정을 통해 잠재 공간의 비지도 표현 학습을 수행합니다. 이 과정은 압축된 잠재 공간에서 정의된 영어 기술적 개념들과 함께, 정보 손실을 최소화하고 의미 있는 표현을 학습하도록 설계되었습니다.
- ***Performance Highlights***: LDAE 모델은 3D 뇌 MRI 데이터의 선형 평가에서 알츠하이머병의 진단 성능(ROC-AUC: 90%, 정확도: 84%) 및 나이 예측(MAE: 4.1년, RMSE: 5.2년)에서 유망한 결과를 보여주었습니다. 또한, 입력 이미지 없이 압축된 잠재 표현에서 새로운 Sample을 생성할 수 있으며, 속도 면에서는 기존의 확산 오토인코더보다 20배 빠른 추론을 지원합니다. 더욱이 중간 스캔의 강력한 재구성을 보여주며, SSIM 점수가 0.969에 도달하는 등 포괄적인 성능을 입증하였습니다.

### [UKBOB: One Billion MRI Labeled Masks for Generalizable 3D Medical Image Segmentation](https://arxiv.org/abs/2504.06908)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.06908.png)

Vote: 1

Authors: Amir Jamaludin, Abdullah Hamdi, Emmanuelle Bourigault

- ***What's New***: UK Biobank Organs and Bones (UKBOB) 데이터셋은 현재까지 가장 큰 레이블링된 의료 영상 데이터셋으로, 51,761개의 3D MRI 샘플과 72개의 장기에 걸친 2D 세그먼테이션 마스크 13억 7천만 개 이상을 포함하고 있습니다. 이 데이터셋은 대규모의 레이블링된 데이터를 통해 3D 의료 영상 분할을 위한 강력하고 일반화 가능한 모델 훈련에 기여할 수 있습니다.
- ***Technical Details***: UKBOB는 자동 레이블링을 사용하여 수집되었으며, 레이블 품질 향상을 위해 장기 특화 필터링 메커니즘과 300개 MRI의 수동 주석을 통한 검증 레이블을 포함합니다. 또한, Entropy Test-time Adaptation (ETTA) 기법을 제안하여 훈련된 모델의 예측을 미세 조정하고 레이블 잡음을 완화합니다. 기반 모델 Swin-BOB는 Swin-UNetr에 기초하여 3D 의료 이미지 분할에서 최첨단 결과를 달성했습니다.
- ***Performance Highlights***: Swin-BOB는 BRATS 뇌종양 MRI 챌린지에서 0.4% 개선, BTCV 복부 CT 스캔 벤치마크에서 1.3% 개선을 이루며 최첨단 성능을 보였습니다. 이는 UKBOB로 훈련된 모델이 다양한 데이터셋에 대한 제로샷 일반화를 보여주는 강력한 성능을 나타냅니다.

### [InteractVLM: 3D Interaction Reasoning from 2D Foundational Models](https://arxiv.org/abs/2504.05303)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.05303.png)

Vote: 0

Authors: Michael J. Black, Shashank Tripathi, Cordelia Schmid, Dimitrios Tzionas, Sai Kumar Dwivedi, Omid Taheri, Dimitrije Antić

- ***What's New***: InteractVLM은 새로운 방법으로 단일 자연 이미지에서 3D 접점 포인트를 추정하여 인간과 객체 간의 상호 작용을 3D로 재구성합니다. 이는 널리 사용되는 Vision-Language Models(VLMs)의 광범위한 시각적 지식을 활용하여 고가의 3D 모션 캡처 데이터의 의존도를 최소화합니다.
- ***Technical Details***: 이 방법의 핵심은 'Render-Localize-Lift' 모듈을 통해 3D 바디 및 객체 표면을 다중 뷰 렌더링을 통해 2D 공간에 포함시키는 것입니다. 새로운 다중 뷰 로컬라이제이션 모델(Multi-View Localization; MV-Loc)은 2D에서 접점을 추론한 후 이를 3D로 변환하여 사용됩니다. 또 다른 중요한 기술적 기여는 객체 레이블에 기초하여 인간의 접점을 추론하는 'Semantic Human Contact'입니다.
- ***Performance Highlights***: InteractVLM은 다양한 데이터셋에서 기존 방법들을 능가하여 3D 접점 추정의 성능을 크게 향상시켰습니다. 특히 'Semantic Human Contact' 추정 작업에서 기존의 이진 접점 예측 방법을 넘어 복잡한 상호작용을 잘 파악합니다.

