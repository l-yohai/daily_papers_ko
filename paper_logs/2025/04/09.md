## Daily Papers (2025-04-09)

### [OmniSVG: A Unified Scalable Vector Graphics Generation Model](https://arxiv.org/abs/2504.06263)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.06263.png)

Vote: 87

Authors: Yiying Yang, Xingjun Ma, Sijin Chen, Xianfang Zeng, Liao Wang, Gang Yu, Wei Cheng, Jiaxu Zhang, Yu-Gang Jiang

- ***What's New***: OmniSVG는 사전 학습된 비전-언어 모델(Vision-Language Models; VLMs)을 활용하여 복잡하고 고품질의 스케일러블 벡터 그래픽스(SVG)를 생성할 수 있는 통합 프레임워크입니다. 이 모델은 다양한 생성 모달리티를 통해 텍스트-대-SVG(Text-to-SVG), 이미지-대-SVG(Image-to-SVG), 캐릭터-참조 SVG(Character-Reference SVG) 등 여러 창의적인 작업에 유연하게 적용될 수 있습니다.
- ***Technical Details***: OmniSVG는 SVG 명령과 좌표를 이산적인 토큰으로 매개변수화하여 구조적 논리를 저수준 기하학에서 분리합니다. 이를 통해 효율적인 학습이 가능하며, 복잡한 SVG 구조의 표현력을 유지합니다. 사전 학습된 VLMs 기반의 OmniSVG 모델은 텍스트와 이미지 입력을 토큰화하여 접두사 토큰으로 사용하고, SVG 토큰화기를 통해 벡터 그래픽 명령을 단일 표현 공간으로 인코딩합니다. 또한 MMSVG-2M이라는 다중 모달 데이터셋을 소개하여 200만 개의 풍부한 주석이 달린 SVG 자산과 표준화된 평가 프로토콜을 제공합니다.
- ***Performance Highlights***: OmniSVG는 기존의 SVG 생성 방법들을 성능 면에서 능가하며 다양한 조건화된 SVG 생성 작업에서 매우 정밀하고 복잡한 SVG 콘텐츠를 생성하는 데 뛰어난 잠재력을 보여줍니다. 3B와 7B 모델을 비교한 실험에서는 모델 크기가 클수록 생성 품질이 향상됨을 나타냈습니다. 특히 텍스트-대-SVG 작업과 이미지-대-SVG 작업에서 더 높은 조화지표 및 낮은 생성 효율성 지표를 보였습니다.

### [Hogwild! Inference: Parallel LLM Generation via Concurrent Attention](https://arxiv.org/abs/2504.06261)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.06261.png)

Vote: 69

Authors: Alina Shutova, Anton Sinitsin, Vage Egiazarian, Dan Alistarh, Denis Kuznedelev, Roman Garipov, Gleb Rodionov, George Yakushev

- ***What's New***: Hogwild! Inferrence는 병렬 LLM 생산을 구현하여 LLM(대형 언어 모델)이 서로의 진행 상황을 즉시 볼 수 있도록 하여 자체 협업 전략을 개발할 수 있도록 설계되었습니다. 이는 Sub-Task를 병렬로 수행하는 기존의 협업 프레임워크와 달리, 각 LLM 인스턴스가 어떤 방식으로 협력할지 동적으로 결정할 수 있게 하여 더욱 유연한 협업을 가능하게 합니다.
- ***Technical Details***: 이 연구에서는 공동 Cross-Attention을 수행하는 병렬 LLM 추론 엔진인 Hogwild! Inferrence를 제안합니다. 주요 기술적 요소로는 Rotary Position Embeddings (RoPE)을 활용한 Key-Value 캐시 메커니즘이 있으며, 이는 병렬 하드웨어 활용도를 개선하고 계산 오버헤드를 줄입니다. 캐시는 여러 블록으로 구성되어 있으며, 각 워커는 자신의 진행 상황을 동기화 없이 업데이트하고, 필요에 따라 그 정보를 기반으로 다른 워커와 협력합니다.
- ***Performance Highlights***: 초기 실험에서는 여러 LLM 인스턴스가 공유 키-값 캐시를 사용하여 수학적 문제와 같이 긴 논리적 추론이 필요한 문제들을 더 빠르며 효율적으로 해결할 수 있음을 보여주었습니다. 병렬 작업자들은 종종 작업을 할당하고 각자의 부분을 처리한 후, 남는 자원을 활용해 남은 작업을 점검하거나 다른 작업자의 실수를 검토하는 방식을 통해 협력하고 있습니다.

### [Skywork R1V: Pioneering Multimodal Reasoning with Chain-of-Thought](https://arxiv.org/abs/2504.05599)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.05599.png)

Vote: 60

Authors: Jiachun Pan, Yang Liu, Ai Jian, Yi Peng, Yichen Wei, Li Ge, Xiaokun Wang, Chris, Yunzhuo Hao, Tianyidan Xie, Weijie Qiu, Xuchen Song, Rongxian Zhuang, Yahui Zhou, Jiangbo Pei

- ***What's New***: Skywork R1V는 R1 시리즈 대형 언어 모델(LLM)을 비주얼 모달리티로 확장하는 멀티모달 추론 모델로, 가벼운 시각 프로젝터(visual projector)를 활용해 기반 언어 모델이나 비전 인코더를 재훈련할 필요 없이 매끄러운 멀티모달 적응을 제공합니다. 시각-텍스트 정렬을 강화하기 위해 반복 학습 최적화(Iterative Supervised Fine-Tuning, SFT)와 그룹 상대 정책 최적화(Group Relative Policy Optimization, GRPO)를 결합한 하이브리드 최적화 전략을 제안합니다. 또한 적응적 길이 생각의 사슬(Chain-of-Thought) 증류 접근 방식을 도입하여 추론 데이터 생성을 최적화합니다.
- ***Technical Details***: Skywork R1V는 효율적인 멀티모달 전이 방법을 사용하여 추론 가능한 LLM을 시각 백본과 효율적으로 정렬합니다. 이 과정에서 다층 퍼셉트론(MLP)을 시각 프로젝터로 활용하며, 기존 대형 언어 모델(fl)과 비전 백본(fv)을 직접 연결하는 대신 대체 언어 모델(f s l)과의 초기 정렬을 통해 기존 멀티모달 데이터셋을 활용합니다. 하이브리드 최적화 프레임워크(Hybrid Optimization Framework)는 반복 학습 최적화와 GRPO를 통합하여 시각-텍스트 표현을 효율적으로 정렬하며, 적응적 길이 생각의 사슬 방법을 통해 생성된 추론 체인을 학습에 사용합니다.
- ***Performance Highlights***: Skywork R1V는 총 38B 파라미터로 구성되어 있으며, MMMU 벤치마크에서 69.0, MathVista에서 67.5 점이라는 경쟁력 있는 성과를 달성했습니다. 또한 AIME에서 72.0, MATH500에서 94.0이라는 우수한 텍스트 추론 성과를 유지합니다. 이 모델은 공개되어 있어 멀티모달 추론 커뮤니티에서의 연구와 혁신을 촉진할 수 있습니다.

### [An Empirical Study of GPT-4o Image Generation Capabilities](https://arxiv.org/abs/2504.05979)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.05979.png)

Vote: 46

Authors: Tao Zhang, Chao Tang, Wenhao Chai, Lu Qi, Qingyu Shi, Yikang Zhou, Xiangtai Li, Jinbin Bai, Xin Lin, Wei Chow, Shilin Xu, Tian Ye, Haobo Yuan, Jianzong Wu, Linfeng Li, Donghao Zhou, Lei Zhu, Zhuoran Zhao, Sixiang Chen

- ***What's New***: 이 연구는 GPT-4o의 이미지 생성 능력을 실증적으로 평가한 최초의 연구로, 텍스트-이미지(텍스트-to-이미지), 이미지-to-이미지, 이미지-to-3D, 이미지-to-X 등의 생성 작업에서 GPT-4o의 성능을 평가하여, 멀티모달 생성 모델의 발전 방향을 탐색합니다.
- ***Technical Details***: GPT-4o는 텍스트 및 이미지 생성 작업을 통합한 통합 생성 아키텍처(Unified Generative Architectures)로 설계되었으며, 다양한 생성 작업에 대한 평가를 통해 여러 상황 내에서의 장단점을 분석합니다. 본 논문에서는 특히 GPT-4o를 상업용 및 오픈 소스 모델들과 비교하기 위해 다양한 설정 하에 테스트를 수행했습니다.
- ***Performance Highlights***: GPT-4o는 텍스트 렌더링, 조건 따라가기(prompt-following) 및 공간 추론(spatial reasoning)에서 뛰어난 성능을 보여주었습니다. 하지만 생성 일관성(inconsistent generation) 및 데이터 편향(data bias) 문제, 그리고 복잡한 텍스트에 대한 반응 능력에서는 한계가 있습니다. 이는 주로 훈련 데이터 및 모델이 담은 세계 지식의 부족에서 기인하며, 이에 대한 개선이 필요함을 강조합니다.

### [COIG-P: A High-Quality and Large-Scale Chinese Preference Dataset for Alignment with Human Values](https://arxiv.org/abs/2504.05535)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.05535.png)

Vote: 33

Authors: Yuchen Jiang, Qunshu Lin, Xinrun Du, Chenglin Cai, Xingyuan Bu, M-A-P Team, Minghao Liu, Zhouliang Yu, Ding Pan, Haoran Que, Jincheng Ren, Wangchunshu Zhou, Boyu Feng, Wenhao Huang, Huaqing Yuan, Shenzhi Wang, Jian Yang, Xingwei Qu, Tiannan Wang, Guoyin Wang, Zekun Moore Wang, Yuelin Bai, Tianyu Zheng, Yunwen Li, Ge Zhang, Siwei Wu, Shuyue Guo, Yiming Liang, Zenith Wang, Chenghua Lin, Jie Liu, Jiaheng Liu

- ***What's New***: COIG-P는 기존의 소규모, 제한된 범위의 중국어 선호 데이터셋의 한계를 극복하기 위해 설계된 대규모 고품질 중국어 선호 데이터셋입니다. 인간 개입 없이 LLM 기반 주석 파이프라인을 설계하여 15개의 주류 LLM을 활용하여 쌍을 생성하고 평가했습니다.
- ***Technical Details***: COIG-P 데이터셋은 6개의 다양한 도메인(채팅, 코드, 수학, 논리, 소설, 역할)을 아우르는 1,006k 중국어 선호 쌍으로 구성되어 있습니다. LLM 시작 92k 중국어 쿼리를 수집 및 필터링한 후, 다양한 응답을 생성하고, 최고 성능 8B 사이즈 모델인 CRM(Chinese Reward Model)을 학습하였습니다.
- ***Performance Highlights***: AlignBench에 따르면 COIG-P는 다른 중국어 선호 데이터셋보다 상당히 우수하며, Qwen2/2.5 및 Infinity-Instruct-3M-0625 모델 시리즈에 대해 2%에서 12% 향상을 가져왔습니다. CRBench에서 CRM은 강력하고 안정적인 스코어링 능력을 보여줍니다. CRM의 필터링 성능은 GPT-4o에 가까운 효과를 보였습니다.

### [Less-to-More Generalization: Unlocking More Controllability by In-Context Generation](https://arxiv.org/abs/2504.02160)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02160.png)

Vote: 27

Authors: Fei Ding, Qian He, Wenxu Wu, Yufeng Cheng, Shaojin Wu, Mengqi Huang

- ***What's New***: 이 연구에서는 다중 주제 이미지 생성(Multi-Subject Image Generation)의 확장성을 해결하기 위해 새로운 데이터 합성 파이프라인을 제안합니다. 그리고 UNO라는 새 모델을 도입하여 텍스트에서 이미지로(Text-to-Image) 모델을 주제 기반 이미지 생성(Subject-to-Image) 모델로 발전시키며, 더 높은 일관성을 유지하면서 단일 주제와 다중 주제 모두를 효과적으로 생성할 수 있는 방법을 제공합니다.
- ***Technical Details***: 이 연구에서는 강력한 데이터 및 모델 동시 진화를 달성하기 위해 점진적인 합성 파이프라인과 다중 슬롯 제어 설계를 활용합니다. 세부적으로, 이 연구는 단일 주제에서 다중 주제로의 전환을 지원하는 합성 데이터 큐레이션 프레임워크 및 데이터와 모델의 발전을 위한 공통 롤링 위치 임베딩 기법(Universal Rotary Position Embedding)을 도입했습니다. UNO 모델은 확산 변환기(Diffusion Transformer) 모델에 반복 학습을 통하여 합성 데이터를 생성하고, 인과적 생성 능력을 발휘하여 최상의 결과를 도출합니다.
- ***Performance Highlights***: UNO는 DreamBench와 다중 주제 생성 벤치마크에서 가장 높은 DINO와 CLIP-I 점수를 기록하며, 솔루션이 탁월한 주제 유사성과 텍스트 제어 가능성을 갖추고 있음을 보여줍니다. UNO는 다른 최첨단 방법들과 비교하여 강력한 성능 향상을 이뤄냈습니다.

### [V-MAGE: A Game Evaluation Framework for Assessing Visual-Centric Capabilities in Multimodal Large Language Models](https://arxiv.org/abs/2504.06148)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.06148.png)

Vote: 10

Authors: Xiangxi Zheng, Ping Yu, Rui Yan, Yuan Yao, Alex Jinpeng Wang, Linjie Li, Zhengyuan Yang, Lijuan Wang

- ***What's New***: V-MAGE는 멀티모달 대형 언어 모델(Multimodal Large Language Models; MLLMs)의 시각적 추론 능력을 평가하기 위한 게임 기반 평가 프레임워크입니다. 5개의 다양한 게임과 30개 이상의 수작업 레벨을 통해 MLLMs의 핵심 시각 기술과 고차원 추론을 테스트합니다. 기존 벤치마크와 비교하여, 시각 중심의 작업과 실제 세계의 의사결정에 필요한 다양한 추론 기술을 포괄적으로 평가할 수 있도록 설계되었습니다.
- ***Technical Details***: V-MAGE 벤치마크는 FlappyBird, RaceGame, SuperMario, PongGame, Tempest Run 등 5개의 게임으로 구성되어 있으며 각 게임은 다양한 난이도의 레벨로 구성되어 있습니다. 모델들은 픽셀 단위의 장면 이해, 물체 추적, 그리고 시공간 추론이 요구되는 시각 입력만을 받습니다. 평가 과정은 모델과 에이전트 전략의 최적화를 가능하게 하는 게임-에이전트-모델의 세 모듈 평가 파이프라인을 통해 수행되며, 적응형 Elo 기반의 성능 비교 시스템을 사용하여 모델의 성능을 동적으로 평가합니다.
- ***Performance Highlights***: V-MAGE에서의 실험 결과, InternVL2.5-78B와 Gemini-2.0 같은 최첨단 MLLMs가 단순한 과제에서는 인간 수준의 성능을 보이지만 복합적인 과제에서는 성능 저하를 보였습니다. 예를 들어, GPT-4o는 FlappyBird의 Level 6에서 인간 점수 ≈10/10에 비해 1.93/10의 점수를 기록했으며, Qwen2VL-72B는 0.61/10을 기록하였습니다. 이는 현재 MLLMs가 시각적 인식 및 다단계 추론에서 인간 수준의 게임 플레이에 도달하지 못하고 있음을 보여줍니다.

### [Generative Evaluation of Complex Reasoning in Large Language Models](https://arxiv.org/abs/2504.02810)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02810.png)

Vote: 10

Authors: Ruilin Yan, James Zou, Haowei Lin, Haotian Ye, Zihao Wang, Jianzhu Ma, Baizhou Huang, Jianhua Zhu, Yitao Liang, Xiangyu Wang

- ***What's New***: 이 논문에서는 대형 언어 모델(Large Language Models; LLMs)의 복잡한 추론 능력을 평가하기 위한 새로운 벤치마크 프레임워크인 KUMO를 소개합니다. KUMO는 LLM과 상징적 엔진을 조화롭게 결합하여 부분적으로 관찰 가능한 다양한 멀티턴 추론 작업을 동적으로 생성합니다. 이는 공개된 벤치마크에 의한 데이터 오염 문제를 해결하기 위한 것입니다.
- ***Technical Details***: KUMO는 복잡한 게임 형식으로 구조화되어 있으며, 참가자가 증거를 모으고 결론을 도출하기 위해 반복적으로 시스템과 상호작용합니다. 시스템은 의료 진단, 교육 평가, 화학 물질 탐지와 같은 여러 도메인에서 실제 추론 작업을 모방합니다. 각 작업은 참가자에게 'Knowledge Book'을 제공하여 도메인 지식을 배제한 순수한 추론 능력을 평가할 수 있도록 설계되었습니다. 이러한 작업은 SAT 기반 상징적 엔진과 협력하는 첨단 LLM에 의해 자동으로 생성됩니다.
- ***Performance Highlights***: KUMO가 생성한 100개의 도메인에서 5,000개의 작업을 통해 23개의 최신 LLM을 평가했습니다. 결과적으로, 많은 LLM이 쉬운 추론 작업에서 대학 수준의 성능을 능가했으며, 추론에 중점을 둔 LLM이 복잡한 추론 문제에서 대학 수준의 성능에 도달했습니다. 또한 KUMO에서의 LLM 성능은 최신의 실제 추론 벤치마크 결과와 강한 상관관계를 보였습니다.

### [Tuning-Free Image Editing with Fidelity and Editability via Unified Latent Diffusion Model](https://arxiv.org/abs/2504.05594)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.05594.png)

Vote: 9

Authors: Ming-Hsuan Yang, Yuchao Gu, Lan Chen, Qi Mao, Mike Zheng Shou

- ***What's New***: 이 논문에서는 UnifyEdit라는 새로운 방법을 제안하여 텍스트 기반 이미지 편집(TIE)에서 충실도(Fidelity)와 편집 가능성(Editability)의 균형을 맞추는 최초의 조정 불필요 프레임워크를 제안합니다. 이 방법은 자가-어텐션(Self-Attention; SA) 보존 제약과 교차-어텐션(Cross-Attention; CA) 정렬 제약을 함께 사용하여, 성능을 지침할 수 있는 적응적 타임스텝 스케줄러를 소개하고 있습니다.
- ***Technical Details***: UnifyEdit는 사전 학습된 텍스트-투-이미지 모델(T2I 모델)의 디퓨전 잠재 최적화를 통해 충실도와 편집 가능성을 균형 있게 통합하는 프레임워크입니다. SA 보존 제약은 구조적 충실도를 유지하고, CA 정렬 제약은 텍스트 정렬을 개선하여 편집 가능성을 높입니다. 그러나 두 제약을 동시에 적용할 경우, 하나의 제약이 지배하여 과잉 또는 부족 편집을 초래할 수 있어, 이를 해결하기 위해 제약의 영향을 동적으로 조정하는 적응적 타임스텝 스케줄러를 도입했습니다.
- ***Performance Highlights***: UnifyEdit는 다양한 편집 작업에서 기존 최신 방법들을 능가하며, 구조 보존과 텍스트 정렬 사이에 견고한 균형을 달성하는 것으로 나타났습니다. 또한, 이 접근 방식은 사용자가 선호하는 편집 스타일에 따라 충실도-편집 가능성의 균형을 조정할 수 있도록 지원합니다.

### [Accelerate Parallelizable Reasoning via Parallel Decoding within One Sequence](https://arxiv.org/abs/2503.20533)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.20533.png)

Vote: 7

Authors: Yijiong Yu

- ***What's New***: 이 논문에서는 LLMs의 병렬 가능한 추론(Parallelizable Reasoning)을 가속화하기 위해 '하나의 시퀀스에서 병렬 디코딩(Parallel Decoding in One Sequence)'이라는 새로운 방법을 제안합니다. 이 방법은 특정 작업에 대해 병렬 가능한 단계들을 식별하고, 이를 병렬로 디코딩함으로써 추론 속도를 증가시킵니다.
- ***Technical Details***: 제안된 방법은 세 단계로 나뉩니다: (1) 병렬 가능한 단계 식별, (2) 병렬 디코딩, (3) 결과 연결 및 계속 생성. 본 방법은 새로운 주의 마스크(attention mask)를 사용하여 각 단계의 시작 토큰을 접두로 사용함으로써 여러 토큰을 한 번의 포워드 패스로 생성할 수 있어 디코딩 과정을 크게 가속화합니다.
- ***Performance Highlights***: 실험 결과, 제안된 방법은 디코딩 속도를 100% 이상 증가시키고 답변의 질을 유지하면서도, 추가적인 메모리 사용 없이 추론 과정을 크게 가속화하였습니다. 특히, 많은 병렬 가능한 단계가 존재하는 작업에서 이는 더욱 두드러졌습니다.

### [HybriMoE: Hybrid CPU-GPU Scheduling and Cache Management for Efficient MoE Inference](https://arxiv.org/abs/2504.05897)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.05897.png)

Vote: 7

Authors: Ling Liang, Ru Huang, Yanfan Sun, Runsheng Wang, Meng Li, Shuzhang Zhong

- ***What's New***: HybriMoE는 대량의 메모리 요구 사항과 비효율적인 자원 활용 문제를 해결하기 위해 CPU와 GPU의 하이브리드 스케줄링 및 캐시 관리 시스템을 도입하여 MoE(Mixture of Experts) 추론의 효율성을 향상시키는 새로운 프레임워크입니다. 동적 내부 레이어 스케줄링과 영향 기반 사전 로딩 알고리즘, 점수 기반 캐싱 알고리즘을 통해 전문가 활성화 불안정성을 줄이고 자원 활용을 극대화합니다.
- ***Technical Details***: HybriMoE는 kTransformers 프레임워크를 바탕으로 구현되었으며 CPU와 GPU 간의 작업 부하를 동적으로 균형 잡기 위한 하이브리드 스케줄링 알고리즘을 도입합니다. 또한, 사전로드의 잠재적인 영향을 시뮬레이션하여 가장 높은 이득이 예상되는 전문가를 우선적으로 사전 로드하는 영향 기반 사전 로딩(Impact-driven prefetching) 메커니즘을 포함하고, 높은 수요가 예상되는 전문가를 우선적으로 캐싱하는 점수 기반 캐싱(Score-based caching) 전략을 활용하여 캐시 미스를 최소화합니다.
- ***Performance Highlights***: 실험 결과, HybriMoE는 기존의 최첨단 하이브리드 MoE 추론 프레임워크와 비교하여 prefill 단계에서 평균 1.33배, decode 단계에서 1.70배의 속도 향상을 달성했습니다. 특히, 공유 전문가의 활용 및 사전적재 전략을 통해 자원 활용도를 극대화하였습니다.

### [CrossWordBench: Evaluating the Reasoning Capabilities of LLMs and LVLMs with Controllable Puzzle Generation](https://arxiv.org/abs/2504.00043)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.00043.png)

Vote: 7

Authors: Haohan Wang, Langlin Huang, Bill Yuchen Lin, Chengsong Huang, Jiaxin Huang, Jixuan Leng, William W. Cohen

- ***What's New***: CrossWordBench는 LLMs와 LVLMs의 추론 능력을 평가하기 위한 새로운 벤치마크로, 십자말 풀이 퍼즐을 이용하여 텍스트 기반의 단서와 시각적인 그리드 구조의 제약을 모두 만족해야 합니다. 이는 기존의 텍스트 또는 비전-언어 이해 능력만을 평가하던 방식에서 벗어나 멀티모달 제약을 포함한 추론 작업을 평가하는 새로운 방법론을 제시합니다.
- ***Technical Details***: CrossWordBench는 여러 형식의 퍼즐을 생성할 수 있는 조정 가능한 퍼즐 생성 프레임워크를 활용하여, 세 가지 출처의 데이터를 수집하고 퍼즐을 생성합니다: 공개 저장소의 다국어 단어-단서 쌍, 사전 기반 정의, 그리고 기존 벤치마크(예: CommonsenseQA)의 적응된 질문-답변 쌍. 평가 모드는 직접 퍼즐 풀이와 상호작용 모드의 두 가지로 나뉘며, 시각적 출력과 함수 호출을 통한 에이전트 평가의 기초를 제공합니다.
- ***Performance Highlights***: 20개 이상의 모델을 평가한 결과, 추론 LLMs는 비추론 모델에 비해 교차 문자 제약을 효과적으로 활용하여 월등한 성능을 보였습니다. 특히 LVLMs는 OCR 및 그리드 파싱 과정에서 어려움을 겪으며 퍼즐 풀기 성능이 낮음을 보였습니다. 이러한 결과는 현재 LLMs와 LVLMs의 추론 능력의 한계를 명확히 하며, 멀티모달 제약을 활용한 새로운 평가 작업의 가능성을 제공합니다.

### [HiFlow: Training-free High-Resolution Image Generation with Flow-Aligned Guidance](https://arxiv.org/abs/2504.06232)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.06232.png)

Vote: 6

Authors: Yujie Zhou, Xiaoyi Dong, Yuhang Zang, Pan Zhang, Pengyang Ling, Tong Wu, Yuhang Cao, Dahua Lin, Jiazi Bu, Jiaqi Wang

- ***What's New***: HiFlow는 사전 훈련된 흐름 모델(Rectified Flow Models)의 해상도 잠재력을 해제하는 새로운 훈련 오프 접근법입니다. 하이레졸루션 이미지를 생성하기 위해 흐름 정렬 가이던스(Flow-Aligned Guidance)를 도입하여 고해상도 이미지 생성을 위한 독창적인 프레임워크를 제공합니다.
- ***Technical Details***: HiFlow는 가상 기준 흐름(Virtual Reference Flow)을 고해상도 공간에서 구축하여 세가지 측면에서 고해상도 샘플링 흐름을 정렬합니다: 초기화 정렬(Initialization Alignment)로 저주파 일관성을 유지하고, 방향 정렬(Direction Alignment)로 구조를 보존하며, 가속 정렬(Acceleration Alignment)로 세부 사항의 충실도를 유지합니다. 이러한 흐름 정렬 가이던스를 통해 T2I 모델의 해상도 이미지를 향상시킵니다.
- ***Performance Highlights***: HiFlow는 2K와 4K 해상도 이미지 생성에서 우수한 성능을 발휘하여 기존의 최신 방법을 능가합니다. 정량적 평가에서 HiFlow는 FID 및 IS와 같은 이미지 품질과 이미지-텍스트 정렬(CLIP 스코어)에서의 경쟁력을 입증하였으며, 실험 결과는 HiFlow의 효율성을 나타냅니다.

### [Efficient Reinforcement Finetuning via Adaptive Curriculum Learning](https://arxiv.org/abs/2504.05520)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.05520.png)

Vote: 5

Authors: Linxin Song, Taiwei Shi, Tianyi Zhou, Yiyang Wu, Jieyu Zhao

- ***What's New***: 이 논문에서는 강화 학습 튜닝(Reinforcement Finetuning; RFT)의 효율성을 크게 향상시키는 ADARFT(Adaptive Curriculum Reinforcement Finetuning)라는 새로운 방법을 제안합니다. 이 방법은 학습 문제의 난이도를 모델의 최신 보상 신호를 기반으로 동적으로 조정하여, 해결 가능하면서도 도전적 문제에 초점을 맞추어 모델의 학습 효율성을 극대화합니다.
- ***Technical Details***: ADARFT는 Proximal Policy Optimization (PPO)와 같은 기존 RFT 알고리즘에 가벼운 커리큘럼 스케줄러를 추가하며, 보상 함수나 모델 구조를 변경하지 않습니다. 학습 데이터셋의 각 문제는 사전에 난이도가 매겨지며, 모델의 성능에 따라 목표 난이도가 조정되어 학습 중 문제들이 모델의 현재 능력에 맞게 조정됩니다.
- ***Performance Highlights***: ADARFT는 다양한 데이터 분포와 모델 크기에 걸쳐 훈련 효율성과 최종 정확도를 개선했습니다. 특히 불균형 데이터 분포에서 ADARFT는 훈련 스텝 수를 50%까지 줄이고 정확도를 크게 향상시키며, 보다 확장 가능하고 효과적인 RFT 프레임워크를 제공합니다.

### [3D Scene Understanding Through Local Random Access Sequence Modeling](https://arxiv.org/abs/2504.03875)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.03875.png)

Vote: 5

Authors: Klemen Kotar, Rahul Mysore Venkatesh, Wanhee Lee, Honglin Chen, Daniel L. K. Yamins, Jared Watrous, Khai Loong Aw

- ***What's New***: 이 연구는 3D 장면 이해에 대한 새로운 자동회귀 모델인 Local Random Access Sequence(LRAS) 모델을 소개합니다. 이 모델은 Local Patch Quantization과 Random Order Decoding을 사용하여 3D 객체 조작 및 깊이 추정 작업을 수행합니다. 특히 Optical Flow를 중간 표현으로 활용하여 기존 방법들의 한계를 극복하고, 최신의 목표 보기 생성 및 삼차원 객체 조작 기능을 달성하였습니다.
- ***Technical Details***: LRAS 모델은 Local Patch Quantization을 사용해 입력 이미지의 로컬 패치들로 이루어진 코드를 생성합니다. Random Order Decoding은 공간적으로 임의의 순서로 이미지를 디코딩하게 하며, 포인터-콘텐츠 표현을 통해 시퀀스를 예측하는 기법을 도입합니다. LRASRGB 모델은 입력 프레임과 Optical Flow 데이터를 바탕으로 다음 RGB 프레임을 예측하며, LRASFLOW 모델은 카메라 움직임에 따라 Optical Flow 필드와 2.5D 깊이 맵을 예측합니다. 이 모델은 대규모 인터넷 비디오 데이터셋(Big Video Dataset; BVD)에서 사전 훈련되었습니다.
- ***Performance Highlights***: LRAS는 다수의 3D 장면 이해 작업에서 최첨단 성능을 달성하며, 특히 3D 객체 조작 실험에서는 기존 Diffusion 모델들보다 장면 구조, 객체 정체성 및 전역 조명을 잘 보존했습니다. 또한, 새로운 Benchmark인 3DEditBench를 도입하여 실세계 데이터에서의 객체 조작 성능을 증명했고, 자가 지도 깊이 추정에서도 기존 방법들보다 우수한 결과를 기록했습니다.

### [Leanabell-Prover: Posttraining Scaling in Formal Reasoning](https://arxiv.org/abs/2504.06122)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.06122.png)

Vote: 3

Authors: Guorui Zhou, Qi Wang, Xingguang Ji, Di Zhang, Jingyuan Zhang, Yang Yue, Kun Gai, Yahui Liu, Fuzheng Zhang

- ***What's New***: Leanabell-Prover는 자동 정리 증명(Automated Theorem Proving; ATP)의 포스트트레이닝 스케일링을 구현하여 형식적 추론에서 최첨단 성능을 달성합니다. 이 연구는 특히 자연어의 추론 모델에서 보이는 돌파구와 ATP를 일치시키는 데 중점을 두고 있으며, Reinforcement Learning(RL)을 사용하여 Lean 4 컴파일러로부터 리워드를 얻는 방식을 탐구합니다.
- ***Technical Details***: Leanabell-Prover의 기초 모델은 DeepSeek-Prover-v1.5-SFT와 Goedel-Prover-SFT입니다. 이 두 모델에 향상된 초슷트레닝 프레임워크를 적용하여 성능을 개선합니다. 참고로 이 모델들은 7 billion parameters로 이루어져 있습니다. 포스트트레이닝은 복합 데이터 세트를 사용하여 진행되며, 인간의 인지적 행동을 모방하는 데이터가 추가로 포함됩니다. Reinforcement Learning 과정에서는 GRPO(Generalized Policy Optimization; Shao et al., 2024)를 사용하여 Lean 4 증명 보조원으로부터 피드백을 받고 이를 리워드 신호로 사용하게 됩니다.
- ***Performance Highlights***: Leanabell-Prover의 RL 버전은 MiniF2F 벤치마크에서 59.8%의 통과율(pass@32)을 달성해 조합적 데이터와 연속적 트레이닝을 통해 기존 DeepSeek-Prover-v1.5-RL과 Goedel-Prover-SFT에 비해 각각 6.6%와 2.2%의 성능 개선을 이루었습니다. 특히, 증명 생성시에는 신경망의 탐색 능력이 효과적으로 활용되었다는 점에서 RL에 의한 더 큰 성능 이득을 보여줍니다.

### [ProtoGCD: Unified and Unbiased Prototype Learning for Generalized Category Discovery](https://arxiv.org/abs/2504.03755)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.03755.png)

Vote: 1

Authors: Fei Zhu, Xu-Yao Zhang, Shijie Ma, Cheng-Lin Liu

- ***What's New***: Airxiv 논문 'ProtoGCD: Unified and Unbiased Prototype Learning for Generalized Category Discovery'는 기존의 문제를 해결하기 위해 old classes와 new classes를 공동으로 모델링하는 통합된 프로토타입 학습 프레임워크인 ProtoGCD를 제안합니다. 이 연구에서는 학습 목표를 통합하여, old class와 new class 간의 불균형 성능과 편향된 표현 문제를 해결하고자 하며, 새로운 클래스 수를 추정하기 위한 기준도 고안합니다.
- ***Technical Details***: 이 논문에서 제안한 ProtoGCD는 old classes와 new classes를 동일한 특징 공간(shared feature space)에서 학습 가능한 프로토타입(prototypes)으로 모델링하여, 단일 클러스터링 목표를 사용한 비편향된(unbiased) 결과를 얻습니다. 이를 위해 dual-level adaptive pseudo-labeling(DAPL) 메커니즘을 제안하여 보장 바이어스(confirmation bias)를 줄이는 동시에, 보다 적절한 표현을 학습할 수 있도록 두 개의 정규화 용어를 도입합니다. 또한, 실용적 고려를 위해 새로운 클래스의 수를 추정하는 기준을 설계하며, ProtoGCD를 unseen outlier를 탐지할 수 있도록 확장합니다.
- ***Performance Highlights***: ProtoGCD는 일반적인(generic) 및 세분화된(fine-grained) 데이터 세트에서 state-of-the-art 성능을 달성하며, 더 균형 잡힌 정확도를 보입니다. 특히, 기존 방식들에서 노출되었던 old class와 new class 간 불균형 문제를 해결하고, 전반적인 군집 정확도(clustering accuracy)를 상당히 향상시켰습니다. 또한, 이는 open-world 학습 시 발생할 수 있는 unseen classes 탐지에서도 강력한 성능을 발휘합니다.

