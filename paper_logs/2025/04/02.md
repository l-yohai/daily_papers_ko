## Daily Papers (2025-04-02)

### [Any2Caption:Interpreting Any Condition to Caption for Controllable Video Generation](https://arxiv.org/abs/2503.24379)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.24379.png)

Vote: 43

Authors: Pengfei Wan, Xintao Wang, Weicai Ye, Shengqiong Wu, Di Zhang, Shuicheng Yan, Quande Liu, Jiahao Wang, Tat-Seng Chua, Kun Gai, Hao Fei

- ***What's New***: Any2Caption은 제어 가능한 비디오 생성에서 사용자 의도 해석의 병목 현상을 해결하기 위해 다양한 입력 조건을 밀집된, 구조화된 캡션으로 해석하는 혁신적인 프레임워크를 제안합니다. 이는 다중 모달 대형 언어 모델(MLLMs)을 활용함으로써 이루어지며, 비디오 합성 단계와 조건 해석 단계를 분리하여 더 나은 가이드라인을 제공합니다.
- ***Technical Details***: Any2Caption은 텍스트, 이미지, 비디오, 모션 및 카메라 위치와 같은 조건을 처리하는 모듈을 갖춘 MLLM 기반의 보편적 조건 해석기로 설계되었습니다. 또한, 다양한 입력 조건을 캡션으로 변환하여 고품질 비디오 생성을 지원할 수 있는 Any2CapIns라는 대규모 데이터셋을 소개합니다. 이 데이터셋은 337,000개의 사례와 407,000개의 조건을 포함하며, 체계적인 캡션 생성과 사용자 의도 평가를 위한 종합적인 평가 전략을 제공합니다.
- ***Performance Highlights***: Any2Caption은 여러 SOTA(SOTA, State-Of-The-Art) 비디오 생성기와 통합하여 임의 조건 하에서 고품질 비디오 생성을 위한 중요한 역할을 하며, 실험 결과에서 구조화된 캡션이 비디오 생성 품질과 제어성을 일관되게 향상시키는 것을 입증하였습니다. 특히 여러 조합된 조건을 효과적으로 처리하며 사용자의 기대치에 근접한 캡션을 생성하는 데 명확한 이점을 보여주었습니다.

### [JudgeLRM: Large Reasoning Models as a Judge](https://arxiv.org/abs/2504.00050)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.00050.png)

Vote: 31

Authors: Nuo Chen, Bingsheng He, Jiaying Wu, Bryan Hooi, Qingyun Zou, Zhiyuan Hu, Qian Wang

- ***What's New***: JudgeLRM은 복잡한 추론 능력을 요구하는 영역에서 LLM(대형 언어 모델)을 평가자로 활용할 수 있도록 고안된 새로운 접근법입니다. 기존의 지도 기반 미세 조정(SFT) 접근 방식의 한계를 극복하고, 강화 학습(RL)을 통해 평가 작업에 적합한 LLM을 개발한 것이 특징입니다.
- ***Technical Details***: JudgeLRM은 평가 작업의 요구사항을 세밀히 분석하여, 판단 지향의 LLM을 judge-wise, outcome-driven 보상을 통해 훈련합니다. 구조적 보상과 콘텐츠 기반 보상을 결합한 보상 함수를 사용하여, 모델이 구조화된 추론과 정확한 판단을 수행할 수 있도록 합니다. JudgeLRM은 3B 및 7B 파라미터 크기의 모델로 구성되며 Group Relative Policy Optimization(GRPO) 알고리즘을 통해 학습됩니다.
- ***Performance Highlights***: JudgeLRM 모델은 SFT 기반 모델뿐만 아니라 최신의 추론 모델을 초과하는 성능을 보여주었습니다. 특히 JudgeLRM-3B는 GPT-4의 성능을 넘어섰으며, JudgeLRM-7B는 DeepSeek-R1보다 F1 점수에서 2.79% 더 높은 성과를 기록했습니다. 이러한 결과는 높은 수준의 추론을 요구하는 평가 작업에서도 일관된 성능 향상을 입증하였으며, JudgeLRM의 평가 능력을 강화하는 데 있어 RL 접근법의 효과성을 강조합니다.

### [CodeARC: Benchmarking Reasoning Capabilities of LLM Agents for Inductive Program Synthesis](https://arxiv.org/abs/2503.23145)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.23145.png)

Vote: 28

Authors: Anjiang Wei, Naveen Kannan, Kai Yan, Tarun Suresh, Ke Wang, Alex Aiken, Thiago S. F. X. Teixeira, Yuheng Wu, Jiannan Cao

- ***What's New***: CodeARC는 프로그래밍 예제로부터 함수 합성을 요구하는 유도적 프로그램 합성(Inductive Program Synthesis)을 평가하기 위한 새로운 체계를 제공합니다. 기존의 정적 평가 프로토콜의 한계를 극복하고, 숨겨진 목표 함수를 새로운 입력을 통해 쿼리하여 상호 작용을 통해 해결책을 점진적으로 개선하는 상호작용적 환경을 제공합니다.
- ***Technical Details***: CodeARC는 에이전트가 숨겨진 목표 함수와 상호작용하고 차별화 테스트 오라클(Differential Testing Oracle)와 함께 후보 함수를 합성하여 자가 수정을 통해 해결책을 개선하는 형태로 평가를 제공합니다. 이는 발견된 반례로 후보 함수를 지속적으로 수정하여 정확성을 높이며, 1114개의 함수를 포괄하는 일반 목적 유도 프로그램 합성 벤치마크를 구축합니다.
- ***Performance Highlights***: 18개의 모델을 평가한 결과, OpenAI의 o3-mini 모델이 최고 성능을 보였으며, 성공률 52.7%를 기록했습니다. LLaMA-3.1-8B-Instruct 모델을 의도적 합성 추적을 사용하여 미세 조정한 경우 성능이 최대 31% 상대적으로 향상되었습니다. 이러한 결과는 현재 모델이 유도적 추론과 코드 합성에서 얼마나 어려움을 겪고 있는지를 보여줍니다.

### [Exploring the Effect of Reinforcement Learning on Video Understanding: Insights from SEED-Bench-R1](https://arxiv.org/abs/2503.24376)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.24376.png)

Vote: 24

Authors: Yuying Ge, Rui Wang, Lu Qiu, Yixiao Ge, Ying Shan, Xihui Liu, Yi Chen

- ***What's New***: SEED-Bench-R1은 비디오 이해에서 멀티모달 대형 언어 모델(Multimodal Large Language Models; MLLMs)의 사후 훈련 방법을 체계적으로 평가하기 위해 설계된 벤치마크입니다. SEED-Bench-R1은 실제 영상과 복잡한 일상 계획 작업을 포함하며, 일반적인 인지와 추론을 요구합니다. 이 벤치마크를 통해 RL(강화 학습; Reinforcement Learning)이 주어진 데이터 내 및 범용 질문에서의 성능 향상에서 SFT(지도 세부 조정; Supervised Fine-Tuning)를 능가하는지를 평가합니다.
- ***Technical Details***: SEED-Bench-R1은 현실적인 일상 활동을 포착하는 자기중심 비디오에서 시각적 데이터를 활용합니다. 이 벤치마크는 3단계의 일반화 평가 계층 구조를 가지고, 쉽게 검증 가능한 정답을 제공하는 대규모 훈련 데이터세트를 포함합니다. 기준 모델로 Qwen2-VL-Instruct-7B를 사용하여 RL 방법인 GRPO(Group Relative Policy Optimization)와 SFT 간의 성능을 비교하였으며, RL이 데이터 효율성을 증대시키고 특히 비전통적인 시나리오에서 우수한 일반화 능력을 보여줍니다.
- ***Performance Highlights***: 실험 결과 RL을 통한 MLLMs는 범용 비디오 이해 벤치마크(LongVideoBench)에서 SFT보다 우수한 성능을 보였으며, 특히 비전통적인(out-of-distribution; OOD) 시나리오에서 큰 차이를 보였습니다. Table 2와 Table 3에서 보듯이, RL을 통해 훈련된 모델은 L1과 OOD(L2, L3) 시나리오에 걸쳐 향상된 성능을 나타냈습니다. 그러나 RL 훈련 모델은 종종 시각적 단서들을 무시하거나 이해의 논리적 일관성을 결여하는 등 한계를 보였습니다.

### [Z1: Efficient Test-time Scaling with Code](https://arxiv.org/abs/2504.00810)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.00810.png)

Vote: 17

Authors: Yilun Zhao, Yinghao Wu, Zhaojian Yu, Arman Cohan, Xiao-Ping Zhang

- ***What's New***: Z1 모델은 코드와 관련된 추론 경로를 통해 대형 언어 모델(LLMs)에서 테스트 시 컴퓨팅 규모 확장을 효율적으로 구현하는 새로운 방법을 제시합니다. 이 방법은 추론 토큰을 절감하면서도 성능을 유지하도록 도와줍니다.
- ***Technical Details***: Z1 모델은 107K개의 간단하고 복잡한 코딩 문제와 그에 따른 짧고 긴 해결 경로로 구성된 Z1-Code-Reasoning-107K 데이터세트로 학습됩니다. 이 모델은 Shifted Thinking Window라는 새로운 방법을 도입하여 맥락 구분 태그(e.g., <think>...</think>)를 제거하고 추론 토큰의 최대치를 한정함으로써 과도한 추론을 방지합니다.
- ***Performance Highlights***: Z1-7B 모델은 다양한 추론 작업에서 R1-Distill-Qwen-7B와 비슷한 성능을 나타내면서 평균 추론 토큰의 약 30%로 효율적인 테스트 시 확장을 보여줍니다. 또한 Z1-7B 모델은 코드 경로로만 미세 조정된 상태에서도 다른 추론 작업에서 높은 일반화 성과를 보입니다(GPQA Diamond에서 47.5% 성과).

### [Open-Qwen2VL: Compute-Efficient Pre-Training of Fully-Open Multimodal LLMs on Academic Resources](https://arxiv.org/abs/2504.00595)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.00595.png)

Vote: 16

Authors: Xifeng Yan, Linjie Yang, Yu Tian, Weizhi Wang, Heng Wang

- ***What's New***: Open-Qwen2VL은 완전히 오픈 소스로 개발된 2B-parameter 크기의 새로운 멀티모달 대형 언어 모델(Multimodal Large Language Model; MLLM)입니다. 이 모델은 매우 효율적인 방식으로 약 29M의 이미지-텍스트 쌍에서 학습되었으며, 뛰어난 학습 효율성을 보장합니다. 오픈소스로 출시된 모든 작업물에는 데이터 필터링 기술부터 훈련 코드베이스까지 포함되며, 이는 멀티모달 LLMs의 '완전한 개방성'의 새로운 기준을 제시합니다.
- ***Technical Details***: Open-Qwen2VL은 저해상도에서 고해상도로의 동적 이미지 해상도 적용과 멀티모달 시퀀스 패킹(multimodal sequence packing)을 통해 훈련 효율성을 크게 향상시켰습니다. 8xA100-40G GPU를 사용하여 대략 5B의 멀티모달 토큰으로 UCSB에서 학습을 수행하였으며, 데이터는 MLLM 기반의 필터링 기법과 전통적인 CLIP 기반의 필터링 방법을 사용하여 고품질 데이터로 엄선되었습니다. 또한, 모델 아키텍처에는 적응형 평균 풀링 레이어가 포함되어 있어 시각적 토큰을 144개로 축소해 학습 속도를 높였습니다.
- ***Performance Highlights***: 최종적으로 명령어 튜닝된 Open-Qwen2VL은 최신 사례 연구에서 매우 높은 효율성을 보여줍니다. 다양한 멀티모달 벤치마크에서 Partially-open SOTA MLLM인 Qwen2-VL-2B를 능가하는 성능을 보였으며, 이 모든 성과는 Qwen2-VL의 멀티모달 프리트레이닝 토큰의 단 0.36%만 사용하여 이뤄진 것입니다.

### [Command A: An Enterprise-Ready Large Language Model](https://arxiv.org/abs/2504.00698)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.00698.png)

Vote: 16

Authors: Utsav Garg, Arnav Kumar Jain, JJ Jordan, Omar Darwiche Domingues, John Dang, Walter Beller-Morales, Victoria Spooner, Tom Hosking, Jeffrey Li, Yixuan Su, Joan Devassy, Viraat Aryabumi, Marysia Winkels, Kevin Luong, Ben Cyrus, Sudip Roy, Mathieu Simard, Edmond Wen, Roman Castagné, Matthieu Geist, Kris Cao, Nathan Grinsztajn, Shauna Nehra, Manoj Govindassamy, Siddhartha Rao Kamalakara, Aakanksha, Patrick Gu, Wojciech Galuba, Eugene Cho, Aleksandra Piktus, Alexis Chevalier, Justin T. Chiu, Ritika Jain, Trushant Kalyanpur, Marina Machado, Brenda Malacara Lopez, Ava Spataru, Aurélien Rodriguez, Zhenyu Zhao, Eujeong Choi, Vivek Muppalla, Dennis Padilla, Wei-Yin Ko, Giannis Chatziveroglou, Andrew Berneshawi, Irem Ergün, Edward Kim, Michael Lasby, Zahara Aviv, Dominic McLoughlin, Hongyu Chen, Sanal Shivaprasad, Devon Crouse, Michael Kozakov, Alexandre Bérard, Lucas Fayoux, Pierre Clavier, Marwan Ahmed, Tomas Goldsack, Andrew Poulton, Minjie Xu, David Cairuz, Tim Hawes, Miguel Ramos, Mohammad Gheshlaghi Azar, Shijian Wang, Abdullah Elkady, Jingyi He, Samuel Cahyawijaya, Olivia Markham, Sophia Althammer, Billy Trend, Björn Bebensee, Boyu Fan, Maximilian Mozes, Jason Ozuzu, Matthias Gallé, Laura Ruis, Tan Yi-Chern, Lukas Mach, Jay Alammar, Ahmet Üstün, Jeremy Pekmez, Jon Ander Campos, Rishit Dholakia, Priyanka Sen, Florian Strub, Saurabh Baji, Jennifer Tracey, Louise Rust, Ana Cismaru, Tom Sherborne, Andres Felipe Cruz-Salinas, Josh Netto-Rosen, Alex Wang, Victor Machado Gonzaga, Raphaël Avalos, Wojciech Kryściński, Lucas Crawhall-Stein, Pierre Richemond, Ali Edalati, Nikolas Gritsch, Joon Kim, Adrien Morisot, Chen Xia, Neeral Beladia, Isha Satyakam, Yash Chandak, Shangmin Guo, Alexandre Matton, Gokce Keskin, Evren Tumer, Aidan Gomez, Tom Kocmi, Sara Hooker, Trisha Starostina, Kilian Haefeli, Jessica Xie, Hemant Jain, Madeline Jenkins, Daniel Ohashi, Gloria Park, Beyza Ermis, Lidiya Murakhovska, Joanne Magbitang, Bharat Venkitesh, Daniel D'souza, Hemangani Nagarajan, Diane Chang, Sholeh Sepehri, Eugene Choi, Hangyu Lin, Anirudh Shrinivason, Adi Bongale, Sungjin Hong, Seraphina Goldfarb-Tarrant, Preethi Seshadri, Inna Shteinbuk, Jesse Willman, Zhoujie Zhao, Leila Chan Currie, Hugo Dalla-Torre, Jozef Mokry, Shubha Raghvendra, Dennis Aumiller, James Owers-Bardsley, Nithya Govindarajan, Sylvie Chang Shi, Julián Cendrero, Ivan Zhang, Dhruti Joshi, Ace Eldeib, Dwarak Talupuru, Alexandre Barbet, Acyr Locatelli, Shaan Desai, Naomi White, Julia Kedrzycki, Sebastian Hofstätter, Kelly Marchisio, Saurabh Dash, Cécile Robert-Michon, Antoine Debugne, Autumn Moulder, Laura Penstone, Ekagra Ranjan, Jonathan Li, Théo Dehaze, Aryan Mann, Vladyslav Shmyhlo, Max Bartolo, Sam Braun, Henry Conklin, Alice Schoenauer Sebag, Eric Hu, Anubhav Sachan, Stephanie Howe, Patrick Lewis, Alekhya Nandula, Alejandro Salamanca, Amir Shukayev, Bowen Yang, Raymond Ma, Sammie Bae, Tim Chung, Cassie Cao, Kyle Duffy, Hisham Nasir, Team Cohere, Olivia Lasche, Ye Shen, Claire Cheng, Matt Bobkin, Komal Kumar Teru, Elena Tommasone, Harry Moynehan, Alex McKinney, William Darling, Renjie Huang, Case Ploeg, Yannis Flet-Berliac, Jason Jung, Donglu Wang, Youran Qi, Maxime Voisin, Volkan Cirik, Arash Ahmadian, Marzieh Fadaee, Rod Hajjar, Sarah Elsharkawy, Sander Land, Pat Verga, Maxime Brunet, Kailash Karthik Saravanakumar, Eugene Tarassov, Anna Bialas, Justin Lee, Phil Blunsom, David Venuto, Nick Jakobi, Yazeed Alnumay, Sam Passaglia, Ella Snyder, Nick Frosst, Arkady Arkhangorodsky, Jimin Sun

- ***What's New***: Command A는 실제 기업에서의 사용 사례에 최적화된 강력한 대형 언어 모델로 개발되었습니다. 이 모델은 에이전트 최적화 및 다국어 지원 기능을 갖추고 있으며, 23개의 세계 비즈니스 언어를 지원합니다. 또한 효율성과 고성능을 균형 잡은 새로운 하이브리드 아키텍처를 채택하고 있습니다. 정보 검색 기반 생성(Retrieval Augmented Generation; RAG) 및 도구 사용을 통해 복잡한 비즈니스 프로세스를 자동화할 수 있습니다.
- ***Technical Details***: Command A는 분산형 훈련 접근 방식을 통해 개발되었으며, 자가 개선 알고리즘 및 모델 병합 기법을 포함합니다. 이 모델은 111B 파라미터를 갖춘 것으로, 성능을 극대화하기 위한 데이터 및 아키텍처 최적화, 전문가 수준의 성능을 낼 수 있는 모델 병합 기반 접근 방식을 갖추고 있습니다. 또한, 23개의 주요 언어를 지원하며 다국어 설정에서 우수한 성능을 발휘합니다.
- ***Performance Highlights***: Command A는 다양한 공공 벤치마크에서 동급 최고의 결과를 나타내며, 특히 70B 모델과 같은 크기에서 최고 성능을 자랑합니다. 제안하는 모델은 기업 관련 에이전트 벤치마크인 Taubench에서 높은 성능을 보이며, 단 두 개의 A100또는 H100 GPU에서 실행이 가능하여 경쟁 모델에 비해 낮은 계산 오버헤드가 특징입니다. GPT-4o와의 비교에서 156 tokens/sec의 속도를 보여 1.75배 높은 성능을 기록했습니다.

### [Agent S2: A Compositional Generalist-Specialist Framework for Computer Use Agents](https://arxiv.org/abs/2504.00906)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.00906.png)

Vote: 15

Authors: Xin Eric Wang, Jiachen Yang, Vincent Tu, Ang Li, Kyle Wong, Saaket Agashe

- ***What's New***: Agent S2는 컴퓨터 사용 에이전트(computer use agents)가 서로 다른 일반 모델(generalist models)과 특화 모델(specialist models)을 활용해 인지 책임을 분배하도록 설계된 새로운 구성적 프레임워크(compositional framework)입니다. 이 시스템은 GUI 요소의 정확한 현지화를 위한 새로운 Mixture-of-Grounding 기술과 변화하는 관측에 동적으로 대응하기 위한 Proactive Hierarchical Planning을 도입하였습니다.
- ***Technical Details***: Agent S2는 기획(management), 실행(execution), 그리고 구체적 현지화에 대해 서로 다른 일반 및 특화 모듈로 인지 작업을 분배하는 시스템입니다. Mixture-of-Grounding 기법은 GUI의 정확한 요소 위치 확인을 가능하게 하고, Proactive Hierarchical Planning은 임의의 길이로 설정된 행동 계획을 동적으로 재조정하여 적응성을 향상시킵니다. 이러한 구성적 접근은 단일 모델에 의존한 기존 방식의 제약을 극복할 수 있습니다.
- ***Performance Highlights***: Agent S2는 OSWorld의 15 및 50단계 평가에서 각각 18.9%, 32.7%의 상대적 성능 개선을 기록하며, Claude Computer Use와 UI-TARS와 같은 기존 에이전트를 능가했습니다. 또한, WindowsAgentArena와 AndroidWorld 벤치마크에서 각각 52.8% 및 16.5%의 상대적 성능 향상으로 다른 운영 체제 및 응용 프로그램으로 확장되는 일반화 능력을 확인했습니다.

### [GeometryCrafter: Consistent Geometry Estimation for Open-world Videos with Diffusion Priors](https://arxiv.org/abs/2504.01016)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.01016.png)

Vote: 15

Authors: Xiangjun Gao, Song-Hai Zhang, Tian-Xing Xu, Xiaoyu Li, Wenbo Hu, Ying Shan

- ***What's New***: GeometryCrafter는 개방형 비디오에서 고품질의 시간에 걸쳐 일관된 포인트 맵(Point Maps)을 추론하는 혁신적인 프레임워크를 제안합니다. 이로써 3D/4D 재구성과 깊이 기반 비디오 편집 및 생성 등 다양한 응용 분야에서 활용 가능합니다.
- ***Technical Details***: GeometryCrafter의 핵심은 비디오 디퓨전 모델(Video Diffusion Model)과 포인트 맵 VAE를 통합하여, 시간적으로 일관된 포인트 맵을 생성하는 것입니다. 포인트 맵은 로그 공간 심도를 기반으로 한 분리 표현(Disentangled Representation)으로 인코딩하며, 잔차 인코더(Residual Encoder)를 통해 추가 정보를 잠재 공간(Latent Space)에 저장합니다. 이를 통해 영상의 고유한 분포를 활용한 강력한 제로샷 일반화를 달성합니다.
- ***Performance Highlights***: GeometryCrafter는 다양한 데이터셋에서 최첨단의 3D 정확성과 시간적 일관성, 일반화 능력을 입증하며, 기존 방법들보다 상당한 성능 향상을 보여줍니다. 특히, Monkaa와 Sintel과 같은 도전적인 데이터셋에서 큰 성능 향상을 이뤄냈습니다. 실험 결과, 제안된 방법은 3D/4D 포인트 클라우드 복원 및 카메라 포즈 추정에 있어 탁월한 성능을 보입니다.

### [MixerMDM: Learnable Composition of Human Motion Diffusion Models](https://arxiv.org/abs/2504.01019)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.01019.png)

Vote: 13

Authors: German Barquero, José García-Rodríguez, Pablo Ruiz-Ponce, Cristina Palmero, Sergio Escalera

- ***What's New***: MixerMDM은 사전학습된 텍스트 조건부 인간 모션 확산 모델(Text-Conditioned Human Motion Diffusion Models)을 결합하기 위한 최초의 학습 가능한 모델 조합 기술입니다. 특별한 조건에 따라 여러 모션 확산 모델의 디노이징 프로세스를 동적으로 결합하여 더욱 정밀한 인간 상호작용을 생성할 수 있습니다.
- ***Technical Details***: MixerMDM은 사전학습된 모델에서 생성된 모션을 혼합하여 새로운 모션 시퀀스를 생성합니다. 이 과정은 디노이징 체인의 각 단계마다 수행됩니다. 텍스트 조건부 모델에서 입력으로 받은 모션은 Transformer 인코더를 통해 고차원 표현으로 변환되며, 이 표현은 Multi-Layer Perceptron(MLP)으로 처리되어 혼합 가중치(Mixing Weights)를 출력합니다. Mixer는 다양한 혼합 가중치 변형(Global, Temporal, Spatial, Spatio-Temporal)을 지원하여 각각의 모델 출력의 특수성에 맞게 동적이고 독특한 결합을 할 수 있습니다.
- ***Performance Highlights***: MixerMDM은 이전의 고정 가중치 방식과 달리 학습된 혼합 전략을 통해 이전 방식보다 높은 혼합 품질을 보여줍니다. 이 방법은 탁월한 조정력으로 여러 조건에 맞춰 모션 결합이 가능하며, 제안된 평가지표에서 높은 적합도를 기록하며 상위 결과를 달성했습니다.

### [Multi-Token Attention](https://arxiv.org/abs/2504.00927)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.00927.png)

Vote: 13

Authors: Olga Golovneva, Jason Weston, Tianlu Wang, Sainbayar Sukhbaatar

- ***What's New***: 이번 연구에서는 멀티 토큰 어텐션(Multi-Token Attention; MTA) 기법을 제안하여, 대형 언어 모델(LLMs)이 여러 개의 쿼리와 키 벡터를 동시에 조건화하여 더 정확한 주의를 가능하게 합니다. 기존의 싱글 토큰 어텐션의 한계를 넘어서, 보다 풍부하고 미세한 정보를 활용하여 콘텍스트 내 관련된 내용을 보다 정확하게 식별할 수 있습니다.
- ***Technical Details***: MTA는 기존 어텐션 메커니즘의 일부를 수정하여 구현됩니다. 키-쿼리 조건화(key-query convolution), 헤드 혼합 컨볼루션(head mixing convolution), 그룹 정규화(group normalization)를 도입하여 어텐션 가중치를 다중 쿼리 및 키 벡터 그리고 여러 헤드 간에 결합하는 방식입니다. 이러한 새로운 접근법은 특히 긴 문맥 내에서 정확한 정보를 검색하는 작업에 유용하게 설계되었습니다.
- ***Performance Highlights***: MTA는 다양한 인기 벤치마크들에서 기존의 트랜스포머 기반 모델을 능가하는 성능을 보여줍니다. 특히 긴 문맥에서의 정보 검색이 필요한 작업(예: Needle-in-the-Haystack, BabiLong)에서 MTA는 보다 높은 정확도와 효율성을 제공합니다. 이러한 실험은 MTA가 현존하는 모델 대비 보다 강화된 성능을 제공함을 입증합니다.

### [Towards Trustworthy GUI Agents: A Survey](https://arxiv.org/abs/2503.23434)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.23434.png)

Vote: 12

Authors: Wenlin Yao, Yucheng Shi, Wenhu Chen, Wenhao Yu, Ninghao Liu

- ***What's New***: 이 논문은 GUI Agent(사용자 인터페이스 에이전트)의 신뢰성을 강화하기 위한 주요 방향성을 제시합니다. 특히 대규모 기반 모델(Large Foundation Models; LFM)을 사용하는 GUI Agent의 보편성과 그에 따른 보안, 프라이버시, 투명성 등의 이슈에 대해 포괄적 조사를 수행했습니다.
- ***Technical Details***: GUI Agent는 다중 모달 입력을 해석하고 동적 인터페이스에서 정확한 동작을 수행할 수 있어야 합니다. 이러한 Agent는 클릭과 화면 해석을 통해 GUI와 상호작용하며, 모바일 네비게이션, 소프트웨어 테스트 등 다양한 응용 프로그램에 적용될 수 있습니다. 특히, 'ST-WebAgentBench'나 'WebPI'와 같은 평가 프레임워크를 통해 실제 웹 기반 환경에서의 정책 준수 및 리스크 완화 전략을 강조하고 있습니다.
- ***Performance Highlights***: 현대의 GUI Agent들은 보안 취약성과 환경적 요인에 의한 리스크에 여전히 민감합니다. 예를 들어, 'Adversarial Image Perturbations'는 인식 모듈을 오도할 수 있으며, 웹 페이지의 악성 요소는 Agent의 행동을 조장할 수 있습니다. 더욱이, GUI Agent는 설명력과 안전성 측면에서 매우 복잡하고 동적인 환경에 적응해야 하며, 이것은 많은 연구와 개선이 필요함을 시사합니다.

### [Recitation over Reasoning: How Cutting-Edge Language Models Can Fail on Elementary School-Level Reasoning Problems?](https://arxiv.org/abs/2504.00509)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.00509.png)

Vote: 12

Authors: Zheyu Wang, Kai Yan, Xuesong Yao, Jiecao Chen, Zhengyin Du, Yufei Xu, Xiaowen Guo

- ***What's New***: RoR-Bench는 최신 LLM(Large Language Models)이 초등학교 수준의 간단한 추론 문제에서 조차도 문제의 조건이 미묘하게 변형될 때 심각한 암기(recitation) 행동을 드러내는 새로운 멀티모달 벤치마크입니다. 벤치마크는 158개의 텍스트 문제와 57개의 이미지 문제로 구성되어 있으며, 모든 문제는 사람에 의해 작성된 간단한 추론 문제와 조건이 미묘하게 변형된 쌍으로 편집되었습니다.
- ***Technical Details***: RoR-Bench는 문제들의 원래 버전과 변형된 버전을 포함하여 각 쌍이 다른 해결 패러다임과 답을 가지도록 설계되었습니다. 데이터는 온라인 블로그와 어린이를 위한 추리 퍼즐 컬렉션에서 수집되었으며, 문제의 수정은 해결 패러다임을 완전히 다르게 하면서 모호성이 없도록 하는 원칙을 따르고 있습니다. 실험은 OpenAI-o1 및 DeepSeek-R1과 같은 최신 LLM을 대상으로 수행되었으며, 'Forced Correct' (FC) 프롬프트와 변형된 여러 문제를 few-shots으로 추가하며 문제 인식 능력을 측정했습니다.
- ***Performance Highlights***: 벤치마크 결과, 변형된 문제에 대해 LLM의 평균 성능은 원래 문제에 비해 50% 이상 감소했습니다. 또한, FC 프롬프트를 사용해도 변형된 문제에서 성능 저하가 45% 이상 발생했으며, 이러한 문제는 LLM 정렬 기술만으로는 쉽게 해결되지 않는다고 결론지었습니다. 수정된 문제별로 '해결 불가' 문제 인식 능력도 매우 낮아, 현재 LLM이 추론보다는 암기에 지나치게 의존하고 있음을 보여줍니다.

### [OmniMMI: A Comprehensive Multi-modal Interaction Benchmark in Streaming Video Contexts](https://arxiv.org/abs/2503.22952)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.22952.png)

Vote: 11

Authors: Yuxuan Wang, Tong Wu, Yueqian Wang, Zilong Zheng, Bo Chen, Dongyan Zhao

- **What's New**: 이 논문은 스트리밍 비디오 상황에서 멀티모달 언어 모델들의 상호작용 능력을 평가하기 위한 OmniMMI라는 포괄적인 멀티모달 상호작용 벤치마크를 소개하고 있습니다. OmniMMI는 스트리밍 비디오 이해와 능동적 추론이라는 두 가지 중요한 도전 과제를 포함하고 있습니다.
- **Technical Details**: OmniMMI는 YouTube와 오픈소스 비디오 오디오 데이터를 사용하여 평균 324초 길이의 1,121개 비디오와 2,290개의 질문으로 구성되어 있으며, 다중 턴 의존성 추론을 포함한 다양한 점검 과제로 멀티턴 질의를 평가합니다. 제안된 M4(Multi-modal Multiplexing Modeling) 프레임워크는 스트리밍 모델의 효율적인 추론을 가능하게 하고, 능동적 턴테이킹(Turn-taking)와 능동적 응답 생성을 개선합니다.
- **Performance Highlights**: 기존의 멀티모달 대형 모델(Multi-modal Large Language Models; MLLMs)은 비디오-오디오 상호작용에서 상당한 도전에 직면했으며, 멀티턴 작업에서는 특히 제약이 있었습니다. 제안된 M4는 비록 경량이지만 능동적 작업과 실시간 상호작용 처리에서 유의미한 성능 개선을 보였습니다.

### [Harnessing the Reasoning Economy: A Survey of Efficient Reasoning for Large Language Models](https://arxiv.org/abs/2503.24377)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.24377.png)

Vote: 11

Authors: Jiahao Qiu, Hongru Wang, Jianhui Pang, Rui Wang, Heng Ji, Derek Fai Wong, Shudong Liu, Boyang Xue, Kam-Fai Wong, Yi Chen

- ***What's New***: 이 논문은 대형 언어 모델(LLMs)의 합리적인 추론 능력을 분석하는 최초의 포괄적인 조사를 제시합니다. '추론 경제(reasoning economy)'라는 개념을 도입하여 성능(benefits)과 계산 비용(budgets) 간의 균형을 탐구합니다.
- ***Technical Details***: 이 논문에서는 LLMs의 포스트 트레이닝(post-training)과 테스트 시간 추론(test-time inference) 단계에서의 성능과 비효율성을 분석합니다. 구체적으로, 비효율적 모델 행동(inefficient model behavior)의 원인과 다양한 추론 패턴을 분석하고 추론 경제를 달성하기 위한 잠재 솔루션을 제공합니다. 행동 규제(behavior regulation)를 통해 불필요한 추론을 줄이고 데이터와 알고리즘을 포함한 포스트 트레이닝 과정에서 모델의 성능을 향상시키는 방법을 제시합니다.
- ***Performance Highlights***: LLMs는 다양한 추론 작업에 대해 뛰어난 성능을 보여주지만, 복잡한 문제에서는 느리고 깊은 추론에 의존해야 하는 경우가 있어 계산 비용이 증가합니다. 특히 테스트 시간 방법론은 수천 번 반복된 샘플링을 통해 LLM 성능을 크게 향상시킬 수 있음을 보여주었습니다. 그러나, 모든 작업이 이러한 깊은 추론을 필요로 하지 않으며 이는 자원 낭비로 이어질 수 있습니다.

### [When To Solve, When To Verify: Compute-Optimal Problem Solving and Generative Verification for LLM Reasoning](https://arxiv.org/abs/2504.01005)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.01005.png)

Vote: 10

Authors: Aditya Grover, Arian Hosseini, Anna Rohrbach, Marcus Rohrbach, Kai-Wei Chang, Nishad Singhi, Hritik Bansal

- ***What's New***: 이 논문은 대형 언어 모델(LLMs)의 추론 능력을 향상시키기 위한 테스트 시 계산 자원의 배분 전략에 관한 새로운 접근을 제시합니다. 특히, '자기 일관성(Self-Consistency; SC)'과 '생성 보상 모델(Generative Reward Models; GenRM)'의 비교를 통해 계산 자원 최적화에 대한 새로운 통찰력을 제공합니다.
- ***Technical Details***: 이 논문에서는 SC와 GenRM가 동일한 계산 자원 하에서 다양한 모델 및 데이터셋을 통해 성능을 비교합니다. SC는 다수결 투표를 기반으로 한 해결책을 선택하는 반면, GenRM은 일련의 생각(Chain-of-Thought)을 생성하여 각 해결책을 검증합니다. 주어진 테스트 시간 계산 자원 하에서 두 접근방식의 성능을 비교하며, GenRM이 SC에 비해 실제로 더 많은 자원을 필요로 한다는 것을 발견했습니다.
- ***Performance Highlights***: 저자들은 SC가 낮은 계산 예산에서 GenRM보다 더 효율적임을 보여주었으며, GenRM은 더 높은 예산에서만 우수한 성능을 발휘합니다. 예를 들어, GenRM이 SC와 성능을 맞추기 위해서는 최대 8x의 계산력을 필요로 하며, 3.8%의 성능 향상을 위해서는 128x의 계산력이 필요합니다. 이 결과는 다양한 모델군과 크기, 그리고 추론 과제에서 일관성을 보였습니다.

### [Efficient LLaMA-3.2-Vision by Trimming Cross-attended Visual Features](https://arxiv.org/abs/2504.00557)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.00557.png)

Vote: 9

Authors: Wooksu Shin, Jaeyeon Kim, Yong Jae Lee, Ki-Ung Song, Donguk Lim, Bo-Kyeong Kim, Seungmin Yang, Tae-Ho Kim, Jewon Lee

- **What's New**: 이 논문에서는 크로스 어텐션 기반의 대형 비전-언어 모델(Large Vision-Language Models; LVLMs)의 연산 효율성을 향상시키기 위해 'Trimmed Llama'라는 혁신적인 방법을 소개합니다. 이 방법은 크로스 어텐션 맵의 스파시티(sparsity)를 활용해 불필요한 시각적 특징을 제거함으로써, 추가 훈련 없이도 추론 지연 시간과 메모리 사용량을 줄일 수 있습니다.
- **Technical Details**: Trimmed Llama는 크로스 어텐션 레이어 내에서 시각적 특징의 반복적이며 일관된 스파스 패턴을 활용하여 중요도가 낮은 시각적 특징을 필터링합니다. 첫 번째 크로스 어텐션 레이어에서 주의(attention) 가중치를 기준으로 불필요한 시각적 특징을 제거하고, 이를 통해 키-벨류(KV) 캐시의 크기를 줄이고 계산 비용을 절감합니다. 이 접근 방식은 모듈화되어 추가 훈련 없이 다른 대형 비전-언어 모델에 적용할 수 있습니다.
- **Performance Highlights**: 제안된 방법은 다양한 벤치마크에서 40%에서 50%의 시각적 특징만을 사용하면서도 원래 모델과 동등한 성능을 유지하는 것으로 나타났습니다. 또한, 대용량 배치(batch)에서 KV 캐시 메모리를 효과적으로 줄이는 등 높은 처리량 조건에서 탁월한 효율성을 달성했습니다.

### [Scaling Language-Free Visual Representation Learning](https://arxiv.org/abs/2504.01017)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.01017.png)

Vote: 9

Authors: Yann LeCun, Nicolas Ballas, Michael Rabbat, Shengbang Tong, Amir Bar, Saining Xie, Koustuv Sinha, David Fan, Xinlei Chen, Zhuang Liu, Jiachen Zhu

- ***What's New***: 이 논문은 언어가 아닌 방식으로 전적으로 시각적인 자기지도학습(Self-Supervised Learning; SSL)을 통해 대규모 시각 표현을 학습하는 방법을 소개합니다. SSL은 CLIP와 같은 언어 감독 방법과 같은 성능을 시각적 질문 응답(Visual Question Answering; VQA) 및 다양한 컴퓨터 비전 벤치마크에서 달성할 수 있음을 보여주며, 언어 감독 없이도 시각 중심의 표현 학습이 가능하다는 새로운 가능성을 열어줍니다.
- ***Technical Details***: 연구진은 SSL 및 CLIP 모델을 동일한 MetaCLIP 데이터를 사용하여 교육하고 VQA를 비전 인코더의 다양한 테스트베드로 활용했습니다. 1억 개 이상의 웹 이미지 데이터를 사용하여 SSL 모델을 트레이닝했으며, 이 방법이 모델 용량 추가와 데이터 스케일링 측면에서 더욱 나은 성능 확장을 보여준다는 것을 발견했습니다. 특히, 이 연구에서는 다양한 VQA 과제에서 SSL이 CLIP와 유사한 성능을 달성함을 보여줍니다.
- ***Performance Highlights***: SSL 모델들은 데이터와 모델 용량 측면에서 CLIP 모델보다 더 잘 확장됩니다. 모델 파라미터를 1B에서 7B로 확장하였을 때, SSL의 성능은 눈에 띄게 향상되었으며, VQA의 모든 평가 영역에서 CLIP와 비슷하거나 더 나은 결과를 보여주었습니다. 특히, 텍스트 관련 작업인 OCR & Chart 이해 영역에서 언어 감독 없는 SSL 모델이 뛰어난 성과를 보였습니다.

### [Landscape of Thoughts: Visualizing the Reasoning Process of Large Language Models](https://arxiv.org/abs/2503.22165)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.22165.png)

Vote: 8

Authors: Xuan Li, Zhanke Zhou, Jian Tang, Bo Han, Mikhail Galkin, Zhaocheng Zhu, Xiao Feng, Sanmi Koyejo

- ***What's New***: Landscape of Thoughts는 대형 언어 모델(Large Language Models; LLMs)의 연쇄 사고(Chain of Thought; CoT)와 기타 단계별 추론 알고리즘의 추론 경로를 시각화하는 첫 번째 도구입니다. 이를 통해 사용자들은 LLMs의 추론 패턴을 성공 및 실패 사례에서 모두 발견할 수 있습니다.
- ***Technical Details***: 이 도구는 t-SNE를 사용하여 상태를 이차원 공간에 시각화합니다. 각 상태는 모든 답안 선택지와의 거리로 특징지어지는 피처 벡터로 나타나며 perplexity metric에 따라 측정됩니다. 이를 통해 사용자들은 모델 추론의 불안정한 패턴인 낮은 일관성과 높은 불확실성을 발견할 수 있습니다.
- ***Performance Highlights***: 도구를 사용하여 다양한 모델 크기 및 디코딩 알고리즘을 평가한 결과, 더 큰 모델일수록 보다 빠른 수렴 속도로 인해 더 높은 정확도를 보였으며, 일관성은 증가하고 불확실성과 perplexity는 감소했습니다. 이는 대형 모델이 더 많은 지식을 활용하여 자신감 있는 결론을 도출할 수 있음을 시사합니다.

### [AdaMMS: Model Merging for Heterogeneous Multimodal Large Language Models with Unsupervised Coefficient Optimization](https://arxiv.org/abs/2503.23733)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.23733.png)

Vote: 8

Authors: Fei Huang, Chi Chen, Xiaochen Wang, Maosong Sun, Zhifang Sui, Yiyang Du, Jiabo Ye, Peng Li, Yiru Wang, Ji Zhang, Yang Liu, Ming Yan

- ***What's New***: 이 논문에서는 이질적인 멀티모달 대형 언어 모델(Heterogeneous Multimodal Large Language Models; HMLLMs)을 비지도 하이퍼파라미터 최적화 기법으로 병합하는 새로운 접근 방식인 AdaMMS를 제안합니다. AdaMMS는 다양한 아키텍처를 가진 모델들도 병합할 수 있도록 설계되어, 비지도 학습 환경에서도 모델 성능을 최적화할 수 있습니다.
- ***Technical Details***: AdaMMS는 매핑(Mapping), 병합(Merging), 탐색(Searching) 세 단계로 이질적인 MLLMs를 병합합니다. 첫째, 매핑 단계에서는 다른 아키텍처를 가진 MLLMs 간에 매핑 기능을 설계합니다. 둘째, 병합 단계에서는 모델 가중치에 대한 선형 보간법을 적용하여 이질적인 MLLMs의 비대칭성을 적극적으로 적응합니다. 마지막으로, 탐색 단계에서는 레이블이 없는 데이터에서도 병합을 위한 하이퍼파라미터를 선택할 수 있는 비지도 하이퍼파라미터 선택 방법을 제안합니다.
- ***Performance Highlights***: AdaMMS는 Qwen 및 LLaMA 아키텍처 기반의 이종 MLLM 쌍에서 기존의 모델 병합 방법보다 우수한 성능을 보였습니다. AdaMMS는 다양한 비전-언어(vision-language) 작업에서 기존 병합 방법보다 성능이 향상되었고, 이전 모델이 필요로 했던 레이블된 데이터 없이도 효과적인 성능을 발휘하였습니다. 이는 AdaMMS가 이질적인 MLLMs 병합에서 어려움을 성공적으로 해결하는 방법임을 입증합니다.

### [Inference-Time Scaling for Complex Tasks: Where We Stand and What Lies Ahead](https://arxiv.org/abs/2504.00294)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.00294.png)

Vote: 6

Authors: Neel Joshi, Safoora Yousefi, Yash Lara, Vidhisha Balachandran, John Langford, Lingjiao Chen, Vibhav Vineet, Shivam Garg, Yue Wu, Jingya Chen, Besmira Nushi

- ***What's New***: 이번 연구에서는 복잡한 작업 수행 중 추론 능력을 향상시키기 위해 'Inference-Time Scaling' 방법론을 대형 언어 모델(LLM) 및 다양한 복잡한 작업에 적용하여 그 성능을 종합적으로 분석했습니다. 특히, 기존의 일반 모델과 추론-시간 스케일링에 맞춰 미세 조정된 모델을 비교하고, 추론-시간 스케일링의 잠재적 성능 향상 가능성을 검토하였습니다.
- ***Technical Details***: 이번 연구에서는 9개의 최신 모델과 8개의 도전적인 작업을 대상으로, 모델의 대규모 언어 모델(LLM) 추론 성능을 평가했습니다. 모델을 독립적 또는 순차적으로 호출하며 피드백을 사용하는 평가 프로토콜을 통해 각 모델의 상한과 하한 성능을 대략적으로 추정했습니다. 다양한 복잡한 문제가 포함된 기존 수학 및 STEM 관련 작업과 새로운 NP-완전 문제 벤치마크를 도입해 성능을 제시했습니다. 모델은 평균적 성능뿐만 아니라 Best-of-n 및 Worst-of-n 방식으로도 평가되었습니다.
- ***Performance Highlights***: 추론-시간 스케일링의 이점은 작업별로 상이하게 나타났고, 문제의 복잡도가 클수록 효과가 감소하는 경향을 보였습니다. 완벽한 검증자를 통한 향상된 추론에서는 상당한 성능 향상을 보이며, 특히 O1 및 O3-mini와 같은 고급 모델의 경우 Best-of-n 성능에서 유의미한 성과 증가를 기록했습니다. 여러 Task에서 inference-time scaling 기반 모델이 기존 모델보다 월등한 성능을 보여 선두를 달렸습니다.

### [m1: Unleash the Potential of Test-Time Scaling for Medical Reasoning with Large Language Models](https://arxiv.org/abs/2504.00869)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.00869.png)

Vote: 5

Authors: Xianfeng Tang, Yuyin Zhou, Hui Liu, Juncheng Wu, Xiaoke Huang

- ***What's New***: 이번 연구에서는 대규모 언어 모델(Large Language Models; LLMs)의 의료 추론 능력을 향상시키기 위한 테스트-타임 스케일링(Test-Time Scaling) 기법의 효과를 처음으로 포괄적으로 조사하였습니다. 연구 결과, '생각하기' 토큰 예산을 증가시키는 것이 의료 추론에 일관되게 좋은 성능을 보임을 밝혔고, 그 결과 10B 미만의 경량 모델로도 기존의 최첨단 성능을 경신할 수 있음을 확인하였습니다.
- ***Technical Details***: m1은 의료 QA 타스크에 적합하도록 설계된 경량 방법론으로, 1K/23K의 예제 데이터셋을 사용하여 오픈 LLM을 미세 조정(Fine-Tune)하고, 테스트-타임 제어를 통해 모델이 충분히 문제를 '생각할 수 있도록' 유도합니다. 이를 통해 모델은 더 긴 사고 과정을 생성하면서 다양한 의료 벤치마크에서 일관되게 정확도가 향상됩니다. 4K 토큰이 최적의 추론 한계로 보인다는 것을 발견하였으며, 초과할 경우 성능이 저하될 수 있음을 확인하였습니다.
- ***Performance Highlights***: m1-7B-23K 모델은 7B 파라미터 규모의 기존 복잡한 RL-튜닝 방법을 사용한 모델을 능가하며, 평균 정확도가 60.32%로 기록되었습니다. 이보다 큰 모델인 m1-32B-1K는 심지어 70B 규모의 맞춤형 모델과 비교해도 견줄 수 있는 성능을 보여주었으며, 이는 더 큰 용량의 베이스 모델과 단순한 생각의 흔적을 결합하여 강력한 결과를 얻을 수 있음을 시사합니다.

### [Chapter-Llama: Efficient Chaptering in Hour-Long Videos with LLMs](https://arxiv.org/abs/2504.00072)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.00072.png)

Vote: 4

Authors: Lucas Ventura, Cordelia Schmid, Gül Varol, Antoine Yang

- ***What's New***: Chapter-Llama는 대형 영상에 대한 챕터 생성 작업에서 강력한 성능을 발휘하며, 텍스트 기반 접근 방식을 통해 장시간의 영상을 효율적으로 처리합니다. 특히, 음성 기록(ASR)과 영상 프레임의 설명을 활용하여 챕터 경계를 예측하고 제목을 생성하는 역량을 갖추었습니다. 더불어, VidChapters-7M 데이터셋에서 기존 성능을 크게 능가하는 결과를 보여 주었습니다.
- ***Technical Details***: Chapter-Llama 프레임워크는 광범위한 컨텍스트 이해 능력을 갖춘 사전 학습된 대형 언어 모델(LLM; Large Language Models)을 활용하여 영상의 프레임을 선택하고 설명하는 방법을 사용합니다. 음성 전사를 통해 선택된 프레임에 대해 비전 캡션을 생성하고, 캡션과 음성 전사를 LLM에 입력하여 챕터 경계와 제목을 생성합니다. 본 문맥 창 한계를 극복하기 위해, 장시간의 영상을 처리할 때는 순차적으로 LLM의 예측을 수행하는 반복적 예측 방법을 사용할 수 있습니다.
- ***Performance Highlights***: Chapter-Llama는 최근 VidChapters-7M 벤치마크에서 기존의 최첨단 모델인 Vid2Seq 대비 상당한 개선을 보였으며, 특히 중간 및 긴 영상에서 눈에 띄는 개선 효과(예: F1 점수 45.3 대 26.7)를 확인했습니다. 이 과정에서 프레임 선택 모델의 잘 정의된 위치 및 캡션 정보가 좋은 품질의 챕터링 성과를 달성하는 데 중요한 역할을 했습니다.

### [Discovering Knowledge Deficiencies of Language Models on Massive Knowledge Base](https://arxiv.org/abs/2503.23361)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.23361.png)

Vote: 4

Authors: Xuwei Ding, Ryotaro Shimizu, Jian Kang, Jieyu Zhang, Rahul Gupta, Yang Liu, Jieyu Zhao, Linxin Song, Taiwei Shi

- ***What's New***: 이 논문에서는 대규모 지식 기반(Massive Knowledge Base)에서 언어 모델들이 가지고 있는 지식의 결핍을 발견하는 새로운 프레임워크인 확률적 오류 상승(Stochastic Error Ascent; SEA)을 제안합니다. SEA는 대규모 언어 모델(LLMs)의 폐쇄 가중치 환경에서 엄격한 쿼리 예산 내에서 효율적으로 오류를 발견하도록 설계되었습니다.
- ***Technical Details***: SEA는 지식 결핍을 발견하기 위한 확률적 최적화 과정으로 구현되며, 이전에 관찰된 실패와 유사한 고오류 후보를 반복적으로 검색합니다. 문서 및 단락 수준에서의 계층적 검색을 통해 검색 효율성을 높이고, 오류 전파를 모델링하고 체계적인 실패 모드를 식별하기 위해 관계 DAG(Directed Acyclic Graph)를 구성합니다.
- ***Performance Highlights***: 실험 결과, SEA는 Automated Capability Discovery 대비 40.7배, AutoBencher 대비 26.7% 더 많은 지식 오류를 발견하고, 오류당 비용을 각각 599배와 9배 감소시켰습니다. 이는 LLM의 지식 결핍을 발견하는 데 있어 SEA의 효율성을 입증하며, 인간 평가를 통해 생성된 질문의 높은 품질을 확인했습니다.

### [DiET-GS: Diffusion Prior and Event Stream-Assisted Motion Deblurring 3D Gaussian Splatting](https://arxiv.org/abs/2503.24210)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.24210.png)

Vote: 2

Authors: Gim Hee Lee, Seungjun Lee

- ***What's New***: DiET-GS는 새로운 프레임워크로, event streams와 사전 학습된 diffusion 모델의 prior를 사용하여 3D Gaussian Splatting(3DGS)에서 모션 블러를 제거합니다. 이 프레임워크는 EDI(Event Double Integral) prior를 활용하여 정확한 색상과 정교한 세부사항을 복원하도록 설계되었습니다.
- ***Technical Details***: DiET-GS는 두 스테이지로 구성됩니다. 첫 번째 스테이지에서는 EDI 제약 조건과 pretrained diffusion model에 기반한 prior 지식을 결합하여 3D Gaussian을 생성합니다. 두 번째 스테이지에서는 추가적인 learnable parameters를 통해 diffusion prior의 효과를 극대화합니다. 이를 통해 각 3D Gaussian에서 latent residual을 직접 생성하며, RSD(Renoised Score Distillation) 최적화를 간단하게 수행합니다.
- ***Performance Highlights***: DiET-GS와 DiET-GS++는 기존의 베이스라인을 뛰어넘어 높은 PSNR 및 SSIM 점수를 기록했습니다. 또한, DiET-GS++는 No-Reference Image Quality Assessment(NR-IQA) 메트릭, MUSIQ 및 CLIP-IQA에서도 우수한 성능을 보여줍니다.

### [Reasoning-SQL: Reinforcement Learning with SQL Tailored Partial Rewards for Reasoning-Enhanced Text-to-SQL](https://arxiv.org/abs/2503.23157)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.23157.png)

Vote: 2

Authors: Ruoxi Sun, Azalia Mirhoseini, Hailong Li, Sercan "O. Arik, Xingchen Wan, Amin Saberi, Shayan Talaei, Mohammadreza Pourreza

- ***What's New***: Reasoning-SQL은 텍스트를 SQL로 변환하는 과정에서 강화 학습을 적용하여, 모델이 스스로 추론 과정을 최적화할 수 있도록 하는 새로운 프레임워크를 제공합니다. 이 접근 방식은 SQL 쿼리 생성의 정확성과 추론 능력을 향상시키기 위해 특수하게 설계된 부분 보상(partial rewards) 집합을 도입합니다.
- ***Technical Details***: Reasoning-SQL은 다양한 부분 보상 메커니즘을 사용하여 텍스트-투-SQL 작업에서 보상의 희소성 문제를 해결합니다. 이 메커니즘에는 LLM-as-a-Judge, Syntax Check, Schema Linking, N-Gram Similarity 보상이 포함되어 있습니다. Group Relative Policy Optimization (GRPO)을 활용하여 이러한 부분 보상들이 효과적으로 통합되며, 각 입력에 대해 여러 후보 쿼리를 생성하고 서로 비교하여 최종 실행 정확성과 중간 추론 과정을 함께 최적화합니다.
- ***Performance Highlights***: 우리의 14B-파라미터 모델은 BIRD 벤치마크에서 독점 모델보다 3~4% 더 높은 성능을 달성했으며, SFT를 통해 훈련된 거대 규제 모델보다 더욱 뛰어난 일반화 능력을 보여줍니다. 함께 사용되는 CHASE-SQL 파이프라인에서는 병렬적인 독점 모델을 대체할 수 있는 비용 효율적인 솔루션을 제공하며, 이는 93% 낮은 추론 비용을 유지하면서 성능을 유지합니다.

### [ManipTrans: Efficient Dexterous Bimanual Manipulation Transfer via Residual Learning](https://arxiv.org/abs/2503.21860)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.21860.png)

Vote: 2

Authors: Siyuan Huang, Yuyang Li, Tengyu Liu, Puhao Li, Kailin Li

- ***What's New***: MANIPTRANS는 잔여 학습(Residual Learning)을 통한 효율적이고 복잡한 양손 조작 기술 전이를 위한 새로운 두 단계 방법론을 소개합니다. 본 연구에서는 인간의 양손 협조 기술을 시뮬레이션 상황의 로봇 손으로 효율적으로 옮기기 위한 최초의 프레임워크로, 이는 높은 성공률과 충실도, 효율성을 보여주었습니다.
- ***Technical Details***: MANIPTRANS는 초기 손 동작 모방을 통해 일반 궤적 모방 모델을 사전 훈련하고, 이후 상호작용 제약 조건 아래 특정 잔여 모듈을 미세 조정하여 복잡한 물체와의 상호작용 및 두 손의 정밀한 협조를 보장합니다. 이 방법은 대규모 사전훈련 단계에서 손동작의 차이를 줄여주고, 잔여 학습 모듈은 물리적 제약 조건을 지속적으로 만족시키도록 로봇의 행동을 수정하는데 중점을 둡니다.
- ***Performance Highlights***: 실험 결과, MANIPTRANS는 기존 최첨단 기법들보다 높은 수준의 전송 성공률과 운동 정밀도를 보였으며, 다양한 손 유형을 위하여 손쉽게 일반화할 수 있음을 입증하였습니다. 특히, 현실 세계에서도 뛰어난 조작 능력을 갖춘 것으로 평가되었습니다.

### [MB-ORES: A Multi-Branch Object Reasoner for Visual Grounding in Remote Sensing](https://arxiv.org/abs/2503.24219)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.24219.png)

Vote: 1

Authors: Karim Radouane, Mustapha lebbah, Hanane Azzag

- ***What's New***: MB-ORES는 원격 감시(Remote Sensing; RS) 이미지에서 객체 탐지(Object Detection; OD)와 시각적 고정(Visual Grounding; VG)을 통합한 새로운 프레임워크를 제안합니다. 이 통합 모델은 공간적, 시각적, 범주적 특징을 결합하는 멀티 브랜치 네트워크(Multi-branch Network)를 통해 상태-of-the-art 성능을 보여줍니다.
- ***Technical Details***: MB-ORES는 개방형 객체 탐지기를 참조 표현 데이터로 세밀하게 조정하여 부분적으로 지도되는 OD 작업으로 처리합니다. 각 이미지의 그래프 표현을 구축하고, 시각적, 공간적, 범주적 속성을 가진 노드로 구성된 그래프 입력을 처리하여 특정 객체를 대상으로 하고 박스 좌표를 회귀하는 상위 설계가 포함됩니다. 이 과정은 1단계와 2단계로 나뉘며, 1단계는 Graph 기반의 시각적 인식을, 2단계는 Task-감지 디자인을 통해 참조 객체를 추출하고 박스를 회귀하는 방식으로 작동합니다.
- ***Performance Highlights***: OPT-RSVG와 DIOR-RSVG 데이터셋에서 MB-ORES는 다양한 평가 메트릭에서 두드러지게 향상된 성능을 보여줍니다. DIOR-RSVG 데이터셋에서는 LPVA 모델을 넘어 MeanIoU에서 +5.38%의 증가를 보였으며, OPT-RSVG에서는 MeanIoU에서 6.98% 향상된 결과를 기록했습니다. 이를 기반으로 MB-ORES는 현재 상태-of-the-art 성능을 달성하며 미래 연구 방향을 제시합니다.

