## Daily Papers (2025-04-24)

### [VisuLogic: A Benchmark for Evaluating Visual Reasoning in Multi-modal Large Language Models](https://arxiv.org/abs/2504.15279)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15279.png)

Vote: 54

Authors: Xiaohua Wang, Jifeng Dai, Aijun Yang, Weiyun Wang, Lewei Lu, Wengang Zhou, Zhe Chen, Xizhou Zhu, Jinguo Zhu, Jiahao Wang, Weiye Xu, Wenhai Wang, Houqiang Li

- ***What's New***: VisuLogic은 멀티모달 대형 언어 모델(Multimodal Large Language Models; MLLMs)의 시각적 추론 능력을 평가하기 위한 새로운 벤치마크입니다. 시각 정보 중심의 평가가 이루어지도록 설계되어, 기존 텍스트 중심의 평가 한계를 넘어섭니다.
- ***Technical Details***: VisuLogic 벤치마크는 수량적 이동, 공간적 관계, 속성 비교 등 다양한 시각적 추론 카테고리에서 인간이 검증한 1,000개의 문제로 구성되어 있습니다. MLLMs는 이 문제들을 통해 여러 시각적 추론 능력을 평가받으며, 코드와 데이터는 https://visulogic-benchmark.github.io/VisuLogic에서 제공됩니다.
- ***Performance Highlights***: 평가 결과, 대부분의 모델은 30% 이하의 정확도를 기록하며, 이는 무작위 25%와 크게 다르지 않고, 인간의 51.4%에 비해 현저히 낮은 성능을 보여줍니다. 강화 학습을 통한 간단한 튜닝 단계를 적용한 경우, 정확도가 25.5%에서 31.1%로 상승하였습니다.

### [DreamID: High-Fidelity and Fast diffusion-based Face Swapping via Triplet ID Group Learning](https://arxiv.org/abs/2504.14509)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.14509.png)

Vote: 36

Authors: Xinghui Li, Qian He, Xinglong Wu, Songtao Zhao, Pengze Zhang, Miao Hua, Qichao Sun, Fulong Ye

- ***What's New***: DreamID는 새로운 고화질 및 빠른 얼굴 교환(face swapping) 모델로, Triplet ID Group 학습을 통해 ID 유사성 및 속성 보존을 강화했습니다. 이는 특히 복잡한 조명, 큰 각도, 메이크업 보존과 같은 도전적 상황에서도 뛰어난 성능을 발휘합니다.
- ***Technical Details***: DreamID는 SD Turbo 가속 확산 모델을 활용하여 단일 반복에서 얼굴 교환을 가능하게 만듭니다. SwapNet, FaceNet, ID Adapter로 구성된 개선된 아키텍처는 Triplet ID Group 데이터를 통한 명시적 감독을 최대한 활용합니다. 또한, 안경과 얼굴 형태와 같은 특정 속성을 훈련 중 명시적으로 조정하여 속성을 미세 조정하고 보존할 수 있습니다.
- ***Performance Highlights***: DreamID는 FFHQ 데이터셋을 비롯한 다양한 실험에서 최신 기술(SoTA)을 초월하여 ID 유사성(+), 포즈 및 표현 보존(+), 이미지 품질(+)에서 우수한 성능을 보여주었습니다. 단일 추론은 단 0.6초 소요되며, 이는 다른 확산 기반 모델들보다 훨씬 빠릅니다.

### [Trillion 7B Technical Report](https://arxiv.org/abs/2504.15431)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15431.png)

Vote: 25

Authors: Suyeong An, Wonsuk Yang, Hyungguk Kim, Juyoung Suk, Kyuseok Kim, Seungtaek Choi, Jamin Shin, Sungjun Han

- ***What's New***: Trillion-7B는 새로운 크로스-링구얼 문서 주의 메커니즘(Cross-lingual Document Attention; XLDA)을 도입하여 효율적인 영어에서 한국어나 일본어와 같은 대상 언어로의 지식 전이를 가능하게 합니다. 또한 최적화된 데이터 혼합, 언어별 필터링, 맞춤형 토크나이저 구성으로 멀티링구얼 대규모 언어 모델(LLM) 분야에서 token 효율성을 최대화한 한국 중심의 다중언어 모델입니다.
- ***Technical Details***: Trillion-7B는 2T training tokens 중 10%만 다중언어 데이터로 사용하였습니다. 특히, XLDA 메커니즘은 서로 다른 언어간의 문서가 혼합된 학습 Batch를 통해 언어 간 패턴을 식별할 수 있는 크로스-링구얼 학습 환경을 생성합니다. 모델은 Transformer 디코더 아키텍처 기반으로 RoPE, SwiGLU, RMSNorm 등의 최신 기술을 적용하였으며, 맞춤형 토크나이저는 128,256 byte-level 토큰으로 구성되어 있어 한국어 처리 속도를 약 35% 증대시켰습니다.
- ***Performance Highlights***: Trillion-7B는 27개 벤치마크에서 강력한 멀티링구얼 성능을 보였으며, 특히 멀티링구얼 채팅 및 명령어 따르기 부문에서 탁월한 성과를 기록했습니다. H100 GPU를 사용해 59.4K 시간($148K)을 소모하며 효율적으로 학습되었고, 이전 모델들과 비교해 한국어 성능에서 특별한 향상을 이끌어냈습니다.

### [I-Con: A Unifying Framework for Representation Learning](https://arxiv.org/abs/2504.16929)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.16929.png)

Vote: 23

Authors: Mark Hamilton, John Hershey, Shaden Alshammari, Axel Feldmann, William T. Freeman

- ***What's New***: I-Con은 정보이론적 관점을 통해 머신러닝의 다양한 손실 함수들에 대한 단일 방정식을 제안하는 새로운 프레임워크입니다. 이 프레임워크는 클러스터링, 스펙트럴 방법, 차원 축소, 대조 학습 및 지도 학습을 통합하여 기존에 분리되어 보였던 여러 방법들이 동일한 손실 함수의 특수 사례임을 보여줍니다.
- ***Technical Details***: I-Con은 두 조건부 분포 간의 KL 발산을 최소화하는 과정으로 표현 학습 알고리즘을 통합합니다. 이 방법론은 클러스터링, 대조 학습, 차원 축소, 스펙트럴 그래프 이론, 지도학습의 여러 기법들을 포괄합니다. 각 알고리즘은 학습된 분포(qϕ)와 감독된 분포(pθ)의 맞춤을 최적화하여 이뤄집니다.
- ***Performance Highlights***: 실험 결과, I-Con 프레임워크를 통해 개발된 새로운 무감독 이미지 분류기가 ImageNet-1K에서 이전의 최첨단 기술들보다 8% 높은 성능 향상을 보였습니다. CIFAR-100과 STL-10에서도 각각 3%, 2%의 개선이 관찰되었습니다. 이는 I-Con이 제공하는 공유프레임워크가 다양한 도메인 간의 아이디어 전환을 가능케 하여 성능 개선을 촉진할 수 있음을 시사합니다.

### [Tina: Tiny Reasoning Models via LoRA](https://arxiv.org/abs/2504.15777)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15777.png)

Vote: 21

Authors: Enes Burak Bilgin, Ollie Liu, Willie Neiswanger, Ömer Faruk Akgül, Shangshang Wang, Julian Asilis

- ***What's New***: Tina는 LoRA(Low-rank Adaptation)를 통해 작은 1.5B 파라미터 기반 모델에서 최소한의 리소스로 상당한 추론 성능을 얻는 파라미터 효율적인 업데이트를 활용한 새로운 작은 추론 모델 패밀리입니다.
- ***Technical Details***: Tina 모델은 RL(강화학습)에서 LoRA를 사용하여 최소한의 파라미터 변화로 추론 형식을 빠르게 적응시켜 기본 모델의 지식을 크게 유지하며, 품질 높은 RL 기법을 경제적으로 실행합니다.
- ***Performance Highlights***: 최고의 Tina 모델은 AIME24 벤치마크에서 >20% 성능 증가와 43.33% Pass@1 정확도를 기록하며, 비용은 단지 9달러로 기존 SOTA 모델에 비해 약 260배의 비용 절감을 보여줍니다.

### [Pre-DPO: Improving Data Utilization in Direct Preference Optimization Using a Guiding Reference Model](https://arxiv.org/abs/2504.15843)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15843.png)

Vote: 16

Authors: Yue Zhang, Wei Shen, Shulin Huang, Junshu Pan, Qiji Zhou

- ***What's New***: Pre-DPO는 Direct Preference Optimization(DPO)의 데이터 활용성을 높이기 위해 안내(reference) 모델을 활용하는 새로운 트레이닝 패러다임을 제안합니다. 이는 선호 최적화를 향상시키면서 별도의 외부 모델이나 추가적인 데이터를 필요로 하지 않습니다.
- ***Technical Details***: Pre-DPO는 기존의 DPO와 SimPO(Simple Preference Optimization)의 한계를 극복하기 위해 개발된 방식으로, 가이드(reference)가 되는 안내(reference) 모델을 통해 학습 초기 단계부터 더 효율적으로 데이터를 재가중(reweighting)합니다. 초기 정책 모델을 최적화한 후 그 결과를 가이드 모델로 설정하고, 이를 기반으로 DPO를 진행하여 더 나은 최적화된 정책을 도출합니다.
- ***Performance Highlights***: AlpacaEval 2 및 Arena-Hard v0.1 벤치마크에서 Pre-DPO는 기존의 DPO 및 SimPO에 비해 평균 2.5~2.6포인트의 성능 향상을 보여줍니다. Pre-DPO는 가이드(reference) 모델을 통한 데이터 재가중을 통해 효율적인 학습을 가능하게 하여 전통적인 레퍼런스 모델 설정으로 인한 성능 한계를 극복합니다. 또한, 외부 모델이나 추가 데이터 없이도 쉽게 배포 가능합니다.

### [Decoupled Global-Local Alignment for Improving Compositional Understanding](https://arxiv.org/abs/2504.16801)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.16801.png)

Vote: 14

Authors: Yupei Wang, Ziyong Feng, Kaicheng Yang, Xiaoxing Hu, Haoran Xu, Jun Wang

- ***What's New***: DeGLA(Decoupled Global-Local Alignment) 프레임워크는 일반적인 능력을 크게 손상시키지 않고 조합적 이해를 개선하여 CLIP 모델의 제한을 극복합니다. Self-Distillation 메커니즘과 대량의 고품질 부정 캡션을 생성하는 LLM 기반의 부정 캡션 생성 파이프라인을 도입하여 모델의 고유한 일반적 이해 능력을 최적화합니다.
- ***Technical Details***: DeGLA는 두 가지 손실을 도입하여 시각-언어 조합성을 향상시킵니다: Image-Grounded Contrast(IGC) 손실과 Text-Grounded Contrast(TGC) 손실. IGC 손실은 이미지 임베딩을 부정 텍스트 임베딩에서 멀어지게 하면서 긍정 텍스트 임베딩 쪽으로 끌어당깁니다. TGC 손실은 강력한 텍스트 임베딩을 통해 부정 텍스트와의 구분을 향상시키고 조합적 이해를 개선합니다.
- ***Performance Highlights***: DeGLA는 VALSE, SugarCrepe 및 ARO 벤치마크에서 이전 최첨단 기법 대비 평균 3.5%의 개선을 달성했습니다. 또한 11개의 데이터셋에서 zero-shot 분류 타스크에서 평균 성능이 13.0% 향상되었습니다. 이 실험 결과는 구성적 추론 및 코드 생성에서 현재 LLM의 도전 과제를 잘 드러냅니다.

### [PHYBench: Holistic Evaluation of Physical Perception and Reasoning in Large Language Models](https://arxiv.org/abs/2504.16074)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.16074.png)

Vote: 14

Authors: Haoling Chang, Zhou Liang, Ziyang Ni, Ming-xing Luo, Xianqi Yin, Shutao Zhang, Laifu Man, Tianyu Zhang, Yutong Ren, Bohan Zhang, Fan Cui, Anqi Lv, Chenyang Wang, Zeyu Cai, Jing-Jun Zhang, Xiaotian Li, Xingqi Xia, Feiyu Tao, Xudong Tian, Yushu Mu, Yixuan Yin, Chencheng Tang, Boxuan Jing, Zhangyi Liu, Weike Wang, Tianyu Luo, Yuku Zhang, Zhongxuan Li, Jiawei Lin, Zheyu Shen, Zhuo-Yang Song, Binran Wang, Yunbo Sun, Ziheng Zhou, Fengyuan Wang, Changkun Shao, Jianxiang Li, Muhan Zhang, Yi Hu, Qihua Sun, Minghao Li, Hua Xing Zhu, Qi Liu, Haoxu Zhang, Zizhuo Fu, Jiahang Chen, Shaoyang Guo, Qiuhao Xiong, Qing-Hong Cao, Jingtian Zhang, Shi Qiu, Jiashen Wei

- ***What's New***: PHYBench는 대형 언어 모델(LLMs)의 물리적 이해 및 추론 능력을 평가하기 위한 새로운 고품질 벤치마크입니다. 500개의 실제 물리 시나리오에 기반한 물리 문제로 구성되어 있으며, 모델이 현실적인 물리 과정을 이해하고 추론하는 능력을 평가하도록 설계되었습니다. 이 벤치마크는 난이도의 범위가 고등학교 문제부터 학부 수준이나 물리학 올림피아드 문제까지 다양합니다. 또한, 수학적 표현의 수정 거리(Edit Distance)에 기반한 새로운 평가 지표인 표현(Edit Distance) 점수(EED)를 제안하여 모델의 세부적인 추론 과정을 평가합니다.
- ***Technical Details***: PHYBench는 역학, 전자기학, 열역학, 광학, 현대 물리학 및 고급 물리학 등의 다양한 분야를 포함합니다. 각 문제는 명확히 정의된 물리적 표현을 정답으로 사용하여 모델의 물리적 지각 및 강력한 추론 능력을 정확하게 평가합니다. 표현 수정 거리(EED) 점수는 심피(Sympy)의 트리 표현 및 편집 거리를 활용하여 모델이 제공한 표현과 지상 진실(ground truth)간의 유사성을 자동으로 평가합니다. 데이터셋은 공개되어 있으며, 북경대의 학부 물리학 학생들을 모집하여 사람 기준을 설정하였습니다.
- ***Performance Highlights***: 실험 결과, 최첨단 추론 모델도 사람 전문가에 비해 성능이 크게 뒤떨어짐을 알 수 있었습니다. 예컨대, Gemini 2.5 Pro 모델은 36.9%의 정확도를 기록하여 사람 기준의 61.9%와 상당한 차이를 보였습니다. 최신 발전 모델도 복잡한 물리 추론 시나리오에서 여전히 많은 개선이 필요함을 시사합니다. EED 점수를 통한 샘플 효율성은 304% 향상되어, EED를 사용한 500 문제 평가가 잠재적으로 정확도를 기준으로 했을 때 약 1500 문제를 평가하는 것과 동일한 구별력을 제공합니다.

### [DreamO: A Unified Framework for Image Customization](https://arxiv.org/abs/2504.16915)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.16915.png)

Vote: 13

Authors: Xinghui Li, Jian Zhang, Qian He, Yanze Wu, Fei Ding, Shiwen Zhang, Yufeng Cheng, Chong Mou, Songtao Zhao, Xinglong Wu, Zinan Guo, Wenxu Wu, Yiming Luo, Mengtian Li, Pengze Zhang

- ***What's New***: DreamO는 이미지 커스터마이제이션을 위한 통합 프레임워크로, 다양한 조건들을 매끄럽게 통합할 수 있도록 설계되었습니다. 특히, 이 프레임워크는 Diffusion Transformer(DiT)를 활용하여 입력 데이터를 균일하게 처리하며, 풍부한 커스터마이제이션 작업을 처리할 수 있도록 새로운 기능 라우팅 제약(Feture Routing Constraint)을 도입했습니다.
- ***Technical Details***: DreamO는 사전 훈련된 DiT 모델을 기반으로 다양한 일관성 조건(e.g., 정체성, 주제, 의상, 스타일)을 지원하며, 훈련 데이터로는 대규모 커스터마이제이션 작업을 포함하고 있습니다. 훈련 과정은 세 가지 단계로 이루어지며, 단순 작업부터 시작하여 전체 데이터로 확장한 후, 마지막 품질 조정 단계를 통해 모델의 적합성과 이미지 품질을 최적화합니다.
- ***Performance Highlights***: 제안된 DreamO 프레임워크는 다양한 이미지 커스터마이제이션 작업에서 높은 품질의 결과를 달성했으며, 멀티 조건 시나리오에서도 강력한 적응력을 보여주었습니다. 특히 가벼운 LoRA 기반 설계로 인해 상대적으로 적은 컴퓨팅 비용으로도 효율적인 배치가 가능함을 확인했습니다.

### [Rethinking the Generation of High-Quality CoT Data from the Perspective of LLM-Adaptive Question Difficulty Grading](https://arxiv.org/abs/2504.11919)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11919.png)

Vote: 9

Authors: Fang Tan, Keyu Wu, Yongsheng Du, Chushu Zhang, Manlin Mei, Qianjin Yu, Kunlin Liu, Yurui Zhu, Zihan Chen, Lingjun Huang

- ***What's New***: 본 연구는 LLM 적응형(LLM-Adaptive) 질문 난이도 등급에 기반한 고품질 Chain-of-Thought(CoT) 데이터를 생성하는 새로운 방법을 제안합니다. 이를 통해 DeepSeek-R1 모델의 복잡한 수학 및 코딩 문제 해결 능력을 소규모 LLM에서도 활용할 수 있습니다. 이 접근법은 데이터 생성 비용을 절감하고 모델의 감독된 미세 조정(SFT)의 효율성을 향상시킵니다.
- ***Technical Details***: 제안된 방법은 LLM의 추론 능력을 바탕으로 질문의 난이도를 평가하고, 이를 기반으로 다양하고 포괄적인 문제 라이브러리를 구축합니다. 이후 생성된 난이도 분포에 따라 질문을 샘플링하고, DeepSeek-R1 모델을 활용해 수학적 추론 및 코드 생성 작업을 각각 대상으로 고품질 CoT 데이터를 생성합니다.
- ***Performance Highlights***: ZMath-32B 모델은 단 2천 개의 고품질 수학적 CoT 데이터를 통해 DeepSeek-Distill-32B 모델을 수학 추론 작업에서 능가했습니다. 유사하게, ZCode-32B 모델도 코드 생성 작업에서 동일한 수의 데이터로 DeepSeek-Distill-32B를 능가했으며, 이는 제안된 LLM-적응형 CoT 데이터 생성의 효과를 입증합니다.

### [AIMO-2 Winning Solution: Building State-of-the-Art Mathematical Reasoning Models with OpenMathReasoning dataset](https://arxiv.org/abs/2504.16891)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.16891.png)

Vote: 9

Authors: Christof Henkel, Igor Gitman, Benedikt Schifferer, Ivan Moshkov, Ivan Sorokin, Wei Du, Shubham Toshniwal, Darragh Hanley

- ***What's New***: 이 연구는 AI Mathematical Olympiad - Progress Prize 2 (AIMO-2) 대회에서 우승한 최첨단 수학적 추론 모델을 소개합니다. 이 모델은 OpenMathReasoning 데이터셋을 활용하여 수학적 문제 해결 능력을 향상시킵니다.
- ***Technical Details***: 모델 구축을 위해 540K의 고품질 수학 문제와 3.2M의 장문의 추론 해결책을 포함한 대규모 데이터셋을 생성했습니다. 그 후, 코드 실행을 장기 추론 모델에 통합하는 혁신적인 방법을 개발하였고, 이를 통해 1.7M 고품질 도구 통합 추론(tool-integrated reasoning; TIR) 해결책을 형성했습니다. 마지막으로 가장 유망한 해결책을 선택하기 위한 파이프라인을 구축하였으며, 이러한 생성적 선택(gen-select) 방식은 전통적인 다수결 투표보다 성능을 크게 향상시킵니다.
- ***Performance Highlights***: OpenMath-Nemotron은 1.5B, 7B, 14B, 32B 등 파라미터 크기의 다양한 모델로 구성되어 있으며, 각각 CoT, TIR, GenSelect 추론 모드를 지원합니다. 특히, 14B 모델은 AIMO-2에서 50문제 중 34문제를 정확히 해결하여 대회에서 우승하는 성과를 거두었습니다.

### [A Comprehensive Survey in LLM(-Agent) Full Stack Safety: Data, Training and Deployment](https://arxiv.org/abs/2504.15585)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15585.png)

Vote: 9

Authors: Bhavya Kailkhura, Junyuan Mao, Luu Anh Tuan, Jingyi Wang, Yaodong Yang, Shiqian Zhao, Qing Guo, Qing Li, Yanwei Yue, Jinhu Fu, Xinyun Zhou, Tianlin Li, Jen-tse Huang, Cong Wu, Xinfeng Li, Guowen Xu, Yizhou Sun, Chengwei Liu, Wenyuan Xu, Kun Wang, Xiaofeng Wang, Hao Wu, Hui Xiong, Yu-Gang Jiang, Fan Zhang, Yibo Yan, Zhenhong Zhou, Bo Li, Shuicheng Yan, Yifan Jiang, Wenke Huang, Junfeng Fang, Yalan Qin, Qingsong Wen, Ke Tang, Liang Lin, Mohit Bansal, Hanjun Luo, Hongwei Li, Lei Bai, Haolang Lu, Yuval Elovici, Yifan Zhang, Chenlong Yin, Xiaojun Jia, Yi Ding, Weisong Sun, Xiang Wang, Yihao Huang, Dacheng Tao, Philip S. Yu, Qiankun Li, Shirui Pan, Xuming Hu, Ningyu Zhang, Dongxia Wang, Joey Tianyi Zhou, Fanci Meng, Minghe Wang, Chongye Guo, Felix Juefei-Xu, Yufei Guo, Yang Liu, Zhihao Xu, Xinye Cao, Tianlong Chen, Bo An, Xiao Wang, Xingjun Ma, Weifei Jin, Jun Sun, Guibin Zhang, Miao Yu, Jiahao Wu, Jing Chen, Jiaming Ji, Yiming Li, Donghai Hong, Tianwei Zhang, Wei Wang, Jie Zhang, Guancheng Wan

- ***What's New***: 이 논문은 LLM(대형 언어 모델)의 전체 수명 주기에서 발생하는 보안 문제, 즉 데이터 준비, 사전 훈련, 후속 훈련, 배포 및 상업화까지의 모든 단계를 포괄적으로 조사하는 최초의 '풀 스택 안전성' 개념을 소개합니다. 이는 기존의 특정 단계에 집중된 연구와 대비하여, LLM의 전체 수명 주기에 걸친 보안 문제를 체계적으로 이해하려는 시도를 보여줍니다.
- ***Technical Details***: 연구는 800개 이상의 논문을 바탕으로 수행되었으며, 데이터 생성, 모델 정렬, 모델 편집 및 LLM 기반 에이전트 시스템 등 각 단계에서 발생할 수 있는 보안 이슈를 종합적으로 분석합니다. 보안 취약점에 대한 다각적인 시각을 제공하며, 특히 데이터 독성, 사생활 침해, 악의적인 미세 조정 공격 및 배포 후의 도전 과제를 중점적으로 다룹니다.
- ***Performance Highlights***: 이 논문은 데이터 안전성, 정렬 기술, 모델 편집 및 에이전트 결합 시스템을 통한 LLM 보안 문제를 파악하며, 미래 연구의 방향으로 데이터 생성의 안전성과 후속 훈련의 보안 강화를 제안합니다. 특히, 에이전트 도구 안전성과 구체적인 시나리오에서의 신뢰성 보장 문제를 앞으로의 주요 연구 과제로 제시합니다.

### [RePOPE: Impact of Annotation Errors on the POPE Benchmark](https://arxiv.org/abs/2504.15707)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15707.png)

Vote: 8

Authors: Yannic Neuhaus, Matthias Hein

- ***What's New***: RePOPE는 MSCOCO 데이터셋의 라벨 오류가 POPE 벤치마크에 미치는 영향을 평가한 연구로, 라벨의 품질이 모델 평가 결과에 얼마나 큰 영향을 미치는지를 보여줍니다. 이를 위해, RePOPE라는 교정된 라벨 셋을 제공하며, 코드와 데이터는 GitHub (https://github.com/YanNeu/RePOPE)에서 제공합니다.
- ***Technical Details***: POPE 벤치마크에서는 MSCOCO 데이터셋의 원래 주석을 기반으로 라벨이 제공되었지만, 이 데이터셋은 많은 주석 오류를 포함하고 있습니다. 연구팀은 500개의 이미지를 다시 주석하여 RePOPE를 구성하고, 두 명의 인간 레이블러의 합의에 따라 'Yes', 'No', 'Ambiguous' 라벨을 할당합니다. 주석 오류 및 모호한 케이스의 예시는 이미지와 함께 제공합니다.
- ***Performance Highlights***: RePOPE를 사용한 실험 결과, 원래의 POPE 벤치마크에서 긍정 문항에 대한 라벨 오류가 발견되었으며, 9.3%의 라벨 오류와 13.8%의 모호한 라벨이 있음을 확인했습니다. 반면에 부정 문항에서는 1.7%의 라벨 오류와 4.3%의 모호한 사례가 발견되었습니다. 재라벨링 후, 모델의 정밀도는 감소하였으나 TPR(참 양성 비율)은 개선되었으며, 이는 F1 점수 순위에 영향을 미치게 됩니다.

### [Unchecked and Overlooked: Addressing the Checkbox Blind Spot in Large Language Models with CheckboxQA](https://arxiv.org/abs/2504.10419)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10419.png)

Vote: 4

Authors: Michał Turski, Łukasz Borchmann, Mateusz Chiliński

- ***What's New***: CheckboxQA는 대규모 비전-언어 모델(LVLMs)이 시각적으로 풍부한 문서 내 체크박스를 처리하는 능력을 평가하기 위한 특화된 데이터셋입니다. 문서 처리에서 체크박스의 해석 오류가 실질적인 운영 실수로 이어질 수 있다는 점에서 이 과제의 중요성이 큽니다. CheckboxQA는 기존 벤치마크에서 간과되는 체크박스의 해석을 중심으로 성능을 개선하고자 합니다.
- ***Technical Details***: CheckboxQA 데이터셋은 다양한 문서와 정확한 체크박스 해석에 기반한 질문-답변(QA) 쌍으로 구성되어 있습니다. 주어진 문서 이미지와 체커블 콘텐츠에 대한 질문에 올바른 텍스트 답변을 생성해야 합니다. ANLS(평균 정규화 레벤쉬타인 유사도)를 활용하여 모델의 예측력을 평가합니다. 문서의 박스화 및 텍스트 문맥을 파악하는데 중점을 두고 있으며, 상용 LVLMs 및 공개 소스 모델들이 시험되었습니다. 총 600개의 질문-답변 쌍을 생성하여 모델 성능의 기본 수치를 제공합니다.
- ***Performance Highlights***: Qwen 2.5 VL 72B 모델은 83.2%라는 높은 점수를 기록하며, GPT-4o보다 월등히 우수한 성능을 보였습니다. 그 외 Gemini와 Pixtral 모델들은 43.6%에서 71.9% 사이의 성과를 기록하였고, 인간 평가자의 성능은 97.5%로, 여전히 기계 학습 모델보다 우월함을 보여주었습니다. 이러한 결과는 현재의 LVLMs가 여전히 세부적인 시각적 추론과 레이아웃 이해에서 제한이 있음을 나타냅니다.

### [CRUST-Bench: A Comprehensive Benchmark for C-to-safe-Rust Transpilation](https://arxiv.org/abs/2504.15254)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15254.png)

Vote: 3

Authors: Qiaochu Chen, Anirudh Khatry, Greg Durrett, Jia Pan, Ziteng Wang, Isil Dillig, Robert Zhang

- ***What's New***: CRUST-Bench라는 새로운 벤치마크가 소개되었습니다. 이 벤치마크는 100개의 C 프로젝트를 대상으로 하며, 수작업으로 작성된 안전한 Rust 인터페이스와 Rust 테스트 케이스를 포함하여 C-to-safe-Rust 트랜스파일레이션(Transpilation)을 평가할 수 있게 합니다. CRUST-Bench는 리포지토리 단위의 프로젝트를 포함하여 대규모 코드를 다루며, 안전하고 관용적인 Rust 패턴을 지키도록 강제합니다.
- ***Technical Details***: CRUST-Bench는 100개의 C 리포지토리로 구성되어 있으며, 각 리포지토리는 수작업으로 만든 Rust 인터페이스와 테스트 케이스가 제공됩니다. Rust 인터페이스는 함수 시그니처와 타입 주석, 소유권 제한을 정의하여 트랜스파일레이션 프로세스를 안내합니다. 이는 Rust의 안전성 보장을 따르게 하며, 동반된 테스트 케이스는 기능적 정확성을 검증합니다. 벤치마크는 하이브리드 주석 프로세스를 통해 생성되었으며, 자동화 도구와 인간의 전문 지식을 결합하여 제작되었습니다.
- ***Performance Highlights***: OpenAI의 o1 모델이 가장 좋은 성과를 내었으며 37%의 테스트 작업을 성공적으로 트랜스파일링 할 수 있었습니다. Claude 3.7 Sonnet과 같은 프론티어 모델들은 컴파일러 에러와 테스트 실패를 통한 반복적인 수정을 거쳐, 각각 32%까지 성공률을 향상시켰습니다. 이는 현재의 자동화된 코드 마이그레이션에 대해 상당한 개선의 여지가 있음을 시사합니다.

### [Progressive Language-guided Visual Learning for Multi-Task Visual Grounding](https://arxiv.org/abs/2504.16145)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.16145.png)

Vote: 1

Authors: Hong Wang, Yefeng Zheng, Wenlong Zhang, Dingjiang Huang, Jingchao Wang, Kunhua Ji

- ***What's New***: 이 논문은 다중 작업 시각적 그라운딩(Multi-Task Visual Grounding; MTVG)을 위한 점진적인 언어 안내 시각 학습(Progressive Language-guided Visual Learning; PLVL) 프레임워크를 제안했습니다. 이 프레임워크는 시각적 모달리티의 고유한 특징 표현을 탐색하면서 언어 정보를 점진적으로 통합하여 언어 관련 시각 특징 학습을 강화합니다. 이를 통해 추가적인 교차 모달 융합 모듈 없이도 언어 지침을 완전히 도입할 수 있습니다.
- ***Technical Details***: PLVL은 ViTDet를 기반으로 한 시각 백본에 언어 정보를 통합하여, 시각적 모달리티의 특징 표현을 강화합니다. 로컬 블록(Local Block)에서는 자기 주의 메커니즘(Self-attention Mechanism)을 사용하여 시각 모달리티 내의 관계를 탐구하고, 글로벌 블록(Global Block)에서는 교차 주의(Cross-attention) 메커니즘을 통해 언어 토큰의 지침을 시각 백본에 점진적으로 도입합니다. 또한, REC와 RES 작업 간의 상호 관계를 탐색하고 협력적 다중 작업 헤드(Collaborative Multi-task Head)를 제안하여, 이들 두 작업의 공동 예측 정확도를 높입니다. PLVL 프레임워크는 BER트-Base와 ViTDet 기반의 백본을 사용하며, PyTorch와 NVIDIA A100 GPU에서 전체적으로 최적화되었습니다.
- ***Performance Highlights***: PLVL 프레임워크는 RefCOCO, RefCOCO+, RefCOCOg 데이터셋에서 기존의 대표적인 방법들을 능가하는 성능을 보였습니다. 전통적인 설정과 사전 훈련된 설정 모두에서 REC 및 RES 작업에 대해 탁월한 성능을 기록하며, 사전 훈련 설정에서는 특히 최고 성능을 기록했습니다(예: RefCOCO에서 92.65% REC 정확도). 또한 채널 구별 기반의 협력적 다중 작업 헤드는 성능 향상을 보였습니다.

### [Causal-Copilot: An Autonomous Causal Analysis Agent](https://arxiv.org/abs/2504.13263)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.13263.png)

Vote: 0

Authors: Parjanya Prashant, Saloni Patnaik, Songyao Jin, Shivam Singh, Wenyi Wu, Hou Zhu, Biwei Huang, Qian Shen, Xinyue Wang, Aryan Philip, Har Simrat Singh, Fang Nan, Kun Zhou

- ***What's New***: Causal-Copilot는 독립적인 인공지능 에이전트로, 대형 언어 모델 프레임워크(Large Language Model Framework; LLM)를 활용해 고급 인과 분석을 자동화 합니다. 이 시스템은 테이블형 및 시계열 데이터를 위한 인과 발견, 인과 추론, 알고리즘 선택, 하이퍼파라미터 최적화, 결과 해석 및 실행 가능한 통찰력 생성을 자동으로 수행합니다.
- ***Technical Details***: Causal-Copilot은 20개 이상의 최첨단 인과 분석 기법을 통합하여 다각적으로 분석을 하고, 학습된 인과 그래프를 통계적 신뢰도 및 사용자 피드백을 통해 재구성합니다. 또한, 자연어 피드백을 통해 상호작용할 수 있도록 설계되어 비전문가도 쉽게 접근할 수 있습니다.
- ***Performance Highlights***: 경쟁 알고리즘과 비교했을 때, Causal-Copilot은 복잡한 데이터 품질 문제와 대규모 네트워크에서 뛰어난 정확도와 효율성을 보여주며, 특히 실시간 데이터 스트리밍을 다룰 때도 높은 성능을 발휘합니다. 또한 다양한 테스트 시나리오에서 일관된 성능 이점을 가짐을 입증했습니다.

