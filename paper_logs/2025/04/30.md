## Daily Papers (2025-04-30)

### [Reinforcement Learning for Reasoning in Large Language Models with One Training Example](https://arxiv.org/abs/2504.20571)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.20571.png)

Vote: 46

Authors: Lucas Liu, Zhiyuan Zeng, Kuan Wang, Jianfeng Gao, Weizhu Chen, Yiping Wang, Simon Shaolei Du, Yelong Shen, Liliang Ren, Xuehai He, Hao Cheng, Qing Yang, Baolin Peng, Shuohang Wang

- ***What's New***: 이 연구는 하나의 예제 데이터로 대형 언어 모델(Large Language Models; LLMs)의 수리적 추론 능력을 강화할 수 있다는 것을 보여줍니다. RLVR(강화학습과 검증 가능한 보상을 사용함)을 통해 모델 성능이 MATH500에서 36.0%에서 73.6%로, 여섯 가지 수리적 추론 벤치마크에서 평균 17.6%에서 35.7%로 향상되었습니다.
- ***Technical Details***: RLVR(Reinforcement Learning with Verifiable Reward) 기술이 적용되어 모델이 단일 예제를 통해 높은 성능을 내면서 데이터 효율성을 입증하였습니다. 정책 그래디언트 손실(Policy Gradient Loss)의 기여가 크고, 엔트로피 손실(Entropy Loss)을 추가하면 성능이 더욱 향상된다는 것이 밝혀졌습니다.
- ***Performance Highlights***: 한 개의 예제 데이터를 사용한 학습(RLVR)으로도 다수의 데이터셋을 사용한 것과 비슷한 성능을 낼 수 있었습니다. 특히, Qwen2.5-Math-1.5B 모델은 MATH500에서 성능이 73.6%까지 향상되었습니다. 또한, 다양한 모델과 알고리즘(PPO, GRPO) 및 수학적 예제에 대해 30% 이상의 성능 향상이 관찰되었습니다.

### [UniversalRAG: Retrieval-Augmented Generation over Multiple Corpora with Diverse Modalities and Granularities](https://arxiv.org/abs/2504.20734)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.20734.png)

Vote: 42

Authors: Kangsan Kim, Woongyeong Yeo, Sung Ju Hwang, Jinheon Baek, Soyeong Jeong

- ***What's New***: UniversalRAG는 서로 다른 모달리티와 세분성을 가진 다양한 코퍼스에서 지식을 검색하고 통합하여 쿼리에 대한 응답을 생성하는 새로운 Retrieval-Augmented Generation(RAG) 프레임워크를 제공합니다. 현실 세계의 쿼리 요구 사항을 충족하기 위해 다중 모달리티의 이질적인 자료원을 목표로 합니다.
- ***Technical Details***: UniversalRAG는 쿼리의 모달리티에 따라 적절한 모달리티 전용 코퍼스를 동적으로 식별하고, 정보를 검색하는 모달리티 인식 라우팅 메커니즘을 제안합니다. 각 모달리티를 여러 세분화 수준으로 구성하여 쿼리의 복잡성 및 범위에 맞게 조정된 검색을 가능하게 합니다. 텍스트, 이미지, 비디오 등 다중 모달리티에 걸쳐 여러 벤치마크에서 검증되었습니다.
- ***Performance Highlights***: UniversalRAG는 8개의 다양한 모달리티 벤치마크에서 평가되었으며, 모달리티 전용 및 Unified 방식의 베이스라인을 능가하는 성능을 보였습니다. 특히, 쿼리별로 적절한 모달리티와 세분성을 효과적으로 결정하여 다양한 쿼리에 대해 강력한 성능을 발휘했습니다.

### [ReasonIR: Training Retrievers for Reasoning Tasks](https://arxiv.org/abs/2504.20595)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.20595.png)

Vote: 35

Authors: Pang Wei Koh, Sewon Min, Niklas Muennighoff, Rui Qiao, Rulin Shao, Wen-tau Yih, Daniela Rus, Luke Zettlemoyer, Xi Victoria Lin, Varsha Kishore, Bryan Kian Hsiang Low

- ***What's New***: ReasonIR-8B는 복잡한 추론 과제를 위한 최초의 리트리버 모델로, 기존의 리트리버들이 간단한 사실 기반 데이터셋에 의해 훈련된 것과 달리, 이 모델은 데이터 유출 방지를 위한 합성 데이터로 훈련되어 뛰어난 추론 능력을 제공합니다. BRIGHT라는 복잡한 정보 검색 벤치마크에서 기존의 모델 대비 우수한 성능을 기록합니다.
- ***Technical Details***: ReasonIR-8B는 LLAMA3.1-8B 모델을 기반으로 하고, REASONIR-SYNTHESIZER라는 합성 데이터 생성 파이프라인을 통해 훈련되었습니다. 이 파이프라인은 문서로부터 다양한 길이의 질의와 관련성 없는 부정적 사례를 생성하여, 모델이 이러한 데이터를 통해 더 효과적으로 훈련되도록 합니다. 이 과정은 다중 회전 접근 방식으로 수행되어 이전의 부정적 사례 탐색 방법보다 강화된 결과를 제공합니다.
- ***Performance Highlights***: ReasonIR-8B는 BRIGHT 벤치마크에서 reranker를 사용하지 않을 때 nDCG@10 29.9, reranker와 함께 사용했을 시 36.9라는 새로운 최첨단 성능을 달성했습니다. RAG 과제에서도 MMLU와 GPQA에서 각각 6.4%와 22.6%의 성능 향상을 기록하며 기존 리트리버와 검색 엔진을 능가합니다. 이 모델은 주어진 쿼리를 길고 세부적으로 작성할수록 성능이 향상되며, LLM reranker와 결합하면 더욱 우수한 성능을 보입니다.

### [The Leaderboard Illusion](https://arxiv.org/abs/2504.20879)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.20879.png)

Vote: 32

Authors: Shayne Longpre, Daniel D'Souza, Beyza Ermis, Sanmi Koyejo, Sara Hooker, Shivalika Singh, Marzieh Fadaee, Yiyang Nan, Alex Wang, Ahmet Üstün, Sayash Kapoor, Noah Smith, Yuntian Deng

- ***What's New***: 이 논문은 Chatbot Arena라는 벤치마크가 생성 AI 모델의 평가에서 발생하는 편향 문제를 조사합니다. 저자들은 메타, 구글, 오픈AI와 같은 일부 공급자들이 비공개 테스트를 통해 여러 변종을 테스트하고 최상의 결과만 공개하여 점수에 편향이 발생한다고 주장합니다.
- ***Technical Details***: 연구팀은 공정성을 높이기 위해 Chatbot Arena의 평가 프레임워크 개선을 권장합니다. 논문은 Bradley-Terry 모델이 어떻게 비공개 테스트와 선택적 공개에 의해 왜곡될 수 있는지를 시뮬레이션 실험을 통해 보여주며, 표준화된 모델 네트워크가 충분히 연결되지 않을 때 순위의 신뢰성이 감소한다고 강조합니다.
- ***Performance Highlights***: Chatbot Arena에 대한 데이터 접근 권한이 성능에 큰 영향을 미칠 수 있음을 발견했습니다. 약간의 추가적인 데이터 접근만으로도 Arena의 성능 이득이 최대 112%까지 증가할 수 있습니다. 이는 Arena가 특정 공급자에게 비대칭적인 이점을 부여할 수 있으며, 이러한 비대칭성이 Arena 전용 동태에 과적합을 초래할 수 있음을 보여줍니다.

### [Toward Evaluative Thinking: Meta Policy Optimization with Evolving Reward Models](https://arxiv.org/abs/2504.20157)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.20157.png)

Vote: 28

Authors: Dongyeop Kang, Vipul Raheja, Zae Myung Kim, Chanwoo Park

- ***What's New***: 이 연구는 보상 기반 정렬 방법론의 한계를 극복하고자 Meta Policy Optimization (MPO)을 도입했습니다. MPO는 메타 보상 모델을 활용하여 보상 모델의 프롬프트를 동적으로 조정함으로써 보상 신호의 취약점을 개선하고, 수작업 보상 프롬프트 설계의 필요성을 대폭 줄였습니다.
- ***Technical Details***: MPO는 메타 보상 모델을 통해 강화 학습의 시각 및 다양한 문맥을 분석하고 프롬프트를 최적화합니다. 이 과정에서 메타 보상 모델은 훈련 환경을 세밀하게 관찰하고 평가 프롬프트를 조정하며, 교육적 측면에서는 메타인지 기법을 통해 인지 사고와 결정 방식의 지속적인 향상을 모방합니다.
- ***Performance Highlights***: 실험 결과, MPO는 정적인 수작업 프롬프트에 의존하는 전통적인 모델 대비 강화 학습 보상 모델의 안정성을 높였고, 수많은 작업에 걸쳐 향상된 정렬 성능을 달성했습니다. 특히 에세이 작성, 요약, 윤리적 추론 및 수학적 문제 해결을 포함한 다양한 작업에서 MPO의 효과가 입증되었습니다.

### [TesserAct: Learning 4D Embodied World Models](https://arxiv.org/abs/2504.20995)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.20995.png)

Vote: 11

Authors: Hongxin Zhang, Chuang Gan, Haoyu Zhen, Yilun Du, Qiao Sun, Junyan Li, Siyuan Zhou

- ***What's New***: TesserAct는 새로운 4D 구현된 세계 모델(Embodied World Models)을 학습하여, 체화된 에이전트의 동작에 따라 3D 장면의 역동적 변화 을 예측합니다. 이는 공간적 및 시간적 일관성을 제공합니다. 기존 2D 모델을 초월하여 모양, 구성, 시간적 변화를 세밀하게 예측할 수 있습니다. 특히 RGB-DN(RGB, Depth, Normal) 비디오를 학습하여 효율적으로 4D 세계 모델을 구축하고, 이는 비전 기반 세계 모델보다 뛰어난 성과를 보입니다.
- ***Technical Details***: TesserAct 모델은 기존 로봇 조작 비디오 데이터셋에 깊이와 노멀 정보(Depth and Normal Information)를 확장하고, 주석이 달린 데이터셋에서 비디오 생성을 미세 조정합니다. 각 프레임에 대해 RGB-DN을 공동으로 예측하고, 생성된 RGB, Depth, Normal 비디오를 직접 고품질 4D 장면으로 변환하는 알고리즘을 제공합니다. 일관성을 유지하기 위해 두 가지 새로운 손실 함수(Loss Function)를 소개하며, 이는 장면 전반에 걸쳐 시간적, 공간적 일관성을 보장합니다.
- ***Performance Highlights***: TesserAct는 기존의 비전 기반 세계 모델들과 비교하여, 고품질의 4D 장면을 예측하는 데 있어 뛰어난 성과를 보였습니다. 실제 및 합성 데이터셋 모두에서 텍스처, 기하학, 표면을 정확하게 캡처하여 많은 추론 작업에서 매우 낮은 챔퍼 거리(Chamfer Distance)를 기록했습니다. 이는 복잡한 동적 장면에서 세밀한 세부 사항을 포착하는 데 있어서 탁월한 성능을 보여줍니다.

### [Certified Mitigation of Worst-Case LLM Copyright Infringement](https://arxiv.org/abs/2504.16046)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.16046.png)

Vote: 9

Authors: Marc Marone, Daniel Khashabi, Benjamin Van Durme, Jingyu Zhang, Jiacan Yu

- ***What's New***: BLOOMSCRUB는 LLM의 저작권 침해 위험을 감소시키기 위한 간단하지만 매우 효과적인 추론 시간 접근 방식입니다. 저작권이 있는 자료로부터 긴 인용구를 탐지하고 재작성하는 과정을 반복하여 위험을 줄입니다. 이는 대규모 실사용 말뭉치에서도 확장 가능한 저작권 검사 기능을 수행합니다.
- ***Technical Details***: BLOOMSCRUB는 Bloom 필터를 사용하여 모델 응답에서 고위험 인용구를 추출하고, 이를 '스크럽'하여 위반 인용구를 수정하는 반복적인 프로세스를 사용합니다. 이 과정은 저작권 제한을 준수하면서 유창성과 일관성을 유지하며, 고위험 인용구가 사라질 때까지 반복됩니다. 만약 응답이 낮은 위험으로 분류되지 않으면 답변을 포기하여 위험 감소를 인증합니다.
- ***Performance Highlights***: BLOOMSCRUB는 다른 방법들보다 저작권 침해를 줄이는데 더 효과적이며 텍스트 유틸리티를 잘 보존합니다. 또한, 재작성 반복 수를 조정하여 위험 임계값을 동적으로 조절할 수 있는 확장 가능하고 적응 가능한 솔루션을 제공합니다. 실험 결과, 긴 인용구를 거의 제거하고, 필요 시 위험 감소를 인증하는 거부 응답을 출력합니다.

### [YoChameleon: Personalized Vision and Language Generation](https://arxiv.org/abs/2504.20998)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.20998.png)

Vote: 9

Authors: Trung Bui, Yuheng Li, Krishna Kumar Singh, Jing Shi, Thao Nguyen, Yong Jae Lee

- ***What's New***: Yo'Chameleon은 대형 멀티모달 모델(Large Multimodal Models; LMMs)의 개인화된 비전 및 언어 생성(Vision and Language Generation)을 다루는 첫 번째 시도로, 사용자의 특정 개념을 개인화할 수 있는 연구입니다. 이 모델은 제한된 수의 이미지를 기반으로 새로운 개념에 대한 문제를 해결하고 새로운 컨텍스트에서 해당 개념의 이미지 생성을 지원합니다.
- ***Technical Details***: Yo'Chameleon은 소프트 프롬프트 튜닝(Soft-Prompt Tuning)을 사용하여 주제별 정보를 임베딩하고, 자동 프롬프팅 최적화 메커니즘과 '소프트-포지티브' 이미지 생성(Soft-Positive Image Generation)을 통해 이미지 품질을 향상시킵니다. 이 접근 방식은 다중 모드(Modalities)들 간의 성능 균형을 맞추기 위해 두 개의 소프트 프롬프트 세트를 사용하며, 자가 프롬프팅(Self-Prompting) 메커니즘을 도입하여 모델이 쿼리에 응답하기 전에 작업 타입을 결정합니다.
- ***Performance Highlights***: 실험 결과, Yo'Chameleon은 보다 적은 토큰을 사용하여 개념을 효과적으로 학습하고, 기존 프롬프팅 기법을 능가하는 성과를 보였습니다. 특히 이미지를 이해하고 생성하는 작업에서 GPT-4o를 포함한 다른 모델과 비교하여 우수한 개인화 성능을 달성했으며, 모델의 일반적인 지식을 유지하면서 개인화된 기능을 효과적으로 구현했습니다.

### [ISDrama: Immersive Spatial Drama Generation through Multimodal Prompting](https://arxiv.org/abs/2504.20630)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.20630.png)

Vote: 9

Authors: Changhao Pan, Wenxiang Guo, Zhou Zhao, Yu Zhang, Zhiyuan Zhu, Tao Jin

- ***What's New***: ISDrama는 다중모달 프롬프트(Multimodal Prompting)를 통해 몰입형 공간 드라마를 생성하는 최초의 모델로, 연속적인 다중화자 바이노럴 스피치(Binaural Speech)를 드라마틱한 프로소디(Dramatic Prosody)와 함께 생성합니다. MRSDrama라는 새로운 데이터셋을 구축하여 이 작업을 지원하며, 이는 바이노럴 드라마 오디오, 스크립트, 비디오, 기하학적 자세 및 텍스트 프롬프트를 포함합니다.
- ***Technical Details***: ISDrama는 대조학습(Contrastive Learning)을 기반으로 한 다중모달 포즈 인코더(Multimodal Pose Encoder)와 Mamba-Transformer 모델을 사용하여 몰입형 드라마를 생성합니다. 또한, Drama-MOE(Mixture of Experts)로 전문가를 선택하여 프로소디적 표현력과 포즈 조절을 향상시킵니다. 컨텍스트 일관성 분류자 없는 가이던스(Context-consistent Classifier-Free Guidance) 전략도 설계하여 드라마의 완전한 생성의 일관성을 높입니다.
- ***Performance Highlights***: 객관적 및 주관적인 평가 메트릭에서 ISDrama가 모든 기준 모델을 능가했음을 실험 결과가 보여줍니다. ISDrama는 물리적 원칙을 준수하면서 다양한 프로소디 변화를 보여주는 몰입형 공간 드라마를 성공적으로 생성할 수 있는 성능을 입증했습니다.

### [X-Fusion: Introducing New Modality to Frozen Large Language Models](https://arxiv.org/abs/2504.20996)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.20996.png)

Vote: 7

Authors: Sicheng Mo, Siddharth Srinivasan Iyer, Bolei Zhou, Eli Shechtman, Krishna Kumar Singh, Yijun Li, Yuchen Liu, Yuheng Li, Thao Nguyen, Abhishek Tandon, Yong Jae Lee, Xun Huang

- ***What's New***: X-Fusion은 사전 학습된 대형 언어 모델(LLM; Large Language Models)을 새로운 모달리티로 확장하면서도 언어 능력을 보존하는 새로운 프레임워크입니다. 이 프레임워크는 듀얼 타워(Dual-Tower) 설계를 사용하여 각 모달리티에 특화된 가중치로 LLM의 언어 파라미터는 고정된 상태에서 비전(vision) 전용 정보를 통합합니다.
- ***Technical Details***: X-Fusion은 텍스트 타워(text tower)와 비전 타워(vision tower)로 구성된 듀얼 타워 구조를 채택하였습니다. 텍스트 타워는 LLM의 고정 가중치를 사용해 언어 능력을 보존하며, 비전 타워는 계층별로 별도의 비전 가중치를 사용하여 시각 정보를 처리합니다. 시각 및 텍스트 피처를 입력과 출력뿐만 아니라 중간 처리 수준에서도 정렬합니다.
- ***Performance Highlights***: X-Fusion은 이미지-텍스트 및 텍스트-이미지 작업 모두에서 다른 아키텍처 대비 일관된 성능 향상을 보여줍니다. 이를 통해 이해 중심의 데이터를 포함하면 생성 품질이 개선되고, 이미지 데이터의 노이즈를 줄이면 전체 성능이 향상된다는 것을 실험을 통해 입증하였습니다. 또한, 피처 정렬이 작은 모델에서는 수렴을 가속화하지만 큰 모델에는 미미한 영향을 줍니다.

### [RAGEN: Understanding Self-Evolution in LLM Agents via Multi-Turn Reinforcement Learning](https://arxiv.org/abs/2504.20073)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.20073.png)

Vote: 7

Authors: Zihan Wang, Manling Li, Qineng Wang, Yiping Lu, Jiajun Wu, Linjie Li, Monica Lam, Kangrui Wang, Kefan Yu, Kyunghyun Cho, Li Fei-Fei, Zhengyuan Yang, Lijuan Wang, Eli Gottlieb, Yejin Choi, Licheng Liu, Pingyue Zhang, Minh Nhat Nguyen

- ***What's New***: 이 연구에서는 대형 언어 모델(LLM)을 대화형 에이전트로 훈련하기 위한 새로운 강화를 제안했습니다. StarPO(State-Thinking-Actions-Reward Policy Optimization) 프레임워크를 통해 LLM 에이전트를 다턴 상호작용 환경에서 자율적으로 학습시키는 RAGEN 시스템을 소개했습니다.
- ***Technical Details***: RAGEN은 멀티턴 에이전트 훈련을 위한 통합 시스템으로, 강화 학습(RL)을 통해 에이전트의 추론 및 적응 능력을 훈련하고 평가합니다. StarPO는 다턴 상호작용 궤적을 최적화하기 위한 일반적인 RL 프레임워크로, 보상 할당 및 프롬프트-롤아웃 구조에 유연한 제어를 제공합니다. 이를 통해 에이전트는 환경과의 상호작용을 통해 의사결정 정책을 학습하게 됩니다.
- ***Performance Highlights***: StarPO를 활용한 실험에서, 에이전트는 초기 단계에서는 다양한 추론을 보여주지만 'Echo Trap'이라는 패턴을 통해 교육이 이루어지면서 성능이 점차 하락하는 경향을 보였습니다. 이를 해결하기 위해 StarPO-S라는 안정화 된 버전을 제안하고, 불확실성 기반 필터링 등으로 문제를 해결했습니다.

### [In-Context Edit: Enabling Instructional Image Editing with In-Context Generation in Large Scale Diffusion Transformer](https://arxiv.org/abs/2504.20690)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.20690.png)

Vote: 5

Authors: Yi Yang, Zongxin Yang, Zechuan Zhang, Ji Xie, Yu Lu

- ***What's New***: In-Context Edit는 대형 확산 변환기(Diffusion Transformer, DiT)를 활용하여 자연어 지시에 기반한 이미지 편집을 효과적으로 수행하는 혁신적인 방법을 제안합니다. 제한된 데이터와 파라미터로 SOTA 수준의 성능을 달성하며, 데이터 효율성과 편집 정확성 간의 균형을 이루었습니다.
- ***Technical Details***: 이 방법은 인-컨텍스트 프롬프트(In-Context Prompt)를 활용한 무훈련(zero-shot)의 지시 기반 편집 프레임워크를 제시하며, 모델 아키텍처 변경이나 광범위한 재훈련이 없이도 컨텍스트적 편집이 가능하게 합니다. 또한 LoRA-MoE 하이브리드 튜닝을 통해 전문가 라우팅을 동적으로 활용하여 다양한 편집 작업에서도 높은 성공률을 보여줍니다. 초기 노이즈 선택 과정에서 VLM 기반의 결과 평가를 통합하여 편집 품질을 향상시켰습니다.
- ***Performance Highlights***: MagicBrush 및 Emu Edit 벤치마크에서 본 방법은 기존의 SOTA 방법들을 능가하는 데이터 및 파라미터 효율성을 보여주었습니다. 0.5%의 학습 데이터와 1%의 파라미터만으로 우수한 편집 품질을 보장하며, VIE-스코어 평가에서 상용 시스템과 비교 시 경쟁력을 입증했습니다.

### [Learning Explainable Dense Reward Shapes via Bayesian Optimization](https://arxiv.org/abs/2504.16272)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.16272.png)

Vote: 4

Authors: Dongyeop Kang, Vipul Raheja, Kwang-Sung Jun, Mingyi Hong, Ian Yang, Ryan Koo

- ***What's New***: 이 논문은 설명 가능한 밀집 보상 지형(Learning Explainable Dense Reward Shapes)을 베이시안 최적화(Bayesian Optimization)을 통해 학습하는 새로운 방법론을 제안합니다. 이는 강화 학습이 인간 피드백(Reinforcement Learning from Human Feedback)에서 흔히 발생하는 희소한 보상 문제를 해결하는데 중점을 두고 있습니다.
- ***Technical Details***: 이 연구는 SHAP와 LIME과 같은 설명 가능 방법을 활용하여 보상 모델에서 각 토큰 단위 보상을 추정하는 보상 형상 함수를 제안합니다. 이를 위해 이중 최적화 프레임워크(Bilevel optimization framework)를 사용하며, 베이시안 최적화와 정책 훈련을 통합하여 토큰 보상 추정치의 노이즈를 처리합니다.
- ***Performance Highlights***: 실험 결과, 제안된 방법은 토큰 단위 보상 귀속의 균형을 잘 맞추면 기존 방법보다 하위 작업 성능이 개선되고 훈련 중 최적 정책을 더 빠르게 찾을 수 있음을 보여줍니다. SHAP와 LIME의 기능 가산 변환 함수가 원래의 보상과 동일한 최적의 정책을 유지함을 이론적으로 증명했습니다.

### [Disentangle Identity, Cooperate Emotion: Correlation-Aware Emotional Talking Portrait Generation](https://arxiv.org/abs/2504.18087)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.18087.png)

Vote: 4

Authors: Xiaozhong Ji, Chengjie Wang, Xiaobin Hu, Chuming Lin, Junwei Zhu, FeiFan Xu, Weipeng Tan, Chengming Xu, Yanwei Fu

- ***What's New***: DICE-Talk는 감정을 제어할 수 있는 오디오 기반의 확산 모델(Diffusion Model)에 기반한 최초의 감성 가상 인물 생성 프레임워크입니다. 이 시스템은 감정 표현을 인물의 정체성으로부터 분리하여 감정적으로 풍부한 표정을 제공하면서도 정체성을 유지합니다.
- ***Technical Details***: DICE-Talk 프레임워크는 다양한 데이터의 오디오 및 비주얼 정보를 교차-모달 주의(Cross-modal Attention) 메커니즘을 통해 결합한 정체성 불가 존재의 감정 임베딩을 사용합니다. 감정 은행(Emotion Bank)은 벡터 양자화 방식과 학습 가능한 감정 벡터를 통해 서로 다른 감정의 상관관계를 캡처합니다. 또한, 잠재 공간(Latent Space)에서 감정 일관성을 유지하는 감정 판별 목적을 추가하여 더 정확한 감정 전달을 유도합니다.
- ***Performance Highlights***: DICE-Talk는 MEAD 및 HDTF 데이터셋에서 실험된 결과 기존 방식들보다 감정의 정확성과 립싱크 성능에서 더 나은 성능을 보여주었습니다. FID 및 FVD 같은 다양한 지표에서, DICE-Talk는 감정적 표현의 풍부함과 비디오 생성의 전반적인 품질을 높이 평가받았으며, 비디오의 자연스러움과 부드러움을 사용자 연구에서도 인정받았습니다.

### [TreeHop: Generate and Filter Next Query Embeddings Efficiently for Multi-hop Question Answering](https://arxiv.org/abs/2504.20114)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.20114.png)

Vote: 3

Authors: Xuming Hu, Kunpeng Zhang, Jinghuai Ou, Zhonghao Li, Shuliang Liu

- ***What's New***: TreeHop는 다중 단계 질문 응답(Multi-hop Question Answering; MHQA)에서 반복적인 LLM 기반의 질의 재작성 없이 효율적으로 다음 질의 임베딩(Query Embedding)을 생성하고 필터링하는 새로운 방식입니다. 이 프레임워크는 기존의 'Retrieve-Rewrite-Vectorize-Retrieve' 사이클을 'Retrieve-Embed-Retrieve' 루프로 단순화하여 계산량을 크게 줄이고 있습니다.
- ***Technical Details***: TreeHop는 이전 질의와 검색된 문서의 의미 정보를 융합하여 질의 임베딩을 동적으로 업데이트합니다. 업데이트 된 질의 임베딩은 의미적 유사성에 기반하여 다음 스텝의 검색을 가능하게 합니다. 또한, 문장의 의미적 중복을 방지하고 정보 추출의 초점을 맞추기 위해 게이트 교차 주의(Gated Cross-Attention) 메커니즘을 사용합니다. 트리밍 기준에서는 중복 검색 경로를 제거하고, 매 단계별로 상위 K개의 검색 후보를 유지하여 효율성을 보장하고 있습니다.
- ***Performance Highlights***: TreeHop는 2WikiMultiHop, MuSiQue, MultiHop RAG의 3개 데이터셋에서 99%의 검색 지연을 줄이면서도 첨단 RAG 메소드와 비교해 유사한 성능을 보였습니다. TreeHop의 질의 처리 시간은 0.02초(2번째 반복) 및 0.06초(3번째 반복)를 기록하며, 이는 EfficientRAG 대비 99.2%~99.6% 빠른 속도입니다. 이는 질의-첨단 생성을 피함으로써 얻은 임베딩 수준의 효율성 덕분입니다.

### [LawFlow : Collecting and Simulating Lawyers' Thought Processes](https://arxiv.org/abs/2504.18942)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.18942.png)

Vote: 3

Authors: Daniel Schwarcz, Khanh Chi Le, Brendan Madson, Brett McDonnell, Debarati Das, Daniel H. Moses, Karin De Langis, Robin M. Willis, Ritik Sachin Parkar, Dongyeop Kang, Chad M. Berryman

- ***What's New***: LawFlow는 변호사의 추론 과정을 수집하고 시뮬레이션하는 새로운 데이터셋을 도입하여 AI의 법적 작업 지원 방식을 개선합니다. 법학 교육을 받은 학생들로부터 수집된 전체 법적 워크플로우를 통해 실제 비즈니스 형성 시나리오에 기반하여 법적 추론 과정을 포착하며, 이는 모듈식이고 반복적인 추론 과정을 포함합니다.
- ***Technical Details***: LawFlow 데이터셋은 법학 교육을 받은 학생들이 업무를 수행하는 동안 생성된 모든 결정 경로를 추적합니다. 이 데이터셋은 심도 있는 인터뷰와 모의 시나리오 연기를 통해 수집되며, 각 워크플로우는 클라이언트 정보 수집에서 시작해 사업 운영 계약 초안 작성에 이르는 전체 결정을 포함합니다. LawFlow는 전통적인 체인 오브 쏘트 데이터를 넘어서 '결정의 체인(chain-of-decisions)' 추론을 지원합니다.
- ***Performance Highlights***: 실험 결과, 인간의 워크플로우는 모듈식이고 적응적이며, LLM의 워크플로우는 더 순차적이고 포괄적입니다. LLM은 인간과 비교하여 추론의 유연성이 부족하고 하위 작업 간 전환이 적은 것으로 나타났습니다. 또한, 인간 변호사는 LLM이 보조적 역할을 수행하길 선호하며, AI가 명확성과 창의성, 효율성을 지원하는 하이브리드 계획 및 적응적 실행 전략을 제안합니다.

### [A Review of 3D Object Detection with Vision-Language Models](https://arxiv.org/abs/2504.18738)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.18738.png)

Vote: 2

Authors: Manoj Karkee, Ranjan Sapkota, Konstantinos I Roumeliotis, Rahul Harsha Cheppally, Marco Flores Calero

- ***What's New***: 이 논문은 최초로 비전-언어 모델(Vision-Language Models; VLMs)을 활용한 3D 객체 탐지에 대한 광범위하고 포괄적인 리뷰를 제공합니다. VLMs는 멀티모달 AI 분야에서 빠르게 발전하는 전선으로, 이 모델들은 언어 기반 추론, 제로샷 일반화, 명령 기반 상호작용 등을 통해 공간 이해를 향상시킵니다.
- ***Technical Details***: 이 연구는 VLMs 기반 3D 객체 탐지를 위한 아키텍처적 토대를 조사하며, 사전학습 기법, 공간 정렬 모듈, 크로스 모달 퓨전 전략 등을 다룹니다. Pretraining과 Fine-tuning 단계에서 시각적 토큰과 텍스트 토큰의 통합을 통해 3D 객체 탐지가 이루어지며, 네트워크가 언어 입력을 통해 객체를 식별하는 능력을 제공합니다. 또한, VLMs를 이용한 3D 탐지의 최근 발전 사항으로 무관측 객체 탐지, 오픈 단어 검색 등이 있습니다.
- ***Performance Highlights***: 기존 CNN 기반의 3D 탐지 모델들이 구조화된 환경 내에서 관련성과 실시간 성능에서 높은 점수를 기록하지만, VLMs는 오픈 보캐뷸러리와 동적 환경 변화 및 제로샷 학습에서 탁월함을 보입니다. 그러나 VLMs의 경우 실시간 성능 면에서 15–20% 낮은 프레임 속도를 기록, 컴퓨팅 자원의 높은 요구가 확연한 차이점입니다.

### [Chain-of-Defensive-Thought: Structured Reasoning Elicits Robustness in Large Language Models against Reference Corruption](https://arxiv.org/abs/2504.20769)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.20769.png)

Vote: 1

Authors: Soheil Feizi, Wenxiao Wang, Parsa Hosseini

- ***What's New***: Chain-of-Defensive-Thought는 기존의 대규모 언어 모델(Large Language Models; LLMs)의 참고 자료 손상(Reference Corruption)에 대한 강건성을 단순한 체인-오브-디펜시브-생각(Chain-of-Defensive-Thought) 프롬프팅 방법을 통해 향상시킵니다. 이는 프롬프팅만으로 이루어진 접근 방식으로 복잡한 태스크가 아니더라도 다양한 유형의 언어 모델에서 향상된 강건성을 보여줍니다.
- ***Technical Details***: 체인-오브-디펜시브-생각 프롬프팅은 몇 가지 예시를 통해 언어 모델들이 구조화된 추론 과정을 모방하도록 유도하여 보다 신뢰할 수 있는 응답을 생성하게 합니다. 이러한 프롬프팅은 참고 자료의 손상, 특히 프롬프트 주입 공격과 지식 손상 공격에 대한 강건성을 이끌어 냅니다.
- ***Performance Highlights***: 설명된 실험 결과, 체인-오브-디펜시브-생각은 자연어 질문(Natural Questions) 및 실시간 QA(RealTime QA) 벤치마크에서 GPT-4o의 경우 손상된 참조에서 정확도를 3%에서 50%로 증가시키고, 공격 성공률을 20%로 감소시켰습니다. 이는 여러 모델에 걸쳐 평균적으로 정확도를 23.70% 증가시키고 공격 성공률을 27.31% 감소시키는 것으로 나타났습니다.

### [CaRL: Learning Scalable Planning Policies with Simple Rewards](https://arxiv.org/abs/2504.17838)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.17838.png)

Vote: 1

Authors: Bernhard Jaeger, Daniel Dauner, Andreas Geiger, Simon Gerstenecker, Jens Beißwenger, Kashyap Chitta

- ***What's New***: 이 연구는 자율 주행에서 강화 학습의 효율성을 높이기 위한 새로운 보상 설계를 제안합니다. 기존의 복잡한 형태의 보상 기법 대신 단일 보상 항목인 '경로 완성'에만 최적화하여 보다 직관적인 보상 시스템을 도입했습니다.
- ***Technical Details***: 이 연구에서 제안한 보상 시스템은 '경로 완성'(Route Completion)을 주된 보상으로 사용하며, 위반 사항 발생 시 에피소드를 종료하거나 경로 완성도를 곱셈적으로 감소시키는 방식으로 패널티를 줍니다. 이 보상 설계는 'Proximal Policy Optimization(PPO)' 알고리즘을 사용하여 대규모 데이터 병렬 처리 및 스케일링에 효율적입니다. CARLA와 nuPlan 시뮬레이터에서 각각 300M, 500M 데이터를 사용하여 PPO를 확장하여 학습을 진행했습니다.
- ***Performance Highlights***: 개발된 모델은 CARLA 시뮬레이터의 longest6 v2 벤치마크에서 64 DS를 달성하며, 복잡한 보상을 사용하는 기존 강화 학습 방법들보다 더욱 우수한 성능을 보였습니다. 또한, nuPlan 시뮬레이터의 Val14 벤치마크 평가에서도 91.3의 높은 점수를 기록하며, 기존 학습 기반 방법보다 최대 10배 빠른 추론 속도를 자랑합니다.

