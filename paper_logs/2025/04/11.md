## Daily Papers (2025-04-11)

### [Kimi-VL Technical Report](https://arxiv.org/abs/2504.07491)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.07491.png)

Vote: 69

Authors: Jianzhou Wang, Xinyuan Wang, Kimi Team, Hao Zhang, Sihan Cao, Zihao Huang, Xinyu Zhou, Jinhong Wang, Jianlin Su, Kun Ouyang, Weiran He, Yuxin Wu, Zheng Zhang, Y. Charles, Tongtong Bai, Zhaowei Li, Junjie Yan, Bowei Xing, Yiping Bao, Dongliang Wang, Yiqin Wang, Huabin Zheng, Hongcheng Gao, Yan Zhong, Yanru Chen, Jin Xie, Pengyu Cheng, Jiaming Li, Zhilin Yang, Yibo Miao, Bowen Qu, Bohong Yin, Shaowei Liu, Zaida Zhou, Liang Chen, Longhui Yu, Lin Sui, Yuzi Yan, Tianhui Song, Enzhe Lu, Hao Ding, Heng Wang, Yuanxin Liu, Ziwei Chen, Jiezhong Qiu, Haoyu Lu, Dikang Du, Angang Du, Xinxing Zu, Yibo Liu, Nuo Xu, Haotian Yao, Enming Yuan, Xingcheng Yao, Tao Yu, Zijia Zhao, Qizheng Gu, Yidao Qin, Congcong Wang, Jingyuan Liu, Weixin Xu, Xiaokun Yuan, Dehao Zhang, Guangda Wei, Yimin Chen, Zhejun Jiang, Mengnan Dong, Weixiao Huang, Bowen Wang, Han Zhu, Yang Li, Hao Hu, Hao Yang, Yangyang Hu, Haoning Wu, Chu Wei, Flood Sung, Chenlin Zhang, Yejie Wang, Cheng Chen, Guokun Lai, Fang Li, Wei Song, Xingzhe Wu, Yuzhi Wang, Chenzhuang Du, Yongsheng Kang, Mengfan Dong, Zhiqi Huang, Runjie Zhou, Yulun Du, Jiaqi Deng

- ***What's New***: Kimi-VL은 새로운 비전-언어 모델(Vision-Language Model; VLM)로, 전문가 혼합 모델(Mixture-of-Experts; MoE) 아키텍처를 사용하여 멀티모달 추론 및 장문 맥락 이해에서 강력한 성능을 발휘합니다. 새로운 Kimi-VL-Thinking은 장기 추론(Long-horizon Reasoning) 능력을 강화하여 보다 복잡한 멀티모달 추론 시나리오 수행을 중심으로 개발되었습니다.
- ***Technical Details***: Kimi-VL은 2.8B 활성 파라미터를 가진 문장 디코더와 400M 네이티브 해상도 비전 인코더로 구성됩니다. 문장 디코더에 MoE 아키텍처를 사용하여 추론 능력을 높였으며, 장문 입력을 처리하기 위한 128K 확장된 콘텍스트 윈도우를 지원합니다. Kimi-VL-Thinking은 장기 사유 모델로, 강화 학습(RL) 및 연쇄 사고 훈련(CoT SFT)을 통해 복잡한 멀티모달 추론 능력을 더욱 강화했습니다.
- ***Performance Highlights***: Kimi-VL은 다양한 벤치마크에서 가장 앞선 모델들과 비교해도 뛰어난 성능을 보이며, 특히 비전-언어 추론, 장문 맥락 이해에서 우수한 성과를 거둡니다. InfoVQA에서는 83.2점, ScreenSpot-Pro에서는 34.5점을 기록했으며, Kimi-VL-Thinking은 MMMU에서 61.7점, MathVista에서 71.3점을 달성하며 복잡한 추론 작업에서도 높은 효율성을 보여 줍니다.

### [VCR-Bench: A Comprehensive Evaluation Framework for Video Chain-of-Thought Reasoning](https://arxiv.org/abs/2504.07956)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.07956.png)

Vote: 34

Authors: Wenxuan Huang, Jie Zhao, Zehui Chen, Yukun Qi, Feng Zhao, Xikun Bao, Yu Zeng, Lin Chen, Yiming Zhao, Zhongang Qi

- ***What's New***: VCR-Bench는 비디오 체인-오브-써트(Chain-of-Thought; CoT) 추론의 평가를 위해 설계된 최초의 포괄적 벤치마크입니다. 이 벤치마크는 다양한 비디오 콘텐츠와 기간을 포괄하며, 1,034개의 고품질 질문-답변 쌍을 포함하고 각 쌍은 CoT 추론 논리에 따라 단계별로 수동으로 주석이 달려 있습니다.
- ***Technical Details***: VCR-Bench는 7개의 서로 다른 작업 차원으로 설계되었으며, 이는 LVLMs의 비디오 CoT 추론 능력을 평가하기 위한 CoT 점수를 제안합니다. 데이터 샘플에 대한 표준 답변을 제공하는 것 외에도, 자세하고 정확한 단계별 추론을 주석으로 제공하며, 이를 통해 모델의 추론 능력을 포괄적으로 측정합니다.
- ***Performance Highlights***: 현재 LVLMs의 상당한 한계를 보여주는 실험 결과를 통해 상위 성능 모델 o1도 CoT 점수 62.8%와 정확도 56.7%만을 기록했으며, 대부분의 모델들은 40% 이하를 기록했습니다. 이는 LVLMs의 주요 병목이 복잡한 비디오 추론에 대한 시각적 인식 및 시공간 정보 처리에 있음을 나타냅니다.

### [VisualCloze: A Universal Image Generation Framework via Visual In-Context Learning](https://arxiv.org/abs/2504.07960)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.07960.png)

Vote: 32

Authors: Le Zhuo, Ruoyi Du, Zhong-Yu Li, Zhanyu Ma, Juncheng Yan, Peng Gao, Ming-Ming Cheng, Zhen Li

- ***What's New***: VisualCloze는 시각적 인-컨텍스트 학습(Visual In-Context Learning)을 통해 다양한 이미지를 생성할 수 있는 보편적인 프레임워크를 제안합니다. 이 프레임워크는 기존의 언어 기반 작업 지시 대신 시각적 데모를 통해 모델이 작업을 학습하도록 하여, 보이는 범위 내의 작업뿐만 아니라 보이지 않는 작업에서도 일반화 기능을 향상시킵니다.
- ***Technical Details***: VisualCloze는 Graph200K라는 그래프 구조의 데이터셋을 사용하여 여러 관련 작업을 설정하고, 작업 밀도를 높여서 이동 가능한 지식을 습득합니다. 또한, 이미지 삽입을 기반으로 한 통합 이미지 생성 공식은 기존의 이미지 삽입 모델에서 수립된 강력한 생성적 사전(Generative Priors)을 활용할 수 있도록 해 줍니다.
- ***Performance Highlights***: VisualCloze 모델은 다양한 도메인 내 작업에서 인-컨텍스트 학습을 통해 더 나은 작업 의도와 일반화 능력을 보여주었습니다. 예를 들어, 깊이 지도에서 이미지로의 변환에서 RMSE가 25.06에서 10.31로 감소했습니다. 또한, 주제 주도 이미지 생성에서 DINOv2, CLIP-I, CLIP-T 점수에서 일관된 개선을 보여주었습니다.

### [DeepSeek-R1 Thoughtology: Let's <think> about LLM Reasoning](https://arxiv.org/abs/2504.07128)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.07128.png)

Vote: 31

Authors: Karolina Stańczak, Gaurav Kamath, Austin Kraft, Aditi Khandelwal, Milad Aghajohari, Marius Mosbach, Siva Reddy, Sara Vera Marjanović, Mehar Bhatia, Amirhossein Kazemnejad, Vaibhav Adlakha, Benno Krojer, Parishad BehnamGhader, Nicholas Meade, Dongchan Shin, Arkil Patel, Xing Han Lù

- ***What's New***: DeepSeek-R1은 대형 언어 모델(LLM)에서 복잡한 문제 해결을 위한 새로운 접근법을 제안합니다. 입력에 직접 답을 생성하는 대신, 다단계 추론 체인을 생성하여 문제를 '생각'한 후 답변을 제공합니다. 이를 통해 모델의 추론 행동을 연구할 수 있는 기회를 제공하며, Thoughtology라는 새로운 연구 분야를 개척합니다.
- ***Technical Details***: DeepSeek-R1은 기본적인 추론의 빌딩 블록에 대한 분류 체계를 통해 분석되며, 추론 길이의 영향을 조사하고, 복잡하거나 혼란스러운 컨텍스트에서의 처리, 안전 및 문화적 측면을 고려합니다. DeepSeek-R1은 RL(강화 학습)을 통해 학습되었으며, 추론 체인에 접근할 수 있는 최초의 고성능 LLM입니다.
- ***Performance Highlights***: DeepSeek-R1의 추론 체인은 문제별로 최적의 길이가 존재하며, 이를 넘어가면 성능이 감소합니다. 긴 컨텍스트 처리에서는 최상위 LLM에 비해 약간 떨어지는 성능을 보이지만, 추론 능력은 비추론 모델보다 뛰어납니다. 안전 평가에서는 DeepSeek-R1이 비추론 모델보다 더 높은 위험을 나타냈고, 다양한 언어에서 문화적 가치 차이를 나타냈습니다.

### [MM-IFEngine: Towards Multimodal Instruction Following](https://arxiv.org/abs/2504.07957)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.07957.png)

Vote: 26

Authors: Yuhang Zang, Xiaoyi Dong, Shenxi Wu, Shengyuan Ding, Xiangyu Zhao, Haodong Duan, Yuhang Cao, Jiaqi Wang, Dahua Lin, Pan Zhang

- ***What's New***: MM-IFEngine는 고품질의 이미지-지시 쌍을 생성하기 위한 효과적인 파이프라인을 제안하여 멀티모달 지시 추종(Multimodal Instruction Following; MIF)의 훈련 데이터를 대량으로 만들어 냅니다. 이는 MM-IFInstruct-23k와 MM-IFDPO-23k 데이터셋을 구축하여 MLLMs(멀티모달 대형 언어 모델) 훈련에 사용됩니다.
- ***Technical Details***: MM-IFEngine는 다양한 이미지 소스를 수집하고 16개의 작업 설명과 32개의 제약 조건을 사용하여 개별 이미지에 맞는 지시를 작성합니다. 생성된 MM-IFInstruct-23k는 Supervised Fine-Tuning(SFT)용으로 사용되고, 제약 조건을 제거하여 생성된 MM-IFDPO-23k는 Direct Preference Optimization(DPO)에 사용됩니다.
- ***Performance Highlights***: MM-IFEngine를 통해 훈련된 모델은 기존의 멀티모달 지시 추종 벤치마크에서 평균 10% 이상의 성능 향상을 보여주었고, MM-IFEval에서 선도적인 독점 모델(GPT-4o: 64.6%)과 비교하여 개방형 모델 중 최고 성능을 기록했습니다(Qwen2-VL-72B: 50.8%). 이는 MM-IFEngine을 활용한 고품질 훈련 데이터 세트의 효율성을 입증합니다.

### [HoloPart: Generative 3D Part Amodal Segmentation](https://arxiv.org/abs/2504.07943)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.07943.png)

Vote: 22

Authors: Xihui Liu, Yuan-Chen Guo, Yunhan Yang, Zi-Xin Zou, Zhipeng Yu, Yan-Pei Cao, Yukun Huang, Yangguang Li

- ***What's New***: HoloPart는 3D 파트 암달(3D Part Amodal) 세분화를 위한 새로운 생성 모델입니다. 기존의 2D 암달 세분화 작업에서 받은 영감을 바탕으로 3D 형태로 확장하여 3D 모양을 완벽하게 의미 있는 파트로 분해하는 문제를 해결합니다. 이를 통해 기하학적 편집, 애니메이션, 재료 할당과 같은 다양한 다운스트림 애플리케이션을 지원합니다.
- ***Technical Details***: HoloPart는 두 단계 접근법을 채택합니다. 첫 번째 단계는 초기, 불완전한 파트 세분화를 생성하는 것이고, 두 번째 단계에서는 HoloPart라는 새로운 확산 기반의 모델(Diffusion-based Model)을 도입하여 이러한 세분화를 완전한 3D 파트로 완료합니다. HoloPart의 설계는 세부적인 기하학적 세부 사항을 포착하기 위한 로컬 주의(Local Attention)와 전체적인 모양 일관성을 보장하기 위한 형상 문맥 인지 주의(Shape Context-aware Attention)를 활용합니다. 이 모델은 ABO 및 PartObjaverse-Tiny 데이터셋을 기반으로 한 새로운 벤치마크 테스트를 통해 평가됩니다.
- ***Performance Highlights***: HoloPart는 기존의 형태 완성(shape completion) 방법들을 훨씬 뛰어넘는 탁월한 성능을 보였으며, 특히 보고된 벤치마크 테스트인 Chamfer Distance, IoU, F-Score와 같이 측정된 성능 지표에서 두드러진 향상을 나타냈습니다. HoloPart는 파트 완성과 3D 파트 암달 세분화 작업에서 현저한 성능을 보임으로써 실용적인 응용 가능성과 다양한 다운스트림 애플리케이션에 대한 비전을 제시합니다.

### [C3PO: Critical-Layer, Core-Expert, Collaborative Pathway Optimization for Test-Time Expert Re-Mixing](https://arxiv.org/abs/2504.07964)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.07964.png)

Vote: 16

Authors: Tianyi Zhou, Zhongyang Li, Ziyue Li

- ***What's New***: C3PO는 테스트 시점에서 전문가 믹싱을 최적화하여 Mixture-of-Experts (MoE) 대규모 언어 모델의 경로를 개선하는 새로운 접근법입니다. 이 방법은 기존 경로 셀렉션에서 발생하는 10-20%의 성능 차이를 줄이기 위해 각 테스트 샘플에 대해 전문가를 재조정하는 것입니다.
- ***Technical Details***: C3PO는 경로 최적화를 위해 모드 찾기, 커널 회귀, 손실 가중합 등 세 가지 대리 목표를 도입합니다. 이 방법은 핵심 전문가의 혼합 가중치를 중요 계층에만 적용하여 계산 비용을 절감하면서도 유사한 성능을 유지합니다. 따라서 효율적인 경로 최적화를 달성합니다.
- ***Performance Highlights***: C3PO는 두 가지 최신 MoE 모델에 적용되어 6개 벤치마크에서 7-15%의 정확도 향상을 일관되게 보여주었습니다. 또한, C3PO는 활성 매개변수가 1-3B인 MoE LLM이 7-9B 매개변수를 가진 모델보다 뛰어난 성능을 발휘할 수 있게 하여 MoE의 효율성을 더욱 장려합니다.

### [MOSAIC: Modeling Social AI for Content Dissemination and Regulation in Multi-Agent Simulations](https://arxiv.org/abs/2504.07830)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.07830.png)

Vote: 14

Authors: Elisa Kreiss, Marzyeh Ghassemi, Salman Rahman, Saadia Gabriel, Genglin Liu

- ***What's New***: MOSAIC는 최초로 개방형 소셜 네트워크 시뮬레이션 프레임워크를 제시하여 생성적 언어 에이전트(Generative Language Agents)를 통해 사용자의 콘텐츠 행동을 예측하고, 다중 에이전트 시뮬레이션을 통해 콘텐츠 확산과 참여 역학을 모델링합니다. 이 시뮬레이션은 콘텐츠 불확실성과 사용자의 사회적 상호작용 패턴을 연구하는 데에 활용됩니다.
- ***Technical Details***: MOSAIC 시뮬레이션은 LLM 에이전트와 방향성 소셜 그래프를 결합하여 콘텐츠 확산과 참여 패턴을 분석합니다. 사용자의 다양하고 세밀한 페르소나로부터 사용자 표현을 구성하여 콘텐츠 전달과 참여 역학을 대규모로 모델링합니다. 세 가지 콘텐츠 규제 전략을 평가하여 비사실적 콘텐츠의 확산을 줄이고 사용자의 참여를 증가시키는 것을 목표로 합니다.
- ***Performance Highlights***: 시뮬레이션 결과, 사실 검증 전략은 허위정보의 확산을 완화하면서 사용자 참여를 증가시키는 것으로 나타났습니다. MOSAIC는 커뮤니티 중심의 사실 검증과 제3자 주도의 사실 검증의 조합이 가장 효과적인 것으로 드러났으며, 이는 콘텐츠 규제의 점진적 이점을 증명합니다.

### [Scaling Laws for Native Multimodal Models Scaling Laws for Native Multimodal Models](https://arxiv.org/abs/2504.07951)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.07951.png)

Vote: 12

Authors: Mustafa Shukor, Victor Guilherme Turrisi da Costa, Matthieu Cord, Alaaeldin El-Nouby, Enrico Fini, Joshua Susskind

- ***What's New***: 이번 연구는 초대형 멀티모달 모델(Native Multimodal Models; NMM)의 스케일링 법칙(Scaling Laws)을 폭넓게 조사한 결과, 별도로 사전 훈련된 컴포넌트를 통합한 말기 융합 아키텍처(late-fusion architectures)가 근본적으로 우월하지 않다는 것을 밝혔습니다. 대신, 초기 융합 초기술(early-fusion architectures)이 낮은 파라미터 수에서 더 강력한 성능을 보이며 훈련 효율성 및 배포 용이성을 가지는 것으로 나타났습니다. 더 나아가, 전문가의 혼합(Mixture of Experts; MoEs)을 통합하여 모달리티별로 최적화된 가중치를 학습함으로써 성능을 크게 향상시킬 수 있음을 보여주었습니다.
- ***Technical Details***: 457개의 다양한 아키텍처와 훈련 혼합비를 가진 모델을 훈련하여 NMM의 스케일링 법칙을 연구하였습니다. 초기 융합 아키텍처는 전용 비전 인코더를 사용하지 않고 원시 멀티모달 입력을 처리하며, MoEs는 각 모달리티에 대해 대칭적이고 병렬적인 방법으로 특화된 파라미터를 동적으로 할당할 수 있도록 합니다. 이번 연구는 초기 융합과 말기 융합이 동일한 FLOP(부동소수처리연산) 예산 하에서 유사한 최종 모델 손실을 제공함을 보여주며, 각 아키텍처에서 compute-optimal 모델을 위한 예측이 가능하다는 것을 보였습니다.
- ***Performance Highlights***: 초기 융합 아키텍처는 말기 융합보다 낮은 파라미터 수 및 더 많은 훈련 토큰을 요구하였으며, FLOP 스케일링에 있어서 초기 융합 모델이 말기 융합보다 낮은 추론 비용을 요구함으로써 성능을 더욱 최적화했습니다. MoEs는 동일한 추론 비용 하에서 조밀한 모델보다 더 낮은 손실을 보였으며, 이는 활발한 파라미터의 수보다는 훈련 토큰의 수를 더 크게 스케일링하는 것이 중요함을 시사합니다. 또한, 모달리티 불문 라우팅이 작은 모달리티를 인식한 라우팅보다 일관되게 우수한 성능을 보였습니다.

### [SoTA with Less: MCTS-Guided Sample Selection for Data-Efficient Visual Reasoning Self-Improvement](https://arxiv.org/abs/2504.07934)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.07934.png)

Vote: 10

Authors: Linjie Li, Chung-Ching Lin, Kevin Lin, Zhengyuan Yang, Furong Huang, Xiyao Wang, Lijuan Wang, Hongjin Lu, Chao Feng

- ***What's New***: 이 논문에서는 데이터 효율적인 시각 추론(Self-Improvement of Visual Reasoning)을 달성하는 새로운 방법을 소개합니다. 기존의 지식 증류(Knowledge Distillation)를 피하고 대규모 강화 미세 조정(Reinforcement Fine-Tuning; RFT)만으로 시각언어모델(VLMs)의 추론 성능을 향상시킵니다. 제안된 방법은 MCTS(Monte Carlo Tree Search)를 사용하여 훈련 데이터의 난이도를 정량화하고, 이를 기반으로 데이터 가이디드 샘플 선택(Data-Guided Sample Selection)을 합니다.
- ***Technical Details***: ThinkLite-VL 모델은 70k 오픈 소스 훈련 데이터 샘플에서 시작하여 MCTS 기반 선택 방법을 통해 샘플 난이도를 정량화합니다. VLMs가 문제를 해결하기 위해 필요로 하는 순회 횟수를 기반으로 난이도를 평가하며, 11k개의 고품질 샘플을 필터링하여 RFT에 사용합니다. 이는 Qwen2.5-VL-7B-Instruct 모델을 기반으로 하며, 지식 증류나 추가적인 감독된 미세 조정(SFT) 없이 성능을 향상시킵니다.
- ***Performance Highlights***: ThinkLite-VL-7B는 7B 수준의 기존 시각 추론 모델들을 뛰어넘는 성능을 보이며, 8개의 벤치마크에서 평균적인 성능을 59.69에서 63.89로 향상시켰습니다. 특히 MathVista 벤치마크에서 75.1의 SoTA 정확도를 달성하며, 더 큰 오픈 소스 모델 및 GPT-4o 등을 능가하는 성과를 나타냅니다.

### [Towards Visual Text Grounding of Multimodal Large Language Model](https://arxiv.org/abs/2504.04974)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.04974.png)

Vote: 4

Authors: Yufan Zhou, Jiuxiang Gu, Wanrong Zhu, Tianyi Zhou, Ming Li, Tong Sun, Jian Chen, Franck Dernoncourt, Ruiyi Zhang

- ***What's New***: TRIG는 멀티모달 대형 언어 모델(Multimodal Large Language Models; MLLMs)의 시각 텍스트 그라운딩(Visual Text Grounding) 문제를 극복하기 위한 새로운 태스크로, 문서의 질의응답(Document Question-Answering)을 위한 새로운 벤치마크와 데이터셋을 소개합니다. 이는 텍스트가 풍부한 이미지에서 MLLMs의 공간적 추론 및 그라운딩 능력을 개선하기 위해 설계되었습니다.
- ***Technical Details***: TRIG 벤치마크는 DocVQA, ChartQA, InfographicsVQA 및 TRINS 데이터를 기반으로 한 800개의 수작업으로 주석된 질의응답 쌍과 90,000개의 합성 데이터로 이루어져 있습니다. 이 작업을 위해 OCR-LLM-인간 상호작용 파이프라인이 도입되어, 줌, PaddleOCR 등을 활용하여 과정별로 평가 및 교정합니다. 또한, 일반적인 지시 기반 튜닝(instruction tuning)과 효율적인 임베딩 기반 방법을 기반으로 한 두 가지 TRIG 방법을 제안합니다.
- ***Performance Highlights***: 실험 결과, 본 연구에서 제안한 방법이 강력한 GPT4o 모델을 큰 차이로 뛰어넘었고, 특히 독립적인 임베딩 기반 방법의 경우 평균 IoU가 10.0%로, 지시 기반 방식은 29.98%를 기록했습니다. 이는 공간적 추론과 지시를 성공적으로 수행하는 데 있어 MLLMs의 능력을 보여줍니다. 본 연구는 기존 모델이 복잡한 문서 레이아웃과 텍스트 내용을 다루는 데 있어 실제적인 한계를 드러내며, 이는 향후 모델개선을 위한 가치 있는 기준점을 제공합니다.

### [Compass Control: Multi Object Orientation Control for Text-to-Image Generation](https://arxiv.org/abs/2504.06752)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.06752.png)

Vote: 1

Authors: Rishubh Parihar, R. Venkatesh Babu, Sachidanand VS, Vaibhav Agrawal

- ***What's New***: 이 연구는 텍스트-이미지 디퓨전 모델(Text-to-Image Diffusion Models)에서 오브젝트의 정확한 3D 방향 제어를 가능하게 하는 새로운 방법인 Compass Control을 소개합니다.
- ***Technical Details***: Compass Control은 다중 객체 장면에서 각 객체의 방향 제어를 위한 경량 인코더 네트워크를 사용하여 각 객체의 방향에 기반한 나침반 토큰(Compass Tokens)을 예측합니다. 이 모델은 기본 텍스트 인코더와 확산 U-Net의 학습을 위해 합성 데이터셋을 사용하며, Coupled Attention Localization (CALL) 메커니즘을 도입하여 크로스-어텐션 맵을 제약합니다.
- ***Performance Highlights***: Compass Control은 학습되지 않은 복잡한 객체 및 다중 객체 장면에 대해 강력한 일반화 성능을 보여주며, 개인화 기법과 결합 시 새로운 객체의 방향을 다양한 컨텍스트에서 정확하게 제어할 수 있습니다. 사용자 연구와 체계적인 평가를 통해 첨단 수준의 방향 제어 및 텍스트 정렬 성능을 달성했습니다.

### [TAPNext: Tracking Any Point (TAP) as Next Token Prediction](https://arxiv.org/abs/2504.05579)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.05579.png)

Vote: 1

Authors: Sarath Chandar, Yi Yang, Ignacio Rocco, Viorica Patraucean, Xu Owen He, Mehdi S. M. Sajjadi, Ross Goroshin, Artem Zholus, Carl Doersch, Skanda Koppula

- ***What's New***: TAPNext는 비디오 이미지의 임의 지점 추적(Tracking Any Point)을 다음 토큰 예측(Next Token Prediction) 문제로 변환하는 새로운 접근 방식을 소개합니다. 기존의 추적 모델들이 특정 전처리나 추론 방법에 의존하지 않고도 온라인 및 오프라인 추적에서 최첨단 성능을 달성합니다.
- ***Technical Details***: TAPNext은 임의 지점 추적 작업을 시퀀스 마스킹 토큰 디코딩 방식으로 처리하며, SSM(State-Space Model)과 ViT(Vision Transformer) 블록을 기본 구조로 사용합니다. 특히, 성능을 저하시키는 기존의 복잡한 인덕티브 바이이어스를 배제하고 온라인 추적을 실시간으로 처리하는 간단한 반복 구조를 사용합니다.
- ***Performance Highlights***: TAPNext는 DAVIS와 Kinetics 데이터셋에서 탁월한 성능을 보이며 대부분의 성능 지표에서 최신 기술의 성능을 능가합니다. 특히, 이전에 비해 대규모 비디오 추적을 더 효율적으로 처리하며, 다양한 응용 프로그램에서의 가능성을 입증했습니다.

### [Geo4D: Leveraging Video Generators for Geometric 4D Scene Reconstruction](https://arxiv.org/abs/2504.07961)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.07961.png)

Vote: 1

Authors: Chuanxia Zheng, Zeren Jiang, Diane Larlus, Iro Laina, Andrea Vedaldi

- ***What's New***: Geo4D는 비디오 생성기를 활용하여 동적 장면의 단안 3D 복원을 수행할 수 있는 새로운 방법론을 도입했습니다. 동영상 모델에서 포착한 강력한 동적 선행 데이터를 활용하여, 이 모델은 오로지 합성 데이터로만 학습하고도 실제 데이터에 제로샷으로 일반화가 가능합니다.
- ***Technical Details***: Geo4D는 여러 보완적인 기하학적 형상(geometric modalities)인 포인트 맵, 깊이 맵, 광선 맵을 예측하고, 다중 모달 정렬 알고리즘을 사용하여 이러한 형상을 정렬 및 융합합니다. 또한, 여러 슬라이딩 윈도우를 사용하여 추론 시간을 줄이며 길이가 긴 비디오에 대한 강력하고 정확한 4D 복원을 구현합니다.
- ***Performance Highlights***: 다양한 벤치마크에서 수행된 실험들은 Geo4D가 MonST3R와 같이 동적 장면을 다루도록 설계된 최신 비디오 깊이 추정 방법들을 크게 능가한다는 것을 보여줍니다. Sintel, Bonn, KITTI 데이터셋에서의 실험 결과 Geo4D는 이전의 최첨단 방법보다 절대 상대 오차(Abs Rel) 메트릭에서 더 나은 성능을 보였습니다.

### [MonoPlace3D: Learning 3D-Aware Object Placement for 3D Monocular Detection](https://arxiv.org/abs/2504.06801)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.06801.png)

Vote: 1

Authors: Sarthak Vora, Srinjay Sarkar, Rishubh Parihar, R. Venkatesh Babu, Jogendra Kundu

- ***What's New***: MonoPlace3D는 3D 단안 탐지(Monocular Detection)를 위한 혁신적 3D 인식 객체 배치 방법론을 제안합니다. 이는 기존의 데이터 세트가 가지는 다양성과 규모의 한계를 해결하기 위해 장면 인식 학습을 통한 사실적인 3D 증강을 구현합니다. 이를 통해 실외 환경에서도 현실적인 객체 배치가 가능해졌습니다.
- ***Technical Details***: MonoPlace3D는 3D Scene-Aware Placement Network (SA-PlaceNet)를 도입하여 주어진 이미지에 대해 가능한 3D 경계 상자(Bounding Box)의 분포를 학습합니다. 이 모델은 도로 장면의 물리적 규칙을 준수하여 더욱 다양한 3D 객체 배치를 가능하게 하며, 기존 3D 객체 탐지 데이터 세트를 활용하여 학습합니다. 객체의 사실적인 외형을 구현하기 위해, ControlNet을 활용한 이미지-이미지 변환 모델을 통해 합성 객체가 실제와 유사하게 렌더링 됩니다.
- ***Performance Highlights***: MonoPlace3D는 KITTI와 NuScenes 데이터 세트에서 기존의 단안 3D 탐지 모델의 정확도를 크게 향상시켰으며, 데이터 효율성도 높았습니다. 간단한 배치 방법을 사용한 Lift3D와 비교했을 때, 우리의 증강 방법은 모형의 데이터 효율성을 증가시킴으로써 성능 개선의 주요한 역할을 했습니다. 특정 실험에서는 전체 데이터의 50%만 사용하고도 전체 데이터로 학습한 경우와 유사한 탐지 성능을 달성했습니다.

