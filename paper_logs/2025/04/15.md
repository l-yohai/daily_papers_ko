## Daily Papers (2025-04-15)

### [InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models](https://arxiv.org/abs/2504.10479)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10479.png)

Vote: 167

Authors: Wenqi Shao, Weiye Xu, Jiahao Wang, Kai Chen, Conghui He, Shenglong Ye, Hao Li, Botian Shi, Yi Wang, Yue Cao, Weiyun Wang, Jiapeng Luo, Wenhai Wang, Peng Sun, Zhaoyang Liu, Lijun Wu, Weijie Su, Kaipeng Zhang, Xingcheng Zhang, Lixin Gu, Zhe Chen, Hao Tian, Dahua Lin, Xizhou Zhu, Tong Lu, Erfei Cui, Dengnian Chen, Jie Shao, Min Dou, Yuchen Duan, Huipeng Deng, Limin Wang, Yu Qiao, Yinan He, Jifeng Dai, Tan Jiang, Junjun He, Songze Li, Wenwen Qu, Yangzhou Liu, Lewei Lu, Jinguo Zhu, Penglong Jiao, Han Lv, Yingtong Xiong, Jiaye Ge, Zhangwei Gao

- ***What's New***: InternVL3는 기존의 언어 중심 대형 언어 모델을 멀티모달 대형 언어 모델(MLLM)로 전환하는 복잡한 프로세스를 거치지 않고 단일 프리트레이닝 단계에서 다양한 멀티모달 데이터와 순수 텍스트 코퍼스를 활용하여 멀티모달 및 언어적 역량을 동시에 학습할 수 있는 네이티브 멀티모달 프리트레이닝 패러다임을 도입합니다.
- ***Technical Details***: InternVL3는 가변 시각적 위치 인코딩(Variable Visual Position Encoding, V2PE)을 사용하고, Supervised Fine-Tuning(SFT) 및 혼합 선호 최적화(Mixed Preference Optimization, MPO)와 같은 고급 포스트 트레이닝 기법을 포함합니다. 또한 최적화된 프레임워크를 통해 수백억 개의 매개변수까지 확장이 가능하며, 이를 통해 멀티모달 태스크에서의 효율성과 성능을 대폭 향상시킵니다.
- ***Performance Highlights***: InternVL3-78B는 MMMU 벤치마크에서 72.2점을 기록하며, 기존의 오픈 소스 MLLM을 뛰어넘어 새로운 성능 기준을 세웠습니다. 특히, ChatGPT-4o와 Claude 3.5 Sonnet 등의 상업적 모델과도 경쟁력이 있으며, 언어 능력 또한 유지하여 순수 언어 태스크에서도 뛰어난 성과를 보입니다.

### [PRIMA.CPP: Speeding Up 70B-Scale LLM Inference on Low-Resource Everyday Home Clusters](https://arxiv.org/abs/2504.08791)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.08791.png)

Vote: 92

Authors: Wenjiao Feng, Tao Li, Zonghang Li, Mohsen Guizani, Hongfang Yu

- ***What's New***: PRIMA.CPP는 일상적인 가정용 클러스터에서 70B 규모의 대형 언어 모델(LLM)을 실행하기 위해 설계된 분산 추론 시스템입니다. 이 시스템은 CPU/GPU, 낮은 RAM/VRAM, Wi-Fi를 활용하여 홈 어시스턴트에서 첨단 AI 모델을 사용 가능하게 만듭니다. 코드는 오픈 소스이며 [GitHub 링크](https://github.com/Lizonghang/prima.cpp)에서 확인할 수 있습니다.
- ***Technical Details***: PRIMA.CPP는 mmap을 사용하여 모델 가중치를 관리하고, piped-ring parallelism과 prefetching 기법을 도입하여 디스크 로딩 지연 시간을 숨깁니다. 이 시스템은 이종 시스템의 계층을 CPU와 GPU에 최적으로 할당하여 토큰 지연 시간을 줄입니다. NP-hard 문제인 계층-디바이스 할당 문제를 해결하기 위해 Halda라는 알고리즘을 제안합니다. 이는 ILP로 문제를 변환하여 다항식 시간 내 최적 해를 찾을 수 있도록 합니다.
- ***Performance Highlights***: PRIMA.CPP는 4개의 노드로 구성된 가정용 클러스터에서 평가되었으며, llama.cpp와 다른 분산 대안보다 70B 모델에서 최대 15배 빠른 토큰 지연 시간을 기록했습니다. 이 시스템은 메모리 압력을 6% 이하로 유지하며, 가정용 디바이스로 Llama 3, DeepSeek R1, Qwen 2.5, QwQ 등의 모델을 실행하는 성능을 보여줍니다.

### [Have we unified image generation and understanding yet? An empirical study of GPT-4o's image generation ability](https://arxiv.org/abs/2504.08003)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.08003.png)

Vote: 34

Authors: Ning Li, Justin Cui, Jingran Zhang

- ***What's New***: GPT-4o의 이미지 생성 능력에 대한 체계적인 평가가 진행되어, 이 모델이 세계 지식과 문맥적 추론을 이미지 생성에 얼마나 잘 통합할 수 있는지에 대해 새로운 통찰을 제공합니다. 이번 연구는 글로벌 지시 준수(Global Instruction Adherence), 정밀 편집(Fine-Grained Editing Precision), 생성 후 추론(Post-Generation Reasoning)이라는 세 가지 주요 측면에서 GPT-4o의 능력을 평가하였습니다.
- ***Technical Details***: 본 연구는 GPT-4o의 이미지 생성 능력을 평가하기 위해 세 가지 유형의 프롬프트를 사용하였습니다. 첫째, 글로벌 지시 프롬프트는 모델이 지시 사항을 문자 그대로가 아닌 문맥에 맞게 해석할 수 있는지를 테스트합니다. 둘째, 이미지 편집 프롬프트는 모델이 주어진 이미지의 특정 요소를 세밀하게 편집할 수 있는지를 평가합니다. 셋째, 생성 후 추론 프롬프트는 모델이 이미지 생성 후 문맥적 이해와 논리적 추론을 유지할 수 있는지를 실험합니다.
- ***Performance Highlights***: GPT-4o는 이미지 생성 시 주어진 지시를 문자 그대로 해석하는 경향이 있으며, 추상적 또는 문맥적 논리를 통합하는 데 어려움을 겪고 있다는 점이 발견되었습니다. 특히 숫자 변환이나 선택적 이미지 편집 같은 추상적 글로벌 규칙을 통합하는데 한계를 보였습니다. 이러한 결과는 GPT-4o가 다중 모달 통합 및 이미지 이해의 통합적 능력에서 아직 상당한 개선 여지가 있음을 시사합니다.

### [VL-Rethinker: Incentivizing Self-Reflection of Vision-Language Models with Reinforcement Learning](https://arxiv.org/abs/2504.08837)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.08837.png)

Vote: 34

Authors: Haozhe Wang, Zuming Huang, Fangzhen Lin, Chao Qu, Wei Chu, Wenhu Chen

- ***What's New***: VL-Rethinker는 비전-언어 모델(Vision-Language Model; VLM)의 멀티모달 느린 사고(slow-thinking) 능력을 강화하기 위해 강화 학습(Reinforcement Learning)을 사용하는 혁신적인 접근법입니다. 이 모델은 선택적 샘플 재생(Selective Sample Replay; SSR)과 강제 재고(Forced Rethinking) 기법을 도입하여 VLM의 성능을 크게 개선합니다.
- ***Technical Details***: GRPO(Group Relative Policy Optimization) 알고리즘을 SSR과 결합하여 멀티모달 추론 훈련의 안정성을 향상시켰습니다. SSR은 모든 응답에 동일한 보상이 주어질 때 발생하는 소멸하는 이득(vanishing advantages) 문제를 해결하기 위해, 품질 높은 샘플을 리플레이 버퍼에서 선택적으로 반복 학습합니다. 또한, 초기 모델 응답에 강제 재고 트리거를 추가하여 자발적 자기 반성을 유도합니다. 이 방법은 GRPO 기반의 RL 훈련을 기반으로 실행되며, 모델의 느린 사고 행동을 촉진합니다.
- ***Performance Highlights***: VL-Rethinker는 수학 관련 측정기준(MathVista, MathVerse, MathVision)에서 이전 최첨단 모델들보다 높은 성과(80.3%, 61.7%, 43.9%)를 기록했습니다. 또한, 다양한 멀티디스플린 측정기준(MMMU-Pro, EMMA, MEGA-Bench)에서도 강력한 성능을 발휘하며 GPT-o1과의 격차를 좁혔습니다.

### [FUSION: Fully Integration of Vision-Language Representations for Deep Cross-Modal Understanding](https://arxiv.org/abs/2504.09925)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.09925.png)

Vote: 34

Authors: Bin Cui, Wentao Zhang, Mengjie Liu, Zheng Liu, Conghui He, Jingzhou Chen, Jingwei Xu

- ***What's New***: FUSION은 비전-언어 표현(Vision-Language Representations)을 완전히 통합하여 깊이 있는 교차 모달 이해를 제공하는 멀티모달 및 대형 언어 모델(Multimodal Large Language Models, MLLMs)입니다. 기존의 후처리 모달 상호작용에 의존하지 않고 전처리 전반에 걸쳐 깊고 동적 통합을 이뤄낸 것이 특징입니다.
- ***Technical Details***: FUSION은 텍스트 주도 통합 비전 인코딩(Text-Guided Unified Vision Encoding)을 통해 비전 인코딩 단계에서 텍스트 정보를 포함하여 픽셀 수준의 통합을 실현합니다. 또한, 텍스트 컨텍스트 조건에 따른 시각적 특징을 순차적으로 집계하는 컨텍스트 기반 순환 정렬 디코딩(Context-Aware Recursive Alignment Decoding)을 고안하여 질문 수준의 정밀한 의미적 통합을 가능하게 합니다. 이러한 특징 매핑을 안내하고 모달리티 불일치를 완화하기 위해 이중 지도 의미 매핑 손실(Dual-Supervised Semantic Mapping Loss)을 개발했습니다.
- ***Performance Highlights***: FUSION은 두 가지 크기(3B, 8B)로 훈련되었으며 기존 기법들보다 630개의 비전 토큰만으로도 뛰어난 성능을 보여줍니다. 특히, FUSION 3B 모델은 Cambrian-1 8B 및 Florence-VL 8B를 대부분의 벤치마크에서 능가하였고, FUSION 3B는 비전 토큰 수를 300으로 제한했을 때도 여전히 Cambrian-1 8B보다 뛰어난 성능을 유지하였습니다. 또한, FUSION은 다른 최신 모델들과 비교해도 적은 리소스로 동일한 성능을 유지합니다.

### [Iterative Self-Training for Code Generation via Reinforced Re-Ranking](https://arxiv.org/abs/2504.09643)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.09643.png)

Vote: 29

Authors: Valentin Malykh, Ivan Sedykh, Nikita Sorokin

- ***What's New***: 이 논문은 새로운 기법인 RewardRanker를 소개하여 코드 생성의 품질을 향상시킵니다. 이 접근법은 Proximal Policy Optimization (PPO)를 활용하여 반복적인 자기 학습 루프를 통해 리랭커(RewardRanker) 모델을 개선합니다. 효과적으로 리랭킹 정밀도를 개선하고 생성된 코드의 전체적인 품질을 높입니다. 특히, 이 방법은 더 작은 모델 크기의 효율적인 사용을 통해 더 큰 모델을 능가하는 성능을 보여줍니다.
- ***Technical Details***: RewardRanker는 코드 생성 품질을 향상시키기 위해 반복적인 자기 학습 사이클을 활용하여 리랭커 모델을 사용하는 접근법입니다. 각 사이클은 지도 학습(Supervised Fine-Tuning), 리워드 모델 학습, Proximal Policy Optimization (PPO)을 포함하며, 새로운 예제 생성을 통해 평가와 재학습으로 이어집니다. 이 과정에서 올바른 코드와 오류 코드(하드 네거티브)를 함께 학습시켜 모델의 리랭킹 능력과 일반화 능력을 향상시킵니다.
- ***Performance Highlights***: MultiPL-E 데이터셋에서 RewardRanker는 13.4B 파라미터 모델이 더 큰 33B 모델보다 뛰어난 결과를 보였으며, GPT-4와 유사한 성능을 발휘하고 C++ 프로그래밍 언어에서는 이를 능가했습니다. 이는 적은 자원으로도 높은 성능을 유지할 수 있는 가능성을 보여줍니다.

### [Mavors: Multi-granularity Video Representation for Multimodal Large Language Model](https://arxiv.org/abs/2504.10068)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10068.png)

Vote: 25

Authors: Jingyun Hua, Wenjing Yang, Xinlong Chen, Di Zhang, Wentao Zhang, Zekun Wang, Fuzheng Zhang, Yuanxing Zhang, Jiaheng Liu, Yushuo Guan, Zihao Wang, Weihong Lin, Bohan Zeng, Yang Shi, Zhenhua Wu

- ***What's New***: Mavors는 비디오와 이미지의 높은 해상도와 조밀한 샘플링을 기반으로 멀티모달 대형 언어 모델(MLLM)을 위한 새로운 멀티-그래뉼러리티 비디오 표현(Multi-granularity Video Representation) 방법을 제안했습니다. 이는 MLLM이 긴 비디오 컨텍스트에서 공간적 세부사항과 시간적 연속성을 모두 효율적으로 유지할 수 있게 해주는 프레임워크입니다.
- ***Technical Details***: Mavors는 두 가지 주요 컴포넌트로 구성됩니다: 1) 인트라 청크 비전 인코더(Intra-chunk Vision Encoder; IVE)는 3D 컨볼루션과 비전 트랜스포머(Visual Transformers)를 이용하여 비디오 청크 내의 고해상도 시각적 특징을 추출합니다. 2) 인터 청크 특징 결합기(Inter-chunk Feature Aggregator; IFA)는 트랜스포머 기반의 의존성 모델링을 통해 청크 간 시간적 연관성을 구축하고 청크 레벨 회전 위치 인코딩을 활용하여 시간적 패턴을 유지합니다.
- ***Performance Highlights***: Mavors는 여러 벤치마크 실험에서 기존의 방법들에 비해 공간적 충실도와 시간적 연속성을 더 잘 유지하여 뛰어난 성능을 보였습니다. 특히, 캡션 생성을 위한 복잡한 장면에서 기존 방법ologies들보다 월등한 개선을 보여주어 전체 비디오 이벤트에 대한 정확하고 포괄적인 이해를 달성하는데 효과적임을 입증했습니다.

### [AgentRewardBench: Evaluating Automatic Evaluations of Web Agent Trajectories](https://arxiv.org/abs/2504.08942)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.08942.png)

Vote: 18

Authors: Alejandra Zambrano, Karolina Stańczak, Arkil Patel, Xing Han Lù, Amirhossein Kazemnejad, Dongchan Shin, Christopher J. Pal, Siva Reddy, Nicholas Meade, Peter Shaw

- ***What's New***: AGENTREWARDBENCH는 웹 에이전트 웹사이트 상의 작업 경로를 평가하는 LLM 판사의 성능을 평가하기 위한 최초의 벤치마크입니다. 이 벤치마크는 총 1302개의 경로를 포함하며, 웹 에이전트의 성공 여부, 부작용, 반복성 등을 전문가가 직접 검토하여 평가합니다.
- ***Technical Details***: AGENTREWARDBENCH는 5개의 다양한 웹 환경과 작업을 통해 수집된 4개의 LLM 에이전트의 경로로 구성되어 있습니다. 각 경로는 화면 캡처와 설명에 따라 성공, 부작용, 반복 등의 보조 라벨이 부여됩니다. 웹 에이전트의 평가를 위해 설계된 AGENTREWARDBENCH는 기존과 새로운 LLM 판사를 평가하며, 전문가 평가와의 비교를 통해 효과성을 판단합니다.
- ***Performance Highlights***: LLM 판사의 성능은 70% 이상의 정밀도를 보이지 못해, 상당 수의 경로가 성공적으로 마크되었습니다. 이는 AGENTREWARDBENCH가 제공하는 경로를 통해 학습하는데 있어 LLM 판사의 현 상태가 최적이지 않음을 보여주며, 정확한 자동 평가의 필요성을 강조합니다. 또한, 규칙 기반 평가 방식은 전문가의 판단에 비해 웹 에이전트의 능력을 과소평가하는 경향이 있음을 나타냅니다.

### [S1-Bench: A Simple Benchmark for Evaluating System 1 Thinking Capability of Large Reasoning Models](https://arxiv.org/abs/2504.10368)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10368.png)

Vote: 16

Authors: Zefeng Zhang, Wenyuan Zhang, Shuaiyi Nie, Xinghua Zhang, Tingwen Liu

- ***What's New***: S1-Bench는 대형 추론 모델(Large Reasoning Models; LRMs)의 시스템 1 사고 능력을 평가하기 위한 새로운 벤치마크로, 직관적 사고 및 간단한 태스크에 집중합니다. 이는 현재 LRMs의 복잡한 추론에 대한 의존 방식이 시스템 1 사고 능력의 제한을 초래할 수 있다는 점을 탐구합니다.
- ***Technical Details***: S1-Bench는 다양한 도메인과 언어에 걸쳐 간단하고 명확한 질문들로 구성되어 있으며, LRMs의 시스템 1 사고 능력을 평가하는 데 주목적을 둡니다. 이 벤치마크는 4개의 주요 카테고리와 28개의 하위 카테고리로 구성된 422개의 질문-정답 쌍을 포함하고 있으며, 각 질문은 평이하여 인간과 소규모 LLMs 모두 쉽게 해결할 수 있습니다.
- ***Performance Highlights***: 22개의 LRMs를 평가한 결과, 현재 LRMs는 평균 15.5배 더 긴 응답을 생성하며 효율성이 낮다는 경향을 보였습니다. 또한 중간 과정에서 정답을 미리 식별하지만, 불필요한 추론을 지속하는 경향도 관찰되었습니다. 이는 현재의 LRMs가 단순 문제에서 정확도가 떨어지는 경향을 보이며, 단순 문제에서도 여러 오류를 발생시키기도 했습니다.

### [DUMP: Automated Distribution-Level Curriculum Learning for RL-based LLM Post-training](https://arxiv.org/abs/2504.09710)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.09710.png)

Vote: 13

Authors: Zhenting Wang, Kun Wan, Wentian Zhao, Guofeng Cui

- ***What's New***: 이 논문에서는 강화 학습(Reinforcement Learning; RL)을 이용한 LLM 사후 훈련에 있어 데이터 분포에 적응적으로 훈련을 스케줄링하는 자동화된 커리큘럼 학습 프레임워크 DUMP를 제안하고 있습니다. 이는 데이터 분포의 학습 가능성을 반영하는 정책 이점의 크기를 활용하여 상위 신뢰 경계(UCB) 원칙을 통해 샘플링 확률을 동적으로 조정합니다.
- ***Technical Details***: DUMP는 GRPO 알고리즘을 기반으로 하며, 분포별로 샘플링 가중치를 UCB 스코어를 활용해 업데이트하여 학습을 진행합니다. 구체적으로, 각 분포의 최근 이점의 절대값을 추적하여 학습 가능성을 측정하고, 이를 바탕으로 데이터 분포 간의 샘플링 우선순위를 설정합니다. 이로 인해 특정 분포에서 관측된 높은 이점(착취) 또는 낮은 샘플 수(탐험)에 따라 가중치를 할당하여 학습 효율성을 극대화합니다.
- ***Performance Highlights***: 실험 결과, DUMP는 K&K 퍼즐 데이터셋의 다양한 난이도 분포에 대해 고른 성능 향상을 나타냈습니다. 특히, 중~고난이도(캐릭터 6~12개) 분포에서 정책 향상을 가속화하고, 테스트 성능을 높이는 데 탁월한 효과를 보였습니다. 이러한 성과는 학습 데이터 분포의 다양성과 실시간 학습 신호에 유동적으로 적응하는 능력 덕분입니다.

### [Breaking the Data Barrier -- Building GUI Agents Through Task Generalization](https://arxiv.org/abs/2504.10127)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10127.png)

Vote: 12

Authors: Junxian He, Zijie Chen, Zhenzhong Lan, Chang Ma, Qiushi Sun, Junlei Zhang, Zichen Ding

- ***What's New***: 이 논문은 GUI 에이전트의 성능 향상을 위해 새로운 방식의 중간 학습(mid-training) 단계를 제안합니다. 이 과정에서 비 GUI 도메인에서 수집한 풍부한 데이터를 활용하여 GUI 작업의 일반화 성능을 향상시킵니다. 특히, 수학적 추론이나 코딩과 같은 텍스트 기반 자료가 멀티모달 GUI 에이전트의 성능을 개선하는 데 큰 효과가 있음을 발견했습니다.
- ***Technical Details***: 제안된 중간 학습 단계는 Vision Language Models(VLMs)의 기본적인 에이전트 능력을 강화하는 것을 목표로 하며, GUI 트레젝토리 데이터에 대해 미세 조정을 하기 전에 수행됩니다. 11개의 비 GUI 데이터 세트를 포함하여, 이유리(Reasoning), 지식 검색(Knowledge Retrieval), 인지(Perception) 과제를 중심으로 데이터를 수집하고 분석하였습니다. 모델의 계획 이전에 고차원적인 '생각'을 생성하도록 표준화된 절차를 활용하며, 최적화 알고리즘을 통해 학습 과정의 전이와 망각을 최소화했습니다.
- ***Performance Highlights***: 논문에서는 중간 학습을 통해 AndroidWorld에서 12.2%, WebArena에서는 8.0%의 절대적인 성능 향상이 있음을 보여줍니다. 특히, 언어 기반 수학 데이터는 AndroidWorld에서 5.4%, WebArena에서 5.6%의 향상을 가져왔으며, 멀티모달 수학 데이터 역시 상당한 퍼포먼스 개선을 달성했습니다. GUI 인지 데이터가 성능에 미치는 효과가 제한적임을 발견한 것도 주목할 만한 점입니다.

### [Executable Functional Abstractions: Inferring Generative Programs for Advanced Math Problems](https://arxiv.org/abs/2504.09763)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.09763.png)

Vote: 11

Authors: Archiki Prasad, Jaemin Cho, Elias Stengel-Eskin, Mohit Bansal, Zaid Khan

- ***What's New***: 이 논문은 고급 수학 문제를 위해 자동으로 생성적인 프로그램을 추론하는 '실행 가능한 기능적 추상화' (Executable Functional Abstractions; EFAs)라는 개념을 도입합니다. EFAs는 수학 문제의 논리를 캡슐화하여 문제 변형을 자동으로 샘플링할 수 있는 프로그램입니다.
- ***Technical Details***: EFAGen은 큰 언어 모델(Large Language Model; LLM)에 기반하여 프로그래밍 합성 작업으로 EFAs를 생성합니다. 주어진 수학 문제와 그 해결 절차에 대해 여러 후보 프로그램을 생성하고 자동화된 테스트를 통해 유효성을 검사하며 잘못된 프로그램을 필터링 합니다. 이를 통해 규명된 모든 EFAs의 수학적 속성을 단위 테스트로 형식화하고 이러한 테스트를 보상을 통해 LLM을 훈련시키는 데 사용합니다.
- ***Performance Highlights***: EFAGen을 통해 생성된 EFAs는 원래의 시드 문제에 충실하며 모델이 추가 문제 변형을 통해 개선할 수 있도록 돕습니다. EFAs는 또한 MATH-Hard와 FnEval과 같은 다양한 문제 소스에서 성공적으로 도출될 수 있었으며, EFA 기반 데이터 증강은 모델 성능을 일관되게 향상시켰습니다.

### [SocioVerse: A World Model for Social Simulation Powered by LLM Agents and A Pool of 10 Million Real-World Users](https://arxiv.org/abs/2504.10157)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10157.png)

Vote: 11

Authors: Yao Hu, Yu Wang, Yihang Yang, Guanying Li, Shiyue Yang, Jingxuan Huang, Jiebo Luo, Jiayu Lin, Hanjia Lyu, Baohua Zhou, Zhongyu Wei, Libo Sun, Yue Chen, Libo Wu, Shiping Tang, Xiawei Liu, Xinyi Mou, Xinnong Zhang, Siming Chen, Weihong Qi, Ling Yan

- ***What's New***: SocioVerse는 대규모 실세계 사용자 풀(10 Million Real-World Users)을 바탕으로 LLM 에이전트(Agents)를 구동하여 사회 시뮬레이션을 가능하게 하는 모델로 소개되었습니다. 이 프레임워크는 정치, 뉴스, 경제 등 다양한 분야에서 시뮬레이션 실험을 통해 대규모 인구 동태를 반영하며, 다양성과 신뢰성을 갖춘 사회 시뮬레이션을 보장합니다.
- ***Technical Details***: SocioVerse는 네 가지 주요 정렬 모듈(Alignment Components)로 구성되어 있으며, 각 모듈은 실세계의 정보와 시뮬레이션 환경을 동기화시킵니다. 사용자 엔진(User Engine)은 대규모 사용자 풀에서 현실의 사용자를 기반으로 시뮬레이션 에이전트를 생성하고, 시나리오 엔진(Scenario Engine)은 다양한 시뮬레이션 구조를 실제 세계와 맞추며, 행동 엔진(Behavior Engine)은 에이전트가 실사용자의 행동을 재현하도록 설계되었습니다.
- ***Performance Highlights***: 세 가지 시뮬레이션 시나리오에서 SocioVerse는 높은 정확도와 현실성을 보여주었습니다. 특정 모델 예측에서 전반적인 키러(RMSE)가 최소화되었고, 특히 대선 예측 시뮬레이션에서 높은 매크로 정확성을 달성하였습니다. 다양한 LLM 모델들이 서로 다른 사회적 맥락에서 인간의 태도 및 이념을 잘 시뮬레이션할 수 있었으나, GPT-4o-mini와 같은 특정 모델의 경우 모델별 편향이 관찰되었습니다. 이는 시뮬레이션 정확성에 모델 선택이 중요한 영향을 미침을 시사합니다.

### [MIEB: Massive Image Embedding Benchmark](https://arxiv.org/abs/2504.10471)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10471.png)

Vote: 10

Authors: Kenneth Enevoldsen, Isaac Chung, Noura Al Moubayed, Jamie Stirling, Roman Solomatin, Niklas Muennighoff, Xin Zhang, Chenghao Xiao, Imene Kerboua, Márton Kardos

- ***What's New***: MIEB(Massive Image Embedding Benchmark)는 현재까지 가장 광범위한 스펙트럼에 걸쳐 이미지와 이미지-텍스트 임베딩 모델의 성능을 평가하기 위해 도입되었습니다. 38개 언어로 130개의 개별 작업을 포함하여 총 8개의 상위 카테고리로 분류된 벤치마크로, 기존의 평가 프로토콜보다 더욱 종합적인 평가를 제공합니다.
- ***Technical Details***: MIEB 벤치마크는 MTEB의 코드베이스를 확장하여 이미지와 이미지-텍스트 임베딩 모델을 평가하도록 구성되어 있습니다. 평가 카테고리는 클러스터링(Clustering), 분류(Classification), 검색(Retrieval) 등을 포함하고 있으며, 일부 세부적인 측면으로는 시각적 표현(visual representation)을 평가하는 Visual STS 및 문서 이해(Document Understanding) 등이 있습니다.
- ***Performance Highlights***: MIEB에서 평가된 50개 모델 중, 개별 모델이 모든 작업 카테고리에서 우수한 성능을 보이는 경우는 없었습니다. MLLM 기반 모델은 시각적 텍스트 이해 및 다국어 작업에서 매우 우수한 성능을 보였으며, CLIP 스타일 모델은 전통적인 작업(예: linear probing)에서 강점을 보였습니다. 그러나 시각적 텍스트 표현 및 다국어 작업에서는 상대적으로 낮은 성능을 보였습니다.

### [TinyLLaVA-Video-R1: Towards Smaller LMMs for Video Reasoning](https://arxiv.org/abs/2504.09641)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.09641.png)

Vote: 8

Authors: Wenjun Wu, Xingjian Zhang, Siwei Wen, Lei Huang

- ***What's New***: TinyLLaVA-Video-R1은 작은 규모의 멀티모달 모델(Small-scale Multimodal Models; MMM)로서 비디오 추론(Video Reasoning)의 능력을 강화하기 위해 설계된 새로운 모델입니다. 4억 개 이하의 파라미터로 구성되어 비용 효율적인 환경에서 비디오 이해를 향상시킬 수 있습니다. 강화 학습을 통해 '아하 모먼트'를 포착하고, 이는 단순한 정답 생성보다 깊은 의미를 제공합니다.
- ***Technical Details***: 이 모델은 GRPO (Group Relative Policy Optimization) 알고리즘을 사용하여 Video-QA 데이터셋에서 학습되었습니다. 이 과정에서 응답 포맷과 정확도에 기반한 리워드 규칙을 정의하였으며, 특히 응답 포맷 맞춤, 연속 길이 리워드, 그리고 부정확한 응답에 대한 패널티를 포함합니다. 모델은 비디오 시나리오를 분석하고 각 옵션을 체계적으로 평가하여 답을 도출합니다.
- ***Performance Highlights***: TinyLLaVA-Video-R1은 여러 벤치마크(MVBench, VideoMME, MLVU, MMVU)에서 뛰어난 성능을 보여주었습니다. 특히 3B 파라미터의 모델임에도 불구하고 다양한 비디오 이해 및 추론 벤치마크에서 기존의 대형 모델들과 맞먹는 성능을 기록하였습니다. 이는 강화 학습을 통해 더 깊은 추론 능력을 획득한 결과로 평가됩니다.

### [The AI Scientist-v2: Workshop-Level Automated Scientific Discovery via Agentic Tree Search](https://arxiv.org/abs/2504.08066)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.08066.png)

Vote: 7

Authors: Jakob Foerster, Robert Tjarko Lange, Chris Lu, Yutaro Yamada, Jeff Clune, Shengran Hu, David Ha, Cong Lu

- ***What's New***: The AI Scientist-v2는 AI를 활용한 과학 발견을 자동화하는 시스템으로, 최초로 AI만으로 생성된 논문이 동료 리뷰를 통해 워크숍에서 수락된 사례를 소개합니다. 이 시스템은 과학 가설을 수립하고, 실험을 설계 및 실행하며, 데이터를 분석하고 시각화하여 과학 논문을 자동으로 작성하는 기능을 가지고 있습니다.
- ***Technical Details***: The AI Scientist-v2는 인간이 작성한 코드 템플릿에 의존하지 않으며, 일반적인 머신러닝 도메인에 걸쳐 효과적으로 일반화할 수 있는 능력을 갖추고 있습니다. 이를 위해 실험 관리자 에이전트(experiment manager agent)가 관리하는 혁신적인 점진적 에이전트 트리 탐색(progressive agentic tree-search) 방법론을 활용합니다. 또한, 비전-언어 모형(Vision-Language Model; VLM) 피드백 루프를 통합하여 내용을 개선하고 시각적 요소의 미적 요소를 세련합니다.
- ***Performance Highlights***: 세 개의 AI가 완전히 생성한 원고가 동료 리뷰에 제출되었으며, 그 중 하나가 사람의 평균 수락 기준을 초과하여 워크숍에서 수락되었습니다. 이 성과는 AI가 과학 연구의 모든 측면을 수행하는 능력이 증가하고 있음을 시사하며, 향후 기술 발전이 인간 지식 생성에 큰 영향을 미칠 것으로 예상됩니다.

### [LLM-SRBench: A New Benchmark for Scientific Equation Discovery with Large Language Models](https://arxiv.org/abs/2504.10415)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10415.png)

Vote: 6

Authors: Amir Barati Farimani, Kazem Meidani, Ngoc-Hieu Nguyen, Khoa D Doan, Chandan K Reddy, Parshin Shojaee

- ***What's New***: LLM-SRBench는 대형 언어 모델(Large Language Models; LLMs)을 활용한 과학적 방정식 발견을 평가하기 위해 특별히 설계된 새로운 벤치마크입니다. 기존 벤치마크의 문제를 피하기 위해, 변형된 수학적 표현(LSR-Transform)과 합성, 발견 중심의 문제(LSR-Synth)로 구성된 239개의 도전적인 과제들을 포함합니다.
- ***Technical Details***: LLM-SRBench는 두 가지 주요 문제 유형으로 구성됩니다. LSR-Transform은 피드백 기반 논리 체계를 요구하는 인지적 추론(Reasoning beyond Memorized Forms)을 통해 LLM의 한계를 테스트하기 위해 기존의 물리학 모델을 변형된 수학적 표현으로 변환합니다. LSR-Synth는 기존 방정식의 용어와 새로운 합성 용어를 결합하여 데이터 중심의 과학적 추론을 평가하며, 화학, 생물학, 물리학, 재료과학 등 네 가지 과학 분야를 다룹니다.
- ***Performance Highlights***: 여러 최첨단 방법들의 광범위한 평가 결과에서, 가장 우수한 시스템의 상징적 정확도는 31.5%에 불과했습니다. 이는 과학 방정식 발견의 현재 어려움을 강조하며, LLM-SRBench가 미래 연구에 가치 있는 자원이 될 수 있음을 시사합니다.

### [VisuoThink: Empowering LVLM Reasoning with Multimodal Tree Search](https://arxiv.org/abs/2504.09130)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.09130.png)

Vote: 5

Authors: Qinyuan Cheng, Liang Ding, Dacheng Tao, Qipeng Guo, Yikun Wang, Siyin Wang, Zhaoye Fei, Xipeng Qiu

- ***What's New***: VisuoThink는 대형 시각 언어 모델(Large Vision-Language Models; LVLMs)이 복잡한 추론 작업을 수행할 수 있도록 돕기 위한 새로운 다중 모드 트리 검색(multi-modal tree search) 프레임워크를 제안합니다. 이 프레임워크는 시각적 및 언어적(reasoning) 경로를 통합하여 테스트 시간(test-time scaling) 동안 예측 롤아웃(predictive rollout)을 적용하여 성능을 향상시킵니다.
- ***Technical Details***: VisuoThink는 시각-텍스트(interleaved) 추론을 통해 여러 추론 경로를 체계적으로 탐색하며, 예측 롤아웃 메커니즘을 사용하여 여러 추론 경로의 결과를 시뮬레이션하고 가장 유망한 경로를 선택합니다. 이 과정은 인간의 느린 사고 과정처럼 진행되며, 시각적 및 언어적 힌트를 도구로 생성하고 활용합니다.
- ***Performance Highlights***: VisuoThink는 Geomverse-109과 Geometry3K와 같은 복잡한 기하학 및 공간 추론 작업에서 기존 방법을 능가하는 것으로 나타났습니다. 예를 들어, Geomverse-109에서 Visual Sketchpad와 비교해 정확도(accuracy@1)에서 최대 21.8%의 개선을 보였습니다. 이는 다중 단계 시각 추론이 필요한 문제에서 강력한 성능을 보여주었습니다.

### [How new data permeates LLM knowledge and how to dilute it](https://arxiv.org/abs/2504.09522)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.09522.png)

Vote: 4

Authors: Ulrich Rueckert, Mark Sandler, Been Kim, Chen Sun, Nolan Andrew Miller, Max Vladymyrov, Renat Aksitov, Andrey Zhmoginov

- ***What's New***: 본 연구는 LLM(Large Language Models)의 지식에서 새로운 정보가 어떻게 영향을 미치는지를 조사하며, 'priming' 효과를 통해 이러한 과정을 체계적으로 연구했습니다. 연구진은 Outlandish라는 데이터셋을 도입하여 새로운 정보가 LLM에 미치는 영향을 측정할 수 있었습니다.
- ***Technical Details***: Outlandish는 1,320개의 다양한 텍스트 샘플로 구성된 데이터셋으로, 각 샘플은 핵심 키워드가 포함된 문맥을 제공하여 LLM이 새로 습득한 정보를 적절히 학습했는지 또는 부적절하게 기존 지식에 영향을 주는 'priming'이 발생했는지를 평가했습니다. LLM은 PALM-2, Gemma, Llama 등의 다양한 모델 아키텍처와 크기에서 시험되었습니다. 새로운 기술로는 'stepping-stone' 텍스트 증강 전략과 'ignore-k' 업데이트 가지치기 방법이 개발되어 'priming' 영향을 조절할 수 있었습니다.
- ***Performance Highlights***: 연구 결과, 키워드의 사전 학습 확률을 기반으로 'priming' 정도를 예측할 수 있는 것으로 나타났습니다. 이러한 관계는 다양한 모델 아키텍처 및 크기에서 일관되게 관찰되었습니다. 'stepping-stone' 방법은 'priming' 효과를 50-95% 감소시켰으며, 모델의 새로운 정보 학습 능력을 유지하였습니다.

### [EmoAgent: Assessing and Safeguarding Human-AI Interaction for Mental Health Safety](https://arxiv.org/abs/2504.09689)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.09689.png)

Vote: 4

Authors: Jiahao Qiu, Yinghui He, Yiming Wang, Zixin Yao, Xun Jiang, Yue Wu, Yuhan Liu, Ling Yang, Xinzhe Juan, Mengdi Wang

- ***What's New***: EmoAgent는 인간과 AI 간의 상호작용에서 정신 건강 안전을 보장하기 위해 설계된 새로운 멀티에이전트 AI 프레임워크입니다. 이는 특히 심리적 취약성을 가진 인간 사용자들에게 위험이 되는 요소를 식별하고 이를 완화하기 위한 평가 및 보호 메커니즘을 제공합니다.
- ***Technical Details***: EmoAgent는 두 가지 주요 구성 요소인 EmoEval과 EmoGuard로 나누어집니다. EmoEval은 대화형 AI 시스템의 정신적 스트레스 유발 위험을 평가하며, PHQ-9, PDI, PANSS와 같은 임상적으로 검증된 도구를 활용하여 사용자의 정신 건강 변화를 시뮬레이션하고 평가합니다. EmoGuard는 실시간 보호 프레임워크로서 사용자의 정신 상태를 모니터링하고 잠재적 위험을 예측하며 교정 피드백을 제공하여 위험을 완화하는 역활을 합니다.
- ***Performance Highlights***: 실험에서는 EmoAgent가 캐릭터 기반 챗봇과의 상호작용에서 더 안전한 대화 환경을 제공할 수 있음을 보여주었습니다. EmoGuard는 심리적으로 취약한 사용자의 악화를 방지하는 데 효과적이며, 기존 악화율을 50% 이상 감소시키는 성과를 입증했습니다. 이는 EmoGuard가 대화형 AI 시스템에서 맥락을 감안한 실시간 개입을 통해 정신적 위험을 줄이는 데 크게 기여한다는 것을 시사합니다.

### [M1: Towards Scalable Test-Time Compute with Mamba Reasoning Models](https://arxiv.org/abs/2504.10449)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10449.png)

Vote: 3

Authors: Daniele Paliotta, Junxiong Wang, Daniel Ritter, Wen-Ding Li, Tri Dao, Alexander M. Rush

- ***What's New***: 새로운 하이브리드 선형 RNN 모델인 M1 모델은 Mamba 아키텍처에 기반하여 개발되었습니다. 이 모델은 대규모 수학적 문제 해결에서 향상된 성능을 제공하며, 대규모 Transformer 모델의 한계를 극복하고자 메모리 효율적인 추론 방식을 활용합니다.
- ***Technical Details***: M1 모델은 스칼라블한 추론을 위해 복잡한 Pretrained Transformer 모델을 Mamba 아키텍처로 증류(Distillation)하여 만듭니다. 수퍼바이즈드 파인튜닝(Supervised Fine-Tuning; SFT)과 강화학습(RL)을 통해 성능을 더욱 향상시켰습니다. 전반적인 학습 과정은 500억 토큰 이하로 이루어집니다.
- ***Performance Highlights***: M1 모델은 AIME 및 MATH 벤치마크에서 기존의 선형 RNN 모델들을 능가하는 성능을 기록하며, Deepseek R1 증류 모델과 비슷한 성능을 보였습니다. 대규모 배치 설정에서 Transformer 모델에 비해 3배 빠른 추론 속도를 보여주며, 동일한 시간 내에 높은 정확도를 달성했습니다.

### [LLM Can be a Dangerous Persuader: Empirical Study of Persuasion Safety in Large Language Models](https://arxiv.org/abs/2504.10430)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10430.png)

Vote: 1

Authors: Qi Zhang, Heajun An, Lifu Huang, Sarvech Qadir, Minqian Liu, Jin-Hee Cho, Sang Won Lee, Pamela J. Wisniewski, Ruoxi Jia, Xinyi Zhang, Zhiyang Xu

- ***What's New***: 본 연구는 대형 언어 모델(Large Language Models; LLMs)의 설득 안전성을 체계적으로 평가하기 위한 프레임워크인 PERSUSAFETY를 소개합니다. 이를 통해 LLM이 비윤리적 설득 작업을 거부하거나 비윤리적 전략을 피할 수 있는지를 조사합니다.
- ***Technical Details***: PERSUSAFETY는 설득 신(Scene) 생성, 설득 대화 시뮬레이션, 설득 안전성 평가의 세 단계로 구성되어 있습니다. 다양한 비윤리적 설득 주제와 일반적인 15가지 비윤리적 전략을 다루며, 8개의 주요 LLM을 대상으로 광범위한 실험이 수행되었습니다.
- ***Performance Highlights***: 대부분의 LLM은 해로운 설득 작업을 식별하지 못하고 다양한 비윤리적 설득 전략을 활용하는 안전상 우려를 보여주었습니다. 특히 Claude-3.5-Sonnet 모델은 거부가 수월하더라도, 설득 과정에서 높은 수준의 비윤리적 전략을 사용하는 모순점을 드러냈습니다.

### [MDK12-Bench: A Multi-Discipline Benchmark for Evaluating Reasoning in Multimodal Large Language Models](https://arxiv.org/abs/2504.05782)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.05782.png)

Vote: 1

Authors: Zhaopan Xu, Wenqi Shao, Yansheng Qiu, Haoquan Zhang, Kaipeng Zhang, Zizhen Li, Zhen Li, Ming Li, Chuanhao Li, Fanrui Zhang, Xiaofeng Mao, Xiaopeng Peng, Jiaxin Ai, Yukang Feng, Wangbo Zhao, Yang You, Pengfei Zhou, Kai Wang, Xiaojun Chang, Jianwen Sun

- ***What's New***: MDK12-Bench는 다중 모달 대형 언어 모델(Multimodal Large Language Models; MLLMs)의 추론 역량을 K-12 수준의 실제 시험을 통해 평가하는 다학문 벤치마크입니다. 6개 학문 영역(수학, 물리, 화학, 생물학, 지리학, 정보 과학)에 걸쳐 140,000개 이상의 추론 사례를 포함하며, 체계적인 지식 구조를 기반으로 한 세분화된 지식 포인트 주석과 상세한 정답 설명을 제공합니다.
- ***Technical Details***: MDK12-Bench는 초등학교부터 12학년에 이르는 시험 문제들을 포함하고 있으며, 각 문제에 세분화된 난이도 라벨과 교차 연도 구분을 제공합니다. 동적 평가 프레임워크를 도입하여 데이터 오염 문제를 완화하고, 부트스트래핑 기법을 통해 질문 형식, 질문 유형, 이미지 스타일을 변형함으로써 더욱 공정한 평가 체계를 구축합니다.
- ***Performance Highlights***: 다양한 MLLMs의 평가 결과, Gemini2.0-flash-thinking 모델이 59.4%의 전체 정확도로 가장 높은 성능을 기록했으며, 특히 화학과 생물 분야에서 두드러진 성과를 보였습니다. 동적 평가에서는 대부분의 모델이 텍스트와 이미지 부트스트래핑 조합에 취약한 모습을 보이며 성능 저하가 나타났고, 이는 현재 MLLMs가 맥락적 변화와 과제 복잡성에 대해 민감함을 시사합니다.

### [DeepSeek vs. o3-mini: How Well can Reasoning LLMs Evaluate MT and Summarization?](https://arxiv.org/abs/2504.08120)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.08120.png)

Vote: 1

Authors: Sotaro Takeshita, Yanran Chen, Daniil Larionov, Christian Greisinger, Ran Zhang, Zhipin Wang, Steffen Eger, Christoph Leiter

- ***What's New***: 이 연구는 Reasoning LLMs가 자연어 생성(NLG)의 기계 번역(MT) 및 요약 평가 작업에 얼마나 효과적인지를 처음으로 체계적으로 비교합니다. 다양한 파라미터와 아키텍처를 가진 Reasoning 모델(DeepSeek-R1, OpenAI o3)과 비Reasoning 모델을 대비하여 이들의 성능을 WMT23 및 SummEval 벤치마크에서 평가합니다.
- ***Technical Details***: 실험에서는 Reasoning LLMs와 그 증류된 변형(Distilled Variants), 그리고 동등한 비Reasoning LLMs를 포함하는 세 가지 아키텍처 카테고리로 구성된 8개의 모델을 평가합니다. MT 평가에서는 GEMBA-MQM 프로세스와 Summarization 평가에서는 G-Eval을 사용하였습니다. Reasoning 모델의 Distillation이 중간 크기 모델에서는 성능을 유지하나, 작은 변형에서는 크게 저하됨을 보여줍니다.
- ***Performance Highlights***: 실험 결과, Reasoning 기능의 유효성은 모델 및 작업에 따라 다릅니다. OpenAI o3-mini 모델은 Reasoning 강도가 증가함에 따라 일관된 성능 향상을 보였으나, DeepSeek-R1은 비Reasoning 변형에 비해 대부분의 평가 작업에서 부진했습니다. Reasoning 토큰 사용이 증가할수록 평가 품질이 향상되는 경향을 보였으며, Reasoning 역량의 증류 또한 중형 모델(32B)에서는 합리적인 성능을 유지하지만, 소형 변형(8B)에서는 성능이 상당히 저하되었습니다.

### [3D CoCa: Contrastive Learners are 3D Captioners](https://arxiv.org/abs/2504.09518)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.09518.png)

Vote: 1

Authors: Zeyu Zhang, Ting Huang, Yemin Wang, Hao Tang

- ***What's New***: 3D CoCa는 대조적 비전-언어 학습(contrastive vision-language learning)을 3D 캡션 생성과 통합한 최초의 통합 프레임워크로, 물체 제안이나 외부 감지기 없이 포인트 클라우드에서 직접 3D 장면을 기술할 수 있도록 설계되었습니다. 이 모델은 대규모 비전-언어 사전학습에서 얻은 강력한 시각-언어 사전 정보(priors)를 활용하여 복잡한 3D 장면에 대한 더욱 풍부하고 정확한 캡션을 생성할 수 있습니다.
- ***Technical Details***: 3D CoCa는 프로즌 CLIP 비전-언어 백본을 사용하여 풍부한 의미론적 사전 정보를 제공하고, 기하학적 맥락을 캡처하는 공간 인식 3D 장면 인코더와 설명적 캡션을 생성하는 다중 모달 디코더를 결합하여 설계되었습니다. 대조적 학습(contrastive learning)과 캡션 생성 목표를 공유된 특징 공간에서 최적화하여 강력한 공간적 추론과 의미론적 기반을 얻습니다.
- ***Performance Highlights***: 3D CoCa는 ScanRefer와 Nr3D 벤치마크에서 각각 CIDEr@0.5IoU에서 77.13% 및 52.84%의 성능으로 현재 최고 성능을 기록하며 두드러진 성과를 보였습니다. 이 실험 결과는 대조적 학습(strategy)이 3D 캡션 생성에서 중요한 이점을 제공함을 보여줍니다.

### [SpecReason: Fast and Accurate Inference-Time Compute via Speculative Reasoning](https://arxiv.org/abs/2504.07891)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.07891.png)

Vote: 1

Authors: Zhihao Zhang, Zhihao Jia, Rui Pan, Ravi Netravali, Yinwei Dai, Gabriele Oliaro

- ***What's New***: SpecReason은 대형 추론 모델(Large Reasoning Models; LRMs)의 추론 시간(inference-time) 계산을 가속화하는 혁신적인 시스템으로, 경량 모델을 활용하여 간단한 중급 추론 단계를 사전적으로 수행하고, 비용이 많이 드는 기본 모델(base model)은 사전 출력물을 평가(및 수정)할 때만 사용하는 방법을 제시합니다. 이는 기존의 사전적 디코딩(speculative decoding)이 각 단계에서 토큰 레벨의 등가성을 요구했던 것과 달리, 최종 답변의 정확성을 유지하는데 필요한 의미적 유연성을 활용한다는 점에서 차별화됩니다.
- ***Technical Details***: SpecReason은 경량 추론 모델이 개별 추론 단계를 생성하고, 기본 모델이 그 출력을 평가하는 구조로 설계되었습니다. 평가에는 기본 모델이 리플렉션(reflection) 기능을 통해 중간 단계의 오류를 수정할 수 있도록 경량화된 검증 과정을 사용합니다. 이러한 방법론은 CoT(Chain of Thought)의 생각 토큰(thinking tokens)을 생성하는 데 있어 근사값에 대한 높은 허용 오차를 이용하여, 추론의 속도를 크게 향상시킵니다. 이 시스템은 의미 수준의 유사성을 활용하여 최종 출력의 정확도를 유지하면서도 큰 지연 시간을 줄일 수 있습니다.
- ***Performance Highlights***: SpecReason은 기본 LRM 추론과 비교하여 1.5~2.5배의 속도 향상을 이룩하면서도, 정확도는 1.0~9.9% 향상시켰습니다. 또한, 사전적 디코딩과 결합하여 추가적으로 19.4~44.2%의 지연 시간을 감소시켰습니다. 특히, MATH 데이터셋에서 가장 큰 성능 향상이 관찰되었으며, 경량 모델이 사전에 추정한 단계의 수락률 증가로 인해 종단 간 지연 시간을 크게 줄일 수 있었습니다.

### [Reasoning Models Can Be Effective Without Thinking](https://arxiv.org/abs/2504.09858)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.09858.png)

Vote: 1

Authors: Sewon Min, Jingxuan He, Charlie Snell, Wenjie Ma, Tyler Griggs, Matei Zaharia

- ***What's New***: 이 논문은 최첨단 추론 모델에서 의도적인 사고 과정 없이도 효과적인 결과를 얻을 수 있음을 보여줍니다. NoThinking이라는 새로운 접근법을 통해 특정 프롬프트로 사고 과정을 생략하고도 다양한 문제 집합에서 성능을 입증했습니다.
- ***Technical Details***: DeepSeek-R1-Distill-Qwen 모델을 기반으로 한 NoThinking 방법은, 미리 설정된 더미 사고 블록을 사용하는 단순한 프롬프트를 통해 추론 과정을 생략합니다. 이 방법은 AMC, AIME, OlympiadBench와 같은 문제 집합에서 실험적으로 우수성을 입증하였으며, 패스 앳 K (pass@k) 메트릭을 사용하여 다양한 데이터셋에서 성능을 비교했습니다. 패스 앳 K 지표에서 NoThinking이 특히 K 값이 증가할수록 더 경쟁력 있다는 결과가 나타났습니다.
- ***Performance Highlights***: NoThinking은 작은 예산 범위에서의 성능을 예로 제시하며 특히 AMC 23에서 700개의 토큰 사용 시 51.3%의 정확도로 기존 'Thinking' 방법의 28.9% 성능을 능가했습니다. 최고 성능인 k 값에서는 NoThinking이 일반적인 추론 방식보다 최대 9배 낮은 지연 시간과 최대 4배 적은 토큰 사용량을 보였습니다.

### [DiffuMural: Restoring Dunhuang Murals with Multi-scale Diffusion](https://arxiv.org/abs/2504.09513)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.09513.png)

Vote: 0

Authors: Puyu Han, Zeyu Zhang, Zhichen Liu, Juntao Jiang, Yuhang Pan, Luqi Gong, Erting Pan, Qunchao Jin, Jiaju Kang

- ***What's New***: DiffuMural은 대형 손상된 고대 벽화를 복원하기 위해 멀티스케일 확산 메커니즘(Multi-scale Diffusion Mechanism)을 활용한 혁신적인 방법을 소개합니다. 이는 특히 큰 결함이 있는 벽화 복원에서 뛰어난 성과를 보이며, 23개의 동시대 둔황 벽화를 이용해 시각적 일관성을 유지하며 훈련된 모델입니다.
- ***Technical Details***: DiffuMural은 손상된 영역의 윤곽 정보를 조건적 가이드로 활용하여 복원을 진행하며, 유넷(U-Net) 기반의 벽화 공간 주의 메커니즘(Mural Spatial Attention Mechanism)을 도입해 문맥적 추론을 강화합니다. 또, 협력적 확산(Co-Diffusion)을 통해 멀티스케일에서 고해상도의 시각 정보를 결합하여 자연스럽고 정밀한 복원을 실현합니다. 이러한 프로세스를 통해 최적의 결과를 얻기 위해 주파수 도메인 처리(Frequency Domain Processing; FDP) 모듈을 사용하여 텍스처 및 컬러 정보도 개선합니다.
- ***Performance Highlights***: DiffuMural은 여러 첨단 복원 기법들과 비교하여 구조적 유사성(SSIM)과 에지 일관성(ECON) 지표에서 최고 성능을 기록하였으며, 평균 6.47% 차이로 색상 일관성(CCON)과 텍스처 일관성(TCON) 지표에서도 경쟁력을 보입니다. 전문가 평가 결과에서도 뛰어난 시각적 일관성과 예술적 진정성을 유지하며, 제안된 방법은 실제 벽화 보존 작업에 적합한 솔루션으로 인정받았습니다.

