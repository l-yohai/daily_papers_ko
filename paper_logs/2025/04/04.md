## Daily Papers (2025-04-04)

### [Advances and Challenges in Foundation Agents: From Brain-Inspired Intelligence to Evolutionary, Collaborative, and Safe Systems](https://arxiv.org/abs/2504.01990)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.01990.png)

Vote: 97

Authors: Yuheng Cheng, Ollie Liu, Haochen Shi, Jianyun Nie, Qingyun Wu, Xiangru Tang, Ian Foster, Xinfeng Li, Logan Ward, Kunlun Zhu, Shaokun Zhang, Sirui Hong, Haibo Jin, Jiayi Zhang, Chenglin Wu, Xiaoliang Qi, Tanjin He, Huan Zhang, Jiaqi Chen, Kaitao Song, Yuyu Luo, Tianming Liu, Yizhang Lin, Tongliang Liu, Mingchen Zhuge, Qiang Yang, Chi Wang, Glen Berseth, Boyan Li, Hongzhang Liu, Xiaoqiang Wang, Jiawei Xu, Yu Gu, Fengwei Teng, Xiaojun Jia, Suyuchen Wang, Zhaoyang Yu, Dekun Wu, Huan Sun, Haohan Wang, Jinlin Wang, Jinyu Xiang, Jiaxuan You, Yu Su, Peiyan Zhang, Jian Pei, Bang Liu

- ***What's New***: 이 논문은 대규모 언어 모델(LLMs)을 중심으로 한 지능형 에이전트 시스템의 자기 진화(Self-Evolution)에 대해 조명합니다. 이는 에이전트 시스템 개발의 자동화를 목표로 하며, 수작업의 필요성을 최소화하여 인간의 노력을 대체합니다. 특히, LLMs을 최적화 엔진으로 사용하여 프롬프트 최적화, 워크플로우 개선, 도구 최적화 등의 다양한 최적화 문제를 해결하려는 최신 연구 사례를 소개합니다.
- ***Technical Details***: 에이전트 최적화는 프롬프트 최적화, 워크플로우 최적화, 도구 최적화 등으로 구성된 두 계층 구조로 설명됩니다. 최적화에는 LLMs의 자연어 처리 능력을 활용하며, 이를 통해 지연과 비용을 최소화하는 행동 조정, 즉각적인 피드백을 기반으로 한 실시간 적응성 강화 등 다양한 방법을 사용합니다. 또한, 오프라인의 체계적 학습과 온라인의 지속적 업데이트를 결합한 하이브리드 최적화 전략도 다룹니다.
- ***Performance Highlights***: 성능 향상을 위해 LLM 기반의 최적화 전략이 강조되며, 이는 비판적 피드백 루프를 통한 에이전트의 지속 개선을 이루도록 합니다. 과학적 발견 시나리오에서 에이전트는 여러 상호작용 단계에서 도구 활용, 최적의 워크플로우 구현 등을 통해 지식 발견과 지능 발전을 지속적으로 향상시킬 수 있습니다. 이러한 최적화 기법은 지능형 에이전트의 자율성을 크게 높이며 다양한 과학적 연구 영역에서 혁신을 가능하게 합니다.

### [Envisioning Beyond the Pixels: Benchmarking Reasoning-Informed Visual Editing](https://arxiv.org/abs/2504.02826)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02826.png)

Vote: 55

Authors: Guangtao Zhai, Kexian Tang, Hua Yang, Peiyuan Zhang, Xue Yang, Xiangyu Zhao, Zicheng Zhang, Haodong Duan, Junchi Yan, Hao Li

- ***What's New***: RISEBench라는 최초의 Reasoning-Informed Visual Editing (RISE)을 평가하기 위한 벤치마크가 소개되었습니다. 이는 복잡한 조작 지시 사항을 따르거나, 외형의 일관성을 유지하며, 유연한 입력 형식을 지원하는 등의 일반적인 비주얼 편집에서의 한계를 해소하려는 목적입니다.
- ***Technical Details***: RISEBench는 Temporal, Causal, Spatial, Logical Reasoning의 네 가지 주요 Reasoning 유형에 중점을 두고 있습니다. 각 범주의 고품질 테스트 케이스를 큐레이션하고, 인간 심판과 멀티모달 모델(LMM)-as-a-judge 접근 방식을 포함하는 평가 프레임워크를 제안합니다.
- ***Performance Highlights***: GPT-4o-Native는 다른 모델들에 비해 월등히 성능이 우수한 것으로 밝혀졌지만, 여전히 logical reasoning 작업에서는 어려움을 겪고 있습니다. 특히, 이 벤치마크는 현재의 모델들이 시각적 편집에서의 정교한 reasoning 요구를 충족하기 위해서는 추가적인 연구가 필요함을 강조합니다.

### [ZClip: Adaptive Spike Mitigation for LLM Pre-Training](https://arxiv.org/abs/2504.02507)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02507.png)

Vote: 48

Authors: Abhay Kumar, Nilabhra Roy Chowdhury, Louis Owen, Fabian Güra

- ***What's New***: ZClip은 대형 언어 모델(LLM)의 사전 훈련(Pre-Training) 중 손실 스파이크(loss spikes)를 효과적으로 완화하기 위한 적응형 그래디언트 클리핑(adaptive gradient clipping) 알고리즘입니다. 기존의 고정 임계값 기반의 클리핑법이 진화하는 훈련 동적에 적응하지 못하는 한계를 넘어서, ZClip는 z-score 기반의 이상 탐지를 사용하여 훈련 중 발생하는 큰 그래디언트 스파이크를 능동적으로 식별하고 완화합니다.
- ***Technical Details***: ZClip는 그래디언트 분포의 최근 통계에 기반하여 클리핑 임계값을 동적으로 조정합니다. 구체적으로, 지수 이동 평균(EMA)을 사용하여 그래디언트 노름의 평균과 분산을 추적하며, z-score 통계치를 기반으로 스파이크를 탐지하고 이를 수정합니다. ZClip는 z-score 계산을 통해 현재 그래디언트 노름이 평균에서 얼마나 떨어져 있는지를 평가하며, 임계 z-score (zthres) 값을 초과할 경우, 그래디언트를 조정하여 안정적인 훈련을 유지합니다.
- ***Performance Highlights***: 실험 결과, ZClip는 고정 임계값 그래디언트 클리핑과 비교하여 손실 스파이크를 완전히 제거하면서도 하위 작업 성능에서 향상을 보여주었습니다. 특히 학습률이 높은 '공격적인' 훈련 환경에서도 모델의 수렴 속도를 크게 증가시켰으며, 더 적은 컴퓨팅 자원과 토큰 비용으로 안정적인 최적화를 달성하였습니다.

### [GPT-ImgEval: A Comprehensive Benchmark for Diagnosing GPT4o in Image Generation](https://arxiv.org/abs/2504.02782)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02782.png)

Vote: 34

Authors: Weijia Li, Kaiqing Lin, Xiangyang He, Li Yuan, Jun He, Conghui He, Junyan Ye, Shenghai Yuan, Zilong Huang, Zhiyuan Yan

- ***What's New***: GPT-ImgEval은 GPT-4o의 이미지 생성 성능을 평가하기 위한 최초의 포괄적인 벤치마크입니다. 이 벤치마크는 이미지 생성 질, 편집 능력, 세계 지식에 기반한 의미 합성을 포함하여 세 가지 중요한 차원에서 평가합니다. GPT-4o는 생성 품질, 이미지 편집, 그리고 의미적 합성에서 기존 방법들을 능가하는 성과를 보입니다.
- ***Technical Details***: GPT-ImgEval은 세 가지 주요 데이터셋인 GenEval, Reason-Edit, 그리고 WISE를 사용하여 GPT-4o의 성능을 평가합니다. GenEval은 객체 중심의 프레임워크로, 이미지 속성 조합을 평가하며, Reason-Edit는 텍스트 지침 기반의 이미지 편집을, WISE는 세계 지식에 기반한 의미 평가를 위한 것입니다. 실험 결과, GPT-4o는 diffusion-based 헤드를 활용한 이미지 디코딩을 사용하는 것으로 보입니다.
- ***Performance Highlights***: GPT-4o는 생성 품질에서 0.84의 높은 점수를 기록하며, 특히 색상 인식에서 0.92, 공간 위치에서 0.75, 속성 바인딩에서 0.61로 우수한 성능을 보여주었습니다. 이미지 편집에서는 Reason-Edit 벤치마크에서 0.929의 점수를 기록하여 기존의 모든 방법을 능가했습니다.

### [Rethinking RL Scaling for Vision Language Models: A Transparent, From-Scratch Framework and Comprehensive Evaluation Scheme](https://arxiv.org/abs/2504.02587)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02587.png)

Vote: 24

Authors: Yiran Zhong, Yan Ma, Xuyang Shen, Steffi Chern, Pengfei Liu

- ***What's New***: 이 논문은 대형 비전-언어 모델(Vision-Language Models; VLMs)의 추론 능력을 개선하기 위한 강화 학습(RL) 스케일링 문제를 투명하고 기초부터 구축된 프레임워크를 통해 접근하며, 포괄적인 평가 체계를 제안합니다. 이러한 새로운 프레임워크는 재현성을 중시하며 기존의 복잡한 라이브러리에 의존하지 않고 표준 라이브러리만을 활용하여 구현되었습니다.
- ***Technical Details***: 제안된 RL 프레임워크는 Transformers, FSDP2, vLLM과 같은 표준 라이브러리를 사용하여 구현되며 데이터 흐름, 응답 수집, 궤적 생성, 정책 업데이트의 네 가지 주요 컴포넌트로 구성됩니다. 이 시스템은 YAML 파일을 통해 실험 구성을 로드한 후, 정책 및 참조 모델, 데이터로더, 옵티마이저 등을 초기화하는 과정으로 시작됩니다. 이에 더해, 표준화된 RL의 평가는 이해하기 쉬운 학습 곡선(learning curve)과 반영 행동(reflective behavior)의 비율 등을 통해 다각적으로 이루어집니다.
- ***Performance Highlights***: RL은 mm_math5k 및 geometry3k 데이터셋에서 테스트 시 평균 1.35배에서 최대 1.76배, 다른 비전-언어 모델에서는 평균 1.36배에서 최대 1.51배의 정확도 향상을 보였습니다. 특히, Qwen2.5-VL-Instruct-7B 모델은 이미 높은 성능을 가진 기본 모델임에도 불구하고 RL 학습을 통해 일반화 능력을 더욱 향상시켰습니다. SFT(Supervised Fine-Tuning)와 비교했을 때 RL은 더 강력한 데이터 효율성과 일반화 능력을 보여주었습니다.

### [WikiVideo: Article Generation from Multiple Videos](https://arxiv.org/abs/2504.00939)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.00939.png)

Vote: 22

Authors: Alexander Martin, Hannah Recknor, Kate Sanders, Eugene Yang, Reno Kriz, William Gantt Walden, Francis Ferraro, Benjamin Van Durme

- ***What's New***: 이 논문은 WikiVideo라는 새로운 벤치마크를 통해 비디오에서 고차원의 Wikipedia 스타일 기사를 생성하는 도전적인 과제를 제시하고 있습니다. WikiVideo는 다중 비디오를 통해 실제 사건에 대한 정보를 집계하여 생성된 전문가 작성 기사와 밀집하게 주석된 비디오들로 구성되어 있으며, 다중모달 소스를 기반으로 한 고차원의 콘텐츠 생성을 가능하게 합니다.
- ***Technical Details***: WikiVideo 벤치마크는 52개의 사건을 다루며 약 400개의 관련 비디오로 구성되어 있습니다. 각 비디오는 사건을 중심으로 정보가 집계된 전문가 작성 참조 기사를 제공합니다. 이를 통해 단일 비디오 내의 정보뿐 아니라 동일 주제의 여러 비디오에서 정보를 종합할 수 있도록 시스템이 요구됩니다. 이 논문에서는 Collaborative Article Generation (CAG)이라는 새로운 방법을 제안하여 비디오의 시각, 오디오, OCR 콘텐츠에 근거하여 문서를 생성합니다.
- ***Performance Highlights***: CAG는 최첨단 VideoLLM들과 비교하여 콘텐츠 생성 실험에서 일관되게 우수한 결과를 보여주었습니다. 테스트에서는 CAG가 다른 방법을 능가했으며, WikiVideo가 도전적인 벤치마크라는 것을 보여줍니다. CAG는 더 나은 기간 내 성능을 위해 반복적인 상호작용을 사용하여 특히 우수한 성과를 보였습니다.

### [Scaling Analysis of Interleaved Speech-Text Language Models](https://arxiv.org/abs/2504.02398)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02398.png)

Vote: 21

Authors: Michael Hassid, Yossi Adi, Gallil Maimon, Amit Roth

- ***What's New***: 이 논문은 스피치-텍스트 혼합 학습 셋업에서의 확장 가능성(scaling behavior)을 분석하여 고품질의 스피치 언어 모델(SLMs)을 훈련할 수 있는 가능성을 제시합니다. 텍스트 및 합성 데이터를 통해 지식을 전이시킴으로써 기존의 textless-SLMs보다 효율적인 학습이 가능하다는 것을 확인했습니다.
- ***Technical Details***: 논문에서는 다양한 크기와 계산 예산(compute budgets), 모델 패밀리(model families)를 고려하여 여러 interleaved SLMs을 훈련하고, 텍스트 초기화(TextLMs)로부터 시작하여, 스피치 및 텍스트 토큰이 서로 혼합된 시퀀스에서 학습을 수행합니다. 또한, k-군집화(k-means clustering)를 이용해 연속적인 음성을 이산적 단위로 변환한 후 텍스트 토큰화(tokenization) 및 스피치-텍스트 간 간섭 효과를 분석합니다.
- ***Performance Highlights***: Sims라는 방식으로 훈련된 모델은 7B 파라미터에서 15B 토큰을 사용하여 최신 SLM과 비교해 의미론적 지표에서 상위 성능을 발휘합니다. 또한, 기존 SLM을 뛰어넘는 퍼포먼스를 보여주며, 특히 multi-speaker tSC 메트릭에서 두드러진 성능 향상을 입증했습니다. 이는 약 6e20의 계산 예산에서 이루어진 토큰 활용의 최적화를 잘 보여줍니다.

### [SkyReels-A2: Compose Anything in Video Diffusion Transformers](https://arxiv.org/abs/2504.02436)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02436.png)

Vote: 19

Authors: Rui Wang, Debang Li, Yahui Zhou, Zhengcong Fei, Yikun Dou, Jiahua Wang, Jingtao Xu, Guibin Chen, Di Qiu, Yang Li, Mingyuan Fan

- ***What's New***: SkyReels-A2는 텍스트 프롬프트에 따라 다양한 시각적 요소(예: 캐릭터, 객체, 배경)를 비디오로 조합하여 합성하는 최초의 오픈 소스 상용 등급 모델입니다. 이 모델은 주어진 참조 이미지와의 엄격한 일관성을 유지하며, '요소에서 비디오로(elements-to-video)' 작업을 수행합니다. 높은 품질의 비디오 생성과 정밀한 요소 제어를 가능하게 합니다.
- ***Technical Details***: SkyReels-A2는 텍스트-참조-비디오 삼중항 데이터 구조를 구성하고, 공동 이미지-텍스트 임베딩 모델을 통해 여러 요소의 표현을 결합하여 가시적 요소별 일관성과 전체적인 코히어런스를 유지합니다. 성능 최적화를 위해 추론 파이프라인을 개선하고, 체계적인 평가를 위한 A2 Bench라는 벤치마크를 도입했습니다. 다양한 GPU 병렬 처리를 통해 모델의 효율성을 높이고 메모리 소비를 줄입니다.
- ***Performance Highlights***: SkyReels-A2는 고품질의 다양한 비디오를 생성하며, commercial closed-source 모델과의 정량적, 정성적 비교에서 긍정적인 결과를 보였습니다. 특히 대상 일관성 및 모션 자연스러움에서 우수한 성능을 보여, user preference study에서 높은 평가를 받았습니다.

### [Inference-Time Scaling for Generalist Reward Modeling](https://arxiv.org/abs/2504.02495)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02495.png)

Vote: 18

Authors: Yang Liu, Runxin Xu, Zijun Liu, Yu Wu, Shirong Ma, Peiyi Wang, Chong Ruan, Peng Li

- ***What's New***: 본 연구는 대규모 언어 모델(LLMs)에서 강화 학습(RL)을 통한 보상 모델링(RM)의 추론 시 확장성을 처음으로 탐구하였습니다. 특히, 다양한 입력 유형과 추론 시 확장 가능성을 고려한 생성 보상 모델링(GRM)을 채택하여 일반적인 도메인에서 보상 생성의 유연성을 확보하였으며, 온라인 RL을 통해 scalable reward generation behavior를 학습하도록 설계한 Self-Principled Critique Tuning(SPCT)을 제안합니다.
- ***Technical Details***: 이 연구에서는 다양한 입력과 도메인에서 유연성과 정확한 보상 생성을 가능하게 하는 Pointwise Generative Reward Modeling(GRM)을 사용합니다. SPCT는 온라인 RL을 통해 원칙을 적응적으로 생성하고 비판을 정확하게 하여 GRMs에서 scalable behavior를 foster합니다. 또한, 컴퓨팅 사용량을 확장하기 위해 평행 샘플링을 사용하고 메타 RM을 도입하여 더 나은 확장 성능을 발휘합니다.
- ***Performance Highlights***: SPCT를 적용한 DeepSeek-GRM 모델은 다양한 RM 벤치마크에서 기존 메서드를 능가하였으며, 모델 사이즈 확장에 비해 추론 시간 확장이 더 나은 성능을 보여줍니다. 특히, 메타 RM을 통한 투표 프로세스를 통해 고품질의 샘플을 필터링하여 성능을 더욱 향상시킬 수 있었습니다.

### [ShortV: Efficient Multimodal Large Language Models by Freezing Visual Tokens in Ineffective Layers](https://arxiv.org/abs/2504.00502)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.00502.png)

Vote: 17

Authors: Yanjiang Liu, Qingyu Zhang, Xianpei Han, Jiawei Chen, Jia Zheng, Le Sun, Yaojie Lu, Hongyu Lin, Qianhao Yuan

- ***What's New***: ShortV는 MLLM(Multimodal Large Language Models)에서 비효율적인 레이어에 대한 시각적 토큰을 동결함으로써 계층적 중복성을 탐구하고, MLLM의 컴퓨팅 효율성을 크게 개선할 수 있는 첫 번째 비훈련식 방법을 제안합니다. ShortV는 약 60%의 MLLM 레이어에서 시각적 토큰을 동결하고도 성능 저하가 없이 FLOPs를 50%까지 줄일 수 있습니다.
- ***Technical Details***: ShortV는 Layer Contribution(LC)이라는 새로운 지표를 도입하여 각 레이어가 시각적 및 텍스트 토큰에 미치는 변환 효과를 정량적으로 측정합니다. LC는 모델 출력에서 레이어의 변환을 제거한 결과물의 발산을 측정합니다. 이를 통해 비효율적인 레이어를 식별하고, 해당 레이어에서 시각적 토큰의 업데이트를 동결하여 연산량을 감소시킵니다.
- ***Performance Highlights***: ShortV는 LLaVA-NeXT-13B 모델에서 FLOPs를 50% 줄이고도 뛰어난 성능을 유지합니다. 다양한 벤치마크에서 경쟁력 있는 성능을 보여주며, 특히 비효율적인 레이어에서 시각적 토큰의 동결로 인해 약간의 성능 저하 없이 효율성이 크게 증가했음을 증명하였습니다.

### [JavisDiT: Joint Audio-Video Diffusion Transformer with Hierarchical Spatio-Temporal Prior Synchronization](https://arxiv.org/abs/2503.23377)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.23377.png)

Vote: 16

Authors: Hao Fei, Jiayi Ji, Fan Zhou, Tat-Seng Chua, Yanhao Zheng, Rongxin Jiang, Kai Liu, Shengqiong Wu, Jiebo Luo, Lai Chen, Wei Li

- ***What's New***: JavisDiT는 새로운 융합 오디오-비디오 확산 변환기(Joint Audio-Video Diffusion Transformer)로, 동기화된 오디오-비디오 생성(JAVG)을 위한 혁신적인 메커니즘을 도입하였습니다. Hierarchical Spatial-Temporal Synchronized Prior (HiST-Sypo) Estimator를 통해 시각적 및 청각적 요소 사이의 정밀한 동기화를 달성합니다.
- ***Technical Details***: JavisDiT는 Diffusion Transformer (DiT) 아키텍처를 기반으로 구축되어 있으며, 시각적 및 청각적 구성 요소의 동기화를 위한 Hierarchical Spatial-Temporal Synchronized Prior (HiST-Sypo) estimator를 이용해 전역 및 세부 수준의 시공간 우선조건을 추출합니다. 이 모델은 10,140개의 고품질 텍스트 자막이 있는 비디오로 구성된 JavisBench라는 새로운 벤치마크에서 검증되었습니다.
- ***Performance Highlights***: 실험 결과는 JavisDiT가 기존 방법보다 뛰어난 성능을 보이며, 특히 복잡한 장면 비디오를 처리하는 데 효과적임을 보여줍니다. JavisScore라는 새로운 평가 지표를 통해 시공간 동기화를 보다 정확하게 평가하였습니다. JavisDiT는 다양하고 복잡한 현실 세계의 콘텐츠를 포함한 벤치마크에서 두드러진 성능을 보입니다.

### [Audio-visual Controlled Video Diffusion with Masked Selective State Spaces Modeling for Natural Talking Head Generation](https://arxiv.org/abs/2504.02542)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02542.png)

Vote: 13

Authors: Xiu Li, Qin Lin, Fa-Ting Hong, Zixiang Zhou, Zunnan Xu, Dan Xu, Jun Zhou, Qinglin Lu

- ***What's New***: ACTalker는 대화형 얼굴 영상을 생성하기 위해 멀티신호와 단일신호 모두를 제어할 수 있는 오디오-비주얼 제어 비디오 디퓨전(ACT; Audio-visual Controlled Video Diffusion) 모델을 소개합니다. 오디오 및 표정 신호를 통한 충돌 없는 자연스러운 영상 생성을 위한 마스크된 선택적 상태 공간 모델(Masked Selective State Spaces; SSM)을 활용한 병렬-컨트롤 맘바 구조를 도입하여 독특한 모델링을 제안했습니다.
- ***Technical Details***: ACTalker는 공간-시간적 특징과 컨트롤 신호를 통합하여 포토리얼리스틱하고 표현력 있는 대화형 머리 영상을 생성하는 프레임워크입니다. 이 모델은 SSM 구조를 사용하여 신호들이 공간 및 시간 차원에서 중간 특징 토큰과 상호작용할 수 있도록 하고, 마스크 드롭 전략(mask-drop strategy)을 통해 각 신호별로 담당하는 얼굴 영역을 독립적으로 제어함으로써 제어 충돌을 해결합니다. PMC(Parallel-control Mamba Layer) 레이어는 다중 신호의 공간적, 시간적 매너로 단일 브랜치 내에서 중간 특징과 결합하여, 훈련 중에는 랜덤 설정된 게이트 메커니즘을 통해 영상 생성을 유연하게 제어합니다.
- ***Performance Highlights***: ACTalker는 단일-신호 제어에서는 기존 방법보다 뛰어난 성능을 보이며, 다중 신호 제어에서도 제어 충돌 문제를 해결합니다. 실험 결과 최적화된 음성-시각 동기화 및 비디오 품질을 확인할 수 있으며, 세부적인 얼굴 표정을 세밀하게 모사하여 보다 자연스러운 대화형 얼굴 영상을 생성할 수 있음을 입증했습니다.

### [Efficient Model Selection for Time Series Forecasting via LLMs](https://arxiv.org/abs/2504.02119)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02119.png)

Vote: 9

Authors: Franck Dernoncourt, Hongjie Chen, Wang Wei, Tiankai Yang, Hoda Eldardiry, Yue Zhao, Ryan A. Rossi

- ***What's New***: 이 연구는 LLMs(Large Language Models)를 활용한 시계열 예측 모델 선택의 효율화를 다루고 있습니다. 기존의 메타러닝 접근법이 성능 매트릭스를 필요로 하는 반면, LLMs를 사용하여 명시적인 성능 매트릭스 없이 모델 선택을 수행하는 방법을 제안합니다. 이를 통해 시계열 예측에서 LLMs의 잠재력을 보여줍니다.
- ***Technical Details***: 이번 연구에서는 다양한 LLMs(Llama, GPT, Gemini)를 사용하여 제로샷(Zero-shot) 프롬프트 설정을 통해 모델 선택을 구현하였습니다. 320개 이상의 데이터셋에서 실험을 진행하였고, 다양한 프롬프트 디자인을 평가하여 LLM 기반 선택이 기존의 메타러닝 기법보다 성능이 우수함을 확인하였습니다. 시계열 데이터에 대한 메타피처와 CoT(Chain-of-Thought) 프롬프팅을 활용하여 모델 선택에 미치는 영향을 분석하였습니다.
- ***Performance Highlights***: LLM 기반 방법은 hit@k 정확도와 평균 제곱 오차(MSE) 측면에서 기존의 모든 베이스라인과 메타러닝 기반 방법보다 뛰어난 성능을 보였습니다. 특히 Llama3.2 모델은 용량이 큰 성능 매트릭스를 생성하지 않고도 89배 더 빠른 추론 속도를 달성했습니다. 이는 다양한 데이터셋에 대해 시계열 예측 모델을 선택할 때 LLM 방법의 효율성을 보여줍니다.

### [Scaling Laws in Scientific Discovery with AI and Robot Scientists](https://arxiv.org/abs/2503.22444)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.22444.png)

Vote: 9

Authors: Animesh Garg, Zhibin Li, Arash Ajoudani, Pengsong Zhang, Xinyu Liu, Zhenting Wang, Huazhe Xu, Renjun Xu, Cong Wang, Heng Zhang

- ***What's New***: 이 논문은 AI와 로봇 과학자(Robot Scientists)를 활용한 과학 발견의 스케일링 법칙에 대한 이론적 틀을 제시합니다. 제안된 자율적 일반 과학자(Autonomous Generalist Scientist; AGS) 개념은 에이전트형 AI(Agentic AI)와 물리적 로봇을 통합하여 과학 연구의 모든 단계를 자동화합니다.
- ***Technical Details***: AGS 시스템은 문헌 검토, 제안서 생성, 실험, 원고 작성 등 다섯 가지 주요 기능 모듈로 구성되어 있으며, 상호작용과 반성 메커니즘을 결합해 연구의 모든 측면을 지원합니다. 이 시스템은 물리적 및 가상 환경과 상호작용하며 다양한 과학 분야에서의 지식 통합을 촉진합니다.
- ***Performance Highlights***: 제안된 AGS는 기존의 인간 연구자 중심 방법을 넘어 연구 생산성을 획기적으로 향상시킬 수 있습니다. 컴퓨팅 및 로봇 플랫폼의 재현 가능성은 새로운 지식 발견의 스케일링 법칙을 암시하며, 전통적 연구 방법에 비해 연구력을 크게 발전시킬 수 있습니다.

### [Interpreting Emergent Planning in Model-Free Reinforcement Learning](https://arxiv.org/abs/2504.01871)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.01871.png)

Vote: 7

Authors: Thomas Bush, Stephen Chung, Usman Anwar, Adrià Garriga-Alonso, David Krueger

- ***What's New***: 이 연구는 모델-프리 강화 학습 에이전트가 내적으로 계획을 세울 수 있다는 메커니즘적 증거를 처음으로 제시합니다. 구체적으로, DRC라는 모델-프리 에이전트가 Sokoban 게임에서 장기적인 행동 결과를 예측하고 그에 따라 행동을 선택하는 내부 플랜을 형성한다는 것을 보여줍니다.
- ***Technical Details***: 이 연구에서는 개념 기반 해석 방법론을 사용하여, Sokoban을 플레이하는 DRC 에이전트가 계획에 관련된 개념을 학습하고 이를 이용해 테스트 시간에 '계획'을 구성하는지 확인했습니다. 구체적으로, (1) 계획 관련 개념을 탐색하고 (2) 에이전트의 표현 내에서 계획 형성을 조사하며 (3) 발견된 계획이 에이전트의 행동에 미치는 인과적 영향을 확인했습니다.
- ***Performance Highlights***: GPT-4와 같은 모델을 벤치마크로 평가한 결과와 달리, DRC 에이전트가 실제로 내적으로 계획을 세운다는 것을 최초로 입증했습니다. 이는 이러한 에이전트들이 추가적인 테스트 계산을 통해 성과를 개선할 수 있으며, 이러한 특성은 대규모 언어 모델에서 출현하는 계획 및 추론 능력과 유사함을 시사합니다.

### [GenPRM: Scaling Test-Time Compute of Process Reward Models via Generative Reasoning](https://arxiv.org/abs/2504.00891)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.00891.png)

Vote: 6

Authors: Kaiyan Zhang, Xiu Li, Zhimu Zhou, Dong Li, Zhouyi Qian, Jiafei Lyu, Biqing Qi, Jian Zhao, Runze Liu, Junqi Gao, Bowen Zhou

- ***What's New***: GenPRM은 대형 언어 모델(LLMs)의 성능을 강화하기 위한 프로세스 리워드 모델(Process Reward Models; PRMs)로서, 체인 오브 사고(Chain-of-Thought; CoT)와 코드 검증을 수행하는 생성적 방식을 도입했습니다. 이 모델은 테스트 시간(TTS)을 확장하여 보다 작은 모델이 더 큰 모델의 성능을 뛰어넘도록 합니다.
- ***Technical Details***: GenPRM은 체계적인 생성적 프로세스 검증 프레임워크로, (1) 자연어 추론과 코드 생성 및 실행을 통합한 다단계 추론 프로세스를 제공합니다. (2) 상대적 진행 추정(Relative Progress Estimation; RPE)과 새로운 근거 데이터 합성 프레임워크를 통합하고, (3) 유연한 계산 확장을 통해 샘플링 및 추론 구성 요소 선택을 가능하게 합니다.
- ***Performance Highlights***: 23,000개의 MATH 데이터세트로 학습된 1.5B 크기의 GenPRM은 GPT-4o 및 Qwen2.5-Math-PRM-72B를 뛰어넘는 성능을 보이며, 기존의 PRMs보다 월등한 성능을 나타냅니다. 특히 GenPRM은 테스트 시간 확장을 통해 보다 작은 PRMs가 10배 큰 PRMs보다 우수한 성능을 발휘할 수 있음을 보여줍니다.

### [Instruction-Guided Autoregressive Neural Network Parameter Generation](https://arxiv.org/abs/2504.02012)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02012.png)

Vote: 6

Authors: Soro Bedionita, Sung Ju Hwang, Bruno Andreis, Song Chong

- ***What's New***: 본 연구에서는 IGPG(Instruction-Guided Parameter Generation)라는 새로운 프레임워크를 소개했습니다. IGPG는 VQ-VAE와 Autoregressive 모델링을 결합하여 다양한 작업과 아키텍처에 맞춰 뉴럴 네트워크 파라미터를 생성합니다. 이는 기존 방식에서 대규모 아키텍처의 확장성 한계와 비일관적인 레이어 간 파라미터 생성을 해결하고자 하는 최초의 시도로, 단일 생성 프레임워크 내에서 다양한 사전 학습된 모델을 통합하는 데 중점을 두고 있습니다.
- ***Technical Details***: IGPG는 VQ-VAE(Vector Quantized Variational Autoencoders)와 Autoregressive 모델을 통합하여 뉴럴 네트워크 파라미터를 생성합니다. 이는 작업 설명, 데이터셋, 아키텍처 세부정보를 조건으로 고려하여, 모델 간 레이어 일관성을 보장하면서 다양한 모델과 데이터셋에 대한 적응성을 향상시킵니다. 이를 통해 데이터세트 및 아키텍처별 파라미터를 효율적으로 생성할 수 있습니다.
- ***Performance Highlights***: IGPG는 다양한 사전 학습된 모델의 집합체를 단일 유연한 생성 프레임워크로 통합하여, 미지의 작업에서도 경쟁력 있거나 우수한 성능을 달성합니다. 특히 대규모 아키텍처에 적용할 때 확장성과 효율성 면에서 기존 최첨단 방법들과 비교하여 우수함을 보였습니다. 이는 파라미터 공간 내에서 더 높은 성능의 구성 요소를 찾는데 유효성을 보였습니다.

### [Sparse Autoencoders Learn Monosemantic Features in Vision-Language Models](https://arxiv.org/abs/2504.02821)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02821.png)

Vote: 5

Authors: Zeynep Akata, Serge Belongie, Mateusz Pach, Quentin Bouniot, Shyamgopal Karthik

- ***What's New***: 이 연구는 Sparse Autoencoders(SAEs)를 Vision-Language Models(VLMs)에 적용하여 개별 뉴런의 단의미성(monosemanticity)을 향상시키는 방법을 제안합니다. VLM에서의 단의미성 평가를 위한 포괄적인 프레임워크를 도입하며, 이 프레임워크는 시각 표현의 전문가 정의 구조와의 정렬을 확인합니다.
- ***Technical Details***: 이 연구에서는 단의미성 점수(Monosemanticity Score; MS)를 제안하여 SAEs가 뉴런 활성화-가중 이미지 임베딩의 유사도를 기반으로 시각 과제에서 개념을 얼마나 잘 분리하는지 평가합니다. 또한, 가시적 인코더(Vision Encoder) 위에 훈련된 SAEs의 경우 뉴런 단계의 개념 분리를 개선하는 데 있어 충분한 SPARSITY와 확장 계수의 영향을 시험합니다.
- ***Performance Highlights***: SAEs를 적용한 후, 원래 VLM 뉴런보다 더 높은 단의미성 점수를 기록하였습니다. 예를 들어, CLIP 모델에서 90% 이상의 뉴런이 개선된 단의미성 점수를 기록했으며, 이는 SAE 뉴런이 단일 개념에 보다 집중되는 효과를 보여줍니다. 또한, 다양한 확장 계수에서 SAE의 성능을 비교하여 SPARSITY가 개선 효과를 가져오는 것을 확인했습니다.

### [FreSca: Unveiling the Scaling Space in Diffusion Models](https://arxiv.org/abs/2504.02154)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02154.png)

Vote: 5

Authors: Chao Huang, Yunlong Tang, Chenliang Xu, Susan Liang, Yapeng Tian, Li Ma

- ***What's New***: FreSca는 기존 확산 모델(Diffusion Models)의 성능을 향상시키기 위한 플러그 앤 플레이(Plug-and-Play) 방식의 강화 방법을 제안합니다. 이 방법은 추가적인 학습 없이 이미지 편집과 심도 예측과 같은 다양한 작업에 적용 가능합니다.
- ***Technical Details***: FreSca는 확산 과정에서 델타 노이즈 예측(∆ϵ)의 푸리에 해석을 통해 각기 다른 주파수 대역에 독립적인 가이드 스케일링(Guidance Scaling)을 적용하는 방법입니다. 이 방법은 델타 노이즈 예측을 저주파 및 고주파 구성요소로 분해한 후 각각 독립적으로 조정하여 보다 정교한 제어를 가능하게 합니다.
- ***Performance Highlights***: FreSca를 기존의 Image Editing 및 심도 예측 방법에 통합했을 때, 스타일 Transfer 혹은 물체 크기 조작 등 다양한 편집 작업에서 품질이 개선되었습니다. 추가적으로, 마리골드(Marigold)와 같은 최신 모델에 적용하여 자율주행이나 로봇 공학에 중요한 심도 예측 작업의 정확성을 향상시켰습니다.

### [NeuralGS: Bridging Neural Fields and 3D Gaussian Splatting for Compact 3D Representations](https://arxiv.org/abs/2503.23162)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.23162.png)

Vote: 4

Authors: Xinhua Cheng, Yuan Liu, Zhenyu Tang, Junwu Zhang, Chaoran Feng, Xiaoxiao Long, Li Yuan, Wenping Wang, Wangbo Yu

- ***What's New***: NeuralGS는 Neural Fields와 3D Gaussian Splatting(3DGS)를 결합하여 컴팩트하면서도 효율적인 3D 장면 표현을 제공하는 새로운 방법론입니다. 이 방법은 복잡한 클러스터링이나 양자화 전략 없이도 원본 3DGS를 효율적으로 압축할 수 있습니다. Neural Fields를 활용하여 대규모 장면에서도 적은 저장공간을 필요로 하며, 네트워크 가중치를 통해 Gaussians의 속성을 인코딩합니다.
- ***Technical Details***: Neural Fields를 네트워크 구조의 중심으로 하고 이를 통해 Gaussian 속성을 인코딩하는 다양한 전략을 채택합니다. 여기에는 중요도에 따른 Pruning(자르기), 속성 기반 클러스터링, 그리고 Fine-tuning 과정에서의 주파수 손실(Frequency Loss) 도입이 포함됩니다. 각 클러스터는 작은 MLP(다층 퍼셉트론)을 통해 Gaussian의 위치를 인코딩하며, 이는 위치에서 속성을 추정하는 과정을 효율화합니다. 이 접근법은 NeRF와 유사한 compact함을 보장하면서도, SOTA 압축 방법과 견줄 수 있는 성능을 제공합니다.
- ***Performance Highlights***: NeuralGS는 Mip-NeRF 360 데이터셋에서 기존의 3DGS와 비교하여 모델 크기를 약 45배 줄이면서도 고품질의 렌더링 성능을 유지합니다. Deep Blending 데이터셋에서 PSNR 기준으로 원본 3DGS 대비 0.49 dB 높은 성능을 기록하였으며, 대부분의 기존 압축 방법보다 높은 데이터를 제공합니다. 이러한 성과는 주로 클러스터 기반의 Neural Fields를 통해 얻어졌으며, 적용된 데이터세트 전반에 걸쳐 높은 압축률을 보여주고 있습니다.

### [Whisper-LM: Improving ASR Models with Language Models for Low-Resource Languages](https://arxiv.org/abs/2503.23542)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.23542.png)

Vote: 3

Authors: Xabier de Zuazo, Ibon Saratxaga, Eva Navas, Inma Hernáez Rioja

- ***What's New***: 본 연구는 소수 언어에서 Whisper라는 대형 다국어 ASR(Automatic Speech Recognition) 모델의 성능을 향상시키기 위해 새로운 언어 모델(Language Models; LMs)의 통합을 소개합니다. 언어 모델은 Whisper의 음향 모델 출력을 조정하여 단어 오류율(WER)을 크게 줄일 수 있음을 시연하였습니다.
- ***Technical Details***: Whisper 모델은 다양한 언어 데이터에 대해 세밀하게 튜닝되었으며, 언어 모델은 추론 시점에 내부 점수를 기반으로 임베딩되어 성능을 향상시킵니다. N-그램 모델과 대형 언어 모델(LLM)은 Whisper의 빔 서치(Beam Search) 과정에 통합되어 개선된 출력을 제공합니다.
- ***Performance Highlights***: Whisper 모델에 언어 모델을 통합한 결과, 저자들이 제안한 방식을 통해 단어 오류율(WER)이 최대 51%까지 개선되었습니다. 이 모델은 특히 소수 언어의 OOD(Out-of-Distribution) 데이터 세트에서 더 나은 성능을 보여주며, 대형 언어 모델(LLM)을 사용하면 안정성이 더욱 강화됩니다.

### [Scene-Centric Unsupervised Panoptic Segmentation](https://arxiv.org/abs/2504.01955)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.01955.png)

Vote: 1

Authors: Stefan Roth, Daniel Cremers, Oliver Hahn, Christian Rupprecht, Christoph Reich, Nikita Araslanov

- ***What's New***: 자연스러운 장면 중심의 이미지를 기반으로 훈련하는 첫 번째 비지도 전반적 세분화(scene-centric unsupervised panoptic segmentation)를 제안합니다. 이 접근 방식은 물체 중심 데이터에 의존하지 않고 복잡한 장면을 이해할 수 있도록 합니다. 시각적 표현, 깊이(depth), 그리고 움직임(motion) 신호를 결합하여 고해상도의 전반적 의사 라벨(panoptic pseudo labels)을 생성합니다.
- ***Technical Details***: CUPS라는 새로운 기법은 깊이와 움직임 신호를 사용하여 오토스테레오(stereo) 이미지에서 장면 흐름(scene flow)을 통해 이동하는 물체를 감지하고 비지도 전반적 의사 라벨을 생성합니다. 이를 통해 의사 라벨과 자신을 참조하는 훈련 방법으로 전반적 세분화 네트워크를 훈련합니다. 높은 해상도에서 시맨틱 및 인스턴스 라벨을 병합하여 전반적 의사 라벨을 생성합니다.
- ***Performance Highlights***: CUPS는 Cityscapes 데이터셋에서 이전 비지도 전반적 세분화 기법에 비해 9.4%의 PQ 향상을 보여주며, 다른 여러 장면 중심 데이터셋에서도 우수한 성능을 입증했습니다. 특정 데이터셋에서는 기존 방법에 비해 최대 17.1%의 PQ 성능 향상을 보이며, 이는 현재 비지도 전반적 세분화 기법 중 최고의 성능입니다.

### [OpenCodeReasoning: Advancing Data Distillation for Competitive Coding](https://arxiv.org/abs/2504.01943)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.01943.png)

Vote: 0

Authors: Wasi Uddin Ahmad, Aleksander Ficek, Vahid Noroozi, Jocelyn Huang, Siddhartha Jain, Boris Ginsburg, Somshubra Majumdar, Sean Narenthiran

- ***What's New***: OpenCodeReasoning은 경쟁 코딩을 위한 데이터 증류(Data Distillation) 접근법을 발전시키기 위해 구축된 가장 큰 추론 기반 합성 데이터셋입니다. 이 데이터셋은 28,904개의 고유한 경쟁 프로그래밍 질문과 736,712개의 파이썬 샘플을 포함하고 있습니다.
- ***Technical Details***: OPENCODEREASONING 데이터셋은 `Qwen2.5` 모델들(7B, 14B, 32B)을 `슈퍼바이즈드 파인 튜닝(Supervised Fine-Tuning; SFT)` 기법으로 학습시켜 뛰어난 성능을 달성했습니다. 특히 `라이브코드벤치(LiveCodeBench)`에서 기존의 모든 SFT-전용 모델을 뛰어넘는 결과를 기록했습니다. 데이터셋 구축 과정에서는 다양한 소스로부터 경쟁 코딩 질문을 수집하고, 대형 언어 모델(LLM)이 생성한 응답을 후처리하여 풀이를 검증하고 해설의 일부를 추출하였습니다.
- ***Performance Highlights***: Qwen2.5 모델은 `LiveCodeBench`에서 파라미터 규모에 따라 다른 모델들과 `pass@1` 정확도를 비교했을 때, 7B 모델이 51.3, 14B 모델이 59.4, 32B 모델이 61.8을 기록하며 `DeepSeek-R1`과의 성능 차이를 크게 좁혔습니다. 특히, 실행 기반 필터링을 거치지 않은 부정확한 풀이도 성능 개선에 기여할 수 있다는 흥미로운 관찰이 있었습니다.

